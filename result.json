{"traceEvents":[{"ph":"M","pid":31447,"tid":31447,"name":"process_name","args":{"name":"MainProcess"}},{"ph":"M","pid":31447,"tid":377863,"name":"thread_name","args":{"name":"MainThread"}},{"pid":31447,"tid":377863,"ts":48727577471.0,"dur":5.0,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577477.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577478.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577478.04,"dur":0.96,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577479.02,"dur":0.98,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577480.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577482.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577485.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577484.0,"dur":2.0,"name":"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577488.0,"dur":2.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:437)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577494.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577491.0,"dur":4.0,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:458)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577497.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577499.0,"dur":2.0,"name":"list.remove","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577502.0,"dur":1.0,"name":"list.insert","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577503.02,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577498.0,"dur":5.06,"name":"_add_filter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:181)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577496.0,"dur":7.08,"name":"simplefilter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:165)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577504.0,"dur":1.0,"name":"numpy.asarray","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577507.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577506.0,"dur":1.04,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:477)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577508.02,"dur":0.98,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577509.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577508.0,"dur":2.0,"name":"_ensure_no_complex_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:475)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577515.0,"dur":0.02,"name":"str.rpartition","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577513.0,"dur":3.0,"name":"parent (<frozen importlib._bootstrap>:398)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577523.0,"dur":1.0,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577522.0,"dur":2.02,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577525.0,"dur":0.02,"name":"dict.copy","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577521.0,"dur":4.04,"name":"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577526.0,"dur":0.02,"name":"numpy.asanyarray","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577531.0,"dur":1.0,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577530.0,"dur":3.0,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:284)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577534.02,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577534.0,"dur":0.06,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:284)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577535.0,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577529.0,"dur":6.04,"name":"issubdtype (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:358)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577537.0,"dur":1.0,"name":"_sum_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2118)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577540.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577543.0,"dur":1.0,"name":"dict.items","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577544.02,"dur":1.98,"name":"<dictcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:70)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577547.0,"dur":10.0,"name":"numpy.ufunc.reduce","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577542.0,"dur":15.02,"name":"_wrapreduction (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:69)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577539.0,"dur":19.0,"name":"sum (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2123)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577538.02,"dur":20.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577536.0,"dur":22.04,"name":"sum (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577528.0,"dur":31.0,"name":"_safe_accumulator_op (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:869)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577512.0,"dur":56.0,"name":"_assert_all_finite (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:89)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577573.0,"dur":1.0,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577574.02,"dur":0.98,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577575.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577576.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577582.02,"dur":0.98,"name":"_abc._abc_subclasscheck","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577582.0,"dur":1.02,"name":"__subclasscheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:121)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577579.02,"dur":4.98,"name":"_abc._abc_instancecheck","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577579.0,"dur":5.02,"name":"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:117)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577577.0,"dur":7.04,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577569.0,"dur":15.06,"name":"_num_samples (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:252)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577470.0,"dur":115.0,"name":"check_array (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:485)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577588.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577589.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577589.0,"dur":1.0,"name":"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577591.0,"dur":3.0,"name":"get_patch_message (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/_utils.py:96)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577596.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577598.0,"dur":1.0,"name":"isEnabledFor (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py:1689)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577597.0,"dur":3.0,"name":"info (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py:1436)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577595.0,"dur":5.02,"name":"info (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py:2089)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577605.02,"dur":0.98,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577605.0,"dur":1.02,"name":"isclass (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/inspect.py:73)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577606.04,"dur":0.96,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577607.02,"dur":0.98,"name":"builtins.vars","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577609.02,"dur":0.98,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577610.02,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577611.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577611.04,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577611.08,"dur":0.92,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577612.02,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577612.06,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577612.1,"dur":0.9,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577613.02,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577613.06,"dur":0.94,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577614.02,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577615.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577615.04,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577616.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577616.04,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577616.08,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577617.0,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577617.04,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577617.08,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577618.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577618.04,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577619.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577619.04,"dur":1.96,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577609.0,"dur":12.02,"name":"<listcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:1199)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577603.0,"dur":19.0,"name":"check_is_fitted (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:1138)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577625.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577626.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577628.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577628.04,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577628.08,"dur":0.92,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577629.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577630.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577630.04,"dur":0.96,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577631.02,"dur":0.98,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577632.04,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577632.02,"dur":0.06,"name":"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577633.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:437)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577636.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577635.0,"dur":2.0,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:458)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577638.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577639.0,"dur":0.02,"name":"list.remove","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577640.0,"dur":1.0,"name":"list.insert","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577641.02,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577638.04,"dur":3.02,"name":"_add_filter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:181)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577637.02,"dur":4.98,"name":"simplefilter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:165)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577642.02,"dur":0.98,"name":"numpy.asarray","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577644.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577643.02,"dur":1.02,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:477)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577645.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577645.06,"dur":0.94,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577645.0,"dur":1.02,"name":"_ensure_no_complex_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:475)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577648.02,"dur":0.02,"name":"str.rpartition","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577648.0,"dur":1.0,"name":"parent (<frozen importlib._bootstrap>:398)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577652.04,"dur":0.96,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577652.02,"dur":1.0,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577653.04,"dur":0.02,"name":"dict.copy","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577652.0,"dur":2.0,"name":"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577654.02,"dur":0.02,"name":"numpy.asanyarray","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577656.04,"dur":0.96,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577656.02,"dur":1.98,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:284)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577659.02,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577659.0,"dur":0.06,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:284)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577659.08,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577656.0,"dur":3.12,"name":"issubdtype (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:358)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577661.0,"dur":0.02,"name":"_sum_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2118)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577662.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577664.02,"dur":0.02,"name":"dict.items","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577665.0,"dur":0.02,"name":"<dictcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:70)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577666.0,"dur":6.0,"name":"numpy.ufunc.reduce","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577664.0,"dur":11.0,"name":"_wrapreduction (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:69)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577662.0,"dur":13.02,"name":"sum (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2123)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577661.04,"dur":14.96,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577660.0,"dur":16.02,"name":"sum (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577655.0,"dur":21.04,"name":"_safe_accumulator_op (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:869)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577647.0,"dur":33.0,"name":"_assert_all_finite (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:89)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577683.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577683.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577684.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577684.04,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577687.02,"dur":0.98,"name":"_abc._abc_subclasscheck","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577687.0,"dur":1.02,"name":"__subclasscheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:121)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577686.02,"dur":2.02,"name":"_abc._abc_instancecheck","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577686.0,"dur":2.06,"name":"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:117)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577685.0,"dur":3.08,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577681.0,"dur":8.0,"name":"_num_samples (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:252)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577627.0,"dur":62.02,"name":"check_array (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:485)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577690.0,"dur":0.02,"name":"dict.get","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577695.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577695.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577695.08,"dur":0.92,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577696.02,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577692.02,"dur":4.98,"name":"_num_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:199)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577697.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577692.0,"dur":6.0,"name":"_check_n_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:349)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577624.0,"dur":74.02,"name":"_validate_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:395)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577709.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577709.0,"dur":0.06,"name":"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577700.0,"dur":9.08,"name":"safe_sparse_dot (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:120)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577601.0,"dur":113.0,"name":"_decision_function (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/linear_model/_base.py:342)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577466.02,"dur":248.0,"name":"_predict_ridge (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/linear_model/_ridge_0_22.py:136)","ph":"X","cat":"FEE"},{"pid":31447,"tid":377863,"ts":48727577466.0,"dur":248.04,"name":"predict (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/linear_model/_ridge_0_22.py:197)","ph":"X","cat":"FEE"}],"viztracer_metadata":{"version":"0.13.4"},"displayTimeUnit":"us","file_info":{"files":{"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py":["\"\"\"Base class for sparse matrices\"\"\"\nimport numpy as np\n\nfrom .sputils import (isdense, isscalarlike, isintlike,\n                      get_sum_dtype, validateaxis, check_reshape_kwargs,\n                      check_shape, asmatrix)\n\n__all__ = ['spmatrix', 'isspmatrix', 'issparse',\n           'SparseWarning', 'SparseEfficiencyWarning']\n\n\nclass SparseWarning(Warning):\n    pass\n\n\nclass SparseFormatWarning(SparseWarning):\n    pass\n\n\nclass SparseEfficiencyWarning(SparseWarning):\n    pass\n\n\n# The formats that we might potentially understand.\n_formats = {'csc': [0, \"Compressed Sparse Column\"],\n            'csr': [1, \"Compressed Sparse Row\"],\n            'dok': [2, \"Dictionary Of Keys\"],\n            'lil': [3, \"List of Lists\"],\n            'dod': [4, \"Dictionary of Dictionaries\"],\n            'sss': [5, \"Symmetric Sparse Skyline\"],\n            'coo': [6, \"COOrdinate\"],\n            'lba': [7, \"Linpack BAnded\"],\n            'egd': [8, \"Ellpack-itpack Generalized Diagonal\"],\n            'dia': [9, \"DIAgonal\"],\n            'bsr': [10, \"Block Sparse Row\"],\n            'msr': [11, \"Modified compressed Sparse Row\"],\n            'bsc': [12, \"Block Sparse Column\"],\n            'msc': [13, \"Modified compressed Sparse Column\"],\n            'ssk': [14, \"Symmetric SKyline\"],\n            'nsk': [15, \"Nonsymmetric SKyline\"],\n            'jad': [16, \"JAgged Diagonal\"],\n            'uss': [17, \"Unsymmetric Sparse Skyline\"],\n            'vbr': [18, \"Variable Block Row\"],\n            'und': [19, \"Undefined\"]\n            }\n\n\n# These univariate ufuncs preserve zeros.\n_ufuncs_with_fixed_point_at_zero = frozenset([\n        np.sin, np.tan, np.arcsin, np.arctan, np.sinh, np.tanh, np.arcsinh,\n        np.arctanh, np.rint, np.sign, np.expm1, np.log1p, np.deg2rad,\n        np.rad2deg, np.floor, np.ceil, np.trunc, np.sqrt])\n\n\nMAXPRINT = 50\n\n\nclass spmatrix:\n    \"\"\" This class provides a base class for all sparse matrices.  It\n    cannot be instantiated.  Most of the work is provided by subclasses.\n    \"\"\"\n\n    __array_priority__ = 10.1\n    ndim = 2\n\n    def __init__(self, maxprint=MAXPRINT):\n        self._shape = None\n        if self.__class__.__name__ == 'spmatrix':\n            raise ValueError(\"This class is not intended\"\n                             \" to be instantiated directly.\")\n        self.maxprint = maxprint\n\n    def set_shape(self, shape):\n        \"\"\"See `reshape`.\"\"\"\n        # Make sure copy is False since this is in place\n        # Make sure format is unchanged because we are doing a __dict__ swap\n        new_matrix = self.reshape(shape, copy=False).asformat(self.format)\n        self.__dict__ = new_matrix.__dict__\n\n    def get_shape(self):\n        \"\"\"Get shape of a matrix.\"\"\"\n        return self._shape\n\n    shape = property(fget=get_shape, fset=set_shape)\n\n    def reshape(self, *args, **kwargs):\n        \"\"\"reshape(self, shape, order='C', copy=False)\n\n        Gives a new shape to a sparse matrix without changing its data.\n\n        Parameters\n        ----------\n        shape : length-2 tuple of ints\n            The new shape should be compatible with the original shape.\n        order : {'C', 'F'}, optional\n            Read the elements using this index order. 'C' means to read and\n            write the elements using C-like index order; e.g., read entire first\n            row, then second row, etc. 'F' means to read and write the elements\n            using Fortran-like index order; e.g., read entire first column, then\n            second column, etc.\n        copy : bool, optional\n            Indicates whether or not attributes of self should be copied\n            whenever possible. The degree to which attributes are copied varies\n            depending on the type of sparse matrix being used.\n\n        Returns\n        -------\n        reshaped_matrix : sparse matrix\n            A sparse matrix with the given `shape`, not necessarily of the same\n            format as the current object.\n\n        See Also\n        --------\n        numpy.matrix.reshape : NumPy's implementation of 'reshape' for\n                               matrices\n        \"\"\"\n        # If the shape already matches, don't bother doing an actual reshape\n        # Otherwise, the default is to convert to COO and use its reshape\n        shape = check_shape(args, self.shape)\n        order, copy = check_reshape_kwargs(kwargs)\n        if shape == self.shape:\n            if copy:\n                return self.copy()\n            else:\n                return self\n\n        return self.tocoo(copy=copy).reshape(shape, order=order, copy=False)\n\n    def resize(self, shape):\n        \"\"\"Resize the matrix in-place to dimensions given by ``shape``\n\n        Any elements that lie within the new shape will remain at the same\n        indices, while non-zero elements lying outside the new shape are\n        removed.\n\n        Parameters\n        ----------\n        shape : (int, int)\n            number of rows and columns in the new matrix\n\n        Notes\n        -----\n        The semantics are not identical to `numpy.ndarray.resize` or\n        `numpy.resize`. Here, the same data will be maintained at each index\n        before and after reshape, if that index is within the new bounds. In\n        numpy, resizing maintains contiguity of the array, moving elements\n        around in the logical matrix but not within a flattened representation.\n\n        We give no guarantees about whether the underlying data attributes\n        (arrays, etc.) will be modified in place or replaced with new objects.\n        \"\"\"\n        # As an inplace operation, this requires implementation in each format.\n        raise NotImplementedError(\n            '{}.resize is not implemented'.format(type(self).__name__))\n\n    def astype(self, dtype, casting='unsafe', copy=True):\n        \"\"\"Cast the matrix elements to a specified type.\n\n        Parameters\n        ----------\n        dtype : string or numpy dtype\n            Typecode or data-type to which to cast the data.\n        casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n            Controls what kind of data casting may occur.\n            Defaults to 'unsafe' for backwards compatibility.\n            'no' means the data types should not be cast at all.\n            'equiv' means only byte-order changes are allowed.\n            'safe' means only casts which can preserve values are allowed.\n            'same_kind' means only safe casts or casts within a kind,\n            like float64 to float32, are allowed.\n            'unsafe' means any data conversions may be done.\n        copy : bool, optional\n            If `copy` is `False`, the result might share some memory with this\n            matrix. If `copy` is `True`, it is guaranteed that the result and\n            this matrix do not share any memory.\n        \"\"\"\n\n        dtype = np.dtype(dtype)\n        if self.dtype != dtype:\n            return self.tocsr().astype(\n                dtype, casting=casting, copy=copy).asformat(self.format)\n        elif copy:\n            return self.copy()\n        else:\n            return self\n\n    def asfptype(self):\n        \"\"\"Upcast matrix to a floating point format (if necessary)\"\"\"\n\n        fp_types = ['f', 'd', 'F', 'D']\n\n        if self.dtype.char in fp_types:\n            return self\n        else:\n            for fp_type in fp_types:\n                if self.dtype <= np.dtype(fp_type):\n                    return self.astype(fp_type)\n\n            raise TypeError('cannot upcast [%s] to a floating '\n                            'point format' % self.dtype.name)\n\n    def __iter__(self):\n        for r in range(self.shape[0]):\n            yield self[r, :]\n\n    def getmaxprint(self):\n        \"\"\"Maximum number of elements to display when printed.\"\"\"\n        return self.maxprint\n\n    def count_nonzero(self):\n        \"\"\"Number of non-zero entries, equivalent to\n\n        np.count_nonzero(a.toarray())\n\n        Unlike getnnz() and the nnz property, which return the number of stored\n        entries (the length of the data attribute), this method counts the\n        actual number of non-zero entries in data.\n        \"\"\"\n        raise NotImplementedError(\"count_nonzero not implemented for %s.\" %\n                                  self.__class__.__name__)\n\n    def getnnz(self, axis=None):\n        \"\"\"Number of stored values, including explicit zeros.\n\n        Parameters\n        ----------\n        axis : None, 0, or 1\n            Select between the number of values across the whole matrix, in\n            each column, or in each row.\n\n        See also\n        --------\n        count_nonzero : Number of non-zero entries\n        \"\"\"\n        raise NotImplementedError(\"getnnz not implemented for %s.\" %\n                                  self.__class__.__name__)\n\n    @property\n    def nnz(self):\n        \"\"\"Number of stored values, including explicit zeros.\n\n        See also\n        --------\n        count_nonzero : Number of non-zero entries\n        \"\"\"\n        return self.getnnz()\n\n    def getformat(self):\n        \"\"\"Format of a matrix representation as a string.\"\"\"\n        return getattr(self, 'format', 'und')\n\n    def __repr__(self):\n        _, format_name = _formats[self.getformat()]\n        return \"<%dx%d sparse matrix of type '%s'\\n\" \\\n               \"\\twith %d stored elements in %s format>\" % \\\n               (self.shape + (self.dtype.type, self.nnz, format_name))\n\n    def __str__(self):\n        maxprint = self.getmaxprint()\n\n        A = self.tocoo()\n\n        # helper function, outputs \"(i,j)  v\"\n        def tostr(row, col, data):\n            triples = zip(list(zip(row, col)), data)\n            return '\\n'.join([('  %s\\t%s' % t) for t in triples])\n\n        if self.nnz > maxprint:\n            half = maxprint // 2\n            out = tostr(A.row[:half], A.col[:half], A.data[:half])\n            out += \"\\n  :\\t:\\n\"\n            half = maxprint - maxprint//2\n            out += tostr(A.row[-half:], A.col[-half:], A.data[-half:])\n        else:\n            out = tostr(A.row, A.col, A.data)\n\n        return out\n\n    def __bool__(self):  # Simple -- other ideas?\n        if self.shape == (1, 1):\n            return self.nnz != 0\n        else:\n            raise ValueError(\"The truth value of an array with more than one \"\n                             \"element is ambiguous. Use a.any() or a.all().\")\n    __nonzero__ = __bool__\n\n    # What should len(sparse) return? For consistency with dense matrices,\n    # perhaps it should be the number of rows?  But for some uses the number of\n    # non-zeros is more important.  For now, raise an exception!\n    def __len__(self):\n        raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n                        \" or shape[0]\")\n\n    def asformat(self, format, copy=False):\n        \"\"\"Return this matrix in the passed format.\n\n        Parameters\n        ----------\n        format : {str, None}\n            The desired matrix format (\"csr\", \"csc\", \"lil\", \"dok\", \"array\", ...)\n            or None for no conversion.\n        copy : bool, optional\n            If True, the result is guaranteed to not share data with self.\n\n        Returns\n        -------\n        A : This matrix in the passed format.\n        \"\"\"\n        if format is None or format == self.format:\n            if copy:\n                return self.copy()\n            else:\n                return self\n        else:\n            try:\n                convert_method = getattr(self, 'to' + format)\n            except AttributeError as e:\n                raise ValueError('Format {} is unknown.'.format(format)) from e\n\n            # Forward the copy kwarg, if it's accepted.\n            try:\n                return convert_method(copy=copy)\n            except TypeError:\n                return convert_method()\n\n    ###################################################################\n    #  NOTE: All arithmetic operations use csr_matrix by default.\n    # Therefore a new sparse matrix format just needs to define a\n    # .tocsr() method to provide arithmetic support. Any of these\n    # methods can be overridden for efficiency.\n    ####################################################################\n\n    def multiply(self, other):\n        \"\"\"Point-wise multiplication by another matrix\n        \"\"\"\n        return self.tocsr().multiply(other)\n\n    def maximum(self, other):\n        \"\"\"Element-wise maximum between this and another matrix.\"\"\"\n        return self.tocsr().maximum(other)\n\n    def minimum(self, other):\n        \"\"\"Element-wise minimum between this and another matrix.\"\"\"\n        return self.tocsr().minimum(other)\n\n    def dot(self, other):\n        \"\"\"Ordinary dot product\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from scipy.sparse import csr_matrix\n        >>> A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n        >>> v = np.array([1, 0, -1])\n        >>> A.dot(v)\n        array([ 1, -3, -1], dtype=int64)\n\n        \"\"\"\n        return self * other\n\n    def power(self, n, dtype=None):\n        \"\"\"Element-wise power.\"\"\"\n        return self.tocsr().power(n, dtype=dtype)\n\n    def __eq__(self, other):\n        return self.tocsr().__eq__(other)\n\n    def __ne__(self, other):\n        return self.tocsr().__ne__(other)\n\n    def __lt__(self, other):\n        return self.tocsr().__lt__(other)\n\n    def __gt__(self, other):\n        return self.tocsr().__gt__(other)\n\n    def __le__(self, other):\n        return self.tocsr().__le__(other)\n\n    def __ge__(self, other):\n        return self.tocsr().__ge__(other)\n\n    def __abs__(self):\n        return abs(self.tocsr())\n\n    def __round__(self, ndigits=0):\n        return round(self.tocsr(), ndigits=ndigits)\n\n    def _add_sparse(self, other):\n        return self.tocsr()._add_sparse(other)\n\n    def _add_dense(self, other):\n        return self.tocoo()._add_dense(other)\n\n    def _sub_sparse(self, other):\n        return self.tocsr()._sub_sparse(other)\n\n    def _sub_dense(self, other):\n        return self.todense() - other\n\n    def _rsub_dense(self, other):\n        # note: this can't be replaced by other + (-self) for unsigned types\n        return other - self.todense()\n\n    def __add__(self, other):  # self + other\n        if isscalarlike(other):\n            if other == 0:\n                return self.copy()\n            # Now we would add this scalar to every element.\n            raise NotImplementedError('adding a nonzero scalar to a '\n                                      'sparse matrix is not supported')\n        elif isspmatrix(other):\n            if other.shape != self.shape:\n                raise ValueError(\"inconsistent shapes\")\n            return self._add_sparse(other)\n        elif isdense(other):\n            other = np.broadcast_to(other, self.shape)\n            return self._add_dense(other)\n        else:\n            return NotImplemented\n\n    def __radd__(self,other):  # other + self\n        return self.__add__(other)\n\n    def __sub__(self, other):  # self - other\n        if isscalarlike(other):\n            if other == 0:\n                return self.copy()\n            raise NotImplementedError('subtracting a nonzero scalar from a '\n                                      'sparse matrix is not supported')\n        elif isspmatrix(other):\n            if other.shape != self.shape:\n                raise ValueError(\"inconsistent shapes\")\n            return self._sub_sparse(other)\n        elif isdense(other):\n            other = np.broadcast_to(other, self.shape)\n            return self._sub_dense(other)\n        else:\n            return NotImplemented\n\n    def __rsub__(self,other):  # other - self\n        if isscalarlike(other):\n            if other == 0:\n                return -self.copy()\n            raise NotImplementedError('subtracting a sparse matrix from a '\n                                      'nonzero scalar is not supported')\n        elif isdense(other):\n            other = np.broadcast_to(other, self.shape)\n            return self._rsub_dense(other)\n        else:\n            return NotImplemented\n\n    def __mul__(self, other):\n        \"\"\"interpret other and call one of the following\n\n        self._mul_scalar()\n        self._mul_vector()\n        self._mul_multivector()\n        self._mul_sparse_matrix()\n        \"\"\"\n\n        M, N = self.shape\n\n        if other.__class__ is np.ndarray:\n            # Fast path for the most common case\n            if other.shape == (N,):\n                return self._mul_vector(other)\n            elif other.shape == (N, 1):\n                return self._mul_vector(other.ravel()).reshape(M, 1)\n            elif other.ndim == 2 and other.shape[0] == N:\n                return self._mul_multivector(other)\n\n        if isscalarlike(other):\n            # scalar value\n            return self._mul_scalar(other)\n\n        if issparse(other):\n            if self.shape[1] != other.shape[0]:\n                raise ValueError('dimension mismatch')\n            return self._mul_sparse_matrix(other)\n\n        # If it's a list or whatever, treat it like a matrix\n        other_a = np.asanyarray(other)\n\n        if other_a.ndim == 0 and other_a.dtype == np.object_:\n            # Not interpretable as an array; return NotImplemented so that\n            # other's __rmul__ can kick in if that's implemented.\n            return NotImplemented\n\n        try:\n            other.shape\n        except AttributeError:\n            other = other_a\n\n        if other.ndim == 1 or other.ndim == 2 and other.shape[1] == 1:\n            # dense row or column vector\n            if other.shape != (N,) and other.shape != (N, 1):\n                raise ValueError('dimension mismatch')\n\n            result = self._mul_vector(np.ravel(other))\n\n            if isinstance(other, np.matrix):\n                result = asmatrix(result)\n\n            if other.ndim == 2 and other.shape[1] == 1:\n                # If 'other' was an (nx1) column vector, reshape the result\n                result = result.reshape(-1, 1)\n\n            return result\n\n        elif other.ndim == 2:\n            ##\n            # dense 2D array or matrix (\"multivector\")\n\n            if other.shape[0] != self.shape[1]:\n                raise ValueError('dimension mismatch')\n\n            result = self._mul_multivector(np.asarray(other))\n\n            if isinstance(other, np.matrix):\n                result = asmatrix(result)\n\n            return result\n\n        else:\n            raise ValueError('could not interpret dimensions')\n\n    # by default, use CSR for __mul__ handlers\n    def _mul_scalar(self, other):\n        return self.tocsr()._mul_scalar(other)\n\n    def _mul_vector(self, other):\n        return self.tocsr()._mul_vector(other)\n\n    def _mul_multivector(self, other):\n        return self.tocsr()._mul_multivector(other)\n\n    def _mul_sparse_matrix(self, other):\n        return self.tocsr()._mul_sparse_matrix(other)\n\n    def __rmul__(self, other):  # other * self\n        if isscalarlike(other):\n            return self.__mul__(other)\n        else:\n            # Don't use asarray unless we have to\n            try:\n                tr = other.transpose()\n            except AttributeError:\n                tr = np.asarray(other).transpose()\n            return (self.transpose() * tr).transpose()\n\n    #######################\n    # matmul (@) operator #\n    #######################\n\n    def __matmul__(self, other):\n        if isscalarlike(other):\n            raise ValueError(\"Scalar operands are not allowed, \"\n                             \"use '*' instead\")\n        return self.__mul__(other)\n\n    def __rmatmul__(self, other):\n        if isscalarlike(other):\n            raise ValueError(\"Scalar operands are not allowed, \"\n                             \"use '*' instead\")\n        return self.__rmul__(other)\n\n    ####################\n    # Other Arithmetic #\n    ####################\n\n    def _divide(self, other, true_divide=False, rdivide=False):\n        if isscalarlike(other):\n            if rdivide:\n                if true_divide:\n                    return np.true_divide(other, self.todense())\n                else:\n                    return np.divide(other, self.todense())\n\n            if true_divide and np.can_cast(self.dtype, np.float_):\n                return self.astype(np.float_)._mul_scalar(1./other)\n            else:\n                r = self._mul_scalar(1./other)\n\n                scalar_dtype = np.asarray(other).dtype\n                if (np.issubdtype(self.dtype, np.integer) and\n                        np.issubdtype(scalar_dtype, np.integer)):\n                    return r.astype(self.dtype)\n                else:\n                    return r\n\n        elif isdense(other):\n            if not rdivide:\n                if true_divide:\n                    return np.true_divide(self.todense(), other)\n                else:\n                    return np.divide(self.todense(), other)\n            else:\n                if true_divide:\n                    return np.true_divide(other, self.todense())\n                else:\n                    return np.divide(other, self.todense())\n        elif isspmatrix(other):\n            if rdivide:\n                return other._divide(self, true_divide, rdivide=False)\n\n            self_csr = self.tocsr()\n            if true_divide and np.can_cast(self.dtype, np.float_):\n                return self_csr.astype(np.float_)._divide_sparse(other)\n            else:\n                return self_csr._divide_sparse(other)\n        else:\n            return NotImplemented\n\n    def __truediv__(self, other):\n        return self._divide(other, true_divide=True)\n\n    def __div__(self, other):\n        # Always do true division\n        return self._divide(other, true_divide=True)\n\n    def __rtruediv__(self, other):\n        # Implementing this as the inverse would be too magical -- bail out\n        return NotImplemented\n\n    def __rdiv__(self, other):\n        # Implementing this as the inverse would be too magical -- bail out\n        return NotImplemented\n\n    def __neg__(self):\n        return -self.tocsr()\n\n    def __iadd__(self, other):\n        return NotImplemented\n\n    def __isub__(self, other):\n        return NotImplemented\n\n    def __imul__(self, other):\n        return NotImplemented\n\n    def __idiv__(self, other):\n        return self.__itruediv__(other)\n\n    def __itruediv__(self, other):\n        return NotImplemented\n\n    def __pow__(self, other):\n        if self.shape[0] != self.shape[1]:\n            raise TypeError('matrix is not square')\n\n        if isintlike(other):\n            other = int(other)\n            if other < 0:\n                raise ValueError('exponent must be >= 0')\n\n            if other == 0:\n                from .construct import eye\n                return eye(self.shape[0], dtype=self.dtype)\n            elif other == 1:\n                return self.copy()\n            else:\n                tmp = self.__pow__(other//2)\n                if (other % 2):\n                    return self * tmp * tmp\n                else:\n                    return tmp * tmp\n        elif isscalarlike(other):\n            raise ValueError('exponent must be an integer')\n        else:\n            return NotImplemented\n\n    def __getattr__(self, attr):\n        if attr == 'A':\n            return self.toarray()\n        elif attr == 'T':\n            return self.transpose()\n        elif attr == 'H':\n            return self.getH()\n        elif attr == 'real':\n            return self._real()\n        elif attr == 'imag':\n            return self._imag()\n        elif attr == 'size':\n            return self.getnnz()\n        else:\n            raise AttributeError(attr + \" not found\")\n\n    def transpose(self, axes=None, copy=False):\n        \"\"\"\n        Reverses the dimensions of the sparse matrix.\n\n        Parameters\n        ----------\n        axes : None, optional\n            This argument is in the signature *solely* for NumPy\n            compatibility reasons. Do not pass in anything except\n            for the default value.\n        copy : bool, optional\n            Indicates whether or not attributes of `self` should be\n            copied whenever possible. The degree to which attributes\n            are copied varies depending on the type of sparse matrix\n            being used.\n\n        Returns\n        -------\n        p : `self` with the dimensions reversed.\n\n        See Also\n        --------\n        numpy.matrix.transpose : NumPy's implementation of 'transpose'\n                                 for matrices\n        \"\"\"\n        return self.tocsr(copy=copy).transpose(axes=axes, copy=False)\n\n    def conj(self, copy=True):\n        \"\"\"Element-wise complex conjugation.\n\n        If the matrix is of non-complex data type and `copy` is False,\n        this method does nothing and the data is not copied.\n\n        Parameters\n        ----------\n        copy : bool, optional\n            If True, the result is guaranteed to not share data with self.\n\n        Returns\n        -------\n        A : The element-wise complex conjugate.\n\n        \"\"\"\n        if np.issubdtype(self.dtype, np.complexfloating):\n            return self.tocsr(copy=copy).conj(copy=False)\n        elif copy:\n            return self.copy()\n        else:\n            return self\n\n    def conjugate(self, copy=True):\n        return self.conj(copy=copy)\n\n    conjugate.__doc__ = conj.__doc__\n\n    # Renamed conjtranspose() -> getH() for compatibility with dense matrices\n    def getH(self):\n        \"\"\"Return the Hermitian transpose of this matrix.\n\n        See Also\n        --------\n        numpy.matrix.getH : NumPy's implementation of `getH` for matrices\n        \"\"\"\n        return self.transpose().conj()\n\n    def _real(self):\n        return self.tocsr()._real()\n\n    def _imag(self):\n        return self.tocsr()._imag()\n\n    def nonzero(self):\n        \"\"\"nonzero indices\n\n        Returns a tuple of arrays (row,col) containing the indices\n        of the non-zero elements of the matrix.\n\n        Examples\n        --------\n        >>> from scipy.sparse import csr_matrix\n        >>> A = csr_matrix([[1,2,0],[0,0,3],[4,0,5]])\n        >>> A.nonzero()\n        (array([0, 0, 1, 2, 2]), array([0, 1, 2, 0, 2]))\n\n        \"\"\"\n\n        # convert to COOrdinate format\n        A = self.tocoo()\n        nz_mask = A.data != 0\n        return (A.row[nz_mask], A.col[nz_mask])\n\n    def getcol(self, j):\n        \"\"\"Returns a copy of column j of the matrix, as an (m x 1) sparse\n        matrix (column vector).\n        \"\"\"\n        # Spmatrix subclasses should override this method for efficiency.\n        # Post-multiply by a (n x 1) column vector 'a' containing all zeros\n        # except for a_j = 1\n        from .csc import csc_matrix\n        n = self.shape[1]\n        if j < 0:\n            j += n\n        if j < 0 or j >= n:\n            raise IndexError(\"index out of bounds\")\n        col_selector = csc_matrix(([1], [[j], [0]]),\n                                  shape=(n, 1), dtype=self.dtype)\n        return self * col_selector\n\n    def getrow(self, i):\n        \"\"\"Returns a copy of row i of the matrix, as a (1 x n) sparse\n        matrix (row vector).\n        \"\"\"\n        # Spmatrix subclasses should override this method for efficiency.\n        # Pre-multiply by a (1 x m) row vector 'a' containing all zeros\n        # except for a_i = 1\n        from .csr import csr_matrix\n        m = self.shape[0]\n        if i < 0:\n            i += m\n        if i < 0 or i >= m:\n            raise IndexError(\"index out of bounds\")\n        row_selector = csr_matrix(([1], [[0], [i]]),\n                                  shape=(1, m), dtype=self.dtype)\n        return row_selector * self\n\n    # The following dunder methods cannot be implemented.\n    #\n    # def __array__(self):\n    #     # Sparse matrices rely on NumPy wrapping them in object arrays under\n    #     # the hood to make unary ufuncs work on them. So we cannot raise\n    #     # TypeError here - which would be handy to not give users object\n    #     # arrays they probably don't want (they're looking for `.toarray()`).\n    #     #\n    #     # Conversion with `toarray()` would also break things because of the\n    #     # behavior discussed above, plus we want to avoid densification by\n    #     # accident because that can too easily blow up memory.\n    #\n    # def __array_ufunc__(self):\n    #     # We cannot implement __array_ufunc__ due to mismatching semantics.\n    #     # See gh-7707 and gh-7349 for details.\n    #\n    # def __array_function__(self):\n    #     # We cannot implement __array_function__ due to mismatching semantics.\n    #     # See gh-10362 for details.\n\n    def todense(self, order=None, out=None):\n        \"\"\"\n        Return a dense matrix representation of this matrix.\n\n        Parameters\n        ----------\n        order : {'C', 'F'}, optional\n            Whether to store multi-dimensional data in C (row-major)\n            or Fortran (column-major) order in memory. The default\n            is 'None', indicating the NumPy default of C-ordered.\n            Cannot be specified in conjunction with the `out`\n            argument.\n\n        out : ndarray, 2-D, optional\n            If specified, uses this array (or `numpy.matrix`) as the\n            output buffer instead of allocating a new array to\n            return. The provided array must have the same shape and\n            dtype as the sparse matrix on which you are calling the\n            method.\n\n        Returns\n        -------\n        arr : numpy.matrix, 2-D\n            A NumPy matrix object with the same shape and containing\n            the same data represented by the sparse matrix, with the\n            requested memory order. If `out` was passed and was an\n            array (rather than a `numpy.matrix`), it will be filled\n            with the appropriate values and returned wrapped in a\n            `numpy.matrix` object that shares the same memory.\n        \"\"\"\n        return asmatrix(self.toarray(order=order, out=out))\n\n    def toarray(self, order=None, out=None):\n        \"\"\"\n        Return a dense ndarray representation of this matrix.\n\n        Parameters\n        ----------\n        order : {'C', 'F'}, optional\n            Whether to store multidimensional data in C (row-major)\n            or Fortran (column-major) order in memory. The default\n            is 'None', indicating the NumPy default of C-ordered.\n            Cannot be specified in conjunction with the `out`\n            argument.\n\n        out : ndarray, 2-D, optional\n            If specified, uses this array as the output buffer\n            instead of allocating a new array to return. The provided\n            array must have the same shape and dtype as the sparse\n            matrix on which you are calling the method. For most\n            sparse types, `out` is required to be memory contiguous\n            (either C or Fortran ordered).\n\n        Returns\n        -------\n        arr : ndarray, 2-D\n            An array with the same shape and containing the same\n            data represented by the sparse matrix, with the requested\n            memory order. If `out` was passed, the same object is\n            returned after being modified in-place to contain the\n            appropriate values.\n        \"\"\"\n        return self.tocoo(copy=False).toarray(order=order, out=out)\n\n    # Any sparse matrix format deriving from spmatrix must define one of\n    # tocsr or tocoo. The other conversion methods may be implemented for\n    # efficiency, but are not required.\n    def tocsr(self, copy=False):\n        \"\"\"Convert this matrix to Compressed Sparse Row format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant csr_matrix.\n        \"\"\"\n        return self.tocoo(copy=copy).tocsr(copy=False)\n\n    def todok(self, copy=False):\n        \"\"\"Convert this matrix to Dictionary Of Keys format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant dok_matrix.\n        \"\"\"\n        return self.tocoo(copy=copy).todok(copy=False)\n\n    def tocoo(self, copy=False):\n        \"\"\"Convert this matrix to COOrdinate format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant coo_matrix.\n        \"\"\"\n        return self.tocsr(copy=False).tocoo(copy=copy)\n\n    def tolil(self, copy=False):\n        \"\"\"Convert this matrix to List of Lists format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant lil_matrix.\n        \"\"\"\n        return self.tocsr(copy=False).tolil(copy=copy)\n\n    def todia(self, copy=False):\n        \"\"\"Convert this matrix to sparse DIAgonal format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant dia_matrix.\n        \"\"\"\n        return self.tocoo(copy=copy).todia(copy=False)\n\n    def tobsr(self, blocksize=None, copy=False):\n        \"\"\"Convert this matrix to Block Sparse Row format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant bsr_matrix.\n\n        When blocksize=(R, C) is provided, it will be used for construction of\n        the bsr_matrix.\n        \"\"\"\n        return self.tocsr(copy=False).tobsr(blocksize=blocksize, copy=copy)\n\n    def tocsc(self, copy=False):\n        \"\"\"Convert this matrix to Compressed Sparse Column format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant csc_matrix.\n        \"\"\"\n        return self.tocsr(copy=copy).tocsc(copy=False)\n\n    def copy(self):\n        \"\"\"Returns a copy of this matrix.\n\n        No data/indices will be shared between the returned value and current\n        matrix.\n        \"\"\"\n        return self.__class__(self, copy=True)\n\n    def sum(self, axis=None, dtype=None, out=None):\n        \"\"\"\n        Sum the matrix elements over a given axis.\n\n        Parameters\n        ----------\n        axis : {-2, -1, 0, 1, None} optional\n            Axis along which the sum is computed. The default is to\n            compute the sum of all the matrix elements, returning a scalar\n            (i.e., `axis` = `None`).\n        dtype : dtype, optional\n            The type of the returned matrix and of the accumulator in which\n            the elements are summed.  The dtype of `a` is used by default\n            unless `a` has an integer dtype of less precision than the default\n            platform integer.  In that case, if `a` is signed then the platform\n            integer is used while if `a` is unsigned then an unsigned integer\n            of the same precision as the platform integer is used.\n\n            .. versionadded:: 0.18.0\n\n        out : np.matrix, optional\n            Alternative output matrix in which to place the result. It must\n            have the same shape as the expected output, but the type of the\n            output values will be cast if necessary.\n\n            .. versionadded:: 0.18.0\n\n        Returns\n        -------\n        sum_along_axis : np.matrix\n            A matrix with the same shape as `self`, with the specified\n            axis removed.\n\n        See Also\n        --------\n        numpy.matrix.sum : NumPy's implementation of 'sum' for matrices\n\n        \"\"\"\n        validateaxis(axis)\n\n        # We use multiplication by a matrix of ones to achieve this.\n        # For some sparse matrix formats more efficient methods are\n        # possible -- these should override this function.\n        m, n = self.shape\n\n        # Mimic numpy's casting.\n        res_dtype = get_sum_dtype(self.dtype)\n\n        if axis is None:\n            # sum over rows and columns\n            return (self * asmatrix(np.ones(\n                (n, 1), dtype=res_dtype))).sum(\n                dtype=dtype, out=out)\n\n        if axis < 0:\n            axis += 2\n\n        # axis = 0 or 1 now\n        if axis == 0:\n            # sum over columns\n            ret = asmatrix(np.ones(\n                (1, m), dtype=res_dtype)) * self\n        else:\n            # sum over rows\n            ret = self * asmatrix(\n                np.ones((n, 1), dtype=res_dtype))\n\n        if out is not None and out.shape != ret.shape:\n            raise ValueError(\"dimensions do not match\")\n\n        return ret.sum(axis=(), dtype=dtype, out=out)\n\n    def mean(self, axis=None, dtype=None, out=None):\n        \"\"\"\n        Compute the arithmetic mean along the specified axis.\n\n        Returns the average of the matrix elements. The average is taken\n        over all elements in the matrix by default, otherwise over the\n        specified axis. `float64` intermediate and return values are used\n        for integer inputs.\n\n        Parameters\n        ----------\n        axis : {-2, -1, 0, 1, None} optional\n            Axis along which the mean is computed. The default is to compute\n            the mean of all elements in the matrix (i.e., `axis` = `None`).\n        dtype : data-type, optional\n            Type to use in computing the mean. For integer inputs, the default\n            is `float64`; for floating point inputs, it is the same as the\n            input dtype.\n\n            .. versionadded:: 0.18.0\n\n        out : np.matrix, optional\n            Alternative output matrix in which to place the result. It must\n            have the same shape as the expected output, but the type of the\n            output values will be cast if necessary.\n\n            .. versionadded:: 0.18.0\n\n        Returns\n        -------\n        m : np.matrix\n\n        See Also\n        --------\n        numpy.matrix.mean : NumPy's implementation of 'mean' for matrices\n\n        \"\"\"\n        def _is_integral(dtype):\n            return (np.issubdtype(dtype, np.integer) or\n                    np.issubdtype(dtype, np.bool_))\n\n        validateaxis(axis)\n\n        res_dtype = self.dtype.type\n        integral = _is_integral(self.dtype)\n\n        # output dtype\n        if dtype is None:\n            if integral:\n                res_dtype = np.float64\n        else:\n            res_dtype = np.dtype(dtype).type\n\n        # intermediate dtype for summation\n        inter_dtype = np.float64 if integral else res_dtype\n        inter_self = self.astype(inter_dtype)\n\n        if axis is None:\n            return (inter_self / np.array(\n                self.shape[0] * self.shape[1]))\\\n                .sum(dtype=res_dtype, out=out)\n\n        if axis < 0:\n            axis += 2\n\n        # axis = 0 or 1 now\n        if axis == 0:\n            return (inter_self * (1.0 / self.shape[0])).sum(\n                axis=0, dtype=res_dtype, out=out)\n        else:\n            return (inter_self * (1.0 / self.shape[1])).sum(\n                axis=1, dtype=res_dtype, out=out)\n\n    def diagonal(self, k=0):\n        \"\"\"Returns the kth diagonal of the matrix.\n\n        Parameters\n        ----------\n        k : int, optional\n            Which diagonal to get, corresponding to elements a[i, i+k].\n            Default: 0 (the main diagonal).\n\n            .. versionadded:: 1.0\n\n        See also\n        --------\n        numpy.diagonal : Equivalent numpy function.\n\n        Examples\n        --------\n        >>> from scipy.sparse import csr_matrix\n        >>> A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n        >>> A.diagonal()\n        array([1, 0, 5])\n        >>> A.diagonal(k=1)\n        array([2, 3])\n        \"\"\"\n        return self.tocsr().diagonal(k=k)\n\n    def setdiag(self, values, k=0):\n        \"\"\"\n        Set diagonal or off-diagonal elements of the array.\n\n        Parameters\n        ----------\n        values : array_like\n            New values of the diagonal elements.\n\n            Values may have any length. If the diagonal is longer than values,\n            then the remaining diagonal entries will not be set. If values are\n            longer than the diagonal, then the remaining values are ignored.\n\n            If a scalar value is given, all of the diagonal is set to it.\n\n        k : int, optional\n            Which off-diagonal to set, corresponding to elements a[i,i+k].\n            Default: 0 (the main diagonal).\n\n        \"\"\"\n        M, N = self.shape\n        if (k > 0 and k >= N) or (k < 0 and -k >= M):\n            raise ValueError(\"k exceeds matrix dimensions\")\n        self._setdiag(np.asarray(values), k)\n\n    def _setdiag(self, values, k):\n        M, N = self.shape\n        if k < 0:\n            if values.ndim == 0:\n                # broadcast\n                max_index = min(M+k, N)\n                for i in range(max_index):\n                    self[i - k, i] = values\n            else:\n                max_index = min(M+k, N, len(values))\n                if max_index <= 0:\n                    return\n                for i, v in enumerate(values[:max_index]):\n                    self[i - k, i] = v\n        else:\n            if values.ndim == 0:\n                # broadcast\n                max_index = min(M, N-k)\n                for i in range(max_index):\n                    self[i, i + k] = values\n            else:\n                max_index = min(M, N-k, len(values))\n                if max_index <= 0:\n                    return\n                for i, v in enumerate(values[:max_index]):\n                    self[i, i + k] = v\n\n    def _process_toarray_args(self, order, out):\n        if out is not None:\n            if order is not None:\n                raise ValueError('order cannot be specified if out '\n                                 'is not None')\n            if out.shape != self.shape or out.dtype != self.dtype:\n                raise ValueError('out array must be same dtype and shape as '\n                                 'sparse matrix')\n            out[...] = 0.\n            return out\n        else:\n            return np.zeros(self.shape, dtype=self.dtype, order=order)\n\n\ndef isspmatrix(x):\n    \"\"\"Is x of a sparse matrix type?\n\n    Parameters\n    ----------\n    x\n        object to check for being a sparse matrix\n\n    Returns\n    -------\n    bool\n        True if x is a sparse matrix, False otherwise\n\n    Notes\n    -----\n    issparse and isspmatrix are aliases for the same function.\n\n    Examples\n    --------\n    >>> from scipy.sparse import csr_matrix, isspmatrix\n    >>> isspmatrix(csr_matrix([[5]]))\n    True\n\n    >>> from scipy.sparse import isspmatrix\n    >>> isspmatrix(5)\n    False\n    \"\"\"\n    return isinstance(x, spmatrix)\n\n\nissparse = isspmatrix\n",1235],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py":["\"\"\"Python part of the warnings subsystem.\"\"\"\n\nimport sys\n\n\n__all__ = [\"warn\", \"warn_explicit\", \"showwarning\",\n           \"formatwarning\", \"filterwarnings\", \"simplefilter\",\n           \"resetwarnings\", \"catch_warnings\"]\n\ndef showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"Hook to write a warning to a file; replace if you like.\"\"\"\n    msg = WarningMessage(message, category, filename, lineno, file, line)\n    _showwarnmsg_impl(msg)\n\ndef formatwarning(message, category, filename, lineno, line=None):\n    \"\"\"Function to format a warning the standard way.\"\"\"\n    msg = WarningMessage(message, category, filename, lineno, None, line)\n    return _formatwarnmsg_impl(msg)\n\ndef _showwarnmsg_impl(msg):\n    file = msg.file\n    if file is None:\n        file = sys.stderr\n        if file is None:\n            # sys.stderr is None when run with pythonw.exe:\n            # warnings get lost\n            return\n    text = _formatwarnmsg(msg)\n    try:\n        file.write(text)\n    except OSError:\n        # the file (probably stderr) is invalid - this warning gets lost.\n        pass\n\ndef _formatwarnmsg_impl(msg):\n    category = msg.category.__name__\n    s =  f\"{msg.filename}:{msg.lineno}: {category}: {msg.message}\\n\"\n\n    if msg.line is None:\n        try:\n            import linecache\n            line = linecache.getline(msg.filename, msg.lineno)\n        except Exception:\n            # When a warning is logged during Python shutdown, linecache\n            # and the import machinery don't work anymore\n            line = None\n            linecache = None\n    else:\n        line = msg.line\n    if line:\n        line = line.strip()\n        s += \"  %s\\n\" % line\n\n    if msg.source is not None:\n        try:\n            import tracemalloc\n        # Logging a warning should not raise a new exception:\n        # catch Exception, not only ImportError and RecursionError.\n        except Exception:\n            # don't suggest to enable tracemalloc if it's not available\n            tracing = True\n            tb = None\n        else:\n            tracing = tracemalloc.is_tracing()\n            try:\n                tb = tracemalloc.get_object_traceback(msg.source)\n            except Exception:\n                # When a warning is logged during Python shutdown, tracemalloc\n                # and the import machinery don't work anymore\n                tb = None\n\n        if tb is not None:\n            s += 'Object allocated at (most recent call last):\\n'\n            for frame in tb:\n                s += ('  File \"%s\", lineno %s\\n'\n                      % (frame.filename, frame.lineno))\n\n                try:\n                    if linecache is not None:\n                        line = linecache.getline(frame.filename, frame.lineno)\n                    else:\n                        line = None\n                except Exception:\n                    line = None\n                if line:\n                    line = line.strip()\n                    s += '    %s\\n' % line\n        elif not tracing:\n            s += (f'{category}: Enable tracemalloc to get the object '\n                  f'allocation traceback\\n')\n    return s\n\n# Keep a reference to check if the function was replaced\n_showwarning_orig = showwarning\n\ndef _showwarnmsg(msg):\n    \"\"\"Hook to write a warning to a file; replace if you like.\"\"\"\n    try:\n        sw = showwarning\n    except NameError:\n        pass\n    else:\n        if sw is not _showwarning_orig:\n            # warnings.showwarning() was replaced\n            if not callable(sw):\n                raise TypeError(\"warnings.showwarning() must be set to a \"\n                                \"function or method\")\n\n            sw(msg.message, msg.category, msg.filename, msg.lineno,\n               msg.file, msg.line)\n            return\n    _showwarnmsg_impl(msg)\n\n# Keep a reference to check if the function was replaced\n_formatwarning_orig = formatwarning\n\ndef _formatwarnmsg(msg):\n    \"\"\"Function to format a warning the standard way.\"\"\"\n    try:\n        fw = formatwarning\n    except NameError:\n        pass\n    else:\n        if fw is not _formatwarning_orig:\n            # warnings.formatwarning() was replaced\n            return fw(msg.message, msg.category,\n                      msg.filename, msg.lineno, msg.line)\n    return _formatwarnmsg_impl(msg)\n\ndef filterwarnings(action, message=\"\", category=Warning, module=\"\", lineno=0,\n                   append=False):\n    \"\"\"Insert an entry into the list of warnings filters (at the front).\n\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'message' -- a regex that the warning message must match\n    'category' -- a class that the warning must be a subclass of\n    'module' -- a regex that the module name must match\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(message, str), \"message must be a string\"\n    assert isinstance(category, type), \"category must be a class\"\n    assert issubclass(category, Warning), \"category must be a Warning subclass\"\n    assert isinstance(module, str), \"module must be a string\"\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n\n    if message or module:\n        import re\n\n    if message:\n        message = re.compile(message, re.I)\n    else:\n        message = None\n    if module:\n        module = re.compile(module)\n    else:\n        module = None\n\n    _add_filter(action, message, category, module, lineno, append=append)\n\ndef simplefilter(action, category=Warning, lineno=0, append=False):\n    \"\"\"Insert a simple entry into the list of warnings filters (at the front).\n\n    A simple filter matches all modules and messages.\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'category' -- a class that the warning must be a subclass of\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    _add_filter(action, None, category, None, lineno, append=append)\n\ndef _add_filter(*item, append):\n    # Remove possible duplicate filters, so new one will be placed\n    # in correct place. If append=True and duplicate exists, do nothing.\n    if not append:\n        try:\n            filters.remove(item)\n        except ValueError:\n            pass\n        filters.insert(0, item)\n    else:\n        if item not in filters:\n            filters.append(item)\n    _filters_mutated()\n\ndef resetwarnings():\n    \"\"\"Clear the list of warning filters, so that no filters are active.\"\"\"\n    filters[:] = []\n    _filters_mutated()\n\nclass _OptionError(Exception):\n    \"\"\"Exception used by option processing helpers.\"\"\"\n    pass\n\n# Helper to process -W options passed via sys.warnoptions\ndef _processoptions(args):\n    for arg in args:\n        try:\n            _setoption(arg)\n        except _OptionError as msg:\n            print(\"Invalid -W option ignored:\", msg, file=sys.stderr)\n\n# Helper for _processoptions()\ndef _setoption(arg):\n    parts = arg.split(':')\n    if len(parts) > 5:\n        raise _OptionError(\"too many fields (max 5): %r\" % (arg,))\n    while len(parts) < 5:\n        parts.append('')\n    action, message, category, module, lineno = [s.strip()\n                                                 for s in parts]\n    action = _getaction(action)\n    category = _getcategory(category)\n    if message or module:\n        import re\n    if message:\n        message = re.escape(message)\n    if module:\n        module = re.escape(module) + r'\\Z'\n    if lineno:\n        try:\n            lineno = int(lineno)\n            if lineno < 0:\n                raise ValueError\n        except (ValueError, OverflowError):\n            raise _OptionError(\"invalid lineno %r\" % (lineno,)) from None\n    else:\n        lineno = 0\n    filterwarnings(action, message, category, module, lineno)\n\n# Helper for _setoption()\ndef _getaction(action):\n    if not action:\n        return \"default\"\n    if action == \"all\": return \"always\" # Alias\n    for a in ('default', 'always', 'ignore', 'module', 'once', 'error'):\n        if a.startswith(action):\n            return a\n    raise _OptionError(\"invalid action: %r\" % (action,))\n\n# Helper for _setoption()\ndef _getcategory(category):\n    if not category:\n        return Warning\n    if '.' not in category:\n        import builtins as m\n        klass = category\n    else:\n        module, _, klass = category.rpartition('.')\n        try:\n            m = __import__(module, None, None, [klass])\n        except ImportError:\n            raise _OptionError(\"invalid module name: %r\" % (module,)) from None\n    try:\n        cat = getattr(m, klass)\n    except AttributeError:\n        raise _OptionError(\"unknown warning category: %r\" % (category,)) from None\n    if not issubclass(cat, Warning):\n        raise _OptionError(\"invalid warning category: %r\" % (category,))\n    return cat\n\n\ndef _is_internal_frame(frame):\n    \"\"\"Signal whether the frame is an internal CPython implementation detail.\"\"\"\n    filename = frame.f_code.co_filename\n    return 'importlib' in filename and '_bootstrap' in filename\n\n\ndef _next_external_frame(frame):\n    \"\"\"Find the next frame that doesn't involve CPython internals.\"\"\"\n    frame = frame.f_back\n    while frame is not None and _is_internal_frame(frame):\n        frame = frame.f_back\n    return frame\n\n\n# Code typically replaced by _warnings\ndef warn(message, category=None, stacklevel=1, source=None):\n    \"\"\"Issue a warning, or maybe ignore it or raise an exception.\"\"\"\n    # Check if message is already a Warning object\n    if isinstance(message, Warning):\n        category = message.__class__\n    # Check category argument\n    if category is None:\n        category = UserWarning\n    if not (isinstance(category, type) and issubclass(category, Warning)):\n        raise TypeError(\"category must be a Warning subclass, \"\n                        \"not '{:s}'\".format(type(category).__name__))\n    # Get context information\n    try:\n        if stacklevel <= 1 or _is_internal_frame(sys._getframe(1)):\n            # If frame is too small to care or if the warning originated in\n            # internal code, then do not try to hide any frames.\n            frame = sys._getframe(stacklevel)\n        else:\n            frame = sys._getframe(1)\n            # Look for one frame less since the above line starts us off.\n            for x in range(stacklevel-1):\n                frame = _next_external_frame(frame)\n                if frame is None:\n                    raise ValueError\n    except ValueError:\n        globals = sys.__dict__\n        filename = \"sys\"\n        lineno = 1\n    else:\n        globals = frame.f_globals\n        filename = frame.f_code.co_filename\n        lineno = frame.f_lineno\n    if '__name__' in globals:\n        module = globals['__name__']\n    else:\n        module = \"<string>\"\n    registry = globals.setdefault(\"__warningregistry__\", {})\n    warn_explicit(message, category, filename, lineno, module, registry,\n                  globals, source)\n\ndef warn_explicit(message, category, filename, lineno,\n                  module=None, registry=None, module_globals=None,\n                  source=None):\n    lineno = int(lineno)\n    if module is None:\n        module = filename or \"<unknown>\"\n        if module[-3:].lower() == \".py\":\n            module = module[:-3] # XXX What about leading pathname?\n    if registry is None:\n        registry = {}\n    if registry.get('version', 0) != _filters_version:\n        registry.clear()\n        registry['version'] = _filters_version\n    if isinstance(message, Warning):\n        text = str(message)\n        category = message.__class__\n    else:\n        text = message\n        message = category(message)\n    key = (text, category, lineno)\n    # Quick test for common case\n    if registry.get(key):\n        return\n    # Search the filters\n    for item in filters:\n        action, msg, cat, mod, ln = item\n        if ((msg is None or msg.match(text)) and\n            issubclass(category, cat) and\n            (mod is None or mod.match(module)) and\n            (ln == 0 or lineno == ln)):\n            break\n    else:\n        action = defaultaction\n    # Early exit actions\n    if action == \"ignore\":\n        return\n\n    # Prime the linecache for formatting, in case the\n    # \"file\" is actually in a zipfile or something.\n    import linecache\n    linecache.getlines(filename, module_globals)\n\n    if action == \"error\":\n        raise message\n    # Other actions\n    if action == \"once\":\n        registry[key] = 1\n        oncekey = (text, category)\n        if onceregistry.get(oncekey):\n            return\n        onceregistry[oncekey] = 1\n    elif action == \"always\":\n        pass\n    elif action == \"module\":\n        registry[key] = 1\n        altkey = (text, category, 0)\n        if registry.get(altkey):\n            return\n        registry[altkey] = 1\n    elif action == \"default\":\n        registry[key] = 1\n    else:\n        # Unrecognized actions are errors\n        raise RuntimeError(\n              \"Unrecognized action (%r) in warnings.filters:\\n %s\" %\n              (action, item))\n    # Print message and context\n    msg = WarningMessage(message, category, filename, lineno, source)\n    _showwarnmsg(msg)\n\n\nclass WarningMessage(object):\n\n    _WARNING_DETAILS = (\"message\", \"category\", \"filename\", \"lineno\", \"file\",\n                        \"line\", \"source\")\n\n    def __init__(self, message, category, filename, lineno, file=None,\n                 line=None, source=None):\n        self.message = message\n        self.category = category\n        self.filename = filename\n        self.lineno = lineno\n        self.file = file\n        self.line = line\n        self.source = source\n        self._category_name = category.__name__ if category else None\n\n    def __str__(self):\n        return (\"{message : %r, category : %r, filename : %r, lineno : %s, \"\n                    \"line : %r}\" % (self.message, self._category_name,\n                                    self.filename, self.lineno, self.line))\n\n\nclass catch_warnings(object):\n\n    \"\"\"A context manager that copies and restores the warnings filter upon\n    exiting the context.\n\n    The 'record' argument specifies whether warnings should be captured by a\n    custom implementation of warnings.showwarning() and be appended to a list\n    returned by the context manager. Otherwise None is returned by the context\n    manager. The objects appended to the list are arguments whose attributes\n    mirror the arguments to showwarning().\n\n    The 'module' argument is to specify an alternative module to the module\n    named 'warnings' and imported under that name. This argument is only useful\n    when testing the warnings module itself.\n\n    \"\"\"\n\n    def __init__(self, *, record=False, module=None):\n        \"\"\"Specify whether to record warnings and if an alternative module\n        should be used other than sys.modules['warnings'].\n\n        For compatibility with Python 3.0, please consider all arguments to be\n        keyword-only.\n\n        \"\"\"\n        self._record = record\n        self._module = sys.modules['warnings'] if module is None else module\n        self._entered = False\n\n    def __repr__(self):\n        args = []\n        if self._record:\n            args.append(\"record=True\")\n        if self._module is not sys.modules['warnings']:\n            args.append(\"module=%r\" % self._module)\n        name = type(self).__name__\n        return \"%s(%s)\" % (name, \", \".join(args))\n\n    def __enter__(self):\n        if self._entered:\n            raise RuntimeError(\"Cannot enter %r twice\" % self)\n        self._entered = True\n        self._filters = self._module.filters\n        self._module.filters = self._filters[:]\n        self._module._filters_mutated()\n        self._showwarning = self._module.showwarning\n        self._showwarnmsg_impl = self._module._showwarnmsg_impl\n        if self._record:\n            log = []\n            self._module._showwarnmsg_impl = log.append\n            # Reset showwarning() to the default implementation to make sure\n            # that _showwarnmsg() calls _showwarnmsg_impl()\n            self._module.showwarning = self._module._showwarning_orig\n            return log\n        else:\n            return None\n\n    def __exit__(self, *exc_info):\n        if not self._entered:\n            raise RuntimeError(\"Cannot exit %r without entering first\" % self)\n        self._module.filters = self._filters\n        self._module._filters_mutated()\n        self._module.showwarning = self._showwarning\n        self._module._showwarnmsg_impl = self._showwarnmsg_impl\n\n\n# Private utility function called by _PyErr_WarnUnawaitedCoroutine\ndef _warn_unawaited_coroutine(coro):\n    msg_lines = [\n        f\"coroutine '{coro.__qualname__}' was never awaited\\n\"\n    ]\n    if coro.cr_origin is not None:\n        import linecache, traceback\n        def extract():\n            for filename, lineno, funcname in reversed(coro.cr_origin):\n                line = linecache.getline(filename, lineno)\n                yield (filename, lineno, funcname, line)\n        msg_lines.append(\"Coroutine created at (most recent call last)\\n\")\n        msg_lines += traceback.format_list(list(extract()))\n    msg = \"\".join(msg_lines).rstrip(\"\\n\")\n    # Passing source= here means that if the user happens to have tracemalloc\n    # enabled and tracking where the coroutine was created, the warning will\n    # contain that traceback. This does mean that if they have *both*\n    # coroutine origin tracking *and* tracemalloc enabled, they'll get two\n    # partially-redundant tracebacks. If we wanted to be clever we could\n    # probably detect this case and avoid it, but for now we don't bother.\n    warn(msg, category=RuntimeWarning, stacklevel=2, source=coro)\n\n\n# filters contains a sequence of filter 5-tuples\n# The components of the 5-tuple are:\n# - an action: error, ignore, always, default, module, or once\n# - a compiled regex that must match the warning message\n# - a class representing the warning category\n# - a compiled regex that must match the module that is being warned\n# - a line number for the line being warning, or 0 to mean any line\n# If either if the compiled regexs are None, match anything.\ntry:\n    from _warnings import (filters, _defaultaction, _onceregistry,\n                           warn, warn_explicit, _filters_mutated)\n    defaultaction = _defaultaction\n    onceregistry = _onceregistry\n    _warnings_defaults = True\nexcept ImportError:\n    filters = []\n    defaultaction = \"default\"\n    onceregistry = {}\n\n    _filters_version = 1\n\n    def _filters_mutated():\n        global _filters_version\n        _filters_version += 1\n\n    _warnings_defaults = False\n\n\n# Module initialization\n_processoptions(sys.warnoptions)\nif not _warnings_defaults:\n    # Several warning categories are ignored by default in regular builds\n    if not hasattr(sys, 'gettotalrefcount'):\n        filterwarnings(\"default\", category=DeprecationWarning,\n                       module=\"__main__\", append=1)\n        simplefilter(\"ignore\", category=DeprecationWarning, append=1)\n        simplefilter(\"ignore\", category=PendingDeprecationWarning, append=1)\n        simplefilter(\"ignore\", category=ImportWarning, append=1)\n        simplefilter(\"ignore\", category=ResourceWarning, append=1)\n\ndel _warnings_defaults\n",549],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py":["\"\"\"Utilities for input validation\"\"\"\n\n# Authors: Olivier Grisel\n#          Gael Varoquaux\n#          Andreas Mueller\n#          Lars Buitinck\n#          Alexandre Gramfort\n#          Nicolas Tresegnie\n#          Sylvain Marie\n# License: BSD 3 clause\n\nfrom functools import wraps\nimport warnings\nimport numbers\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom inspect import signature, isclass, Parameter\n\n# mypy error: Module 'numpy.core.numeric' has no attribute 'ComplexWarning'\nfrom numpy.core.numeric import ComplexWarning  # type: ignore\nimport joblib\n\nfrom contextlib import suppress\n\nfrom .fixes import _object_dtype_isnan, parse_version\nfrom .. import get_config as _get_config\nfrom ..exceptions import PositiveSpectrumWarning\nfrom ..exceptions import NotFittedError\nfrom ..exceptions import DataConversionWarning\n\nFLOAT_DTYPES = (np.float64, np.float32, np.float16)\n\n\ndef _deprecate_positional_args(func=None, *, version=\"1.1 (renaming of 0.26)\"):\n    \"\"\"Decorator for methods that issues warnings for positional arguments.\n\n    Using the keyword-only argument syntax in pep 3102, arguments after the\n    * will issue a warning when passed as a positional argument.\n\n    Parameters\n    ----------\n    func : callable, default=None\n        Function to check arguments on.\n    version : callable, default=\"1.1 (renaming of 0.26)\"\n        The version when positional arguments will result in error.\n    \"\"\"\n\n    def _inner_deprecate_positional_args(f):\n        sig = signature(f)\n        kwonly_args = []\n        all_args = []\n\n        for name, param in sig.parameters.items():\n            if param.kind == Parameter.POSITIONAL_OR_KEYWORD:\n                all_args.append(name)\n            elif param.kind == Parameter.KEYWORD_ONLY:\n                kwonly_args.append(name)\n\n        @wraps(f)\n        def inner_f(*args, **kwargs):\n            extra_args = len(args) - len(all_args)\n            if extra_args <= 0:\n                return f(*args, **kwargs)\n\n            # extra_args > 0\n            args_msg = [\n                \"{}={}\".format(name, arg)\n                for name, arg in zip(kwonly_args[:extra_args], args[-extra_args:])\n            ]\n            args_msg = \", \".join(args_msg)\n            warnings.warn(\n                f\"Pass {args_msg} as keyword args. From version \"\n                f\"{version} passing these as positional arguments \"\n                \"will result in an error\",\n                FutureWarning,\n            )\n            kwargs.update(zip(sig.parameters, args))\n            return f(**kwargs)\n\n        return inner_f\n\n    if func is not None:\n        return _inner_deprecate_positional_args(func)\n\n    return _inner_deprecate_positional_args\n\n\ndef _assert_all_finite(X, allow_nan=False, msg_dtype=None):\n    \"\"\"Like assert_all_finite, but only for ndarray.\"\"\"\n    # validation is also imported in extmath\n    from .extmath import _safe_accumulator_op\n\n    if _get_config()[\"assume_finite\"]:\n        return\n    X = np.asanyarray(X)\n    # First try an O(n) time, O(1) space solution for the common case that\n    # everything is finite; fall back to O(n) space np.isfinite to prevent\n    # false positives from overflow in sum method. The sum is also calculated\n    # safely to reduce dtype induced overflows.\n    is_float = X.dtype.kind in \"fc\"\n    if is_float and (np.isfinite(_safe_accumulator_op(np.sum, X))):\n        pass\n    elif is_float:\n        msg_err = \"Input contains {} or a value too large for {!r}.\"\n        if (\n            allow_nan\n            and np.isinf(X).any()\n            or not allow_nan\n            and not np.isfinite(X).all()\n        ):\n            type_err = \"infinity\" if allow_nan else \"NaN, infinity\"\n            raise ValueError(\n                msg_err.format(\n                    type_err, msg_dtype if msg_dtype is not None else X.dtype\n                )\n            )\n    # for object dtype data, we only check for NaNs (GH-13254)\n    elif X.dtype == np.dtype(\"object\") and not allow_nan:\n        if _object_dtype_isnan(X).any():\n            raise ValueError(\"Input contains NaN\")\n\n\ndef assert_all_finite(X, *, allow_nan=False):\n    \"\"\"Throw a ValueError if X contains NaN or infinity.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix}\n\n    allow_nan : bool, default=False\n    \"\"\"\n    _assert_all_finite(X.data if sp.issparse(X) else X, allow_nan)\n\n\ndef as_float_array(X, *, copy=True, force_all_finite=True):\n    \"\"\"Converts an array-like to an array of floats.\n\n    The new dtype will be np.float32 or np.float64, depending on the original\n    type. The function can create a copy or modify the argument depending\n    on the argument copy.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n\n    copy : bool, default=True\n        If True, a copy of X will be created. If False, a copy may still be\n        returned if X's dtype is not a floating point type.\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in X. The\n        possibilities are:\n\n        - True: Force all values of X to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in X.\n        - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot\n          be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n    Returns\n    -------\n    XT : {ndarray, sparse matrix}\n        An array of type float.\n    \"\"\"\n    if isinstance(X, np.matrix) or (\n        not isinstance(X, np.ndarray) and not sp.issparse(X)\n    ):\n        return check_array(\n            X,\n            accept_sparse=[\"csr\", \"csc\", \"coo\"],\n            dtype=np.float64,\n            copy=copy,\n            force_all_finite=force_all_finite,\n            ensure_2d=False,\n        )\n    elif sp.issparse(X) and X.dtype in [np.float32, np.float64]:\n        return X.copy() if copy else X\n    elif X.dtype in [np.float32, np.float64]:  # is numpy array\n        return X.copy(\"F\" if X.flags[\"F_CONTIGUOUS\"] else \"C\") if copy else X\n    else:\n        if X.dtype.kind in \"uib\" and X.dtype.itemsize <= 4:\n            return_dtype = np.float32\n        else:\n            return_dtype = np.float64\n        return X.astype(return_dtype)\n\n\ndef _is_arraylike(x):\n    \"\"\"Returns whether the input is array-like.\"\"\"\n    return hasattr(x, \"__len__\") or hasattr(x, \"shape\") or hasattr(x, \"__array__\")\n\n\ndef _num_features(X):\n    \"\"\"Return the number of features in an array-like X.\n\n    This helper function tries hard to avoid to materialize an array version\n    of X unless necessary. For instance, if X is a list of lists,\n    this function will return the length of the first element, assuming\n    that subsequent elements are all lists of the same length without\n    checking.\n    Parameters\n    ----------\n    X : array-like\n        array-like to get the number of features.\n\n    Returns\n    -------\n    features : int\n        Number of features\n    \"\"\"\n    type_ = type(X)\n    if type_.__module__ == \"builtins\":\n        type_name = type_.__qualname__\n    else:\n        type_name = f\"{type_.__module__}.{type_.__qualname__}\"\n    message = f\"Unable to find the number of features from X of type {type_name}\"\n    if not hasattr(X, \"__len__\") and not hasattr(X, \"shape\"):\n        if not hasattr(X, \"__array__\"):\n            raise TypeError(message)\n        # Only convert X to a numpy array if there is no cheaper, heuristic\n        # option.\n        X = np.asarray(X)\n\n    if hasattr(X, \"shape\"):\n        if not hasattr(X.shape, \"__len__\") or len(X.shape) <= 1:\n            message += f\" with shape {X.shape}\"\n            raise TypeError(message)\n        return X.shape[1]\n\n    first_sample = X[0]\n\n    # Do not consider an array-like of strings or dicts to be a 2D array\n    if isinstance(first_sample, (str, bytes, dict)):\n        message += f\" where the samples are of type {type(first_sample).__qualname__}\"\n        raise TypeError(message)\n\n    try:\n        # If X is a list of lists, for instance, we assume that all nested\n        # lists have the same length without checking or converting to\n        # a numpy array to keep this function call as cheap as possible.\n        return len(first_sample)\n    except Exception as err:\n        raise TypeError(message) from err\n\n\ndef _num_samples(x):\n    \"\"\"Return number of samples in array-like x.\"\"\"\n    message = \"Expected sequence or array-like, got %s\" % type(x)\n    if hasattr(x, \"fit\") and callable(x.fit):\n        # Don't get num_samples from an ensembles length!\n        raise TypeError(message)\n\n    if not hasattr(x, \"__len__\") and not hasattr(x, \"shape\"):\n        if hasattr(x, \"__array__\"):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n\n    if hasattr(x, \"shape\") and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError(\n                \"Singleton array %r cannot be considered a valid collection.\" % x\n            )\n        # Check that shape is returning an integer or default to len\n        # Dask dataframes may not return numeric shape[0] value\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error\n\n\ndef check_memory(memory):\n    \"\"\"Check that ``memory`` is joblib.Memory-like.\n\n    joblib.Memory-like means that ``memory`` can be converted into a\n    joblib.Memory instance (typically a str denoting the ``location``)\n    or has the same interface (has a ``cache`` method).\n\n    Parameters\n    ----------\n    memory : None, str or object with the joblib.Memory interface\n\n    Returns\n    -------\n    memory : object with the joblib.Memory interface\n\n    Raises\n    ------\n    ValueError\n        If ``memory`` is not joblib.Memory-like.\n    \"\"\"\n\n    if memory is None or isinstance(memory, str):\n        if parse_version(joblib.__version__) < parse_version(\"0.12\"):\n            memory = joblib.Memory(cachedir=memory, verbose=0)\n        else:\n            memory = joblib.Memory(location=memory, verbose=0)\n    elif not hasattr(memory, \"cache\"):\n        raise ValueError(\n            \"'memory' should be None, a string or have the same\"\n            \" interface as joblib.Memory.\"\n            \" Got memory='{}' instead.\".format(memory)\n        )\n    return memory\n\n\ndef check_consistent_length(*arrays):\n    \"\"\"Check that all arrays have consistent first dimensions.\n\n    Checks whether all objects in arrays have the same shape or length.\n\n    Parameters\n    ----------\n    *arrays : list or tuple of input objects.\n        Objects that will be checked for consistent length.\n    \"\"\"\n\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError(\n            \"Found input variables with inconsistent numbers of samples: %r\"\n            % [int(l) for l in lengths]\n        )\n\n\ndef _make_indexable(iterable):\n    \"\"\"Ensure iterable supports indexing or convert to an indexable variant.\n\n    Convert sparse matrices to csr and other non-indexable iterable to arrays.\n    Let `None` and indexable objects (e.g. pandas dataframes) pass unchanged.\n\n    Parameters\n    ----------\n    iterable : {list, dataframe, ndarray, sparse matrix} or None\n        Object to be converted to an indexable iterable.\n    \"\"\"\n    if sp.issparse(iterable):\n        return iterable.tocsr()\n    elif hasattr(iterable, \"__getitem__\") or hasattr(iterable, \"iloc\"):\n        return iterable\n    elif iterable is None:\n        return iterable\n    return np.array(iterable)\n\n\ndef indexable(*iterables):\n    \"\"\"Make arrays indexable for cross-validation.\n\n    Checks consistent length, passes through None, and ensures that everything\n    can be indexed by converting sparse matrices to csr and converting\n    non-interable objects to arrays.\n\n    Parameters\n    ----------\n    *iterables : {lists, dataframes, ndarrays, sparse matrices}\n        List of objects to ensure sliceability.\n    \"\"\"\n    result = [_make_indexable(X) for X in iterables]\n    check_consistent_length(*result)\n    return result\n\n\ndef _ensure_sparse_format(\n    spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse\n):\n    \"\"\"Convert a sparse matrix to a given format.\n\n    Checks the sparse format of spmatrix and converts if necessary.\n\n    Parameters\n    ----------\n    spmatrix : sparse matrix\n        Input to validate and convert.\n\n    accept_sparse : str, bool or list/tuple of str\n        String[s] representing allowed sparse matrix formats ('csc',\n        'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). If the input is sparse but\n        not in the allowed format, it will be converted to the first listed\n        format. True allows the input to be any format. False means\n        that a sparse matrix input will raise an error.\n\n    dtype : str, type or None\n        Data type of result. If None, the dtype of the input is preserved.\n\n    copy : bool\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : bool or 'allow-nan'\n        Whether to raise an error on np.inf, np.nan, pd.NA in X. The\n        possibilities are:\n\n        - True: Force all values of X to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in X.\n        - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot\n          be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n    Returns\n    -------\n    spmatrix_converted : sparse matrix.\n        Matrix that is ensured to have an allowed type.\n    \"\"\"\n    if dtype is None:\n        dtype = spmatrix.dtype\n\n    changed_format = False\n\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n\n    # Indices dtype validation\n    _check_large_sparse(spmatrix, accept_large_sparse)\n\n    if accept_sparse is False:\n        raise TypeError(\n            \"A sparse matrix was passed, but dense \"\n            \"data is required. Use X.toarray() to \"\n            \"convert to a dense numpy array.\"\n        )\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\n                \"When providing 'accept_sparse' \"\n                \"as a tuple or list, it must contain at \"\n                \"least one string value.\"\n            )\n        # ensure correct sparse format\n        if spmatrix.format not in accept_sparse:\n            # create new with correct sparse\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        # any other type\n        raise ValueError(\n            \"Parameter 'accept_sparse' should be a string, \"\n            \"boolean or list of strings. You provided \"\n            \"'accept_sparse={}'.\".format(accept_sparse)\n        )\n\n    if dtype != spmatrix.dtype:\n        # convert dtype\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and not changed_format:\n        # force copy\n        spmatrix = spmatrix.copy()\n\n    if force_all_finite:\n        if not hasattr(spmatrix, \"data\"):\n            warnings.warn(\n                \"Can't check %s sparse matrix for nan or inf.\" % spmatrix.format,\n                stacklevel=2,\n            )\n        else:\n            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == \"allow-nan\")\n\n    return spmatrix\n\n\ndef _ensure_no_complex_data(array):\n    if (\n        hasattr(array, \"dtype\")\n        and array.dtype is not None\n        and hasattr(array.dtype, \"kind\")\n        and array.dtype.kind == \"c\"\n    ):\n        raise ValueError(\"Complex data not supported\\n{}\\n\".format(array))\n\n\ndef check_array(\n    array,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_all_finite=True,\n    ensure_2d=True,\n    allow_nd=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    estimator=None,\n):\n\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is checked to be a non-empty 2D array containing\n    only finite values. If the dtype of the array is object, attempt\n    converting to float, raising on failure.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : str, bool or list/tuple of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse=False will cause it to be accepted\n        only if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n\n    order : {'F', 'C'} or None, default=None\n        Whether an array will be forced to be fortran or c-style.\n        When order is None (default), then if copy=False, nothing is ensured\n        about the memory layout of the output array; otherwise (copy=True)\n        the memory layout of the returned array is kept as close as possible\n        to the original array.\n\n    copy : bool, default=False\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n    ensure_2d : bool, default=True\n        Whether to raise a value error if array is not 2D.\n\n    allow_nd : bool, default=False\n        Whether to allow array.ndim > 2.\n\n    ensure_min_samples : int, default=1\n        Make sure that the array has a minimum number of samples in its first\n        axis (rows for a 2D array). Setting to 0 disables this check.\n\n    ensure_min_features : int, default=1\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when the input data has effectively 2\n        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n        disables this check.\n\n    estimator : str or estimator instance, default=None\n        If passed, include the name of the estimator in warning messages.\n\n    Returns\n    -------\n    array_converted : object\n        The converted and validated array.\n    \"\"\"\n    if isinstance(array, np.matrix):\n        warnings.warn(\n            \"np.matrix usage is deprecated in 1.0 and will raise a TypeError \"\n            \"in 1.2. Please convert to a numpy array with np.asarray. For \"\n            \"more information see: \"\n            \"https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\",  # noqa\n            FutureWarning,\n        )\n\n    # store reference to original array to check if copy is needed when\n    # function returns\n    array_orig = array\n\n    # store whether originally we wanted numeric dtype\n    dtype_numeric = isinstance(dtype, str) and dtype == \"numeric\"\n\n    dtype_orig = getattr(array, \"dtype\", None)\n    if not hasattr(dtype_orig, \"kind\"):\n        # not a data type (e.g. a column named dtype in a pandas DataFrame)\n        dtype_orig = None\n\n    # check if the object contains several dtypes (typically a pandas\n    # DataFrame), and store them. If not, store None.\n    dtypes_orig = None\n    has_pd_integer_array = False\n    if hasattr(array, \"dtypes\") and hasattr(array.dtypes, \"__array__\"):\n        # throw warning if columns are sparse. If all columns are sparse, then\n        # array.sparse exists and sparsity will be perserved (later).\n        with suppress(ImportError):\n            from pandas.api.types import is_sparse\n\n            if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n                warnings.warn(\n                    \"pandas.DataFrame with sparse columns found.\"\n                    \"It will be converted to a dense numpy array.\"\n                )\n\n        dtypes_orig = list(array.dtypes)\n        # pandas boolean dtype __array__ interface coerces bools to objects\n        for i, dtype_iter in enumerate(dtypes_orig):\n            if dtype_iter.kind == \"b\":\n                dtypes_orig[i] = np.dtype(object)\n            elif dtype_iter.name.startswith((\"Int\", \"UInt\")):\n                # name looks like an Integer Extension Array, now check for\n                # the dtype\n                with suppress(ImportError):\n                    from pandas import (\n                        Int8Dtype,\n                        Int16Dtype,\n                        Int32Dtype,\n                        Int64Dtype,\n                        UInt8Dtype,\n                        UInt16Dtype,\n                        UInt32Dtype,\n                        UInt64Dtype,\n                    )\n\n                    if isinstance(\n                        dtype_iter,\n                        (\n                            Int8Dtype,\n                            Int16Dtype,\n                            Int32Dtype,\n                            Int64Dtype,\n                            UInt8Dtype,\n                            UInt16Dtype,\n                            UInt32Dtype,\n                            UInt64Dtype,\n                        ),\n                    ):\n                        has_pd_integer_array = True\n\n        if all(isinstance(dtype, np.dtype) for dtype in dtypes_orig):\n            dtype_orig = np.result_type(*dtypes_orig)\n\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == \"O\":\n            # if input is object, convert to float.\n            dtype = np.float64\n        else:\n            dtype = None\n\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            # no dtype conversion required\n            dtype = None\n        else:\n            # dtype conversion required. Let's select the first element of the\n            # list of accepted types.\n            dtype = dtype[0]\n\n    if has_pd_integer_array:\n        # If there are any pandas integer extension arrays,\n        array = array.astype(dtype)\n\n    if force_all_finite not in (True, False, \"allow-nan\"):\n        raise ValueError(\n            'force_all_finite should be a bool or \"allow-nan\". Got {!r} instead'.format(\n                force_all_finite\n            )\n        )\n\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = \"Estimator\"\n    context = \" by %s\" % estimator_name if estimator is not None else \"\"\n\n    # When all dataframe columns are sparse, convert to a sparse array\n    if hasattr(array, \"sparse\") and array.ndim > 1:\n        # DataFrame.sparse only supports `to_coo`\n        array = array.sparse.to_coo()\n        if array.dtype == np.dtype(\"object\"):\n            unique_dtypes = set([dt.subtype.name for dt in array_orig.dtypes])\n            if len(unique_dtypes) > 1:\n                raise ValueError(\n                    \"Pandas DataFrame with mixed sparse extension arrays \"\n                    \"generated a sparse matrix with object dtype which \"\n                    \"can not be converted to a scipy sparse matrix.\"\n                    \"Sparse extension arrays should all have the same \"\n                    \"numeric type.\"\n                )\n\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(\n            array,\n            accept_sparse=accept_sparse,\n            dtype=dtype,\n            copy=copy,\n            force_all_finite=force_all_finite,\n            accept_large_sparse=accept_large_sparse,\n        )\n    else:\n        # If np.array(..) gives ComplexWarning, then we convert the warning\n        # to an error. This is needed because specifying a non complex\n        # dtype to the function converts complex to real dtype,\n        # thereby passing the test made in the lines following the scope\n        # of warnings context manager.\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter(\"error\", ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in \"iu\":\n                    # Conversion float -> int should not contain NaN or\n                    # inf (numpy#14412). We cannot use casting='safe' because\n                    # then conversion float -> int would be disallowed.\n                    array = np.asarray(array, order=order)\n                    if array.dtype.kind == \"f\":\n                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype)\n                    array = array.astype(dtype, casting=\"unsafe\", copy=False)\n                else:\n                    array = np.asarray(array, order=order, dtype=dtype)\n            except ComplexWarning as complex_warning:\n                raise ValueError(\n                    \"Complex data not supported\\n{}\\n\".format(array)\n                ) from complex_warning\n\n        # It is possible that the np.array(..) gave no warning. This happens\n        # when no dtype conversion happened, for example dtype = None. The\n        # result is that np.array(..) produces an array of complex dtype\n        # and we need to catch and raise exception for such cases.\n        _ensure_no_complex_data(array)\n\n        if ensure_2d:\n            # If input is scalar raise error\n            if array.ndim == 0:\n                raise ValueError(\n                    \"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array)\n                )\n            # If input is 1D raise error\n            if array.ndim == 1:\n                raise ValueError(\n                    \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array)\n                )\n\n        # make sure we actually converted to numeric:\n        if dtype_numeric and array.dtype.kind in \"OUSV\":\n            warnings.warn(\n                \"Arrays of bytes/strings is being converted to decimal \"\n                \"numbers if dtype='numeric'. This behavior is deprecated in \"\n                \"0.24 and will be removed in 1.1 (renaming of 0.26). Please \"\n                \"convert your data to numeric values explicitly instead.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            try:\n                array = array.astype(np.float64)\n            except ValueError as e:\n                raise ValueError(\n                    \"Unable to convert array of bytes/strings \"\n                    \"into decimal numbers with dtype='numeric'\"\n                ) from e\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\n                \"Found array with dim %d. %s expected <= 2.\"\n                % (array.ndim, estimator_name)\n            )\n\n        if force_all_finite:\n            _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError(\n                \"Found array with %d sample(s) (shape=%s) while a\"\n                \" minimum of %d is required%s.\"\n                % (n_samples, array.shape, ensure_min_samples, context)\n            )\n\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError(\n                \"Found array with %d feature(s) (shape=%s) while\"\n                \" a minimum of %d is required%s.\"\n                % (n_features, array.shape, ensure_min_features, context)\n            )\n\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n\n    return array\n\n\ndef _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\"\"\"\n    if not accept_large_sparse:\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = [\"col\", \"row\"]\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = [\"indices\", \"indptr\"]\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError(\n                    \"Only sparse matrices with 32-bit integer\"\n                    \" indices are accepted. Got %s indices.\" % indices_datatype\n                )\n\n\ndef check_X_y(\n    X,\n    y,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_all_finite=True,\n    ensure_2d=True,\n    allow_nd=False,\n    multi_output=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    y_numeric=False,\n    estimator=None,\n):\n    \"\"\"Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X to be 2D and y 1D. By\n    default, X is checked to be non-empty and containing only finite values.\n    Standard input checks are also applied to y, such as checking that y\n    does not have np.nan or np.inf targets. For multi-label y, set\n    multi_output=True to allow 2D and sparse y. If the dtype of X is\n    object, attempt converting to float, raising on failure.\n\n    Parameters\n    ----------\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    y : {ndarray, list, sparse matrix}\n        Labels.\n\n    accept_sparse : str, bool or list of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse will cause it to be accepted only\n        if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n\n    order : {'F', 'C'}, default=None\n        Whether an array will be forced to be fortran or c-style.\n\n    copy : bool, default=False\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in X. This parameter\n        does not influence whether y can have np.inf, np.nan, pd.NA values.\n        The possibilities are:\n\n        - True: Force all values of X to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in X.\n        - 'allow-nan': accepts only np.nan or pd.NA values in X. Values cannot\n          be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n    ensure_2d : bool, default=True\n        Whether to raise a value error if X is not 2D.\n\n    allow_nd : bool, default=False\n        Whether to allow X.ndim > 2.\n\n    multi_output : bool, default=False\n        Whether to allow 2D y (array or sparse matrix). If false, y will be\n        validated as a vector. y cannot have np.nan or np.inf values if\n        multi_output=True.\n\n    ensure_min_samples : int, default=1\n        Make sure that X has a minimum number of samples in its first\n        axis (rows for a 2D array).\n\n    ensure_min_features : int, default=1\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when X has effectively 2 dimensions or\n        is originally 1D and ``ensure_2d`` is True. Setting to 0 disables\n        this check.\n\n    y_numeric : bool, default=False\n        Whether to ensure that y has a numeric type. If dtype of y is object,\n        it is converted to float64. Should only be used for regression\n        algorithms.\n\n    estimator : str or estimator instance, default=None\n        If passed, include the name of the estimator in warning messages.\n\n    Returns\n    -------\n    X_converted : object\n        The converted and validated X.\n\n    y_converted : object\n        The converted and validated y.\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    X = check_array(\n        X,\n        accept_sparse=accept_sparse,\n        accept_large_sparse=accept_large_sparse,\n        dtype=dtype,\n        order=order,\n        copy=copy,\n        force_all_finite=force_all_finite,\n        ensure_2d=ensure_2d,\n        allow_nd=allow_nd,\n        ensure_min_samples=ensure_min_samples,\n        ensure_min_features=ensure_min_features,\n        estimator=estimator,\n    )\n\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric)\n\n    check_consistent_length(X, y)\n\n    return X, y\n\n\ndef _check_y(y, multi_output=False, y_numeric=False):\n    \"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\n    if multi_output:\n        y = check_array(\n            y, accept_sparse=\"csr\", force_all_finite=True, ensure_2d=False, dtype=None\n        )\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n        _ensure_no_complex_data(y)\n    if y_numeric and y.dtype.kind == \"O\":\n        y = y.astype(np.float64)\n\n    return y\n\n\ndef column_or_1d(y, *, warn=False):\n    \"\"\"Ravel column or 1d numpy array, else raises an error.\n\n    Parameters\n    ----------\n    y : array-like\n\n    warn : bool, default=False\n       To control display of warnings.\n\n    Returns\n    -------\n    y : ndarray\n\n    \"\"\"\n    y = np.asarray(y)\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn(\n                \"A column-vector y was passed when a 1d array was\"\n                \" expected. Please change the shape of y to \"\n                \"(n_samples, ), for example using ravel().\",\n                DataConversionWarning,\n                stacklevel=2,\n            )\n        return np.ravel(y)\n\n    raise ValueError(\n        \"y should be a 1d array, got an array of shape {} instead.\".format(shape)\n    )\n\n\ndef check_random_state(seed):\n    \"\"\"Turn seed into a np.random.RandomState instance\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        \"%r cannot be used to seed a numpy.random.RandomState instance\" % seed\n    )\n\n\ndef has_fit_parameter(estimator, parameter):\n    \"\"\"Checks whether the estimator's fit method supports the given parameter.\n\n    Parameters\n    ----------\n    estimator : object\n        An estimator to inspect.\n\n    parameter : str\n        The searched parameter.\n\n    Returns\n    -------\n    is_parameter: bool\n        Whether the parameter was found to be a named parameter of the\n        estimator's fit method.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> has_fit_parameter(SVC(), \"sample_weight\")\n    True\n\n    \"\"\"\n    return parameter in signature(estimator.fit).parameters\n\n\ndef check_symmetric(array, *, tol=1e-10, raise_warning=True, raise_exception=False):\n    \"\"\"Make sure that array is 2D, square and symmetric.\n\n    If the array is not symmetric, then a symmetrized version is returned.\n    Optionally, a warning or exception is raised if the matrix is not\n    symmetric.\n\n    Parameters\n    ----------\n    array : {ndarray, sparse matrix}\n        Input object to check / convert. Must be two-dimensional and square,\n        otherwise a ValueError will be raised.\n\n    tol : float, default=1e-10\n        Absolute tolerance for equivalence of arrays. Default = 1E-10.\n\n    raise_warning : bool, default=True\n        If True then raise a warning if conversion is required.\n\n    raise_exception : bool, default=False\n        If True then raise an exception if array is not symmetric.\n\n    Returns\n    -------\n    array_sym : {ndarray, sparse matrix}\n        Symmetrized version of the input array, i.e. the average of array\n        and array.transpose(). If sparse, then duplicate entries are first\n        summed and zeros are eliminated.\n    \"\"\"\n    if (array.ndim != 2) or (array.shape[0] != array.shape[1]):\n        raise ValueError(\n            \"array must be 2-dimensional and square. shape = {0}\".format(array.shape)\n        )\n\n    if sp.issparse(array):\n        diff = array - array.T\n        # only csr, csc, and coo have `data` attribute\n        if diff.format not in [\"csr\", \"csc\", \"coo\"]:\n            diff = diff.tocsr()\n        symmetric = np.all(abs(diff.data) < tol)\n    else:\n        symmetric = np.allclose(array, array.T, atol=tol)\n\n    if not symmetric:\n        if raise_exception:\n            raise ValueError(\"Array must be symmetric\")\n        if raise_warning:\n            warnings.warn(\n                \"Array is not symmetric, and will be converted \"\n                \"to symmetric by average with its transpose.\",\n                stacklevel=2,\n            )\n        if sp.issparse(array):\n            conversion = \"to\" + array.format\n            array = getattr(0.5 * (array + array.T), conversion)()\n        else:\n            array = 0.5 * (array + array.T)\n\n    return array\n\n\ndef check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a NotFittedError with the given message.\n\n    This utility is meant to be used internally by estimators themselves,\n    typically in their own predict / transform methods.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    NotFittedError\n        If the attributes are not found.\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    if attributes is not None:\n        if not isinstance(attributes, (list, tuple)):\n            attributes = [attributes]\n        attrs = all_or_any([hasattr(estimator, attr) for attr in attributes])\n    else:\n        attrs = [\n            v for v in vars(estimator) if v.endswith(\"_\") and not v.startswith(\"__\")\n        ]\n\n    if not attrs:\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})\n\n\ndef check_non_negative(X, whom):\n    \"\"\"\n    Check if there is any negative value in an array.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Input data.\n\n    whom : str\n        Who passed X to this function.\n    \"\"\"\n    # avoid X.min() on sparse matrix since it also sorts the indices\n    if sp.issparse(X):\n        if X.format in [\"lil\", \"dok\"]:\n            X = X.tocsr()\n        if X.data.size == 0:\n            X_min = 0\n        else:\n            X_min = X.data.min()\n    else:\n        X_min = X.min()\n\n    if X_min < 0:\n        raise ValueError(\"Negative values in data passed to %s\" % whom)\n\n\ndef check_scalar(x, name, target_type, *, min_val=None, max_val=None):\n    \"\"\"Validate scalar parameters type and value.\n\n    Parameters\n    ----------\n    x : object\n        The scalar parameter to validate.\n\n    name : str\n        The name of the parameter to be printed in error messages.\n\n    target_type : type or tuple\n        Acceptable data types for the parameter.\n\n    min_val : float or int, default=None\n        The minimum valid value the parameter can take. If None (default) it\n        is implied that the parameter does not have a lower bound.\n\n    max_val : float or int, default=None\n        The maximum valid value the parameter can take. If None (default) it\n        is implied that the parameter does not have an upper bound.\n\n    Raises\n    -------\n    TypeError\n        If the parameter's type does not match the desired type.\n\n    ValueError\n        If the parameter's value violates the given bounds.\n    \"\"\"\n\n    if not isinstance(x, target_type):\n        raise TypeError(\n            \"`{}` must be an instance of {}, not {}.\".format(name, target_type, type(x))\n        )\n\n    if min_val is not None and x < min_val:\n        raise ValueError(\"`{}`= {}, must be >= {}.\".format(name, x, min_val))\n\n    if max_val is not None and x > max_val:\n        raise ValueError(\"`{}`= {}, must be <= {}.\".format(name, x, max_val))\n\n\ndef _check_psd_eigenvalues(lambdas, enable_warnings=False):\n    \"\"\"Check the eigenvalues of a positive semidefinite (PSD) matrix.\n\n    Checks the provided array of PSD matrix eigenvalues for numerical or\n    conditioning issues and returns a fixed validated version. This method\n    should typically be used if the PSD matrix is user-provided (e.g. a\n    Gram matrix) or computed using a user-provided dissimilarity metric\n    (e.g. kernel function), or if the decomposition process uses approximation\n    methods (randomized SVD, etc.).\n\n    It checks for three things:\n\n    - that there are no significant imaginary parts in eigenvalues (more than\n      1e-5 times the maximum real part). If this check fails, it raises a\n      ``ValueError``. Otherwise all non-significant imaginary parts that may\n      remain are set to zero. This operation is traced with a\n      ``PositiveSpectrumWarning`` when ``enable_warnings=True``.\n\n    - that eigenvalues are not all negative. If this check fails, it raises a\n      ``ValueError``\n\n    - that there are no significant negative eigenvalues with absolute value\n      more than 1e-10 (1e-6) and more than 1e-5 (5e-3) times the largest\n      positive eigenvalue in double (simple) precision. If this check fails,\n      it raises a ``ValueError``. Otherwise all negative eigenvalues that may\n      remain are set to zero. This operation is traced with a\n      ``PositiveSpectrumWarning`` when ``enable_warnings=True``.\n\n    Finally, all the positive eigenvalues that are too small (with a value\n    smaller than the maximum eigenvalue multiplied by 1e-12 (2e-7)) are set to\n    zero. This operation is traced with a ``PositiveSpectrumWarning`` when\n    ``enable_warnings=True``.\n\n    Parameters\n    ----------\n    lambdas : array-like of shape (n_eigenvalues,)\n        Array of eigenvalues to check / fix.\n\n    enable_warnings : bool, default=False\n        When this is set to ``True``, a ``PositiveSpectrumWarning`` will be\n        raised when there are imaginary parts, negative eigenvalues, or\n        extremely small non-zero eigenvalues. Otherwise no warning will be\n        raised. In both cases, imaginary parts, negative eigenvalues, and\n        extremely small non-zero eigenvalues will be set to zero.\n\n    Returns\n    -------\n    lambdas_fixed : ndarray of shape (n_eigenvalues,)\n        A fixed validated copy of the array of eigenvalues.\n\n    Examples\n    --------\n    >>> _check_psd_eigenvalues([1, 2])      # nominal case\n    array([1, 2])\n    >>> _check_psd_eigenvalues([5, 5j])     # significant imag part\n    Traceback (most recent call last):\n        ...\n    ValueError: There are significant imaginary parts in eigenvalues (1\n        of the maximum real part). Either the matrix is not PSD, or there was\n        an issue while computing the eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, 5e-5j])  # insignificant imag part\n    array([5., 0.])\n    >>> _check_psd_eigenvalues([-5, -1])    # all negative\n    Traceback (most recent call last):\n        ...\n    ValueError: All eigenvalues are negative (maximum is -1). Either the\n        matrix is not PSD, or there was an issue while computing the\n        eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, -1])     # significant negative\n    Traceback (most recent call last):\n        ...\n    ValueError: There are significant negative eigenvalues (0.2 of the\n        maximum positive). Either the matrix is not PSD, or there was an issue\n        while computing the eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, -5e-5])  # insignificant negative\n    array([5., 0.])\n    >>> _check_psd_eigenvalues([5, 4e-12])  # bad conditioning (too small)\n    array([5., 0.])\n\n    \"\"\"\n\n    lambdas = np.array(lambdas)\n    is_double_precision = lambdas.dtype == np.float64\n\n    # note: the minimum value available is\n    #  - single-precision: np.finfo('float32').eps = 1.2e-07\n    #  - double-precision: np.finfo('float64').eps = 2.2e-16\n\n    # the various thresholds used for validation\n    # we may wish to change the value according to precision.\n    significant_imag_ratio = 1e-5\n    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3\n    significant_neg_value = 1e-10 if is_double_precision else 1e-6\n    small_pos_ratio = 1e-12 if is_double_precision else 2e-7\n\n    # Check that there are no significant imaginary parts\n    if not np.isreal(lambdas).all():\n        max_imag_abs = np.abs(np.imag(lambdas)).max()\n        max_real_abs = np.abs(np.real(lambdas)).max()\n        if max_imag_abs > significant_imag_ratio * max_real_abs:\n            raise ValueError(\n                \"There are significant imaginary parts in eigenvalues (%g \"\n                \"of the maximum real part). Either the matrix is not PSD, or \"\n                \"there was an issue while computing the eigendecomposition \"\n                \"of the matrix.\" % (max_imag_abs / max_real_abs)\n            )\n\n        # warn about imaginary parts being removed\n        if enable_warnings:\n            warnings.warn(\n                \"There are imaginary parts in eigenvalues (%g \"\n                \"of the maximum real part). Either the matrix is not\"\n                \" PSD, or there was an issue while computing the \"\n                \"eigendecomposition of the matrix. Only the real \"\n                \"parts will be kept.\" % (max_imag_abs / max_real_abs),\n                PositiveSpectrumWarning,\n            )\n\n    # Remove all imaginary parts (even if zero)\n    lambdas = np.real(lambdas)\n\n    # Check that there are no significant negative eigenvalues\n    max_eig = lambdas.max()\n    if max_eig < 0:\n        raise ValueError(\n            \"All eigenvalues are negative (maximum is %g). \"\n            \"Either the matrix is not PSD, or there was an \"\n            \"issue while computing the eigendecomposition of \"\n            \"the matrix.\" % max_eig\n        )\n\n    else:\n        min_eig = lambdas.min()\n        if (\n            min_eig < -significant_neg_ratio * max_eig\n            and min_eig < -significant_neg_value\n        ):\n            raise ValueError(\n                \"There are significant negative eigenvalues (%g\"\n                \" of the maximum positive). Either the matrix is \"\n                \"not PSD, or there was an issue while computing \"\n                \"the eigendecomposition of the matrix.\" % (-min_eig / max_eig)\n            )\n        elif min_eig < 0:\n            # Remove all negative values and warn about it\n            if enable_warnings:\n                warnings.warn(\n                    \"There are negative eigenvalues (%g of the \"\n                    \"maximum positive). Either the matrix is not \"\n                    \"PSD, or there was an issue while computing the\"\n                    \" eigendecomposition of the matrix. Negative \"\n                    \"eigenvalues will be replaced with 0.\" % (-min_eig / max_eig),\n                    PositiveSpectrumWarning,\n                )\n            lambdas[lambdas < 0] = 0\n\n    # Check for conditioning (small positive non-zeros)\n    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)\n    if too_small_lambdas.any():\n        if enable_warnings:\n            warnings.warn(\n                \"Badly conditioned PSD matrix spectrum: the largest \"\n                \"eigenvalue is more than %g times the smallest. \"\n                \"Small eigenvalues will be replaced with 0.\"\n                \"\" % (1 / small_pos_ratio),\n                PositiveSpectrumWarning,\n            )\n        lambdas[too_small_lambdas] = 0\n\n    return lambdas\n\n\ndef _check_sample_weight(sample_weight, X, dtype=None, copy=False):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n       Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    dtype : dtype, default=None\n       dtype of the validated `sample_weight`.\n       If None, and the input `sample_weight` is an array, the dtype of the\n       input is preserved; otherwise an array with the default numpy dtype\n       is be allocated.  If `dtype` is not one of `float32`, `float64`,\n       `None`, the output will be of dtype `float64`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n       Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    n_samples = _num_samples(X)\n\n    if dtype is not None and dtype not in [np.float32, np.float64]:\n        dtype = np.float64\n\n    if sample_weight is None:\n        sample_weight = np.ones(n_samples, dtype=dtype)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = np.full(n_samples, sample_weight, dtype=dtype)\n    else:\n        if dtype is None:\n            dtype = [np.float64, np.float32]\n        sample_weight = check_array(\n            sample_weight,\n            accept_sparse=False,\n            ensure_2d=False,\n            dtype=dtype,\n            order=\"C\",\n            copy=copy,\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\"Sample weights must be 1D array or scalar\")\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\n                \"sample_weight.shape == {}, expected {}!\".format(\n                    sample_weight.shape, (n_samples,)\n                )\n            )\n\n    return sample_weight\n\n\ndef _allclose_dense_sparse(x, y, rtol=1e-7, atol=1e-9):\n    \"\"\"Check allclose for sparse and dense data.\n\n    Both x and y need to be either sparse or dense, they\n    can't be mixed.\n\n    Parameters\n    ----------\n    x : {array-like, sparse matrix}\n        First array to compare.\n\n    y : {array-like, sparse matrix}\n        Second array to compare.\n\n    rtol : float, default=1e-7\n        Relative tolerance; see numpy.allclose.\n\n    atol : float, default=1e-9\n        absolute tolerance; see numpy.allclose. Note that the default here is\n        more tolerant than the default for numpy.testing.assert_allclose, where\n        atol=0.\n    \"\"\"\n    if sp.issparse(x) and sp.issparse(y):\n        x = x.tocsr()\n        y = y.tocsr()\n        x.sum_duplicates()\n        y.sum_duplicates()\n        return (\n            np.array_equal(x.indices, y.indices)\n            and np.array_equal(x.indptr, y.indptr)\n            and np.allclose(x.data, y.data, rtol=rtol, atol=atol)\n        )\n    elif not sp.issparse(x) and not sp.issparse(y):\n        return np.allclose(x, y, rtol=rtol, atol=atol)\n    raise ValueError(\n        \"Can only compare two sparse matrices, not a sparse matrix and an array\"\n    )\n\n\ndef _check_fit_params(X, fit_params, indices=None):\n    \"\"\"Check and validate the parameters passed during `fit`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data array.\n\n    fit_params : dict\n        Dictionary containing the parameters passed at fit.\n\n    indices : array-like of shape (n_samples,), default=None\n        Indices to be selected if the parameter has the same size as `X`.\n\n    Returns\n    -------\n    fit_params_validated : dict\n        Validated parameters. We ensure that the values support indexing.\n    \"\"\"\n    from . import _safe_indexing\n\n    fit_params_validated = {}\n    for param_key, param_value in fit_params.items():\n        if not _is_arraylike(param_value) or _num_samples(param_value) != _num_samples(\n            X\n        ):\n            # Non-indexable pass-through (for now for backward-compatibility).\n            # https://github.com/scikit-learn/scikit-learn/issues/15805\n            fit_params_validated[param_key] = param_value\n        else:\n            # Any other fit_params should support indexing\n            # (e.g. for cross-validation).\n            fit_params_validated[param_key] = _make_indexable(param_value)\n            fit_params_validated[param_key] = _safe_indexing(\n                fit_params_validated[param_key], indices\n            )\n\n    return fit_params_validated\n",1589],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py":["\"\"\"Global configuration state and functions for management\n\"\"\"\nimport os\nfrom contextlib import contextmanager as contextmanager\nimport threading\n\n_global_config = {\n    \"assume_finite\": bool(os.environ.get(\"SKLEARN_ASSUME_FINITE\", False)),\n    \"working_memory\": int(os.environ.get(\"SKLEARN_WORKING_MEMORY\", 1024)),\n    \"print_changed_only\": True,\n    \"display\": \"text\",\n}\n_threadlocal = threading.local()\n\n\ndef _get_threadlocal_config():\n    \"\"\"Get a threadlocal **mutable** configuration. If the configuration\n    does not exist, copy the default global configuration.\"\"\"\n    if not hasattr(_threadlocal, \"global_config\"):\n        _threadlocal.global_config = _global_config.copy()\n    return _threadlocal.global_config\n\n\ndef get_config():\n    \"\"\"Retrieve current values for configuration set by :func:`set_config`\n\n    Returns\n    -------\n    config : dict\n        Keys are parameter names that can be passed to :func:`set_config`.\n\n    See Also\n    --------\n    config_context : Context manager for global scikit-learn configuration.\n    set_config : Set global scikit-learn configuration.\n    \"\"\"\n    # Return a copy of the threadlocal configuration so that users will\n    # not be able to modify the configuration with the returned dict.\n    return _get_threadlocal_config().copy()\n\n\ndef set_config(\n    assume_finite=None, working_memory=None, print_changed_only=None, display=None\n):\n    \"\"\"Set global scikit-learn configuration\n\n    .. versionadded:: 0.19\n\n    Parameters\n    ----------\n    assume_finite : bool, default=None\n        If True, validation for finiteness will be skipped,\n        saving time, but leading to potential crashes. If\n        False, validation for finiteness will be performed,\n        avoiding error.  Global default: False.\n\n        .. versionadded:: 0.19\n\n    working_memory : int, default=None\n        If set, scikit-learn will attempt to limit the size of temporary arrays\n        to this number of MiB (per job when parallelised), often saving both\n        computation time and memory on expensive operations that can be\n        performed in chunks. Global default: 1024.\n\n        .. versionadded:: 0.20\n\n    print_changed_only : bool, default=None\n        If True, only the parameters that were set to non-default\n        values will be printed when printing an estimator. For example,\n        ``print(SVC())`` while True will only print 'SVC()' while the default\n        behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n        all the non-changed parameters.\n\n        .. versionadded:: 0.21\n\n    display : {'text', 'diagram'}, default=None\n        If 'diagram', estimators will be displayed as a diagram in a Jupyter\n        lab or notebook context. If 'text', estimators will be displayed as\n        text. Default is 'text'.\n\n        .. versionadded:: 0.23\n\n    See Also\n    --------\n    config_context : Context manager for global scikit-learn configuration.\n    get_config : Retrieve current values of the global configuration.\n    \"\"\"\n    local_config = _get_threadlocal_config()\n\n    if assume_finite is not None:\n        local_config[\"assume_finite\"] = assume_finite\n    if working_memory is not None:\n        local_config[\"working_memory\"] = working_memory\n    if print_changed_only is not None:\n        local_config[\"print_changed_only\"] = print_changed_only\n    if display is not None:\n        local_config[\"display\"] = display\n\n\n@contextmanager\ndef config_context(**new_config):\n    \"\"\"Context manager for global scikit-learn configuration\n\n    Parameters\n    ----------\n    assume_finite : bool, default=False\n        If True, validation for finiteness will be skipped,\n        saving time, but leading to potential crashes. If\n        False, validation for finiteness will be performed,\n        avoiding error.  Global default: False.\n\n    working_memory : int, default=1024\n        If set, scikit-learn will attempt to limit the size of temporary arrays\n        to this number of MiB (per job when parallelised), often saving both\n        computation time and memory on expensive operations that can be\n        performed in chunks. Global default: 1024.\n\n    print_changed_only : bool, default=True\n        If True, only the parameters that were set to non-default\n        values will be printed when printing an estimator. For example,\n        ``print(SVC())`` while True will only print 'SVC()', but would print\n        'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n        when False. Default is True.\n\n        .. versionchanged:: 0.23\n           Default changed from False to True.\n\n    display : {'text', 'diagram'}, default='text'\n        If 'diagram', estimators will be displayed as a diagram in a Jupyter\n        lab or notebook context. If 'text', estimators will be displayed as\n        text. Default is 'text'.\n\n        .. versionadded:: 0.23\n\n    Notes\n    -----\n    All settings, not just those presently modified, will be returned to\n    their previous values when the context manager is exited.\n\n    Examples\n    --------\n    >>> import sklearn\n    >>> from sklearn.utils.validation import assert_all_finite\n    >>> with sklearn.config_context(assume_finite=True):\n    ...     assert_all_finite([float('nan')])\n    >>> with sklearn.config_context(assume_finite=True):\n    ...     with sklearn.config_context(assume_finite=False):\n    ...         assert_all_finite([float('nan')])\n    Traceback (most recent call last):\n    ...\n    ValueError: Input contains NaN, ...\n\n    See Also\n    --------\n    set_config : Set global scikit-learn configuration.\n    get_config : Retrieve current values of the global configuration.\n    \"\"\"\n    old_config = get_config()\n    set_config(**new_config)\n\n    try:\n        yield\n    finally:\n        set_config(**old_config)\n",164],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py":["\"\"\"\nnumerictypes: Define the numeric type objects\n\nThis module is designed so \"from numerictypes import \\\\*\" is safe.\nExported symbols include:\n\n  Dictionary with all registered number types (including aliases):\n    sctypeDict\n\n  Type objects (not all will be available, depends on platform):\n      see variable sctypes for which ones you have\n\n    Bit-width names\n\n    int8 int16 int32 int64 int128\n    uint8 uint16 uint32 uint64 uint128\n    float16 float32 float64 float96 float128 float256\n    complex32 complex64 complex128 complex192 complex256 complex512\n    datetime64 timedelta64\n\n    c-based names\n\n    bool_\n\n    object_\n\n    void, str_, unicode_\n\n    byte, ubyte,\n    short, ushort\n    intc, uintc,\n    intp, uintp,\n    int_, uint,\n    longlong, ulonglong,\n\n    single, csingle,\n    float_, complex_,\n    longfloat, clongfloat,\n\n   As part of the type-hierarchy:    xx -- is bit-width\n\n   generic\n     +-> bool_                                  (kind=b)\n     +-> number\n     |   +-> integer\n     |   |   +-> signedinteger     (intxx)      (kind=i)\n     |   |   |     byte\n     |   |   |     short\n     |   |   |     intc\n     |   |   |     intp            int0\n     |   |   |     int_\n     |   |   |     longlong\n     |   |   \\\\-> unsignedinteger  (uintxx)     (kind=u)\n     |   |         ubyte\n     |   |         ushort\n     |   |         uintc\n     |   |         uintp           uint0\n     |   |         uint_\n     |   |         ulonglong\n     |   +-> inexact\n     |       +-> floating          (floatxx)    (kind=f)\n     |       |     half\n     |       |     single\n     |       |     float_          (double)\n     |       |     longfloat\n     |       \\\\-> complexfloating  (complexxx)  (kind=c)\n     |             csingle         (singlecomplex)\n     |             complex_        (cfloat, cdouble)\n     |             clongfloat      (longcomplex)\n     +-> flexible\n     |   +-> character\n     |   |     str_     (string_, bytes_)       (kind=S)    [Python 2]\n     |   |     unicode_                         (kind=U)    [Python 2]\n     |   |\n     |   |     bytes_   (string_)               (kind=S)    [Python 3]\n     |   |     str_     (unicode_)              (kind=U)    [Python 3]\n     |   |\n     |   \\\\-> void                              (kind=V)\n     \\\\-> object_ (not used much)               (kind=O)\n\n\"\"\"\nimport numbers\nimport warnings\n\nfrom numpy.core.multiarray import (\n        typeinfo, ndarray, array, empty, dtype, datetime_data,\n        datetime_as_string, busday_offset, busday_count, is_busday,\n        busdaycalendar\n        )\nfrom numpy.core.overrides import set_module\n\n# we add more at the bottom\n__all__ = ['sctypeDict', 'sctypes',\n           'ScalarType', 'obj2sctype', 'cast', 'nbytes', 'sctype2char',\n           'maximum_sctype', 'issctype', 'typecodes', 'find_common_type',\n           'issubdtype', 'datetime_data', 'datetime_as_string',\n           'busday_offset', 'busday_count', 'is_busday', 'busdaycalendar',\n           ]\n\n# we don't need all these imports, but we need to keep them for compatibility\n# for users using np.core.numerictypes.UPPER_TABLE\nfrom ._string_helpers import (\n    english_lower, english_upper, english_capitalize, LOWER_TABLE, UPPER_TABLE\n)\n\nfrom ._type_aliases import (\n    sctypeDict,\n    allTypes,\n    bitname,\n    sctypes,\n    _concrete_types,\n    _concrete_typeinfo,\n    _bits_of,\n)\nfrom ._dtype import _kind_name\n\n# we don't export these for import *, but we do want them accessible\n# as numerictypes.bool, etc.\nfrom builtins import bool, int, float, complex, object, str, bytes\nfrom numpy.compat import long, unicode\n\n\n# We use this later\ngeneric = allTypes['generic']\n\ngenericTypeRank = ['bool', 'int8', 'uint8', 'int16', 'uint16',\n                   'int32', 'uint32', 'int64', 'uint64', 'int128',\n                   'uint128', 'float16',\n                   'float32', 'float64', 'float80', 'float96', 'float128',\n                   'float256',\n                   'complex32', 'complex64', 'complex128', 'complex160',\n                   'complex192', 'complex256', 'complex512', 'object']\n\n@set_module('numpy')\ndef maximum_sctype(t):\n    \"\"\"\n    Return the scalar type of highest precision of the same kind as the input.\n\n    Parameters\n    ----------\n    t : dtype or dtype specifier\n        The input data type. This can be a `dtype` object or an object that\n        is convertible to a `dtype`.\n\n    Returns\n    -------\n    out : dtype\n        The highest precision data type of the same kind (`dtype.kind`) as `t`.\n\n    See Also\n    --------\n    obj2sctype, mintypecode, sctype2char\n    dtype\n\n    Examples\n    --------\n    >>> np.maximum_sctype(int)\n    <class 'numpy.int64'>\n    >>> np.maximum_sctype(np.uint8)\n    <class 'numpy.uint64'>\n    >>> np.maximum_sctype(complex)\n    <class 'numpy.complex256'> # may vary\n\n    >>> np.maximum_sctype(str)\n    <class 'numpy.str_'>\n\n    >>> np.maximum_sctype('i2')\n    <class 'numpy.int64'>\n    >>> np.maximum_sctype('f4')\n    <class 'numpy.float128'> # may vary\n\n    \"\"\"\n    g = obj2sctype(t)\n    if g is None:\n        return t\n    t = g\n    base = _kind_name(dtype(t))\n    if base in sctypes:\n        return sctypes[base][-1]\n    else:\n        return t\n\n\n@set_module('numpy')\ndef issctype(rep):\n    \"\"\"\n    Determines whether the given object represents a scalar data-type.\n\n    Parameters\n    ----------\n    rep : any\n        If `rep` is an instance of a scalar dtype, True is returned. If not,\n        False is returned.\n\n    Returns\n    -------\n    out : bool\n        Boolean result of check whether `rep` is a scalar dtype.\n\n    See Also\n    --------\n    issubsctype, issubdtype, obj2sctype, sctype2char\n\n    Examples\n    --------\n    >>> np.issctype(np.int32)\n    True\n    >>> np.issctype(list)\n    False\n    >>> np.issctype(1.1)\n    False\n\n    Strings are also a scalar type:\n\n    >>> np.issctype(np.dtype('str'))\n    True\n\n    \"\"\"\n    if not isinstance(rep, (type, dtype)):\n        return False\n    try:\n        res = obj2sctype(rep)\n        if res and res != object_:\n            return True\n        return False\n    except Exception:\n        return False\n\n\n@set_module('numpy')\ndef obj2sctype(rep, default=None):\n    \"\"\"\n    Return the scalar dtype or NumPy equivalent of Python type of an object.\n\n    Parameters\n    ----------\n    rep : any\n        The object of which the type is returned.\n    default : any, optional\n        If given, this is returned for objects whose types can not be\n        determined. If not given, None is returned for those objects.\n\n    Returns\n    -------\n    dtype : dtype or Python type\n        The data type of `rep`.\n\n    See Also\n    --------\n    sctype2char, issctype, issubsctype, issubdtype, maximum_sctype\n\n    Examples\n    --------\n    >>> np.obj2sctype(np.int32)\n    <class 'numpy.int32'>\n    >>> np.obj2sctype(np.array([1., 2.]))\n    <class 'numpy.float64'>\n    >>> np.obj2sctype(np.array([1.j]))\n    <class 'numpy.complex128'>\n\n    >>> np.obj2sctype(dict)\n    <class 'numpy.object_'>\n    >>> np.obj2sctype('string')\n\n    >>> np.obj2sctype(1, default=list)\n    <class 'list'>\n\n    \"\"\"\n    # prevent abstract classes being upcast\n    if isinstance(rep, type) and issubclass(rep, generic):\n        return rep\n    # extract dtype from arrays\n    if isinstance(rep, ndarray):\n        return rep.dtype.type\n    # fall back on dtype to convert\n    try:\n        res = dtype(rep)\n    except Exception:\n        return default\n    else:\n        return res.type\n\n\n@set_module('numpy')\ndef issubclass_(arg1, arg2):\n    \"\"\"\n    Determine if a class is a subclass of a second class.\n\n    `issubclass_` is equivalent to the Python built-in ``issubclass``,\n    except that it returns False instead of raising a TypeError if one\n    of the arguments is not a class.\n\n    Parameters\n    ----------\n    arg1 : class\n        Input class. True is returned if `arg1` is a subclass of `arg2`.\n    arg2 : class or tuple of classes.\n        Input class. If a tuple of classes, True is returned if `arg1` is a\n        subclass of any of the tuple elements.\n\n    Returns\n    -------\n    out : bool\n        Whether `arg1` is a subclass of `arg2` or not.\n\n    See Also\n    --------\n    issubsctype, issubdtype, issctype\n\n    Examples\n    --------\n    >>> np.issubclass_(np.int32, int)\n    False\n    >>> np.issubclass_(np.int32, float)\n    False\n    >>> np.issubclass_(np.float64, float)\n    True\n\n    \"\"\"\n    try:\n        return issubclass(arg1, arg2)\n    except TypeError:\n        return False\n\n\n@set_module('numpy')\ndef issubsctype(arg1, arg2):\n    \"\"\"\n    Determine if the first argument is a subclass of the second argument.\n\n    Parameters\n    ----------\n    arg1, arg2 : dtype or dtype specifier\n        Data-types.\n\n    Returns\n    -------\n    out : bool\n        The result.\n\n    See Also\n    --------\n    issctype, issubdtype, obj2sctype\n\n    Examples\n    --------\n    >>> np.issubsctype('S8', str)\n    False\n    >>> np.issubsctype(np.array([1]), int)\n    True\n    >>> np.issubsctype(np.array([1]), float)\n    False\n\n    \"\"\"\n    return issubclass(obj2sctype(arg1), obj2sctype(arg2))\n\n\n@set_module('numpy')\ndef issubdtype(arg1, arg2):\n    r\"\"\"\n    Returns True if first argument is a typecode lower/equal in type hierarchy.\n\n    This is like the builtin :func:`issubclass`, but for `dtype`\\ s.\n\n    Parameters\n    ----------\n    arg1, arg2 : dtype_like\n        `dtype` or object coercible to one\n\n    Returns\n    -------\n    out : bool\n\n    See Also\n    --------\n    :ref:`arrays.scalars` : Overview of the numpy type hierarchy.\n    issubsctype, issubclass_\n\n    Examples\n    --------\n    `issubdtype` can be used to check the type of arrays:\n\n    >>> ints = np.array([1, 2, 3], dtype=np.int32)\n    >>> np.issubdtype(ints.dtype, np.integer)\n    True\n    >>> np.issubdtype(ints.dtype, np.floating)\n    False\n\n    >>> floats = np.array([1, 2, 3], dtype=np.float32)\n    >>> np.issubdtype(floats.dtype, np.integer)\n    False\n    >>> np.issubdtype(floats.dtype, np.floating)\n    True\n\n    Similar types of different sizes are not subdtypes of each other:\n\n    >>> np.issubdtype(np.float64, np.float32)\n    False\n    >>> np.issubdtype(np.float32, np.float64)\n    False\n\n    but both are subtypes of `floating`:\n\n    >>> np.issubdtype(np.float64, np.floating)\n    True\n    >>> np.issubdtype(np.float32, np.floating)\n    True\n\n    For convenience, dtype-like objects are allowed too:\n\n    >>> np.issubdtype('S1', np.string_)\n    True\n    >>> np.issubdtype('i4', np.signedinteger)\n    True\n\n    \"\"\"\n    if not issubclass_(arg1, generic):\n        arg1 = dtype(arg1).type\n    if not issubclass_(arg2, generic):\n        arg2 = dtype(arg2).type\n\n    return issubclass(arg1, arg2)\n\n\n# This dictionary allows look up based on any alias for an array data-type\nclass _typedict(dict):\n    \"\"\"\n    Base object for a dictionary for look-up with any alias for an array dtype.\n\n    Instances of `_typedict` can not be used as dictionaries directly,\n    first they have to be populated.\n\n    \"\"\"\n\n    def __getitem__(self, obj):\n        return dict.__getitem__(self, obj2sctype(obj))\n\nnbytes = _typedict()\n_alignment = _typedict()\n_maxvals = _typedict()\n_minvals = _typedict()\ndef _construct_lookups():\n    for name, info in _concrete_typeinfo.items():\n        obj = info.type\n        nbytes[obj] = info.bits // 8\n        _alignment[obj] = info.alignment\n        if len(info) > 5:\n            _maxvals[obj] = info.max\n            _minvals[obj] = info.min\n        else:\n            _maxvals[obj] = None\n            _minvals[obj] = None\n\n_construct_lookups()\n\n\n@set_module('numpy')\ndef sctype2char(sctype):\n    \"\"\"\n    Return the string representation of a scalar dtype.\n\n    Parameters\n    ----------\n    sctype : scalar dtype or object\n        If a scalar dtype, the corresponding string character is\n        returned. If an object, `sctype2char` tries to infer its scalar type\n        and then return the corresponding string character.\n\n    Returns\n    -------\n    typechar : str\n        The string character corresponding to the scalar type.\n\n    Raises\n    ------\n    ValueError\n        If `sctype` is an object for which the type can not be inferred.\n\n    See Also\n    --------\n    obj2sctype, issctype, issubsctype, mintypecode\n\n    Examples\n    --------\n    >>> for sctype in [np.int32, np.double, np.complex_, np.string_, np.ndarray]:\n    ...     print(np.sctype2char(sctype))\n    l # may vary\n    d\n    D\n    S\n    O\n\n    >>> x = np.array([1., 2-1.j])\n    >>> np.sctype2char(x)\n    'D'\n    >>> np.sctype2char(list)\n    'O'\n\n    \"\"\"\n    sctype = obj2sctype(sctype)\n    if sctype is None:\n        raise ValueError(\"unrecognized type\")\n    if sctype not in _concrete_types:\n        # for compatibility\n        raise KeyError(sctype)\n    return dtype(sctype).char\n\n# Create dictionary of casting functions that wrap sequences\n# indexed by type or type character\ncast = _typedict()\nfor key in _concrete_types:\n    cast[key] = lambda x, k=key: array(x, copy=False).astype(k)\n\n\ndef _scalar_type_key(typ):\n    \"\"\"A ``key`` function for `sorted`.\"\"\"\n    dt = dtype(typ)\n    return (dt.kind.lower(), dt.itemsize)\n\n\nScalarType = [int, float, complex, int, bool, bytes, str, memoryview]\nScalarType += sorted(_concrete_types, key=_scalar_type_key)\nScalarType = tuple(ScalarType)\n\n\n# Now add the types we've determined to this module\nfor key in allTypes:\n    globals()[key] = allTypes[key]\n    __all__.append(key)\n\ndel key\n\ntypecodes = {'Character':'c',\n             'Integer':'bhilqp',\n             'UnsignedInteger':'BHILQP',\n             'Float':'efdg',\n             'Complex':'FDG',\n             'AllInteger':'bBhHiIlLqQpP',\n             'AllFloat':'efdgFDG',\n             'Datetime': 'Mm',\n             'All':'?bhilqpBHILQPefdgFDGSUVOMm'}\n\n# backwards compatibility --- deprecated name\n# Formal deprecation: Numpy 1.20.0, 2020-10-19 (see numpy/__init__.py)\ntypeDict = sctypeDict\n\n# b -> boolean\n# u -> unsigned integer\n# i -> signed integer\n# f -> floating point\n# c -> complex\n# M -> datetime\n# m -> timedelta\n# S -> string\n# U -> Unicode string\n# V -> record\n# O -> Python object\n_kind_list = ['b', 'u', 'i', 'f', 'c', 'S', 'U', 'V', 'O', 'M', 'm']\n\n__test_types = '?'+typecodes['AllInteger'][:-2]+typecodes['AllFloat']+'O'\n__len_test_types = len(__test_types)\n\n# Keep incrementing until a common type both can be coerced to\n#  is found.  Otherwise, return None\ndef _find_common_coerce(a, b):\n    if a > b:\n        return a\n    try:\n        thisind = __test_types.index(a.char)\n    except ValueError:\n        return None\n    return _can_coerce_all([a, b], start=thisind)\n\n# Find a data-type that all data-types in a list can be coerced to\ndef _can_coerce_all(dtypelist, start=0):\n    N = len(dtypelist)\n    if N == 0:\n        return None\n    if N == 1:\n        return dtypelist[0]\n    thisind = start\n    while thisind < __len_test_types:\n        newdtype = dtype(__test_types[thisind])\n        numcoerce = len([x for x in dtypelist if newdtype >= x])\n        if numcoerce == N:\n            return newdtype\n        thisind += 1\n    return None\n\ndef _register_types():\n    numbers.Integral.register(integer)\n    numbers.Complex.register(inexact)\n    numbers.Real.register(floating)\n    numbers.Number.register(number)\n\n_register_types()\n\n\n@set_module('numpy')\ndef find_common_type(array_types, scalar_types):\n    \"\"\"\n    Determine common type following standard coercion rules.\n\n    Parameters\n    ----------\n    array_types : sequence\n        A list of dtypes or dtype convertible objects representing arrays.\n    scalar_types : sequence\n        A list of dtypes or dtype convertible objects representing scalars.\n\n    Returns\n    -------\n    datatype : dtype\n        The common data type, which is the maximum of `array_types` ignoring\n        `scalar_types`, unless the maximum of `scalar_types` is of a\n        different kind (`dtype.kind`). If the kind is not understood, then\n        None is returned.\n\n    See Also\n    --------\n    dtype, common_type, can_cast, mintypecode\n\n    Examples\n    --------\n    >>> np.find_common_type([], [np.int64, np.float32, complex])\n    dtype('complex128')\n    >>> np.find_common_type([np.int64, np.float32], [])\n    dtype('float64')\n\n    The standard casting rules ensure that a scalar cannot up-cast an\n    array unless the scalar is of a fundamentally different kind of data\n    (i.e. under a different hierarchy in the data type hierarchy) then\n    the array:\n\n    >>> np.find_common_type([np.float32], [np.int64, np.float64])\n    dtype('float32')\n\n    Complex is of a different type, so it up-casts the float in the\n    `array_types` argument:\n\n    >>> np.find_common_type([np.float32], [complex])\n    dtype('complex128')\n\n    Type specifier strings are convertible to dtypes and can therefore\n    be used instead of dtypes:\n\n    >>> np.find_common_type(['f4', 'f4', 'i4'], ['c8'])\n    dtype('complex128')\n\n    \"\"\"\n    array_types = [dtype(x) for x in array_types]\n    scalar_types = [dtype(x) for x in scalar_types]\n\n    maxa = _can_coerce_all(array_types)\n    maxsc = _can_coerce_all(scalar_types)\n\n    if maxa is None:\n        return maxsc\n\n    if maxsc is None:\n        return maxa\n\n    try:\n        index_a = _kind_list.index(maxa.kind)\n        index_sc = _kind_list.index(maxsc.kind)\n    except ValueError:\n        return None\n\n    if index_sc > index_a:\n        return _find_common_coerce(maxsc, maxa)\n    else:\n        return maxa\n",672],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py":["\"\"\"Module containing non-deprecated functions borrowed from Numeric.\n\n\"\"\"\nimport functools\nimport types\nimport warnings\n\nimport numpy as np\nfrom . import multiarray as mu\nfrom . import overrides\nfrom . import umath as um\nfrom . import numerictypes as nt\nfrom .multiarray import asarray, array, asanyarray, concatenate\nfrom . import _methods\n\n_dt_ = nt.sctype2char\n\n# functions that are methods\n__all__ = [\n    'alen', 'all', 'alltrue', 'amax', 'amin', 'any', 'argmax',\n    'argmin', 'argpartition', 'argsort', 'around', 'choose', 'clip',\n    'compress', 'cumprod', 'cumproduct', 'cumsum', 'diagonal', 'mean',\n    'ndim', 'nonzero', 'partition', 'prod', 'product', 'ptp', 'put',\n    'ravel', 'repeat', 'reshape', 'resize', 'round_',\n    'searchsorted', 'shape', 'size', 'sometrue', 'sort', 'squeeze',\n    'std', 'sum', 'swapaxes', 'take', 'trace', 'transpose', 'var',\n]\n\n_gentype = types.GeneratorType\n# save away Python sum\n_sum_ = sum\n\narray_function_dispatch = functools.partial(\n    overrides.array_function_dispatch, module='numpy')\n\n\n# functions that are now methods\ndef _wrapit(obj, method, *args, **kwds):\n    try:\n        wrap = obj.__array_wrap__\n    except AttributeError:\n        wrap = None\n    result = getattr(asarray(obj), method)(*args, **kwds)\n    if wrap:\n        if not isinstance(result, mu.ndarray):\n            result = asarray(result)\n        result = wrap(result)\n    return result\n\n\ndef _wrapfunc(obj, method, *args, **kwds):\n    bound = getattr(obj, method, None)\n    if bound is None:\n        return _wrapit(obj, method, *args, **kwds)\n\n    try:\n        return bound(*args, **kwds)\n    except TypeError:\n        # A TypeError occurs if the object does have such a method in its\n        # class, but its signature is not identical to that of NumPy's. This\n        # situation has occurred in the case of a downstream library like\n        # 'pandas'.\n        #\n        # Call _wrapit from within the except clause to ensure a potential\n        # exception has a traceback chain.\n        return _wrapit(obj, method, *args, **kwds)\n\n\ndef _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs):\n    passkwargs = {k: v for k, v in kwargs.items()\n                  if v is not np._NoValue}\n\n    if type(obj) is not mu.ndarray:\n        try:\n            reduction = getattr(obj, method)\n        except AttributeError:\n            pass\n        else:\n            # This branch is needed for reductions like any which don't\n            # support a dtype.\n            if dtype is not None:\n                return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\n            else:\n                return reduction(axis=axis, out=out, **passkwargs)\n\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n\n\ndef _take_dispatcher(a, indices, axis=None, out=None, mode=None):\n    return (a, out)\n\n\n@array_function_dispatch(_take_dispatcher)\ndef take(a, indices, axis=None, out=None, mode='raise'):\n    \"\"\"\n    Take elements from an array along an axis.\n\n    When axis is not None, this function does the same thing as \"fancy\"\n    indexing (indexing arrays using arrays); however, it can be easier to use\n    if you need elements along a given axis. A call such as\n    ``np.take(arr, indices, axis=3)`` is equivalent to\n    ``arr[:,:,:,indices,...]``.\n\n    Explained without fancy indexing, this is equivalent to the following use\n    of `ndindex`, which sets each of ``ii``, ``jj``, and ``kk`` to a tuple of\n    indices::\n\n        Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n        Nj = indices.shape\n        for ii in ndindex(Ni):\n            for jj in ndindex(Nj):\n                for kk in ndindex(Nk):\n                    out[ii + jj + kk] = a[ii + (indices[jj],) + kk]\n\n    Parameters\n    ----------\n    a : array_like (Ni..., M, Nk...)\n        The source array.\n    indices : array_like (Nj...)\n        The indices of the values to extract.\n\n        .. versionadded:: 1.8.0\n\n        Also allow scalars for indices.\n    axis : int, optional\n        The axis over which to select values. By default, the flattened\n        input array is used.\n    out : ndarray, optional (Ni..., Nj..., Nk...)\n        If provided, the result will be placed in this array. It should\n        be of the appropriate shape and dtype. Note that `out` is always\n        buffered if `mode='raise'`; use other modes for better performance.\n    mode : {'raise', 'wrap', 'clip'}, optional\n        Specifies how out-of-bounds indices will behave.\n\n        * 'raise' -- raise an error (default)\n        * 'wrap' -- wrap around\n        * 'clip' -- clip to the range\n\n        'clip' mode means that all indices that are too large are replaced\n        by the index that addresses the last element along that axis. Note\n        that this disables indexing with negative numbers.\n\n    Returns\n    -------\n    out : ndarray (Ni..., Nj..., Nk...)\n        The returned array has the same type as `a`.\n\n    See Also\n    --------\n    compress : Take elements using a boolean mask\n    ndarray.take : equivalent method\n    take_along_axis : Take elements by matching the array and the index arrays\n\n    Notes\n    -----\n\n    By eliminating the inner loop in the description above, and using `s_` to\n    build simple slice objects, `take` can be expressed  in terms of applying\n    fancy indexing to each 1-d slice::\n\n        Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n        for ii in ndindex(Ni):\n            for kk in ndindex(Nj):\n                out[ii + s_[...,] + kk] = a[ii + s_[:,] + kk][indices]\n\n    For this reason, it is equivalent to (but faster than) the following use\n    of `apply_along_axis`::\n\n        out = np.apply_along_axis(lambda a_1d: a_1d[indices], axis, a)\n\n    Examples\n    --------\n    >>> a = [4, 3, 5, 7, 6, 8]\n    >>> indices = [0, 1, 4]\n    >>> np.take(a, indices)\n    array([4, 3, 6])\n\n    In this example if `a` is an ndarray, \"fancy\" indexing can be used.\n\n    >>> a = np.array(a)\n    >>> a[indices]\n    array([4, 3, 6])\n\n    If `indices` is not one dimensional, the output also has these dimensions.\n\n    >>> np.take(a, [[0, 1], [2, 3]])\n    array([[4, 3],\n           [5, 7]])\n    \"\"\"\n    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)\n\n\ndef _reshape_dispatcher(a, newshape, order=None):\n    return (a,)\n\n\n# not deprecated --- copy if necessary, view otherwise\n@array_function_dispatch(_reshape_dispatcher)\ndef reshape(a, newshape, order='C'):\n    \"\"\"\n    Gives a new shape to an array without changing its data.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be reshaped.\n    newshape : int or tuple of ints\n        The new shape should be compatible with the original shape. If\n        an integer, then the result will be a 1-D array of that length.\n        One shape dimension can be -1. In this case, the value is\n        inferred from the length of the array and remaining dimensions.\n    order : {'C', 'F', 'A'}, optional\n        Read the elements of `a` using this index order, and place the\n        elements into the reshaped array using this index order.  'C'\n        means to read / write the elements using C-like index order,\n        with the last axis index changing fastest, back to the first\n        axis index changing slowest. 'F' means to read / write the\n        elements using Fortran-like index order, with the first index\n        changing fastest, and the last index changing slowest. Note that\n        the 'C' and 'F' options take no account of the memory layout of\n        the underlying array, and only refer to the order of indexing.\n        'A' means to read / write the elements in Fortran-like index\n        order if `a` is Fortran *contiguous* in memory, C-like order\n        otherwise.\n\n    Returns\n    -------\n    reshaped_array : ndarray\n        This will be a new view object if possible; otherwise, it will\n        be a copy.  Note there is no guarantee of the *memory layout* (C- or\n        Fortran- contiguous) of the returned array.\n\n    See Also\n    --------\n    ndarray.reshape : Equivalent method.\n\n    Notes\n    -----\n    It is not always possible to change the shape of an array without\n    copying the data. If you want an error to be raised when the data is copied,\n    you should assign the new shape to the shape attribute of the array::\n\n     >>> a = np.zeros((10, 2))\n\n     # A transpose makes the array non-contiguous\n     >>> b = a.T\n\n     # Taking a view makes it possible to modify the shape without modifying\n     # the initial object.\n     >>> c = b.view()\n     >>> c.shape = (20)\n     Traceback (most recent call last):\n        ...\n     AttributeError: Incompatible shape for in-place modification. Use\n     `.reshape()` to make a copy with the desired shape.\n\n    The `order` keyword gives the index ordering both for *fetching* the values\n    from `a`, and then *placing* the values into the output array.\n    For example, let's say you have an array:\n\n    >>> a = np.arange(6).reshape((3, 2))\n    >>> a\n    array([[0, 1],\n           [2, 3],\n           [4, 5]])\n\n    You can think of reshaping as first raveling the array (using the given\n    index order), then inserting the elements from the raveled array into the\n    new array using the same kind of index ordering as was used for the\n    raveling.\n\n    >>> np.reshape(a, (2, 3)) # C-like index ordering\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.reshape(np.ravel(a), (2, 3)) # equivalent to C ravel then C reshape\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.reshape(a, (2, 3), order='F') # Fortran-like index ordering\n    array([[0, 4, 3],\n           [2, 1, 5]])\n    >>> np.reshape(np.ravel(a, order='F'), (2, 3), order='F')\n    array([[0, 4, 3],\n           [2, 1, 5]])\n\n    Examples\n    --------\n    >>> a = np.array([[1,2,3], [4,5,6]])\n    >>> np.reshape(a, 6)\n    array([1, 2, 3, 4, 5, 6])\n    >>> np.reshape(a, 6, order='F')\n    array([1, 4, 2, 5, 3, 6])\n\n    >>> np.reshape(a, (3,-1))       # the unspecified value is inferred to be 2\n    array([[1, 2],\n           [3, 4],\n           [5, 6]])\n    \"\"\"\n    return _wrapfunc(a, 'reshape', newshape, order=order)\n\n\ndef _choose_dispatcher(a, choices, out=None, mode=None):\n    yield a\n    yield from choices\n    yield out\n\n\n@array_function_dispatch(_choose_dispatcher)\ndef choose(a, choices, out=None, mode='raise'):\n    \"\"\"\n    Construct an array from an index array and a list of arrays to choose from.\n\n    First of all, if confused or uncertain, definitely look at the Examples -\n    in its full generality, this function is less simple than it might\n    seem from the following code description (below ndi =\n    `numpy.lib.index_tricks`):\n\n    ``np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)])``.\n\n    But this omits some subtleties.  Here is a fully general summary:\n\n    Given an \"index\" array (`a`) of integers and a sequence of ``n`` arrays\n    (`choices`), `a` and each choice array are first broadcast, as necessary,\n    to arrays of a common shape; calling these *Ba* and *Bchoices[i], i =\n    0,...,n-1* we have that, necessarily, ``Ba.shape == Bchoices[i].shape``\n    for each ``i``.  Then, a new array with shape ``Ba.shape`` is created as\n    follows:\n\n    * if ``mode='raise'`` (the default), then, first of all, each element of\n      ``a`` (and thus ``Ba``) must be in the range ``[0, n-1]``; now, suppose\n      that ``i`` (in that range) is the value at the ``(j0, j1, ..., jm)``\n      position in ``Ba`` - then the value at the same position in the new array\n      is the value in ``Bchoices[i]`` at that same position;\n\n    * if ``mode='wrap'``, values in `a` (and thus `Ba`) may be any (signed)\n      integer; modular arithmetic is used to map integers outside the range\n      `[0, n-1]` back into that range; and then the new array is constructed\n      as above;\n\n    * if ``mode='clip'``, values in `a` (and thus ``Ba``) may be any (signed)\n      integer; negative integers are mapped to 0; values greater than ``n-1``\n      are mapped to ``n-1``; and then the new array is constructed as above.\n\n    Parameters\n    ----------\n    a : int array\n        This array must contain integers in ``[0, n-1]``, where ``n`` is the\n        number of choices, unless ``mode=wrap`` or ``mode=clip``, in which\n        cases any integers are permissible.\n    choices : sequence of arrays\n        Choice arrays. `a` and all of the choices must be broadcastable to the\n        same shape.  If `choices` is itself an array (not recommended), then\n        its outermost dimension (i.e., the one corresponding to\n        ``choices.shape[0]``) is taken as defining the \"sequence\".\n    out : array, optional\n        If provided, the result will be inserted into this array. It should\n        be of the appropriate shape and dtype. Note that `out` is always\n        buffered if ``mode='raise'``; use other modes for better performance.\n    mode : {'raise' (default), 'wrap', 'clip'}, optional\n        Specifies how indices outside ``[0, n-1]`` will be treated:\n\n          * 'raise' : an exception is raised\n          * 'wrap' : value becomes value mod ``n``\n          * 'clip' : values < 0 are mapped to 0, values > n-1 are mapped to n-1\n\n    Returns\n    -------\n    merged_array : array\n        The merged result.\n\n    Raises\n    ------\n    ValueError: shape mismatch\n        If `a` and each choice array are not all broadcastable to the same\n        shape.\n\n    See Also\n    --------\n    ndarray.choose : equivalent method\n    numpy.take_along_axis : Preferable if `choices` is an array\n\n    Notes\n    -----\n    To reduce the chance of misinterpretation, even though the following\n    \"abuse\" is nominally supported, `choices` should neither be, nor be\n    thought of as, a single array, i.e., the outermost sequence-like container\n    should be either a list or a tuple.\n\n    Examples\n    --------\n\n    >>> choices = [[0, 1, 2, 3], [10, 11, 12, 13],\n    ...   [20, 21, 22, 23], [30, 31, 32, 33]]\n    >>> np.choose([2, 3, 1, 0], choices\n    ... # the first element of the result will be the first element of the\n    ... # third (2+1) \"array\" in choices, namely, 20; the second element\n    ... # will be the second element of the fourth (3+1) choice array, i.e.,\n    ... # 31, etc.\n    ... )\n    array([20, 31, 12,  3])\n    >>> np.choose([2, 4, 1, 0], choices, mode='clip') # 4 goes to 3 (4-1)\n    array([20, 31, 12,  3])\n    >>> # because there are 4 choice arrays\n    >>> np.choose([2, 4, 1, 0], choices, mode='wrap') # 4 goes to (4 mod 4)\n    array([20,  1, 12,  3])\n    >>> # i.e., 0\n\n    A couple examples illustrating how choose broadcasts:\n\n    >>> a = [[1, 0, 1], [0, 1, 0], [1, 0, 1]]\n    >>> choices = [-10, 10]\n    >>> np.choose(a, choices)\n    array([[ 10, -10,  10],\n           [-10,  10, -10],\n           [ 10, -10,  10]])\n\n    >>> # With thanks to Anne Archibald\n    >>> a = np.array([0, 1]).reshape((2,1,1))\n    >>> c1 = np.array([1, 2, 3]).reshape((1,3,1))\n    >>> c2 = np.array([-1, -2, -3, -4, -5]).reshape((1,1,5))\n    >>> np.choose(a, (c1, c2)) # result is 2x3x5, res[0,:,:]=c1, res[1,:,:]=c2\n    array([[[ 1,  1,  1,  1,  1],\n            [ 2,  2,  2,  2,  2],\n            [ 3,  3,  3,  3,  3]],\n           [[-1, -2, -3, -4, -5],\n            [-1, -2, -3, -4, -5],\n            [-1, -2, -3, -4, -5]]])\n\n    \"\"\"\n    return _wrapfunc(a, 'choose', choices, out=out, mode=mode)\n\n\ndef _repeat_dispatcher(a, repeats, axis=None):\n    return (a,)\n\n\n@array_function_dispatch(_repeat_dispatcher)\ndef repeat(a, repeats, axis=None):\n    \"\"\"\n    Repeat elements of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    repeats : int or array of ints\n        The number of repetitions for each element.  `repeats` is broadcasted\n        to fit the shape of the given axis.\n    axis : int, optional\n        The axis along which to repeat values.  By default, use the\n        flattened input array, and return a flat output array.\n\n    Returns\n    -------\n    repeated_array : ndarray\n        Output array which has the same shape as `a`, except along\n        the given axis.\n\n    See Also\n    --------\n    tile : Tile an array.\n    unique : Find the unique elements of an array.\n\n    Examples\n    --------\n    >>> np.repeat(3, 4)\n    array([3, 3, 3, 3])\n    >>> x = np.array([[1,2],[3,4]])\n    >>> np.repeat(x, 2)\n    array([1, 1, 2, 2, 3, 3, 4, 4])\n    >>> np.repeat(x, 3, axis=1)\n    array([[1, 1, 1, 2, 2, 2],\n           [3, 3, 3, 4, 4, 4]])\n    >>> np.repeat(x, [1, 2], axis=0)\n    array([[1, 2],\n           [3, 4],\n           [3, 4]])\n\n    \"\"\"\n    return _wrapfunc(a, 'repeat', repeats, axis=axis)\n\n\ndef _put_dispatcher(a, ind, v, mode=None):\n    return (a, ind, v)\n\n\n@array_function_dispatch(_put_dispatcher)\ndef put(a, ind, v, mode='raise'):\n    \"\"\"\n    Replaces specified elements of an array with given values.\n\n    The indexing works on the flattened target array. `put` is roughly\n    equivalent to:\n\n    ::\n\n        a.flat[ind] = v\n\n    Parameters\n    ----------\n    a : ndarray\n        Target array.\n    ind : array_like\n        Target indices, interpreted as integers.\n    v : array_like\n        Values to place in `a` at target indices. If `v` is shorter than\n        `ind` it will be repeated as necessary.\n    mode : {'raise', 'wrap', 'clip'}, optional\n        Specifies how out-of-bounds indices will behave.\n\n        * 'raise' -- raise an error (default)\n        * 'wrap' -- wrap around\n        * 'clip' -- clip to the range\n\n        'clip' mode means that all indices that are too large are replaced\n        by the index that addresses the last element along that axis. Note\n        that this disables indexing with negative numbers. In 'raise' mode,\n        if an exception occurs the target array may still be modified.\n\n    See Also\n    --------\n    putmask, place\n    put_along_axis : Put elements by matching the array and the index arrays\n\n    Examples\n    --------\n    >>> a = np.arange(5)\n    >>> np.put(a, [0, 2], [-44, -55])\n    >>> a\n    array([-44,   1, -55,   3,   4])\n\n    >>> a = np.arange(5)\n    >>> np.put(a, 22, -5, mode='clip')\n    >>> a\n    array([ 0,  1,  2,  3, -5])\n\n    \"\"\"\n    try:\n        put = a.put\n    except AttributeError as e:\n        raise TypeError(\"argument 1 must be numpy.ndarray, \"\n                        \"not {name}\".format(name=type(a).__name__)) from e\n\n    return put(ind, v, mode=mode)\n\n\ndef _swapaxes_dispatcher(a, axis1, axis2):\n    return (a,)\n\n\n@array_function_dispatch(_swapaxes_dispatcher)\ndef swapaxes(a, axis1, axis2):\n    \"\"\"\n    Interchange two axes of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis1 : int\n        First axis.\n    axis2 : int\n        Second axis.\n\n    Returns\n    -------\n    a_swapped : ndarray\n        For NumPy >= 1.10.0, if `a` is an ndarray, then a view of `a` is\n        returned; otherwise a new array is created. For earlier NumPy\n        versions a view of `a` is returned only if the order of the\n        axes is changed, otherwise the input array is returned.\n\n    Examples\n    --------\n    >>> x = np.array([[1,2,3]])\n    >>> np.swapaxes(x,0,1)\n    array([[1],\n           [2],\n           [3]])\n\n    >>> x = np.array([[[0,1],[2,3]],[[4,5],[6,7]]])\n    >>> x\n    array([[[0, 1],\n            [2, 3]],\n           [[4, 5],\n            [6, 7]]])\n\n    >>> np.swapaxes(x,0,2)\n    array([[[0, 4],\n            [2, 6]],\n           [[1, 5],\n            [3, 7]]])\n\n    \"\"\"\n    return _wrapfunc(a, 'swapaxes', axis1, axis2)\n\n\ndef _transpose_dispatcher(a, axes=None):\n    return (a,)\n\n\n@array_function_dispatch(_transpose_dispatcher)\ndef transpose(a, axes=None):\n    \"\"\"\n    Reverse or permute the axes of an array; returns the modified array.\n\n    For an array a with two axes, transpose(a) gives the matrix transpose.\n\n    Refer to `numpy.ndarray.transpose` for full documentation.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axes : tuple or list of ints, optional\n        If specified, it must be a tuple or list which contains a permutation of\n        [0,1,..,N-1] where N is the number of axes of a.  The i'th axis of the\n        returned array will correspond to the axis numbered ``axes[i]`` of the\n        input.  If not specified, defaults to ``range(a.ndim)[::-1]``, which\n        reverses the order of the axes.\n\n    Returns\n    -------\n    p : ndarray\n        `a` with its axes permuted.  A view is returned whenever\n        possible.\n\n    See Also\n    --------\n    ndarray.transpose : Equivalent method\n    moveaxis\n    argsort\n\n    Notes\n    -----\n    Use `transpose(a, argsort(axes))` to invert the transposition of tensors\n    when using the `axes` keyword argument.\n\n    Transposing a 1-D array returns an unchanged view of the original array.\n\n    Examples\n    --------\n    >>> x = np.arange(4).reshape((2,2))\n    >>> x\n    array([[0, 1],\n           [2, 3]])\n\n    >>> np.transpose(x)\n    array([[0, 2],\n           [1, 3]])\n\n    >>> x = np.ones((1, 2, 3))\n    >>> np.transpose(x, (1, 0, 2)).shape\n    (2, 1, 3)\n\n    >>> x = np.ones((2, 3, 4, 5))\n    >>> np.transpose(x).shape\n    (5, 4, 3, 2)\n\n    \"\"\"\n    return _wrapfunc(a, 'transpose', axes)\n\n\ndef _partition_dispatcher(a, kth, axis=None, kind=None, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_partition_dispatcher)\ndef partition(a, kth, axis=-1, kind='introselect', order=None):\n    \"\"\"\n    Return a partitioned copy of an array.\n\n    Creates a copy of the array with its elements rearranged in such a\n    way that the value of the element in k-th position is in the\n    position it would be in a sorted array. All elements smaller than\n    the k-th element are moved before this element and all equal or\n    greater are moved behind it. The ordering of the elements in the two\n    partitions is undefined.\n\n    .. versionadded:: 1.8.0\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be sorted.\n    kth : int or sequence of ints\n        Element index to partition by. The k-th value of the element\n        will be in its final sorted position and all smaller elements\n        will be moved before it and all equal or greater elements behind\n        it. The order of all elements in the partitions is undefined. If\n        provided with a sequence of k-th it will partition all elements\n        indexed by k-th  of them into their sorted position at once.\n    axis : int or None, optional\n        Axis along which to sort. If None, the array is flattened before\n        sorting. The default is -1, which sorts along the last axis.\n    kind : {'introselect'}, optional\n        Selection algorithm. Default is 'introselect'.\n    order : str or list of str, optional\n        When `a` is an array with fields defined, this argument\n        specifies which fields to compare first, second, etc.  A single\n        field can be specified as a string.  Not all fields need be\n        specified, but unspecified fields will still be used, in the\n        order in which they come up in the dtype, to break ties.\n\n    Returns\n    -------\n    partitioned_array : ndarray\n        Array of the same type and shape as `a`.\n\n    See Also\n    --------\n    ndarray.partition : Method to sort an array in-place.\n    argpartition : Indirect partition.\n    sort : Full sorting\n\n    Notes\n    -----\n    The various selection algorithms are characterized by their average\n    speed, worst case performance, work space size, and whether they are\n    stable. A stable sort keeps items with the same key in the same\n    relative order. The available algorithms have the following\n    properties:\n\n    ================= ======= ============= ============ =======\n       kind            speed   worst case    work space  stable\n    ================= ======= ============= ============ =======\n    'introselect'        1        O(n)           0         no\n    ================= ======= ============= ============ =======\n\n    All the partition algorithms make temporary copies of the data when\n    partitioning along any but the last axis.  Consequently,\n    partitioning along the last axis is faster and uses less space than\n    partitioning along any other axis.\n\n    The sort order for complex numbers is lexicographic. If both the\n    real and imaginary parts are non-nan then the order is determined by\n    the real parts except when they are equal, in which case the order\n    is determined by the imaginary parts.\n\n    Examples\n    --------\n    >>> a = np.array([3, 4, 2, 1])\n    >>> np.partition(a, 3)\n    array([2, 1, 3, 4])\n\n    >>> np.partition(a, (1, 3))\n    array([1, 2, 3, 4])\n\n    \"\"\"\n    if axis is None:\n        # flatten returns (1, N) for np.matrix, so always use the last axis\n        a = asanyarray(a).flatten()\n        axis = -1\n    else:\n        a = asanyarray(a).copy(order=\"K\")\n    a.partition(kth, axis=axis, kind=kind, order=order)\n    return a\n\n\ndef _argpartition_dispatcher(a, kth, axis=None, kind=None, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_argpartition_dispatcher)\ndef argpartition(a, kth, axis=-1, kind='introselect', order=None):\n    \"\"\"\n    Perform an indirect partition along the given axis using the\n    algorithm specified by the `kind` keyword. It returns an array of\n    indices of the same shape as `a` that index data along the given\n    axis in partitioned order.\n\n    .. versionadded:: 1.8.0\n\n    Parameters\n    ----------\n    a : array_like\n        Array to sort.\n    kth : int or sequence of ints\n        Element index to partition by. The k-th element will be in its\n        final sorted position and all smaller elements will be moved\n        before it and all larger elements behind it. The order all\n        elements in the partitions is undefined. If provided with a\n        sequence of k-th it will partition all of them into their sorted\n        position at once.\n    axis : int or None, optional\n        Axis along which to sort. The default is -1 (the last axis). If\n        None, the flattened array is used.\n    kind : {'introselect'}, optional\n        Selection algorithm. Default is 'introselect'\n    order : str or list of str, optional\n        When `a` is an array with fields defined, this argument\n        specifies which fields to compare first, second, etc. A single\n        field can be specified as a string, and not all fields need be\n        specified, but unspecified fields will still be used, in the\n        order in which they come up in the dtype, to break ties.\n\n    Returns\n    -------\n    index_array : ndarray, int\n        Array of indices that partition `a` along the specified axis.\n        If `a` is one-dimensional, ``a[index_array]`` yields a partitioned `a`.\n        More generally, ``np.take_along_axis(a, index_array, axis=a)`` always\n        yields the partitioned `a`, irrespective of dimensionality.\n\n    See Also\n    --------\n    partition : Describes partition algorithms used.\n    ndarray.partition : Inplace partition.\n    argsort : Full indirect sort.\n    take_along_axis : Apply ``index_array`` from argpartition\n                      to an array as if by calling partition.\n\n    Notes\n    -----\n    See `partition` for notes on the different selection algorithms.\n\n    Examples\n    --------\n    One dimensional array:\n\n    >>> x = np.array([3, 4, 2, 1])\n    >>> x[np.argpartition(x, 3)]\n    array([2, 1, 3, 4])\n    >>> x[np.argpartition(x, (1, 3))]\n    array([1, 2, 3, 4])\n\n    >>> x = [3, 4, 2, 1]\n    >>> np.array(x)[np.argpartition(x, 3)]\n    array([2, 1, 3, 4])\n\n    Multi-dimensional array:\n\n    >>> x = np.array([[3, 4, 2], [1, 3, 1]])\n    >>> index_array = np.argpartition(x, kth=1, axis=-1)\n    >>> np.take_along_axis(x, index_array, axis=-1)  # same as np.partition(x, kth=1)\n    array([[2, 3, 4],\n           [1, 1, 3]])\n\n    \"\"\"\n    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)\n\n\ndef _sort_dispatcher(a, axis=None, kind=None, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_sort_dispatcher)\ndef sort(a, axis=-1, kind=None, order=None):\n    \"\"\"\n    Return a sorted copy of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be sorted.\n    axis : int or None, optional\n        Axis along which to sort. If None, the array is flattened before\n        sorting. The default is -1, which sorts along the last axis.\n    kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n        Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n        and 'mergesort' use timsort or radix sort under the covers and, in general,\n        the actual implementation will vary with data type. The 'mergesort' option\n        is retained for backwards compatibility.\n\n        .. versionchanged:: 1.15.0.\n           The 'stable' option was added.\n\n    order : str or list of str, optional\n        When `a` is an array with fields defined, this argument specifies\n        which fields to compare first, second, etc.  A single field can\n        be specified as a string, and not all fields need be specified,\n        but unspecified fields will still be used, in the order in which\n        they come up in the dtype, to break ties.\n\n    Returns\n    -------\n    sorted_array : ndarray\n        Array of the same type and shape as `a`.\n\n    See Also\n    --------\n    ndarray.sort : Method to sort an array in-place.\n    argsort : Indirect sort.\n    lexsort : Indirect stable sort on multiple keys.\n    searchsorted : Find elements in a sorted array.\n    partition : Partial sort.\n\n    Notes\n    -----\n    The various sorting algorithms are characterized by their average speed,\n    worst case performance, work space size, and whether they are stable. A\n    stable sort keeps items with the same key in the same relative\n    order. The four algorithms implemented in NumPy have the following\n    properties:\n\n    =========== ======= ============= ============ ========\n       kind      speed   worst case    work space   stable\n    =========== ======= ============= ============ ========\n    'quicksort'    1     O(n^2)            0          no\n    'heapsort'     3     O(n*log(n))       0          no\n    'mergesort'    2     O(n*log(n))      ~n/2        yes\n    'timsort'      2     O(n*log(n))      ~n/2        yes\n    =========== ======= ============= ============ ========\n\n    .. note:: The datatype determines which of 'mergesort' or 'timsort'\n       is actually used, even if 'mergesort' is specified. User selection\n       at a finer scale is not currently available.\n\n    All the sort algorithms make temporary copies of the data when\n    sorting along any but the last axis.  Consequently, sorting along\n    the last axis is faster and uses less space than sorting along\n    any other axis.\n\n    The sort order for complex numbers is lexicographic. If both the real\n    and imaginary parts are non-nan then the order is determined by the\n    real parts except when they are equal, in which case the order is\n    determined by the imaginary parts.\n\n    Previous to numpy 1.4.0 sorting real and complex arrays containing nan\n    values led to undefined behaviour. In numpy versions >= 1.4.0 nan\n    values are sorted to the end. The extended sort order is:\n\n      * Real: [R, nan]\n      * Complex: [R + Rj, R + nanj, nan + Rj, nan + nanj]\n\n    where R is a non-nan real value. Complex values with the same nan\n    placements are sorted according to the non-nan part if it exists.\n    Non-nan values are sorted as before.\n\n    .. versionadded:: 1.12.0\n\n    quicksort has been changed to `introsort <https://en.wikipedia.org/wiki/Introsort>`_.\n    When sorting does not make enough progress it switches to\n    `heapsort <https://en.wikipedia.org/wiki/Heapsort>`_.\n    This implementation makes quicksort O(n*log(n)) in the worst case.\n\n    'stable' automatically chooses the best stable sorting algorithm\n    for the data type being sorted.\n    It, along with 'mergesort' is currently mapped to\n    `timsort <https://en.wikipedia.org/wiki/Timsort>`_\n    or `radix sort <https://en.wikipedia.org/wiki/Radix_sort>`_\n    depending on the data type.\n    API forward compatibility currently limits the\n    ability to select the implementation and it is hardwired for the different\n    data types.\n\n    .. versionadded:: 1.17.0\n\n    Timsort is added for better performance on already or nearly\n    sorted data. On random data timsort is almost identical to\n    mergesort. It is now used for stable sort while quicksort is still the\n    default sort if none is chosen. For timsort details, refer to\n    `CPython listsort.txt <https://github.com/python/cpython/blob/3.7/Objects/listsort.txt>`_.\n    'mergesort' and 'stable' are mapped to radix sort for integer data types. Radix sort is an\n    O(n) sort instead of O(n log n).\n\n    .. versionchanged:: 1.18.0\n\n    NaT now sorts to the end of arrays for consistency with NaN.\n\n    Examples\n    --------\n    >>> a = np.array([[1,4],[3,1]])\n    >>> np.sort(a)                # sort along the last axis\n    array([[1, 4],\n           [1, 3]])\n    >>> np.sort(a, axis=None)     # sort the flattened array\n    array([1, 1, 3, 4])\n    >>> np.sort(a, axis=0)        # sort along the first axis\n    array([[1, 1],\n           [3, 4]])\n\n    Use the `order` keyword to specify a field to use when sorting a\n    structured array:\n\n    >>> dtype = [('name', 'S10'), ('height', float), ('age', int)]\n    >>> values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),\n    ...           ('Galahad', 1.7, 38)]\n    >>> a = np.array(values, dtype=dtype)       # create a structured array\n    >>> np.sort(a, order='height')                        # doctest: +SKIP\n    array([('Galahad', 1.7, 38), ('Arthur', 1.8, 41),\n           ('Lancelot', 1.8999999999999999, 38)],\n          dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n\n    Sort by age, then height if ages are equal:\n\n    >>> np.sort(a, order=['age', 'height'])               # doctest: +SKIP\n    array([('Galahad', 1.7, 38), ('Lancelot', 1.8999999999999999, 38),\n           ('Arthur', 1.8, 41)],\n          dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n\n    \"\"\"\n    if axis is None:\n        # flatten returns (1, N) for np.matrix, so always use the last axis\n        a = asanyarray(a).flatten()\n        axis = -1\n    else:\n        a = asanyarray(a).copy(order=\"K\")\n    a.sort(axis=axis, kind=kind, order=order)\n    return a\n\n\ndef _argsort_dispatcher(a, axis=None, kind=None, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_argsort_dispatcher)\ndef argsort(a, axis=-1, kind=None, order=None):\n    \"\"\"\n    Returns the indices that would sort an array.\n\n    Perform an indirect sort along the given axis using the algorithm specified\n    by the `kind` keyword. It returns an array of indices of the same shape as\n    `a` that index data along the given axis in sorted order.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to sort.\n    axis : int or None, optional\n        Axis along which to sort.  The default is -1 (the last axis). If None,\n        the flattened array is used.\n    kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n        Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n        and 'mergesort' use timsort under the covers and, in general, the\n        actual implementation will vary with data type. The 'mergesort' option\n        is retained for backwards compatibility.\n\n        .. versionchanged:: 1.15.0.\n           The 'stable' option was added.\n    order : str or list of str, optional\n        When `a` is an array with fields defined, this argument specifies\n        which fields to compare first, second, etc.  A single field can\n        be specified as a string, and not all fields need be specified,\n        but unspecified fields will still be used, in the order in which\n        they come up in the dtype, to break ties.\n\n    Returns\n    -------\n    index_array : ndarray, int\n        Array of indices that sort `a` along the specified `axis`.\n        If `a` is one-dimensional, ``a[index_array]`` yields a sorted `a`.\n        More generally, ``np.take_along_axis(a, index_array, axis=axis)``\n        always yields the sorted `a`, irrespective of dimensionality.\n\n    See Also\n    --------\n    sort : Describes sorting algorithms used.\n    lexsort : Indirect stable sort with multiple keys.\n    ndarray.sort : Inplace sort.\n    argpartition : Indirect partial sort.\n    take_along_axis : Apply ``index_array`` from argsort\n                      to an array as if by calling sort.\n\n    Notes\n    -----\n    See `sort` for notes on the different sorting algorithms.\n\n    As of NumPy 1.4.0 `argsort` works with real/complex arrays containing\n    nan values. The enhanced sort order is documented in `sort`.\n\n    Examples\n    --------\n    One dimensional array:\n\n    >>> x = np.array([3, 1, 2])\n    >>> np.argsort(x)\n    array([1, 2, 0])\n\n    Two-dimensional array:\n\n    >>> x = np.array([[0, 3], [2, 2]])\n    >>> x\n    array([[0, 3],\n           [2, 2]])\n\n    >>> ind = np.argsort(x, axis=0)  # sorts along first axis (down)\n    >>> ind\n    array([[0, 1],\n           [1, 0]])\n    >>> np.take_along_axis(x, ind, axis=0)  # same as np.sort(x, axis=0)\n    array([[0, 2],\n           [2, 3]])\n\n    >>> ind = np.argsort(x, axis=1)  # sorts along last axis (across)\n    >>> ind\n    array([[0, 1],\n           [0, 1]])\n    >>> np.take_along_axis(x, ind, axis=1)  # same as np.sort(x, axis=1)\n    array([[0, 3],\n           [2, 2]])\n\n    Indices of the sorted elements of a N-dimensional array:\n\n    >>> ind = np.unravel_index(np.argsort(x, axis=None), x.shape)\n    >>> ind\n    (array([0, 1, 1, 0]), array([0, 0, 1, 1]))\n    >>> x[ind]  # same as np.sort(x, axis=None)\n    array([0, 2, 2, 3])\n\n    Sorting with keys:\n\n    >>> x = np.array([(1, 0), (0, 1)], dtype=[('x', '<i4'), ('y', '<i4')])\n    >>> x\n    array([(1, 0), (0, 1)],\n          dtype=[('x', '<i4'), ('y', '<i4')])\n\n    >>> np.argsort(x, order=('x','y'))\n    array([1, 0])\n\n    >>> np.argsort(x, order=('y','x'))\n    array([0, 1])\n\n    \"\"\"\n    return _wrapfunc(a, 'argsort', axis=axis, kind=kind, order=order)\n\n\ndef _argmax_dispatcher(a, axis=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_argmax_dispatcher)\ndef argmax(a, axis=None, out=None):\n    \"\"\"\n    Returns the indices of the maximum values along an axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int, optional\n        By default, the index is into the flattened array, otherwise\n        along the specified axis.\n    out : array, optional\n        If provided, the result will be inserted into this array. It should\n        be of the appropriate shape and dtype.\n\n    Returns\n    -------\n    index_array : ndarray of ints\n        Array of indices into the array. It has the same shape as `a.shape`\n        with the dimension along `axis` removed.\n\n    See Also\n    --------\n    ndarray.argmax, argmin\n    amax : The maximum value along a given axis.\n    unravel_index : Convert a flat index into an index tuple.\n    take_along_axis : Apply ``np.expand_dims(index_array, axis)``\n                      from argmax to an array as if by calling max.\n\n    Notes\n    -----\n    In case of multiple occurrences of the maximum values, the indices\n    corresponding to the first occurrence are returned.\n\n    Examples\n    --------\n    >>> a = np.arange(6).reshape(2,3) + 10\n    >>> a\n    array([[10, 11, 12],\n           [13, 14, 15]])\n    >>> np.argmax(a)\n    5\n    >>> np.argmax(a, axis=0)\n    array([1, 1, 1])\n    >>> np.argmax(a, axis=1)\n    array([2, 2])\n\n    Indexes of the maximal elements of a N-dimensional array:\n\n    >>> ind = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    >>> ind\n    (1, 2)\n    >>> a[ind]\n    15\n\n    >>> b = np.arange(6)\n    >>> b[1] = 5\n    >>> b\n    array([0, 5, 2, 3, 4, 5])\n    >>> np.argmax(b)  # Only the first occurrence is returned.\n    1\n\n    >>> x = np.array([[4,2,3], [1,0,3]])\n    >>> index_array = np.argmax(x, axis=-1)\n    >>> # Same as np.max(x, axis=-1, keepdims=True)\n    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\n    array([[4],\n           [3]])\n    >>> # Same as np.max(x, axis=-1)\n    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\n    array([4, 3])\n\n    \"\"\"\n    return _wrapfunc(a, 'argmax', axis=axis, out=out)\n\n\ndef _argmin_dispatcher(a, axis=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_argmin_dispatcher)\ndef argmin(a, axis=None, out=None):\n    \"\"\"\n    Returns the indices of the minimum values along an axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int, optional\n        By default, the index is into the flattened array, otherwise\n        along the specified axis.\n    out : array, optional\n        If provided, the result will be inserted into this array. It should\n        be of the appropriate shape and dtype.\n\n    Returns\n    -------\n    index_array : ndarray of ints\n        Array of indices into the array. It has the same shape as `a.shape`\n        with the dimension along `axis` removed.\n\n    See Also\n    --------\n    ndarray.argmin, argmax\n    amin : The minimum value along a given axis.\n    unravel_index : Convert a flat index into an index tuple.\n    take_along_axis : Apply ``np.expand_dims(index_array, axis)``\n                      from argmin to an array as if by calling min.\n\n    Notes\n    -----\n    In case of multiple occurrences of the minimum values, the indices\n    corresponding to the first occurrence are returned.\n\n    Examples\n    --------\n    >>> a = np.arange(6).reshape(2,3) + 10\n    >>> a\n    array([[10, 11, 12],\n           [13, 14, 15]])\n    >>> np.argmin(a)\n    0\n    >>> np.argmin(a, axis=0)\n    array([0, 0, 0])\n    >>> np.argmin(a, axis=1)\n    array([0, 0])\n\n    Indices of the minimum elements of a N-dimensional array:\n\n    >>> ind = np.unravel_index(np.argmin(a, axis=None), a.shape)\n    >>> ind\n    (0, 0)\n    >>> a[ind]\n    10\n\n    >>> b = np.arange(6) + 10\n    >>> b[4] = 10\n    >>> b\n    array([10, 11, 12, 13, 10, 15])\n    >>> np.argmin(b)  # Only the first occurrence is returned.\n    0\n\n    >>> x = np.array([[4,2,3], [1,0,3]])\n    >>> index_array = np.argmin(x, axis=-1)\n    >>> # Same as np.min(x, axis=-1, keepdims=True)\n    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\n    array([[2],\n           [0]])\n    >>> # Same as np.max(x, axis=-1)\n    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\n    array([2, 0])\n\n    \"\"\"\n    return _wrapfunc(a, 'argmin', axis=axis, out=out)\n\n\ndef _searchsorted_dispatcher(a, v, side=None, sorter=None):\n    return (a, v, sorter)\n\n\n@array_function_dispatch(_searchsorted_dispatcher)\ndef searchsorted(a, v, side='left', sorter=None):\n    \"\"\"\n    Find indices where elements should be inserted to maintain order.\n\n    Find the indices into a sorted array `a` such that, if the\n    corresponding elements in `v` were inserted before the indices, the\n    order of `a` would be preserved.\n\n    Assuming that `a` is sorted:\n\n    ======  ============================\n    `side`  returned index `i` satisfies\n    ======  ============================\n    left    ``a[i-1] < v <= a[i]``\n    right   ``a[i-1] <= v < a[i]``\n    ======  ============================\n\n    Parameters\n    ----------\n    a : 1-D array_like\n        Input array. If `sorter` is None, then it must be sorted in\n        ascending order, otherwise `sorter` must be an array of indices\n        that sort it.\n    v : array_like\n        Values to insert into `a`.\n    side : {'left', 'right'}, optional\n        If 'left', the index of the first suitable location found is given.\n        If 'right', return the last such index.  If there is no suitable\n        index, return either 0 or N (where N is the length of `a`).\n    sorter : 1-D array_like, optional\n        Optional array of integer indices that sort array a into ascending\n        order. They are typically the result of argsort.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    indices : array of ints\n        Array of insertion points with the same shape as `v`.\n\n    See Also\n    --------\n    sort : Return a sorted copy of an array.\n    histogram : Produce histogram from 1-D data.\n\n    Notes\n    -----\n    Binary search is used to find the required insertion points.\n\n    As of NumPy 1.4.0 `searchsorted` works with real/complex arrays containing\n    `nan` values. The enhanced sort order is documented in `sort`.\n\n    This function uses the same algorithm as the builtin python `bisect.bisect_left`\n    (``side='left'``) and `bisect.bisect_right` (``side='right'``) functions,\n    which is also vectorized in the `v` argument.\n\n    Examples\n    --------\n    >>> np.searchsorted([1,2,3,4,5], 3)\n    2\n    >>> np.searchsorted([1,2,3,4,5], 3, side='right')\n    3\n    >>> np.searchsorted([1,2,3,4,5], [-10, 10, 2, 3])\n    array([0, 5, 1, 2])\n\n    \"\"\"\n    return _wrapfunc(a, 'searchsorted', v, side=side, sorter=sorter)\n\n\ndef _resize_dispatcher(a, new_shape):\n    return (a,)\n\n\n@array_function_dispatch(_resize_dispatcher)\ndef resize(a, new_shape):\n    \"\"\"\n    Return a new array with the specified shape.\n\n    If the new array is larger than the original array, then the new\n    array is filled with repeated copies of `a`.  Note that this behavior\n    is different from a.resize(new_shape) which fills with zeros instead\n    of repeated copies of `a`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be resized.\n\n    new_shape : int or tuple of int\n        Shape of resized array.\n\n    Returns\n    -------\n    reshaped_array : ndarray\n        The new array is formed from the data in the old array, repeated\n        if necessary to fill out the required number of elements.  The\n        data are repeated iterating over the array in C-order.\n\n    See Also\n    --------\n    np.reshape : Reshape an array without changing the total size.\n    np.pad : Enlarge and pad an array.\n    np.repeat : Repeat elements of an array.\n    ndarray.resize : resize an array in-place.\n\n    Notes\n    -----\n    When the total size of the array does not change `~numpy.reshape` should\n    be used.  In most other cases either indexing (to reduce the size)\n    or padding (to increase the size) may be a more appropriate solution.\n\n    Warning: This functionality does **not** consider axes separately,\n    i.e. it does not apply interpolation/extrapolation.\n    It fills the return array with the required number of elements, iterating\n    over `a` in C-order, disregarding axes (and cycling back from the start if\n    the new shape is larger).  This functionality is therefore not suitable to\n    resize images, or data where each axis represents a separate and distinct\n    entity.\n\n    Examples\n    --------\n    >>> a=np.array([[0,1],[2,3]])\n    >>> np.resize(a,(2,3))\n    array([[0, 1, 2],\n           [3, 0, 1]])\n    >>> np.resize(a,(1,4))\n    array([[0, 1, 2, 3]])\n    >>> np.resize(a,(2,4))\n    array([[0, 1, 2, 3],\n           [0, 1, 2, 3]])\n\n    \"\"\"\n    if isinstance(new_shape, (int, nt.integer)):\n        new_shape = (new_shape,)\n\n    a = ravel(a)\n\n    new_size = 1\n    for dim_length in new_shape:\n        new_size *= dim_length\n        if dim_length < 0:\n            raise ValueError('all elements of `new_shape` must be non-negative')\n\n    if a.size == 0 or new_size == 0:\n        # First case must zero fill. The second would have repeats == 0.\n        return np.zeros_like(a, shape=new_shape)\n\n    repeats = -(-new_size // a.size)  # ceil division\n    a = concatenate((a,) * repeats)[:new_size]\n\n    return reshape(a, new_shape)\n\n\ndef _squeeze_dispatcher(a, axis=None):\n    return (a,)\n\n\n@array_function_dispatch(_squeeze_dispatcher)\ndef squeeze(a, axis=None):\n    \"\"\"\n    Remove axes of length one from `a`.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : None or int or tuple of ints, optional\n        .. versionadded:: 1.7.0\n\n        Selects a subset of the entries of length one in the\n        shape. If an axis is selected with shape entry greater than\n        one, an error is raised.\n\n    Returns\n    -------\n    squeezed : ndarray\n        The input array, but with all or a subset of the\n        dimensions of length 1 removed. This is always `a` itself\n        or a view into `a`. Note that if all axes are squeezed,\n        the result is a 0d array and not a scalar.\n\n    Raises\n    ------\n    ValueError\n        If `axis` is not None, and an axis being squeezed is not of length 1\n\n    See Also\n    --------\n    expand_dims : The inverse operation, adding entries of length one\n    reshape : Insert, remove, and combine dimensions, and resize existing ones\n\n    Examples\n    --------\n    >>> x = np.array([[[0], [1], [2]]])\n    >>> x.shape\n    (1, 3, 1)\n    >>> np.squeeze(x).shape\n    (3,)\n    >>> np.squeeze(x, axis=0).shape\n    (3, 1)\n    >>> np.squeeze(x, axis=1).shape\n    Traceback (most recent call last):\n    ...\n    ValueError: cannot select an axis to squeeze out which has size not equal to one\n    >>> np.squeeze(x, axis=2).shape\n    (1, 3)\n    >>> x = np.array([[1234]])\n    >>> x.shape\n    (1, 1)\n    >>> np.squeeze(x)\n    array(1234)  # 0d array\n    >>> np.squeeze(x).shape\n    ()\n    >>> np.squeeze(x)[()]\n    1234\n\n    \"\"\"\n    try:\n        squeeze = a.squeeze\n    except AttributeError:\n        return _wrapit(a, 'squeeze', axis=axis)\n    if axis is None:\n        return squeeze()\n    else:\n        return squeeze(axis=axis)\n\n\ndef _diagonal_dispatcher(a, offset=None, axis1=None, axis2=None):\n    return (a,)\n\n\n@array_function_dispatch(_diagonal_dispatcher)\ndef diagonal(a, offset=0, axis1=0, axis2=1):\n    \"\"\"\n    Return specified diagonals.\n\n    If `a` is 2-D, returns the diagonal of `a` with the given offset,\n    i.e., the collection of elements of the form ``a[i, i+offset]``.  If\n    `a` has more than two dimensions, then the axes specified by `axis1`\n    and `axis2` are used to determine the 2-D sub-array whose diagonal is\n    returned.  The shape of the resulting array can be determined by\n    removing `axis1` and `axis2` and appending an index to the right equal\n    to the size of the resulting diagonals.\n\n    In versions of NumPy prior to 1.7, this function always returned a new,\n    independent array containing a copy of the values in the diagonal.\n\n    In NumPy 1.7 and 1.8, it continues to return a copy of the diagonal,\n    but depending on this fact is deprecated. Writing to the resulting\n    array continues to work as it used to, but a FutureWarning is issued.\n\n    Starting in NumPy 1.9 it returns a read-only view on the original array.\n    Attempting to write to the resulting array will produce an error.\n\n    In some future release, it will return a read/write view and writing to\n    the returned array will alter your original array.  The returned array\n    will have the same type as the input array.\n\n    If you don't write to the array returned by this function, then you can\n    just ignore all of the above.\n\n    If you depend on the current behavior, then we suggest copying the\n    returned array explicitly, i.e., use ``np.diagonal(a).copy()`` instead\n    of just ``np.diagonal(a)``. This will work with both past and future\n    versions of NumPy.\n\n    Parameters\n    ----------\n    a : array_like\n        Array from which the diagonals are taken.\n    offset : int, optional\n        Offset of the diagonal from the main diagonal.  Can be positive or\n        negative.  Defaults to main diagonal (0).\n    axis1 : int, optional\n        Axis to be used as the first axis of the 2-D sub-arrays from which\n        the diagonals should be taken.  Defaults to first axis (0).\n    axis2 : int, optional\n        Axis to be used as the second axis of the 2-D sub-arrays from\n        which the diagonals should be taken. Defaults to second axis (1).\n\n    Returns\n    -------\n    array_of_diagonals : ndarray\n        If `a` is 2-D, then a 1-D array containing the diagonal and of the\n        same type as `a` is returned unless `a` is a `matrix`, in which case\n        a 1-D array rather than a (2-D) `matrix` is returned in order to\n        maintain backward compatibility.\n\n        If ``a.ndim > 2``, then the dimensions specified by `axis1` and `axis2`\n        are removed, and a new axis inserted at the end corresponding to the\n        diagonal.\n\n    Raises\n    ------\n    ValueError\n        If the dimension of `a` is less than 2.\n\n    See Also\n    --------\n    diag : MATLAB work-a-like for 1-D and 2-D arrays.\n    diagflat : Create diagonal arrays.\n    trace : Sum along diagonals.\n\n    Examples\n    --------\n    >>> a = np.arange(4).reshape(2,2)\n    >>> a\n    array([[0, 1],\n           [2, 3]])\n    >>> a.diagonal()\n    array([0, 3])\n    >>> a.diagonal(1)\n    array([1])\n\n    A 3-D example:\n\n    >>> a = np.arange(8).reshape(2,2,2); a\n    array([[[0, 1],\n            [2, 3]],\n           [[4, 5],\n            [6, 7]]])\n    >>> a.diagonal(0,  # Main diagonals of two arrays created by skipping\n    ...            0,  # across the outer(left)-most axis last and\n    ...            1)  # the \"middle\" (row) axis first.\n    array([[0, 6],\n           [1, 7]])\n\n    The sub-arrays whose main diagonals we just obtained; note that each\n    corresponds to fixing the right-most (column) axis, and that the\n    diagonals are \"packed\" in rows.\n\n    >>> a[:,:,0]  # main diagonal is [0 6]\n    array([[0, 2],\n           [4, 6]])\n    >>> a[:,:,1]  # main diagonal is [1 7]\n    array([[1, 3],\n           [5, 7]])\n\n    The anti-diagonal can be obtained by reversing the order of elements\n    using either `numpy.flipud` or `numpy.fliplr`.\n\n    >>> a = np.arange(9).reshape(3, 3)\n    >>> a\n    array([[0, 1, 2],\n           [3, 4, 5],\n           [6, 7, 8]])\n    >>> np.fliplr(a).diagonal()  # Horizontal flip\n    array([2, 4, 6])\n    >>> np.flipud(a).diagonal()  # Vertical flip\n    array([6, 4, 2])\n\n    Note that the order in which the diagonal is retrieved varies depending\n    on the flip function.\n    \"\"\"\n    if isinstance(a, np.matrix):\n        # Make diagonal of matrix 1-D to preserve backward compatibility.\n        return asarray(a).diagonal(offset=offset, axis1=axis1, axis2=axis2)\n    else:\n        return asanyarray(a).diagonal(offset=offset, axis1=axis1, axis2=axis2)\n\n\ndef _trace_dispatcher(\n        a, offset=None, axis1=None, axis2=None, dtype=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_trace_dispatcher)\ndef trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):\n    \"\"\"\n    Return the sum along diagonals of the array.\n\n    If `a` is 2-D, the sum along its diagonal with the given offset\n    is returned, i.e., the sum of elements ``a[i,i+offset]`` for all i.\n\n    If `a` has more than two dimensions, then the axes specified by axis1 and\n    axis2 are used to determine the 2-D sub-arrays whose traces are returned.\n    The shape of the resulting array is the same as that of `a` with `axis1`\n    and `axis2` removed.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array, from which the diagonals are taken.\n    offset : int, optional\n        Offset of the diagonal from the main diagonal. Can be both positive\n        and negative. Defaults to 0.\n    axis1, axis2 : int, optional\n        Axes to be used as the first and second axis of the 2-D sub-arrays\n        from which the diagonals should be taken. Defaults are the first two\n        axes of `a`.\n    dtype : dtype, optional\n        Determines the data-type of the returned array and of the accumulator\n        where the elements are summed. If dtype has the value None and `a` is\n        of integer type of precision less than the default integer\n        precision, then the default integer precision is used. Otherwise,\n        the precision is the same as that of `a`.\n    out : ndarray, optional\n        Array into which the output is placed. Its type is preserved and\n        it must be of the right shape to hold the output.\n\n    Returns\n    -------\n    sum_along_diagonals : ndarray\n        If `a` is 2-D, the sum along the diagonal is returned.  If `a` has\n        larger dimensions, then an array of sums along diagonals is returned.\n\n    See Also\n    --------\n    diag, diagonal, diagflat\n\n    Examples\n    --------\n    >>> np.trace(np.eye(3))\n    3.0\n    >>> a = np.arange(8).reshape((2,2,2))\n    >>> np.trace(a)\n    array([6, 8])\n\n    >>> a = np.arange(24).reshape((2,2,2,3))\n    >>> np.trace(a).shape\n    (2, 3)\n\n    \"\"\"\n    if isinstance(a, np.matrix):\n        # Get trace of matrix via an array to preserve backward compatibility.\n        return asarray(a).trace(offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out)\n    else:\n        return asanyarray(a).trace(offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out)\n\n\ndef _ravel_dispatcher(a, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_ravel_dispatcher)\ndef ravel(a, order='C'):\n    \"\"\"Return a contiguous flattened array.\n\n    A 1-D array, containing the elements of the input, is returned.  A copy is\n    made only if needed.\n\n    As of NumPy 1.10, the returned array will have the same type as the input\n    array. (for example, a masked array will be returned for a masked array\n    input)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.  The elements in `a` are read in the order specified by\n        `order`, and packed as a 1-D array.\n    order : {'C','F', 'A', 'K'}, optional\n\n        The elements of `a` are read using this index order. 'C' means\n        to index the elements in row-major, C-style order,\n        with the last axis index changing fastest, back to the first\n        axis index changing slowest.  'F' means to index the elements\n        in column-major, Fortran-style order, with the\n        first index changing fastest, and the last index changing\n        slowest. Note that the 'C' and 'F' options take no account of\n        the memory layout of the underlying array, and only refer to\n        the order of axis indexing.  'A' means to read the elements in\n        Fortran-like index order if `a` is Fortran *contiguous* in\n        memory, C-like order otherwise.  'K' means to read the\n        elements in the order they occur in memory, except for\n        reversing the data when strides are negative.  By default, 'C'\n        index order is used.\n\n    Returns\n    -------\n    y : array_like\n        y is an array of the same subtype as `a`, with shape ``(a.size,)``.\n        Note that matrices are special cased for backward compatibility, if `a`\n        is a matrix, then y is a 1-D ndarray.\n\n    See Also\n    --------\n    ndarray.flat : 1-D iterator over an array.\n    ndarray.flatten : 1-D array copy of the elements of an array\n                      in row-major order.\n    ndarray.reshape : Change the shape of an array without changing its data.\n\n    Notes\n    -----\n    In row-major, C-style order, in two dimensions, the row index\n    varies the slowest, and the column index the quickest.  This can\n    be generalized to multiple dimensions, where row-major order\n    implies that the index along the first axis varies slowest, and\n    the index along the last quickest.  The opposite holds for\n    column-major, Fortran-style index ordering.\n\n    When a view is desired in as many cases as possible, ``arr.reshape(-1)``\n    may be preferable.\n\n    Examples\n    --------\n    It is equivalent to ``reshape(-1, order=order)``.\n\n    >>> x = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> np.ravel(x)\n    array([1, 2, 3, 4, 5, 6])\n\n    >>> x.reshape(-1)\n    array([1, 2, 3, 4, 5, 6])\n\n    >>> np.ravel(x, order='F')\n    array([1, 4, 2, 5, 3, 6])\n\n    When ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:\n\n    >>> np.ravel(x.T)\n    array([1, 4, 2, 5, 3, 6])\n    >>> np.ravel(x.T, order='A')\n    array([1, 2, 3, 4, 5, 6])\n\n    When ``order`` is 'K', it will preserve orderings that are neither 'C'\n    nor 'F', but won't reverse axes:\n\n    >>> a = np.arange(3)[::-1]; a\n    array([2, 1, 0])\n    >>> a.ravel(order='C')\n    array([2, 1, 0])\n    >>> a.ravel(order='K')\n    array([2, 1, 0])\n\n    >>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a\n    array([[[ 0,  2,  4],\n            [ 1,  3,  5]],\n           [[ 6,  8, 10],\n            [ 7,  9, 11]]])\n    >>> a.ravel(order='C')\n    array([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])\n    >>> a.ravel(order='K')\n    array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n    \"\"\"\n    if isinstance(a, np.matrix):\n        return asarray(a).ravel(order=order)\n    else:\n        return asanyarray(a).ravel(order=order)\n\n\ndef _nonzero_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_nonzero_dispatcher)\ndef nonzero(a):\n    \"\"\"\n    Return the indices of the elements that are non-zero.\n\n    Returns a tuple of arrays, one for each dimension of `a`,\n    containing the indices of the non-zero elements in that\n    dimension. The values in `a` are always tested and returned in\n    row-major, C-style order.\n\n    To group the indices by element, rather than dimension, use `argwhere`,\n    which returns a row for each non-zero element.\n\n    .. note::\n\n       When called on a zero-d array or scalar, ``nonzero(a)`` is treated\n       as ``nonzero(atleast_1d(a))``.\n\n       .. deprecated:: 1.17.0\n\n          Use `atleast_1d` explicitly if this behavior is deliberate.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    Returns\n    -------\n    tuple_of_arrays : tuple\n        Indices of elements that are non-zero.\n\n    See Also\n    --------\n    flatnonzero :\n        Return indices that are non-zero in the flattened version of the input\n        array.\n    ndarray.nonzero :\n        Equivalent ndarray method.\n    count_nonzero :\n        Counts the number of non-zero elements in the input array.\n\n    Notes\n    -----\n    While the nonzero values can be obtained with ``a[nonzero(a)]``, it is\n    recommended to use ``x[x.astype(bool)]`` or ``x[x != 0]`` instead, which\n    will correctly handle 0-d arrays.\n\n    Examples\n    --------\n    >>> x = np.array([[3, 0, 0], [0, 4, 0], [5, 6, 0]])\n    >>> x\n    array([[3, 0, 0],\n           [0, 4, 0],\n           [5, 6, 0]])\n    >>> np.nonzero(x)\n    (array([0, 1, 2, 2]), array([0, 1, 0, 1]))\n\n    >>> x[np.nonzero(x)]\n    array([3, 4, 5, 6])\n    >>> np.transpose(np.nonzero(x))\n    array([[0, 0],\n           [1, 1],\n           [2, 0],\n           [2, 1]])\n\n    A common use for ``nonzero`` is to find the indices of an array, where\n    a condition is True.  Given an array `a`, the condition `a` > 3 is a\n    boolean array and since False is interpreted as 0, np.nonzero(a > 3)\n    yields the indices of the `a` where the condition is true.\n\n    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> a > 3\n    array([[False, False, False],\n           [ True,  True,  True],\n           [ True,  True,  True]])\n    >>> np.nonzero(a > 3)\n    (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n\n    Using this result to index `a` is equivalent to using the mask directly:\n\n    >>> a[np.nonzero(a > 3)]\n    array([4, 5, 6, 7, 8, 9])\n    >>> a[a > 3]  # prefer this spelling\n    array([4, 5, 6, 7, 8, 9])\n\n    ``nonzero`` can also be called as a method of the array.\n\n    >>> (a > 3).nonzero()\n    (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n\n    \"\"\"\n    return _wrapfunc(a, 'nonzero')\n\n\ndef _shape_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_shape_dispatcher)\ndef shape(a):\n    \"\"\"\n    Return the shape of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    Returns\n    -------\n    shape : tuple of ints\n        The elements of the shape tuple give the lengths of the\n        corresponding array dimensions.\n\n    See Also\n    --------\n    len\n    ndarray.shape : Equivalent array method.\n\n    Examples\n    --------\n    >>> np.shape(np.eye(3))\n    (3, 3)\n    >>> np.shape([[1, 2]])\n    (1, 2)\n    >>> np.shape([0])\n    (1,)\n    >>> np.shape(0)\n    ()\n\n    >>> a = np.array([(1, 2), (3, 4)], dtype=[('x', 'i4'), ('y', 'i4')])\n    >>> np.shape(a)\n    (2,)\n    >>> a.shape\n    (2,)\n\n    \"\"\"\n    try:\n        result = a.shape\n    except AttributeError:\n        result = asarray(a).shape\n    return result\n\n\ndef _compress_dispatcher(condition, a, axis=None, out=None):\n    return (condition, a, out)\n\n\n@array_function_dispatch(_compress_dispatcher)\ndef compress(condition, a, axis=None, out=None):\n    \"\"\"\n    Return selected slices of an array along given axis.\n\n    When working along a given axis, a slice along that axis is returned in\n    `output` for each index where `condition` evaluates to True. When\n    working on a 1-D array, `compress` is equivalent to `extract`.\n\n    Parameters\n    ----------\n    condition : 1-D array of bools\n        Array that selects which entries to return. If len(condition)\n        is less than the size of `a` along the given axis, then output is\n        truncated to the length of the condition array.\n    a : array_like\n        Array from which to extract a part.\n    axis : int, optional\n        Axis along which to take slices. If None (default), work on the\n        flattened array.\n    out : ndarray, optional\n        Output array.  Its type is preserved and it must be of the right\n        shape to hold the output.\n\n    Returns\n    -------\n    compressed_array : ndarray\n        A copy of `a` without the slices along axis for which `condition`\n        is false.\n\n    See Also\n    --------\n    take, choose, diag, diagonal, select\n    ndarray.compress : Equivalent method in ndarray\n    extract : Equivalent method when working on 1-D arrays\n    :ref:`ufuncs-output-type`\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> a\n    array([[1, 2],\n           [3, 4],\n           [5, 6]])\n    >>> np.compress([0, 1], a, axis=0)\n    array([[3, 4]])\n    >>> np.compress([False, True, True], a, axis=0)\n    array([[3, 4],\n           [5, 6]])\n    >>> np.compress([False, True], a, axis=1)\n    array([[2],\n           [4],\n           [6]])\n\n    Working on the flattened array does not return slices along an axis but\n    selects elements.\n\n    >>> np.compress([False, True], a)\n    array([2])\n\n    \"\"\"\n    return _wrapfunc(a, 'compress', condition, axis=axis, out=out)\n\n\ndef _clip_dispatcher(a, a_min, a_max, out=None, **kwargs):\n    return (a, a_min, a_max)\n\n\n@array_function_dispatch(_clip_dispatcher)\ndef clip(a, a_min, a_max, out=None, **kwargs):\n    \"\"\"\n    Clip (limit) the values in an array.\n\n    Given an interval, values outside the interval are clipped to\n    the interval edges.  For example, if an interval of ``[0, 1]``\n    is specified, values smaller than 0 become 0, and values larger\n    than 1 become 1.\n\n    Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``.\n\n    No check is performed to ensure ``a_min < a_max``.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing elements to clip.\n    a_min, a_max : array_like or None\n        Minimum and maximum value. If ``None``, clipping is not performed on\n        the corresponding edge. Only one of `a_min` and `a_max` may be\n        ``None``. Both are broadcast against `a`.\n    out : ndarray, optional\n        The results will be placed in this array. It may be the input\n        array for in-place clipping.  `out` must be of the right shape\n        to hold the output.  Its type is preserved.\n    **kwargs\n        For other keyword-only arguments, see the\n        :ref:`ufunc docs <ufuncs.kwargs>`.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    clipped_array : ndarray\n        An array with the elements of `a`, but where values\n        < `a_min` are replaced with `a_min`, and those > `a_max`\n        with `a_max`.\n\n    See Also\n    --------\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    When `a_min` is greater than `a_max`, `clip` returns an \n    array in which all values are equal to `a_max`, \n    as shown in the second example.  \n\n    Examples\n    --------\n    >>> a = np.arange(10)\n    >>> a\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> np.clip(a, 1, 8)\n    array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])\n    >>> np.clip(a, 8, 1)\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n    >>> np.clip(a, 3, 6, out=a)\n    array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])\n    >>> a\n    array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])\n    >>> a = np.arange(10)\n    >>> a\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8)\n    array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])\n\n    \"\"\"\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n\n\ndef _sum_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n                    initial=None, where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_sum_dispatcher)\ndef sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n        initial=np._NoValue, where=np._NoValue):\n    \"\"\"\n    Sum of array elements over a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Elements to sum.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a sum is performed.  The default,\n        axis=None, will sum all of the elements of the input array.  If\n        axis is negative it counts from the last to the first axis.\n\n        .. versionadded:: 1.7.0\n\n        If axis is a tuple of ints, a sum is performed on all of the axes\n        specified in the tuple instead of a single axis or all the axes as\n        before.\n    dtype : dtype, optional\n        The type of the returned array and of the accumulator in which the\n        elements are summed.  The dtype of `a` is used by default unless `a`\n        has an integer dtype of less precision than the default platform\n        integer.  In that case, if `a` is signed then the platform integer\n        is used while if `a` is unsigned then an unsigned integer of the\n        same precision as the platform integer is used.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must have\n        the same shape as the expected output, but the type of the output\n        values will be cast if necessary.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `sum` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n    initial : scalar, optional\n        Starting value for the sum. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.15.0\n\n    where : array_like of bool, optional\n        Elements to include in the sum. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    sum_along_axis : ndarray\n        An array with the same shape as `a`, with the specified\n        axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar\n        is returned.  If an output array is specified, a reference to\n        `out` is returned.\n\n    See Also\n    --------\n    ndarray.sum : Equivalent method.\n\n    add.reduce : Equivalent functionality of `add`.\n\n    cumsum : Cumulative sum of array elements.\n\n    trapz : Integration of array values using the composite trapezoidal rule.\n\n    mean, average\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.\n\n    The sum of an empty array is the neutral element 0:\n\n    >>> np.sum([])\n    0.0\n\n    For floating point numbers the numerical precision of sum (and\n    ``np.add.reduce``) is in general limited by directly adding each number\n    individually to the result causing rounding errors in every step.\n    However, often numpy will use a  numerically better approach (partial\n    pairwise summation) leading to improved precision in many use-cases.\n    This improved precision is always provided when no ``axis`` is given.\n    When ``axis`` is given, it will depend on which axis is summed.\n    Technically, to provide the best speed possible, the improved precision\n    is only used when the summation is along the fast axis in memory.\n    Note that the exact precision may vary depending on other parameters.\n    In contrast to NumPy, Python's ``math.fsum`` function uses a slower but\n    more precise approach to summation.\n    Especially when summing a large number of lower precision floating point\n    numbers, such as ``float32``, numerical errors can become significant.\n    In such cases it can be advisable to use `dtype=\"float64\"` to use a higher\n    precision for the output.\n\n    Examples\n    --------\n    >>> np.sum([0.5, 1.5])\n    2.0\n    >>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n    1\n    >>> np.sum([[0, 1], [0, 5]])\n    6\n    >>> np.sum([[0, 1], [0, 5]], axis=0)\n    array([0, 6])\n    >>> np.sum([[0, 1], [0, 5]], axis=1)\n    array([1, 5])\n    >>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\n    array([1., 5.])\n\n    If the accumulator is too small, overflow occurs:\n\n    >>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n    -128\n\n    You can also start the sum with a value other than zero:\n\n    >>> np.sum([10], initial=5)\n    15\n    \"\"\"\n    if isinstance(a, _gentype):\n        # 2018-02-25, 1.15.0\n        warnings.warn(\n            \"Calling np.sum(generator) is deprecated, and in the future will give a different result. \"\n            \"Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\",\n            DeprecationWarning, stacklevel=3)\n\n        res = _sum_(a)\n        if out is not None:\n            out[...] = res\n            return out\n        return res\n\n    return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n                          initial=initial, where=where)\n\n\ndef _any_dispatcher(a, axis=None, out=None, keepdims=None, *,\n                    where=np._NoValue):\n    return (a, where, out)\n\n\n@array_function_dispatch(_any_dispatcher)\ndef any(a, axis=None, out=None, keepdims=np._NoValue, *, where=np._NoValue):\n    \"\"\"\n    Test whether any array element along a given axis evaluates to True.\n\n    Returns single boolean unless `axis` is not ``None``\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a logical OR reduction is performed.\n        The default (``axis=None``) is to perform a logical OR over all\n        the dimensions of the input array. `axis` may be negative, in\n        which case it counts from the last to the first axis.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a reduction is performed on multiple\n        axes, instead of a single axis or all the axes as before.\n    out : ndarray, optional\n        Alternate output array in which to place the result.  It must have\n        the same shape as the expected output and its type is preserved\n        (e.g., if it is of type float, then it will remain so, returning\n        1.0 for True and 0.0 for False, regardless of the type of `a`).\n        See :ref:`ufuncs-output-type` for more details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `any` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in checking for any `True` values.\n        See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    any : bool or ndarray\n        A new boolean or `ndarray` is returned unless `out` is specified,\n        in which case a reference to `out` is returned.\n\n    See Also\n    --------\n    ndarray.any : equivalent method\n\n    all : Test whether all elements along a given axis evaluate to True.\n\n    Notes\n    -----\n    Not a Number (NaN), positive infinity and negative infinity evaluate\n    to `True` because these are not equal to zero.\n\n    Examples\n    --------\n    >>> np.any([[True, False], [True, True]])\n    True\n\n    >>> np.any([[True, False], [False, False]], axis=0)\n    array([ True, False])\n\n    >>> np.any([-1, 0, 5])\n    True\n\n    >>> np.any(np.nan)\n    True\n\n    >>> np.any([[True, False], [False, False]], where=[[False], [True]])\n    False\n\n    >>> o=np.array(False)\n    >>> z=np.any([-1, 4, 5], out=o)\n    >>> z, o\n    (array(True), array(True))\n    >>> # Check now that z is a reference to o\n    >>> z is o\n    True\n    >>> id(z), id(o) # identity of z and o              # doctest: +SKIP\n    (191614240, 191614240)\n\n    \"\"\"\n    return _wrapreduction(a, np.logical_or, 'any', axis, None, out,\n                          keepdims=keepdims, where=where)\n\n\ndef _all_dispatcher(a, axis=None, out=None, keepdims=None, *,\n                    where=None):\n    return (a, where, out)\n\n\n@array_function_dispatch(_all_dispatcher)\ndef all(a, axis=None, out=None, keepdims=np._NoValue, *, where=np._NoValue):\n    \"\"\"\n    Test whether all array elements along a given axis evaluate to True.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a logical AND reduction is performed.\n        The default (``axis=None``) is to perform a logical AND over all\n        the dimensions of the input array. `axis` may be negative, in\n        which case it counts from the last to the first axis.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a reduction is performed on multiple\n        axes, instead of a single axis or all the axes as before.\n    out : ndarray, optional\n        Alternate output array in which to place the result.\n        It must have the same shape as the expected output and its\n        type is preserved (e.g., if ``dtype(out)`` is float, the result\n        will consist of 0.0's and 1.0's). See :ref:`ufuncs-output-type` for more\n        details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `all` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in checking for all `True` values.\n        See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    all : ndarray, bool\n        A new boolean or array is returned unless `out` is specified,\n        in which case a reference to `out` is returned.\n\n    See Also\n    --------\n    ndarray.all : equivalent method\n\n    any : Test whether any element along a given axis evaluates to True.\n\n    Notes\n    -----\n    Not a Number (NaN), positive infinity and negative infinity\n    evaluate to `True` because these are not equal to zero.\n\n    Examples\n    --------\n    >>> np.all([[True,False],[True,True]])\n    False\n\n    >>> np.all([[True,False],[True,True]], axis=0)\n    array([ True, False])\n\n    >>> np.all([-1, 4, 5])\n    True\n\n    >>> np.all([1.0, np.nan])\n    True\n\n    >>> np.all([[True, True], [False, True]], where=[[True], [False]])\n    True\n\n    >>> o=np.array(False)\n    >>> z=np.all([-1, 4, 5], out=o)\n    >>> id(z), id(o), z\n    (28293632, 28293632, array(True)) # may vary\n\n    \"\"\"\n    return _wrapreduction(a, np.logical_and, 'all', axis, None, out,\n                          keepdims=keepdims, where=where)\n\n\ndef _cumsum_dispatcher(a, axis=None, dtype=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_cumsum_dispatcher)\ndef cumsum(a, axis=None, dtype=None, out=None):\n    \"\"\"\n    Return the cumulative sum of the elements along a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int, optional\n        Axis along which the cumulative sum is computed. The default\n        (None) is to compute the cumsum over the flattened array.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed.  If `dtype` is not specified, it defaults\n        to the dtype of `a`, unless `a` has an integer dtype with a\n        precision less than that of the default platform integer.  In\n        that case, the default platform integer is used.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output\n        but the type will be cast if necessary. See :ref:`ufuncs-output-type` for\n        more details.\n\n    Returns\n    -------\n    cumsum_along_axis : ndarray.\n        A new array holding the result is returned unless `out` is\n        specified, in which case a reference to `out` is returned. The\n        result has the same size as `a`, and the same shape as `a` if\n        `axis` is not None or `a` is a 1-d array.\n\n    See Also\n    --------\n    sum : Sum array elements.\n    trapz : Integration of array values using the composite trapezoidal rule.\n    diff : Calculate the n-th discrete difference along given axis.\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.\n\n    ``cumsum(a)[-1]`` may not be equal to ``sum(a)`` for floating-point\n    values since ``sum`` may use a pairwise summation routine, reducing\n    the roundoff-error. See `sum` for more information.\n\n    Examples\n    --------\n    >>> a = np.array([[1,2,3], [4,5,6]])\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.cumsum(a)\n    array([ 1,  3,  6, 10, 15, 21])\n    >>> np.cumsum(a, dtype=float)     # specifies type of output value(s)\n    array([  1.,   3.,   6.,  10.,  15.,  21.])\n\n    >>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\n    array([[1, 2, 3],\n           [5, 7, 9]])\n    >>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\n    array([[ 1,  3,  6],\n           [ 4,  9, 15]])\n\n    ``cumsum(b)[-1]`` may not be equal to ``sum(b)``\n\n    >>> b = np.array([1, 2e-9, 3e-9] * 1000000)\n    >>> b.cumsum()[-1]\n    1000000.0050045159\n    >>> b.sum()                    \n    1000000.0050000029\n\n    \"\"\"\n    return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out)\n\n\ndef _ptp_dispatcher(a, axis=None, out=None, keepdims=None):\n    return (a, out)\n\n\n@array_function_dispatch(_ptp_dispatcher)\ndef ptp(a, axis=None, out=None, keepdims=np._NoValue):\n    \"\"\"\n    Range of values (maximum - minimum) along an axis.\n\n    The name of the function comes from the acronym for 'peak to peak'.\n\n    .. warning::\n        `ptp` preserves the data type of the array. This means the\n        return value for an input of signed integers with n bits\n        (e.g. `np.int8`, `np.int16`, etc) is also a signed integer\n        with n bits.  In that case, peak-to-peak values greater than\n        ``2**(n-1)-1`` will be returned as negative values. An example\n        with a work-around is shown below.\n\n    Parameters\n    ----------\n    a : array_like\n        Input values.\n    axis : None or int or tuple of ints, optional\n        Axis along which to find the peaks.  By default, flatten the\n        array.  `axis` may be negative, in\n        which case it counts from the last to the first axis.\n\n        .. versionadded:: 1.15.0\n\n        If this is a tuple of ints, a reduction is performed on multiple\n        axes, instead of a single axis or all the axes as before.\n    out : array_like\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output,\n        but the type of the output values will be cast if necessary.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `ptp` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    Returns\n    -------\n    ptp : ndarray\n        A new array holding the result, unless `out` was\n        specified, in which case a reference to `out` is returned.\n\n    Examples\n    --------\n    >>> x = np.array([[4, 9, 2, 10],\n    ...               [6, 9, 7, 12]])\n\n    >>> np.ptp(x, axis=1)\n    array([8, 6])\n\n    >>> np.ptp(x, axis=0)\n    array([2, 0, 5, 2])\n\n    >>> np.ptp(x)\n    10\n\n    This example shows that a negative value can be returned when\n    the input is an array of signed integers.\n\n    >>> y = np.array([[1, 127],\n    ...               [0, 127],\n    ...               [-1, 127],\n    ...               [-2, 127]], dtype=np.int8)\n    >>> np.ptp(y, axis=1)\n    array([ 126,  127, -128, -127], dtype=int8)\n\n    A work-around is to use the `view()` method to view the result as\n    unsigned integers with the same bit width:\n\n    >>> np.ptp(y, axis=1).view(np.uint8)\n    array([126, 127, 128, 129], dtype=uint8)\n\n    \"\"\"\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if type(a) is not mu.ndarray:\n        try:\n            ptp = a.ptp\n        except AttributeError:\n            pass\n        else:\n            return ptp(axis=axis, out=out, **kwargs)\n    return _methods._ptp(a, axis=axis, out=out, **kwargs)\n\n\ndef _amax_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n                     where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_amax_dispatcher)\ndef amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n         where=np._NoValue):\n    \"\"\"\n    Return the maximum of an array or maximum along an axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which to operate.  By default, flattened input is\n        used.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, the maximum is selected over multiple axes,\n        instead of a single axis or all the axes as before.\n    out : ndarray, optional\n        Alternative output array in which to place the result.  Must\n        be of the same shape and buffer length as the expected output.\n        See :ref:`ufuncs-output-type` for more details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `amax` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    initial : scalar, optional\n        The minimum value of an output element. Must be present to allow\n        computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.15.0\n\n    where : array_like of bool, optional\n        Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n        for details.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    amax : ndarray or scalar\n        Maximum of `a`. If `axis` is None, the result is a scalar value.\n        If `axis` is given, the result is an array of dimension\n        ``a.ndim - 1``.\n\n    See Also\n    --------\n    amin :\n        The minimum value of an array along a given axis, propagating any NaNs.\n    nanmax :\n        The maximum value of an array along a given axis, ignoring any NaNs.\n    maximum :\n        Element-wise maximum of two arrays, propagating any NaNs.\n    fmax :\n        Element-wise maximum of two arrays, ignoring any NaNs.\n    argmax :\n        Return the indices of the maximum values.\n\n    nanmin, minimum, fmin\n\n    Notes\n    -----\n    NaN values are propagated, that is if at least one item is NaN, the\n    corresponding max value will be NaN as well. To ignore NaN values\n    (MATLAB behavior), please use nanmax.\n\n    Don't use `amax` for element-wise comparison of 2 arrays; when\n    ``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n    ``amax(a, axis=0)``.\n\n    Examples\n    --------\n    >>> a = np.arange(4).reshape((2,2))\n    >>> a\n    array([[0, 1],\n           [2, 3]])\n    >>> np.amax(a)           # Maximum of the flattened array\n    3\n    >>> np.amax(a, axis=0)   # Maxima along the first axis\n    array([2, 3])\n    >>> np.amax(a, axis=1)   # Maxima along the second axis\n    array([1, 3])\n    >>> np.amax(a, where=[False, True], initial=-1, axis=0)\n    array([-1,  3])\n    >>> b = np.arange(5, dtype=float)\n    >>> b[2] = np.NaN\n    >>> np.amax(b)\n    nan\n    >>> np.amax(b, where=~np.isnan(b), initial=-1)\n    4.0\n    >>> np.nanmax(b)\n    4.0\n\n    You can use an initial value to compute the maximum of an empty slice, or\n    to initialize it to a different value:\n\n    >>> np.max([[-50], [10]], axis=-1, initial=0)\n    array([ 0, 10])\n\n    Notice that the initial value is used as one of the elements for which the\n    maximum is determined, unlike for the default argument Python's max\n    function, which is only used for empty iterables.\n\n    >>> np.max([5], initial=6)\n    6\n    >>> max([5], default=6)\n    5\n    \"\"\"\n    return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n                          keepdims=keepdims, initial=initial, where=where)\n\n\ndef _amin_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n                     where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_amin_dispatcher)\ndef amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n         where=np._NoValue):\n    \"\"\"\n    Return the minimum of an array or minimum along an axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which to operate.  By default, flattened input is\n        used.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, the minimum is selected over multiple axes,\n        instead of a single axis or all the axes as before.\n    out : ndarray, optional\n        Alternative output array in which to place the result.  Must\n        be of the same shape and buffer length as the expected output.\n        See :ref:`ufuncs-output-type` for more details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `amin` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    initial : scalar, optional\n        The maximum value of an output element. Must be present to allow\n        computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.15.0\n\n    where : array_like of bool, optional\n        Elements to compare for the minimum. See `~numpy.ufunc.reduce`\n        for details.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    amin : ndarray or scalar\n        Minimum of `a`. If `axis` is None, the result is a scalar value.\n        If `axis` is given, the result is an array of dimension\n        ``a.ndim - 1``.\n\n    See Also\n    --------\n    amax :\n        The maximum value of an array along a given axis, propagating any NaNs.\n    nanmin :\n        The minimum value of an array along a given axis, ignoring any NaNs.\n    minimum :\n        Element-wise minimum of two arrays, propagating any NaNs.\n    fmin :\n        Element-wise minimum of two arrays, ignoring any NaNs.\n    argmin :\n        Return the indices of the minimum values.\n\n    nanmax, maximum, fmax\n\n    Notes\n    -----\n    NaN values are propagated, that is if at least one item is NaN, the\n    corresponding min value will be NaN as well. To ignore NaN values\n    (MATLAB behavior), please use nanmin.\n\n    Don't use `amin` for element-wise comparison of 2 arrays; when\n    ``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n    ``amin(a, axis=0)``.\n\n    Examples\n    --------\n    >>> a = np.arange(4).reshape((2,2))\n    >>> a\n    array([[0, 1],\n           [2, 3]])\n    >>> np.amin(a)           # Minimum of the flattened array\n    0\n    >>> np.amin(a, axis=0)   # Minima along the first axis\n    array([0, 1])\n    >>> np.amin(a, axis=1)   # Minima along the second axis\n    array([0, 2])\n    >>> np.amin(a, where=[False, True], initial=10, axis=0)\n    array([10,  1])\n\n    >>> b = np.arange(5, dtype=float)\n    >>> b[2] = np.NaN\n    >>> np.amin(b)\n    nan\n    >>> np.amin(b, where=~np.isnan(b), initial=10)\n    0.0\n    >>> np.nanmin(b)\n    0.0\n\n    >>> np.min([[-50], [10]], axis=-1, initial=0)\n    array([-50,   0])\n\n    Notice that the initial value is used as one of the elements for which the\n    minimum is determined, unlike for the default argument Python's max\n    function, which is only used for empty iterables.\n\n    Notice that this isn't the same as Python's ``default`` argument.\n\n    >>> np.min([6], initial=5)\n    5\n    >>> min([6], default=5)\n    6\n    \"\"\"\n    return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n                          keepdims=keepdims, initial=initial, where=where)\n\n\ndef _alen_dispathcer(a):\n    return (a,)\n\n\n@array_function_dispatch(_alen_dispathcer)\ndef alen(a):\n    \"\"\"\n    Return the length of the first dimension of the input array.\n\n    .. deprecated:: 1.18\n       `numpy.alen` is deprecated, use `len` instead.\n\n    Parameters\n    ----------\n    a : array_like\n       Input array.\n\n    Returns\n    -------\n    alen : int\n       Length of the first dimension of `a`.\n\n    See Also\n    --------\n    shape, size\n\n    Examples\n    --------\n    >>> a = np.zeros((7,4,5))\n    >>> a.shape[0]\n    7\n    >>> np.alen(a)\n    7\n\n    \"\"\"\n    # NumPy 1.18.0, 2019-08-02\n    warnings.warn(\n        \"`np.alen` is deprecated, use `len` instead\",\n        DeprecationWarning, stacklevel=2)\n    try:\n        return len(a)\n    except TypeError:\n        return len(array(a, ndmin=1))\n\n\ndef _prod_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n                     initial=None, where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_prod_dispatcher)\ndef prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n         initial=np._NoValue, where=np._NoValue):\n    \"\"\"\n    Return the product of array elements over a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a product is performed.  The default,\n        axis=None, will calculate the product of all the elements in the\n        input array. If axis is negative it counts from the last to the\n        first axis.\n\n        .. versionadded:: 1.7.0\n\n        If axis is a tuple of ints, a product is performed on all of the\n        axes specified in the tuple instead of a single axis or all the\n        axes as before.\n    dtype : dtype, optional\n        The type of the returned array, as well as of the accumulator in\n        which the elements are multiplied.  The dtype of `a` is used by\n        default unless `a` has an integer dtype of less precision than the\n        default platform integer.  In that case, if `a` is signed then the\n        platform integer is used while if `a` is unsigned then an unsigned\n        integer of the same precision as the platform integer is used.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must have\n        the same shape as the expected output, but the type of the output\n        values will be cast if necessary.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left in the\n        result as dimensions with size one. With this option, the result\n        will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `prod` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n    initial : scalar, optional\n        The starting value for this product. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.15.0\n\n    where : array_like of bool, optional\n        Elements to include in the product. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    product_along_axis : ndarray, see `dtype` parameter above.\n        An array shaped as `a` but with the specified axis removed.\n        Returns a reference to `out` if specified.\n\n    See Also\n    --------\n    ndarray.prod : equivalent method\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.  That means that, on a 32-bit platform:\n\n    >>> x = np.array([536870910, 536870910, 536870910, 536870910])\n    >>> np.prod(x)\n    16 # may vary\n\n    The product of an empty array is the neutral element 1:\n\n    >>> np.prod([])\n    1.0\n\n    Examples\n    --------\n    By default, calculate the product of all elements:\n\n    >>> np.prod([1.,2.])\n    2.0\n\n    Even when the input array is two-dimensional:\n\n    >>> np.prod([[1.,2.],[3.,4.]])\n    24.0\n\n    But we can also specify the axis over which to multiply:\n\n    >>> np.prod([[1.,2.],[3.,4.]], axis=1)\n    array([  2.,  12.])\n\n    Or select specific elements to include:\n\n    >>> np.prod([1., np.nan, 3.], where=[True, False, True])\n    3.0\n\n    If the type of `x` is unsigned, then the output type is\n    the unsigned platform integer:\n\n    >>> x = np.array([1, 2, 3], dtype=np.uint8)\n    >>> np.prod(x).dtype == np.uint\n    True\n\n    If `x` is of a signed integer type, then the output type\n    is the default platform integer:\n\n    >>> x = np.array([1, 2, 3], dtype=np.int8)\n    >>> np.prod(x).dtype == int\n    True\n\n    You can also start the product with a value other than one:\n\n    >>> np.prod([1, 2], initial=5)\n    10\n    \"\"\"\n    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n                          keepdims=keepdims, initial=initial, where=where)\n\n\ndef _cumprod_dispatcher(a, axis=None, dtype=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_cumprod_dispatcher)\ndef cumprod(a, axis=None, dtype=None, out=None):\n    \"\"\"\n    Return the cumulative product of elements along a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int, optional\n        Axis along which the cumulative product is computed.  By default\n        the input is flattened.\n    dtype : dtype, optional\n        Type of the returned array, as well as of the accumulator in which\n        the elements are multiplied.  If *dtype* is not specified, it\n        defaults to the dtype of `a`, unless `a` has an integer dtype with\n        a precision less than that of the default platform integer.  In\n        that case, the default platform integer is used instead.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output\n        but the type of the resulting values will be cast if necessary.\n\n    Returns\n    -------\n    cumprod : ndarray\n        A new array holding the result is returned unless `out` is\n        specified, in which case a reference to out is returned.\n\n    See Also\n    --------\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.\n\n    Examples\n    --------\n    >>> a = np.array([1,2,3])\n    >>> np.cumprod(a) # intermediate results 1, 1*2\n    ...               # total product 1*2*3 = 6\n    array([1, 2, 6])\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> np.cumprod(a, dtype=float) # specify type of output\n    array([   1.,    2.,    6.,   24.,  120.,  720.])\n\n    The cumulative product for each column (i.e., over the rows) of `a`:\n\n    >>> np.cumprod(a, axis=0)\n    array([[ 1,  2,  3],\n           [ 4, 10, 18]])\n\n    The cumulative product for each row (i.e. over the columns) of `a`:\n\n    >>> np.cumprod(a,axis=1)\n    array([[  1,   2,   6],\n           [  4,  20, 120]])\n\n    \"\"\"\n    return _wrapfunc(a, 'cumprod', axis=axis, dtype=dtype, out=out)\n\n\ndef _ndim_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_ndim_dispatcher)\ndef ndim(a):\n    \"\"\"\n    Return the number of dimensions of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.  If it is not already an ndarray, a conversion is\n        attempted.\n\n    Returns\n    -------\n    number_of_dimensions : int\n        The number of dimensions in `a`.  Scalars are zero-dimensional.\n\n    See Also\n    --------\n    ndarray.ndim : equivalent method\n    shape : dimensions of array\n    ndarray.shape : dimensions of array\n\n    Examples\n    --------\n    >>> np.ndim([[1,2,3],[4,5,6]])\n    2\n    >>> np.ndim(np.array([[1,2,3],[4,5,6]]))\n    2\n    >>> np.ndim(1)\n    0\n\n    \"\"\"\n    try:\n        return a.ndim\n    except AttributeError:\n        return asarray(a).ndim\n\n\ndef _size_dispatcher(a, axis=None):\n    return (a,)\n\n\n@array_function_dispatch(_size_dispatcher)\ndef size(a, axis=None):\n    \"\"\"\n    Return the number of elements along a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : int, optional\n        Axis along which the elements are counted.  By default, give\n        the total number of elements.\n\n    Returns\n    -------\n    element_count : int\n        Number of elements along the specified axis.\n\n    See Also\n    --------\n    shape : dimensions of array\n    ndarray.shape : dimensions of array\n    ndarray.size : number of elements in array\n\n    Examples\n    --------\n    >>> a = np.array([[1,2,3],[4,5,6]])\n    >>> np.size(a)\n    6\n    >>> np.size(a,1)\n    3\n    >>> np.size(a,0)\n    2\n\n    \"\"\"\n    if axis is None:\n        try:\n            return a.size\n        except AttributeError:\n            return asarray(a).size\n    else:\n        try:\n            return a.shape[axis]\n        except AttributeError:\n            return asarray(a).shape[axis]\n\n\ndef _around_dispatcher(a, decimals=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_around_dispatcher)\ndef around(a, decimals=0, out=None):\n    \"\"\"\n    Evenly round to the given number of decimals.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    decimals : int, optional\n        Number of decimal places to round to (default: 0).  If\n        decimals is negative, it specifies the number of positions to\n        the left of the decimal point.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must have\n        the same shape as the expected output, but the type of the output\n        values will be cast if necessary. See :ref:`ufuncs-output-type` for more\n        details.\n\n    Returns\n    -------\n    rounded_array : ndarray\n        An array of the same type as `a`, containing the rounded values.\n        Unless `out` was specified, a new array is created.  A reference to\n        the result is returned.\n\n        The real and imaginary parts of complex numbers are rounded\n        separately.  The result of rounding a float is a float.\n\n    See Also\n    --------\n    ndarray.round : equivalent method\n\n    ceil, fix, floor, rint, trunc\n\n\n    Notes\n    -----\n    For values exactly halfway between rounded decimal values, NumPy\n    rounds to the nearest even value. Thus 1.5 and 2.5 round to 2.0,\n    -0.5 and 0.5 round to 0.0, etc.\n\n    ``np.around`` uses a fast but sometimes inexact algorithm to round\n    floating-point datatypes. For positive `decimals` it is equivalent to\n    ``np.true_divide(np.rint(a * 10**decimals), 10**decimals)``, which has\n    error due to the inexact representation of decimal fractions in the IEEE\n    floating point standard [1]_ and errors introduced when scaling by powers\n    of ten. For instance, note the extra \"1\" in the following:\n\n        >>> np.round(56294995342131.5, 3)\n        56294995342131.51\n\n    If your goal is to print such values with a fixed number of decimals, it is\n    preferable to use numpy's float printing routines to limit the number of\n    printed decimals:\n\n        >>> np.format_float_positional(56294995342131.5, precision=3)\n        '56294995342131.5'\n\n    The float printing routines use an accurate but much more computationally\n    demanding algorithm to compute the number of digits after the decimal\n    point.\n\n    Alternatively, Python's builtin `round` function uses a more accurate\n    but slower algorithm for 64-bit floating point values:\n\n        >>> round(56294995342131.5, 3)\n        56294995342131.5\n        >>> np.round(16.055, 2), round(16.055, 2)  # equals 16.0549999999999997\n        (16.06, 16.05)\n\n\n    References\n    ----------\n    .. [1] \"Lecture Notes on the Status of IEEE 754\", William Kahan,\n           https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF\n    .. [2] \"How Futile are Mindless Assessments of\n           Roundoff in Floating-Point Computation?\", William Kahan,\n           https://people.eecs.berkeley.edu/~wkahan/Mindless.pdf\n\n    Examples\n    --------\n    >>> np.around([0.37, 1.64])\n    array([0.,  2.])\n    >>> np.around([0.37, 1.64], decimals=1)\n    array([0.4,  1.6])\n    >>> np.around([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value\n    array([0.,  2.,  2.,  4.,  4.])\n    >>> np.around([1,2,3,11], decimals=1) # ndarray of ints is returned\n    array([ 1,  2,  3, 11])\n    >>> np.around([1,2,3,11], decimals=-1)\n    array([ 0,  0,  0, 10])\n\n    \"\"\"\n    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n\n\ndef _mean_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None, *,\n                     where=None):\n    return (a, where, out)\n\n\n@array_function_dispatch(_mean_dispatcher)\ndef mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,\n         where=np._NoValue):\n    \"\"\"\n    Compute the arithmetic mean along the specified axis.\n\n    Returns the average of the array elements.  The average is taken over\n    the flattened array by default, otherwise over the specified axis.\n    `float64` intermediate and return values are used for integer inputs.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose mean is desired. If `a` is not an\n        array, a conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the means are computed. The default is to\n        compute the mean of the flattened array.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a mean is performed over multiple axes,\n        instead of a single axis or all the axes as before.\n    dtype : data-type, optional\n        Type to use in computing the mean.  For integer inputs, the default\n        is `float64`; for floating point inputs, it is the same as the\n        input dtype.\n    out : ndarray, optional\n        Alternate output array in which to place the result.  The default\n        is ``None``; if provided, it must have the same shape as the\n        expected output, but the type will be cast if necessary.\n        See :ref:`ufuncs-output-type` for more details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `mean` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    m : ndarray, see dtype parameter above\n        If `out=None`, returns a new array containing the mean values,\n        otherwise a reference to the output array is returned.\n\n    See Also\n    --------\n    average : Weighted average\n    std, var, nanmean, nanstd, nanvar\n\n    Notes\n    -----\n    The arithmetic mean is the sum of the elements along the axis divided\n    by the number of elements.\n\n    Note that for floating-point input, the mean is computed using the\n    same precision the input has.  Depending on the input data, this can\n    cause the results to be inaccurate, especially for `float32` (see\n    example below).  Specifying a higher-precision accumulator using the\n    `dtype` keyword can alleviate this issue.\n\n    By default, `float16` results are computed using `float32` intermediates\n    for extra precision.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> np.mean(a)\n    2.5\n    >>> np.mean(a, axis=0)\n    array([2., 3.])\n    >>> np.mean(a, axis=1)\n    array([1.5, 3.5])\n\n    In single precision, `mean` can be inaccurate:\n\n    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> np.mean(a)\n    0.54999924\n\n    Computing the mean in float64 is more accurate:\n\n    >>> np.mean(a, dtype=np.float64)\n    0.55000000074505806 # may vary\n\n    Specifying a where argument:\n    >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n    >>> np.mean(a)\n    12.0\n    >>> np.mean(a, where=[[True], [False], [False]])\n    9.0\n\n    \"\"\"\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n    if type(a) is not mu.ndarray:\n        try:\n            mean = a.mean\n        except AttributeError:\n            pass\n        else:\n            return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\n    return _methods._mean(a, axis=axis, dtype=dtype,\n                          out=out, **kwargs)\n\n\ndef _std_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n                    keepdims=None, *, where=None):\n    return (a, where, out)\n\n\n@array_function_dispatch(_std_dispatcher)\ndef std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n        where=np._NoValue):\n    \"\"\"\n    Compute the standard deviation along the specified axis.\n\n    Returns the standard deviation, a measure of the spread of a distribution,\n    of the array elements. The standard deviation is computed for the\n    flattened array by default, otherwise over the specified axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Calculate the standard deviation of these values.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the standard deviation is computed. The\n        default is to compute the standard deviation of the flattened array.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a standard deviation is performed over\n        multiple axes, instead of a single axis or all the axes as before.\n    dtype : dtype, optional\n        Type to use in computing the standard deviation. For arrays of\n        integer type the default is float64, for arrays of float types it is\n        the same as the array type.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must have\n        the same shape as the expected output but the type (of the calculated\n        values) will be cast if necessary.\n    ddof : int, optional\n        Means Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n        By default `ddof` is zero.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `std` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in the standard deviation.\n        See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    standard_deviation : ndarray, see dtype parameter above.\n        If `out` is None, return a new array containing the standard deviation,\n        otherwise return a reference to the output array.\n\n    See Also\n    --------\n    var, mean, nanmean, nanstd, nanvar\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    The standard deviation is the square root of the average of the squared\n    deviations from the mean, i.e., ``std = sqrt(mean(x))``, where\n    ``x = abs(a - a.mean())**2``.\n\n    The average squared deviation is typically calculated as ``x.sum() / N``,\n    where ``N = len(x)``. If, however, `ddof` is specified, the divisor\n    ``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``\n    provides an unbiased estimator of the variance of the infinite population.\n    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n    normally distributed variables. The standard deviation computed in this\n    function is the square root of the estimated variance, so even with\n    ``ddof=1``, it will not be an unbiased estimate of the standard deviation\n    per se.\n\n    Note that, for complex numbers, `std` takes the absolute\n    value before squaring, so that the result is always real and nonnegative.\n\n    For floating-point input, the *std* is computed using the same\n    precision the input has. Depending on the input data, this can cause\n    the results to be inaccurate, especially for float32 (see example below).\n    Specifying a higher-accuracy accumulator using the `dtype` keyword can\n    alleviate this issue.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> np.std(a)\n    1.1180339887498949 # may vary\n    >>> np.std(a, axis=0)\n    array([1.,  1.])\n    >>> np.std(a, axis=1)\n    array([0.5,  0.5])\n\n    In single precision, std() can be inaccurate:\n\n    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> np.std(a)\n    0.45000005\n\n    Computing the standard deviation in float64 is more accurate:\n\n    >>> np.std(a, dtype=np.float64)\n    0.44999999925494177 # may vary\n\n    Specifying a where argument:\n\n    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n    >>> np.std(a)\n    2.614064523559687 # may vary\n    >>> np.std(a, where=[[True], [True], [False]])\n    2.0\n\n    \"\"\"\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n    if type(a) is not mu.ndarray:\n        try:\n            std = a.std\n        except AttributeError:\n            pass\n        else:\n            return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)\n\n\ndef _var_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n                    keepdims=None, *, where=None):\n    return (a, where, out)\n\n\n@array_function_dispatch(_var_dispatcher)\ndef var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n        where=np._NoValue):\n    \"\"\"\n    Compute the variance along the specified axis.\n\n    Returns the variance of the array elements, a measure of the spread of a\n    distribution.  The variance is computed for the flattened array by\n    default, otherwise over the specified axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose variance is desired.  If `a` is not an\n        array, a conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the variance is computed.  The default is to\n        compute the variance of the flattened array.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a variance is performed over multiple axes,\n        instead of a single axis or all the axes as before.\n    dtype : data-type, optional\n        Type to use in computing the variance.  For arrays of integer type\n        the default is `float64`; for arrays of float types it is the same as\n        the array type.\n    out : ndarray, optional\n        Alternate output array in which to place the result.  It must have\n        the same shape as the expected output, but the type is cast if\n        necessary.\n    ddof : int, optional\n        \"Delta Degrees of Freedom\": the divisor used in the calculation is\n        ``N - ddof``, where ``N`` represents the number of elements. By\n        default `ddof` is zero.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `var` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in the variance. See `~numpy.ufunc.reduce` for\n        details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    variance : ndarray, see dtype parameter above\n        If ``out=None``, returns a new array containing the variance;\n        otherwise, a reference to the output array is returned.\n\n    See Also\n    --------\n    std, mean, nanmean, nanstd, nanvar\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    The variance is the average of the squared deviations from the mean,\n    i.e.,  ``var = mean(x)``, where ``x = abs(a - a.mean())**2``.\n\n    The mean is typically calculated as ``x.sum() / N``, where ``N = len(x)``.\n    If, however, `ddof` is specified, the divisor ``N - ddof`` is used\n    instead.  In standard statistical practice, ``ddof=1`` provides an\n    unbiased estimator of the variance of a hypothetical infinite population.\n    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n    normally distributed variables.\n\n    Note that for complex numbers, the absolute value is taken before\n    squaring, so that the result is always real and nonnegative.\n\n    For floating-point input, the variance is computed using the same\n    precision the input has.  Depending on the input data, this can cause\n    the results to be inaccurate, especially for `float32` (see example\n    below).  Specifying a higher-accuracy accumulator using the ``dtype``\n    keyword can alleviate this issue.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> np.var(a)\n    1.25\n    >>> np.var(a, axis=0)\n    array([1.,  1.])\n    >>> np.var(a, axis=1)\n    array([0.25,  0.25])\n\n    In single precision, var() can be inaccurate:\n\n    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> np.var(a)\n    0.20250003\n\n    Computing the variance in float64 is more accurate:\n\n    >>> np.var(a, dtype=np.float64)\n    0.20249999932944759 # may vary\n    >>> ((1-0.55)**2 + (0.1-0.55)**2)/2\n    0.2025\n\n    Specifying a where argument:\n\n    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n    >>> np.var(a)\n    6.833333333333333 # may vary\n    >>> np.var(a, where=[[True], [True], [False]])\n    4.0\n\n    \"\"\"\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n\n    if type(a) is not mu.ndarray:\n        try:\n            var = a.var\n\n        except AttributeError:\n            pass\n        else:\n            return var(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)\n\n\n# Aliases of other functions. These have their own definitions only so that\n# they can have unique docstrings.\n\n@array_function_dispatch(_around_dispatcher)\ndef round_(a, decimals=0, out=None):\n    \"\"\"\n    Round an array to the given number of decimals.\n\n    See Also\n    --------\n    around : equivalent function; see for details.\n    \"\"\"\n    return around(a, decimals=decimals, out=out)\n\n\n@array_function_dispatch(_prod_dispatcher, verify=False)\ndef product(*args, **kwargs):\n    \"\"\"\n    Return the product of array elements over a given axis.\n\n    See Also\n    --------\n    prod : equivalent function; see for details.\n    \"\"\"\n    return prod(*args, **kwargs)\n\n\n@array_function_dispatch(_cumprod_dispatcher, verify=False)\ndef cumproduct(*args, **kwargs):\n    \"\"\"\n    Return the cumulative product over the given axis.\n\n    See Also\n    --------\n    cumprod : equivalent function; see for details.\n    \"\"\"\n    return cumprod(*args, **kwargs)\n\n\n@array_function_dispatch(_any_dispatcher, verify=False)\ndef sometrue(*args, **kwargs):\n    \"\"\"\n    Check whether some values are true.\n\n    Refer to `any` for full documentation.\n\n    See Also\n    --------\n    any : equivalent function; see for details.\n    \"\"\"\n    return any(*args, **kwargs)\n\n\n@array_function_dispatch(_all_dispatcher, verify=False)\ndef alltrue(*args, **kwargs):\n    \"\"\"\n    Check if all elements of input array are true.\n\n    See Also\n    --------\n    numpy.all : Equivalent function; see for details.\n    \"\"\"\n    return all(*args, **kwargs)\n",3789],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py":["\"\"\"\nExtended math utilities.\n\"\"\"\n# Authors: Gael Varoquaux\n#          Alexandre Gramfort\n#          Alexandre T. Passos\n#          Olivier Grisel\n#          Lars Buitinck\n#          Stefan van der Walt\n#          Kyle Kastner\n#          Giorgio Patrini\n# License: BSD 3 clause\n\nimport warnings\n\nimport numpy as np\nfrom scipy import linalg, sparse\n\nfrom . import check_random_state\nfrom ._logistic_sigmoid import _log_logistic_sigmoid\nfrom .fixes import np_version, parse_version\nfrom .sparsefuncs_fast import csr_row_norms\nfrom .validation import check_array\n\n\ndef squared_norm(x):\n    \"\"\"Squared Euclidean or Frobenius norm of x.\n\n    Faster than norm(x) ** 2.\n\n    Parameters\n    ----------\n    x : array-like\n\n    Returns\n    -------\n    float\n        The Euclidean norm when x is a vector, the Frobenius norm when x\n        is a matrix (2-d array).\n    \"\"\"\n    x = np.ravel(x, order=\"K\")\n    if np.issubdtype(x.dtype, np.integer):\n        warnings.warn(\n            \"Array type is integer, np.dot may overflow. \"\n            \"Data should be float type to avoid this issue\",\n            UserWarning,\n        )\n    return np.dot(x, x)\n\n\ndef row_norms(X, squared=False):\n    \"\"\"Row-wise (squared) Euclidean norm of X.\n\n    Equivalent to np.sqrt((X * X).sum(axis=1)), but also supports sparse\n    matrices and does not create an X.shape-sized temporary.\n\n    Performs no input validation.\n\n    Parameters\n    ----------\n    X : array-like\n        The input array.\n    squared : bool, default=False\n        If True, return squared norms.\n\n    Returns\n    -------\n    array-like\n        The row-wise (squared) Euclidean norm of X.\n    \"\"\"\n    if sparse.issparse(X):\n        if not isinstance(X, sparse.csr_matrix):\n            X = sparse.csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum(\"ij,ij->i\", X, X)\n\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms\n\n\ndef fast_logdet(A):\n    \"\"\"Compute log(det(A)) for A symmetric.\n\n    Equivalent to : np.log(nl.det(A)) but more robust.\n    It returns -Inf if det(A) is non positive or is not defined.\n\n    Parameters\n    ----------\n    A : array-like\n        The matrix.\n    \"\"\"\n    sign, ld = np.linalg.slogdet(A)\n    if not sign > 0:\n        return -np.inf\n    return ld\n\n\ndef density(w, **kwargs):\n    \"\"\"Compute density of a sparse vector.\n\n    Parameters\n    ----------\n    w : array-like\n        The sparse vector.\n\n    Returns\n    -------\n    float\n        The density of w, between 0 and 1.\n    \"\"\"\n    if hasattr(w, \"toarray\"):\n        d = float(w.nnz) / (w.shape[0] * w.shape[1])\n    else:\n        d = 0 if w is None else float((w != 0).sum()) / w.size\n    return d\n\n\ndef safe_sparse_dot(a, b, *, dense_output=False):\n    \"\"\"Dot product that handle the sparse matrix case correctly.\n\n    Parameters\n    ----------\n    a : {ndarray, sparse matrix}\n    b : {ndarray, sparse matrix}\n    dense_output : bool, default=False\n        When False, ``a`` and ``b`` both being sparse will yield sparse output.\n        When True, output will always be a dense array.\n\n    Returns\n    -------\n    dot_product : {ndarray, sparse matrix}\n        Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.\n    \"\"\"\n    if a.ndim > 2 or b.ndim > 2:\n        if sparse.issparse(a):\n            # sparse is always 2D. Implies b is 3D+\n            # [i, j] @ [k, ..., l, m, n] -> [i, k, ..., l, n]\n            b_ = np.rollaxis(b, -2)\n            b_2d = b_.reshape((b.shape[-2], -1))\n            ret = a @ b_2d\n            ret = ret.reshape(a.shape[0], *b_.shape[1:])\n        elif sparse.issparse(b):\n            # sparse is always 2D. Implies a is 3D+\n            # [k, ..., l, m] @ [i, j] -> [k, ..., l, j]\n            a_2d = a.reshape(-1, a.shape[-1])\n            ret = a_2d @ b\n            ret = ret.reshape(*a.shape[:-1], b.shape[1])\n        else:\n            ret = np.dot(a, b)\n    else:\n        ret = a @ b\n\n    if (\n        sparse.issparse(a)\n        and sparse.issparse(b)\n        and dense_output\n        and hasattr(ret, \"toarray\")\n    ):\n        return ret.toarray()\n    return ret\n\n\ndef randomized_range_finder(\n    A, *, size, n_iter, power_iteration_normalizer=\"auto\", random_state=None\n):\n    \"\"\"Computes an orthonormal matrix whose range approximates the range of A.\n\n    Parameters\n    ----------\n    A : 2D array\n        The input data matrix.\n\n    size : int\n        Size of the return array.\n\n    n_iter : int\n        Number of power iterations used to stabilize the result.\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n        .. versionadded:: 0.18\n\n    random_state : int, RandomState instance or None, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data, i.e. getting the random vectors to initialize the algorithm.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    Q : ndarray\n        A (size x size) projection matrix, the range of which\n        approximates well the range of the input matrix A.\n\n    Notes\n    -----\n\n    Follows Algorithm 4.3 of\n    Finding structure with randomness: Stochastic algorithms for constructing\n    approximate matrix decompositions\n    Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\n    An implementation of a randomized algorithm for principal component\n    analysis\n    A. Szlam et al. 2014\n    \"\"\"\n    random_state = check_random_state(random_state)\n\n    # Generating normal random vectors with shape: (A.shape[1], size)\n    Q = random_state.normal(size=(A.shape[1], size))\n    if A.dtype.kind == \"f\":\n        # Ensure f32 is preserved as f32\n        Q = Q.astype(A.dtype, copy=False)\n\n    # Deal with \"auto\" mode\n    if power_iteration_normalizer == \"auto\":\n        if n_iter <= 2:\n            power_iteration_normalizer = \"none\"\n        else:\n            power_iteration_normalizer = \"LU\"\n\n    # Perform power iterations with Q to further 'imprint' the top\n    # singular vectors of A in Q\n    for i in range(n_iter):\n        if power_iteration_normalizer == \"none\":\n            Q = safe_sparse_dot(A, Q)\n            Q = safe_sparse_dot(A.T, Q)\n        elif power_iteration_normalizer == \"LU\":\n            Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n            Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)\n        elif power_iteration_normalizer == \"QR\":\n            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode=\"economic\")\n            Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode=\"economic\")\n\n    # Sample the range of A using by linear projection of Q\n    # Extract an orthonormal basis\n    Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode=\"economic\")\n    return Q\n\n\ndef randomized_svd(\n    M,\n    n_components,\n    *,\n    n_oversamples=10,\n    n_iter=\"auto\",\n    power_iteration_normalizer=\"auto\",\n    transpose=\"auto\",\n    flip_sign=True,\n    random_state=\"warn\",\n):\n    \"\"\"Computes a truncated randomized SVD.\n\n    This method solves the fixed-rank approximation problem described in the\n    Halko et al paper (problem (1.5), p5).\n\n    Parameters\n    ----------\n    M : {ndarray, sparse matrix}\n        Matrix to decompose.\n\n    n_components : int\n        Number of singular values and vectors to extract.\n\n    n_oversamples : int, default=10\n        Additional number of random vectors to sample the range of M so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of M is n_components + n_oversamples. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of singular vectors and singular values. Users might wish\n        to increase this parameter up to `2*k - n_components` where k is the\n        effective rank, for large matrices, noisy problems, matrices with\n        slowly decaying spectrums, or to increase precision accuracy. See Halko\n        et al (pages 5, 23 and 26).\n\n    n_iter : int or 'auto', default='auto'\n        Number of power iterations. It can be used to deal with very noisy\n        problems. When 'auto', it is set to 4, unless `n_components` is small\n        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.\n        This improves precision with few components. Note that in general\n        users should rather increase `n_oversamples` before increasing `n_iter`\n        as the principle of the randomized method is to avoid usage of these\n        more costly power iterations steps. When `n_components` is equal\n        or greater to the effective matrix rank and the spectrum does not\n        present a slow decay, `n_iter=0` or `1` should even work fine in theory\n        (see Halko et al paper, page 9).\n\n        .. versionchanged:: 0.18\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n        .. versionadded:: 0.18\n\n    transpose : bool or 'auto', default='auto'\n        Whether the algorithm should be applied to M.T instead of M. The\n        result should approximately be the same. The 'auto' mode will\n        trigger the transposition if M.shape[1] > M.shape[0] since this\n        implementation of randomized SVD tend to be a little faster in that\n        case.\n\n        .. versionchanged:: 0.18\n\n    flip_sign : bool, default=True\n        The output of a singular value decomposition is only unique up to a\n        permutation of the signs of the singular vectors. If `flip_sign` is\n        set to `True`, the sign ambiguity is resolved by making the largest\n        loadings for each component in the left singular vectors positive.\n\n    random_state : int, RandomState instance or None, default='warn'\n        The seed of the pseudo random number generator to use when\n        shuffling the data, i.e. getting the random vectors to initialize\n        the algorithm. Pass an int for reproducible results across multiple\n        function calls. See :term:`Glossary <random_state>`.\n\n        .. versionchanged:: 1.2\n            The previous behavior (`random_state=0`) is deprecated, and\n            from v1.2 the default value will be `random_state=None`. Set\n            the value of `random_state` explicitly to suppress the deprecation\n            warning.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    singular value decomposition using randomization to speed up the\n    computations. It is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    References\n    ----------\n    * Finding structure with randomness: Stochastic algorithms for constructing\n      approximate matrix decompositions (Algorithm 4.3)\n      Halko, et al., 2009 https://arxiv.org/abs/0909.4061\n\n    * A randomized algorithm for the decomposition of matrices\n      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n    * An implementation of a randomized algorithm for principal component\n      analysis\n      A. Szlam et al. 2014\n    \"\"\"\n    if isinstance(M, (sparse.lil_matrix, sparse.dok_matrix)):\n        warnings.warn(\n            \"Calculating SVD of a {} is expensive. \"\n            \"csr_matrix is more efficient.\".format(type(M).__name__),\n            sparse.SparseEfficiencyWarning,\n        )\n\n    if random_state == \"warn\":\n        warnings.warn(\n            \"If 'random_state' is not supplied, the current default \"\n            \"is to use 0 as a fixed seed. This will change to  \"\n            \"None in version 1.2 leading to non-deterministic results \"\n            \"that better reflect nature of the randomized_svd solver. \"\n            \"If you want to silence this warning, set 'random_state' \"\n            \"to an integer seed or to None explicitly depending \"\n            \"if you want your code to be deterministic or not.\",\n            FutureWarning,\n        )\n        random_state = 0\n\n    random_state = check_random_state(random_state)\n    n_random = n_components + n_oversamples\n    n_samples, n_features = M.shape\n\n    if n_iter == \"auto\":\n        # Checks if the number of iterations is explicitly specified\n        # Adjust n_iter. 7 was found a good compromise for PCA. See #5299\n        n_iter = 7 if n_components < 0.1 * min(M.shape) else 4\n\n    if transpose == \"auto\":\n        transpose = n_samples < n_features\n    if transpose:\n        # this implementation is a bit faster with smaller shape[1]\n        M = M.T\n\n    Q = randomized_range_finder(\n        M,\n        size=n_random,\n        n_iter=n_iter,\n        power_iteration_normalizer=power_iteration_normalizer,\n        random_state=random_state,\n    )\n\n    # project M to the (k + p) dimensional space using the basis vectors\n    B = safe_sparse_dot(Q.T, M)\n\n    # compute the SVD on the thin matrix: (k + p) wide\n    Uhat, s, Vt = linalg.svd(B, full_matrices=False)\n\n    del B\n    U = np.dot(Q, Uhat)\n\n    if flip_sign:\n        if not transpose:\n            U, Vt = svd_flip(U, Vt)\n        else:\n            # In case of transpose u_based_decision=false\n            # to actually flip based on u and not v.\n            U, Vt = svd_flip(U, Vt, u_based_decision=False)\n\n    if transpose:\n        # transpose back the results according to the input convention\n        return Vt[:n_components, :].T, s[:n_components], U[:, :n_components].T\n    else:\n        return U[:, :n_components], s[:n_components], Vt[:n_components, :]\n\n\ndef _randomized_eigsh(\n    M,\n    n_components,\n    *,\n    n_oversamples=10,\n    n_iter=\"auto\",\n    power_iteration_normalizer=\"auto\",\n    selection=\"module\",\n    random_state=None,\n):\n    \"\"\"Computes a truncated eigendecomposition using randomized methods\n\n    This method solves the fixed-rank approximation problem described in the\n    Halko et al paper.\n\n    The choice of which components to select can be tuned with the `selection`\n    parameter.\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    M : ndarray or sparse matrix\n        Matrix to decompose, it should be real symmetric square or complex\n        hermitian\n\n    n_components : int\n        Number of eigenvalues and vectors to extract.\n\n    n_oversamples : int, default=10\n        Additional number of random vectors to sample the range of M so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of M is n_components + n_oversamples. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of eigenvectors and eigenvalues. Users might wish\n        to increase this parameter up to `2*k - n_components` where k is the\n        effective rank, for large matrices, noisy problems, matrices with\n        slowly decaying spectrums, or to increase precision accuracy. See Halko\n        et al (pages 5, 23 and 26).\n\n    n_iter : int or 'auto', default='auto'\n        Number of power iterations. It can be used to deal with very noisy\n        problems. When 'auto', it is set to 4, unless `n_components` is small\n        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.\n        This improves precision with few components. Note that in general\n        users should rather increase `n_oversamples` before increasing `n_iter`\n        as the principle of the randomized method is to avoid usage of these\n        more costly power iterations steps. When `n_components` is equal\n        or greater to the effective matrix rank and the spectrum does not\n        present a slow decay, `n_iter=0` or `1` should even work fine in theory\n        (see Halko et al paper, page 9).\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n    selection : {'value', 'module'}, default='module'\n        Strategy used to select the n components. When `selection` is `'value'`\n        (not yet implemented, will become the default when implemented), the\n        components corresponding to the n largest eigenvalues are returned.\n        When `selection` is `'module'`, the components corresponding to the n\n        eigenvalues with largest modules are returned.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data, i.e. getting the random vectors to initialize the algorithm.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    eigendecomposition using randomized methods to speed up the computations.\n\n    This method is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    Strategy 'value': not implemented yet.\n    Algorithms 5.3, 5.4 and 5.5 in the Halko et al paper should provide good\n    condidates for a future implementation.\n\n    Strategy 'module':\n    The principle is that for diagonalizable matrices, the singular values and\n    eigenvalues are related: if t is an eigenvalue of A, then :math:`|t|` is a\n    singular value of A. This method relies on a randomized SVD to find the n\n    singular components corresponding to the n singular values with largest\n    modules, and then uses the signs of the singular vectors to find the true\n    sign of t: if the sign of left and right singular vectors are different\n    then the corresponding eigenvalue is negative.\n\n    Returns\n    -------\n    eigvals : 1D array of shape (n_components,) containing the `n_components`\n        eigenvalues selected (see ``selection`` parameter).\n    eigvecs : 2D array of shape (M.shape[0], n_components) containing the\n        `n_components` eigenvectors corresponding to the `eigvals`, in the\n        corresponding order. Note that this follows the `scipy.linalg.eigh`\n        convention.\n\n    See Also\n    --------\n    :func:`randomized_svd`\n\n    References\n    ----------\n    * Finding structure with randomness: Stochastic algorithms for constructing\n      approximate matrix decompositions (Algorithm 4.3 for strategy 'module')\n      Halko, et al., 2009 https://arxiv.org/abs/0909.4061\n\n    \"\"\"\n    if selection == \"value\":  # pragma: no cover\n        # to do : an algorithm can be found in the Halko et al reference\n        raise NotImplementedError()\n\n    elif selection == \"module\":\n        # Note: no need for deterministic U and Vt (flip_sign=True),\n        # as we only use the dot product UVt afterwards\n        U, S, Vt = randomized_svd(\n            M,\n            n_components=n_components,\n            n_oversamples=n_oversamples,\n            n_iter=n_iter,\n            power_iteration_normalizer=power_iteration_normalizer,\n            flip_sign=False,\n            random_state=random_state,\n        )\n\n        eigvecs = U[:, :n_components]\n        eigvals = S[:n_components]\n\n        # Conversion of Singular values into Eigenvalues:\n        # For any eigenvalue t, the corresponding singular value is |t|.\n        # So if there is a negative eigenvalue t, the corresponding singular\n        # value will be -t, and the left (U) and right (V) singular vectors\n        # will have opposite signs.\n        # Fastest way: see <https://stackoverflow.com/a/61974002/7262247>\n        diag_VtU = np.einsum(\"ji,ij->j\", Vt[:n_components, :], U[:, :n_components])\n        signs = np.sign(diag_VtU)\n        eigvals = eigvals * signs\n\n    else:  # pragma: no cover\n        raise ValueError(\"Invalid `selection`: %r\" % selection)\n\n    return eigvals, eigvecs\n\n\ndef weighted_mode(a, w, *, axis=0):\n    \"\"\"Returns an array of the weighted modal (most common) value in a.\n\n    If there is more than one such value, only the first is returned.\n    The bin-count for the modal bins is also returned.\n\n    This is an extension of the algorithm in scipy.stats.mode.\n\n    Parameters\n    ----------\n    a : array-like\n        n-dimensional array of which to find mode(s).\n    w : array-like\n        n-dimensional array of weights for each value.\n    axis : int, default=0\n        Axis along which to operate. Default is 0, i.e. the first axis.\n\n    Returns\n    -------\n    vals : ndarray\n        Array of modal values.\n    score : ndarray\n        Array of weighted counts for each mode.\n\n    Examples\n    --------\n    >>> from sklearn.utils.extmath import weighted_mode\n    >>> x = [4, 1, 4, 2, 4, 2]\n    >>> weights = [1, 1, 1, 1, 1, 1]\n    >>> weighted_mode(x, weights)\n    (array([4.]), array([3.]))\n\n    The value 4 appears three times: with uniform weights, the result is\n    simply the mode of the distribution.\n\n    >>> weights = [1, 3, 0.5, 1.5, 1, 2]  # deweight the 4's\n    >>> weighted_mode(x, weights)\n    (array([2.]), array([3.5]))\n\n    The value 2 has the highest score: it appears twice with weights of\n    1.5 and 2: the sum of these is 3.5.\n\n    See Also\n    --------\n    scipy.stats.mode\n    \"\"\"\n    if axis is None:\n        a = np.ravel(a)\n        w = np.ravel(w)\n        axis = 0\n    else:\n        a = np.asarray(a)\n        w = np.asarray(w)\n\n    if a.shape != w.shape:\n        w = np.full(a.shape, w, dtype=w.dtype)\n\n    scores = np.unique(np.ravel(a))  # get ALL unique values\n    testshape = list(a.shape)\n    testshape[axis] = 1\n    oldmostfreq = np.zeros(testshape)\n    oldcounts = np.zeros(testshape)\n    for score in scores:\n        template = np.zeros(a.shape)\n        ind = a == score\n        template[ind] = w[ind]\n        counts = np.expand_dims(np.sum(template, axis), axis)\n        mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)\n        oldcounts = np.maximum(counts, oldcounts)\n        oldmostfreq = mostfrequent\n    return mostfrequent, oldcounts\n\n\ndef cartesian(arrays, out=None):\n    \"\"\"Generate a cartesian product of input arrays.\n\n    Parameters\n    ----------\n    arrays : list of array-like\n        1-D arrays to form the cartesian product of.\n    out : ndarray, default=None\n        Array to place the cartesian product in.\n\n    Returns\n    -------\n    out : ndarray\n        2-D array of shape (M, len(arrays)) containing cartesian products\n        formed of input arrays.\n\n    Examples\n    --------\n    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n    array([[1, 4, 6],\n           [1, 4, 7],\n           [1, 5, 6],\n           [1, 5, 7],\n           [2, 4, 6],\n           [2, 4, 7],\n           [2, 5, 6],\n           [2, 5, 7],\n           [3, 4, 6],\n           [3, 4, 7],\n           [3, 5, 6],\n           [3, 5, 7]])\n\n    Notes\n    -----\n    This function may not be used on more than 32 arrays\n    because the underlying numpy functions do not support it.\n    \"\"\"\n    arrays = [np.asarray(x) for x in arrays]\n    shape = (len(x) for x in arrays)\n    dtype = arrays[0].dtype\n\n    ix = np.indices(shape)\n    ix = ix.reshape(len(arrays), -1).T\n\n    if out is None:\n        out = np.empty_like(ix, dtype=dtype)\n\n    for n, arr in enumerate(arrays):\n        out[:, n] = arrays[n][ix[:, n]]\n\n    return out\n\n\ndef svd_flip(u, v, u_based_decision=True):\n    \"\"\"Sign correction to ensure deterministic output from SVD.\n\n    Adjusts the columns of u and the rows of v such that the loadings in the\n    columns in u that are largest in absolute value are always positive.\n\n    Parameters\n    ----------\n    u : ndarray\n        u and v are the output of `linalg.svd` or\n        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`.\n\n    v : ndarray\n        u and v are the output of `linalg.svd` or\n        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`.\n        The input v should really be called vt to be consistent with scipy's\n        ouput.\n\n    u_based_decision : bool, default=True\n        If True, use the columns of u as the basis for sign flipping.\n        Otherwise, use the rows of v. The choice of which variable to base the\n        decision on is generally algorithm dependent.\n\n\n    Returns\n    -------\n    u_adjusted, v_adjusted : arrays with the same dimensions as the input.\n\n    \"\"\"\n    if u_based_decision:\n        # columns of u, rows of v\n        max_abs_cols = np.argmax(np.abs(u), axis=0)\n        signs = np.sign(u[max_abs_cols, range(u.shape[1])])\n        u *= signs\n        v *= signs[:, np.newaxis]\n    else:\n        # rows of v, columns of u\n        max_abs_rows = np.argmax(np.abs(v), axis=1)\n        signs = np.sign(v[range(v.shape[0]), max_abs_rows])\n        u *= signs\n        v *= signs[:, np.newaxis]\n    return u, v\n\n\ndef log_logistic(X, out=None):\n    \"\"\"Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.\n\n    This implementation is numerically stable because it splits positive and\n    negative values::\n\n        -log(1 + exp(-x_i))     if x_i > 0\n        x_i - log(1 + exp(x_i)) if x_i <= 0\n\n    For the ordinary logistic function, use ``scipy.special.expit``.\n\n    Parameters\n    ----------\n    X : array-like of shape (M, N) or (M,)\n        Argument to the logistic function.\n\n    out : array-like of shape (M, N) or (M,), default=None\n        Preallocated output array.\n\n    Returns\n    -------\n    out : ndarray of shape (M, N) or (M,)\n        Log of the logistic function evaluated at every point in x.\n\n    Notes\n    -----\n    See the blog post describing this implementation:\n    http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/\n    \"\"\"\n    is_1d = X.ndim == 1\n    X = np.atleast_2d(X)\n    X = check_array(X, dtype=np.float64)\n\n    n_samples, n_features = X.shape\n\n    if out is None:\n        out = np.empty_like(X)\n\n    _log_logistic_sigmoid(n_samples, n_features, X, out)\n\n    if is_1d:\n        return np.squeeze(out)\n    return out\n\n\ndef softmax(X, copy=True):\n    \"\"\"\n    Calculate the softmax function.\n\n    The softmax function is calculated by\n    np.exp(X) / np.sum(np.exp(X), axis=1)\n\n    This will cause overflow when large values are exponentiated.\n    Hence the largest value in each row is subtracted from each data\n    point to prevent this.\n\n    Parameters\n    ----------\n    X : array-like of float of shape (M, N)\n        Argument to the logistic function.\n\n    copy : bool, default=True\n        Copy X or not.\n\n    Returns\n    -------\n    out : ndarray of shape (M, N)\n        Softmax function evaluated at every point in x.\n    \"\"\"\n    if copy:\n        X = np.copy(X)\n    max_prob = np.max(X, axis=1).reshape((-1, 1))\n    X -= max_prob\n    np.exp(X, X)\n    sum_prob = np.sum(X, axis=1).reshape((-1, 1))\n    X /= sum_prob\n    return X\n\n\ndef make_nonnegative(X, min_value=0):\n    \"\"\"Ensure `X.min()` >= `min_value`.\n\n    Parameters\n    ----------\n    X : array-like\n        The matrix to make non-negative.\n    min_value : float, default=0\n        The threshold value.\n\n    Returns\n    -------\n    array-like\n        The thresholded array.\n\n    Raises\n    ------\n    ValueError\n        When X is sparse.\n    \"\"\"\n    min_ = X.min()\n    if min_ < min_value:\n        if sparse.issparse(X):\n            raise ValueError(\n                \"Cannot make the data matrix\"\n                \" nonnegative because it is sparse.\"\n                \" Adding a value to every entry would\"\n                \" make it no longer sparse.\"\n            )\n        X = X + (min_value - min_)\n    return X\n\n\n# Use at least float64 for the accumulating functions to avoid precision issue\n# see https://github.com/numpy/numpy/issues/9393. The float64 is also retained\n# as it is in case the float overflows\ndef _safe_accumulator_op(op, x, *args, **kwargs):\n    \"\"\"\n    This function provides numpy accumulator functions with a float64 dtype\n    when used on a floating point input. This prevents accumulator overflow on\n    smaller floating point dtypes.\n\n    Parameters\n    ----------\n    op : function\n        A numpy accumulator function such as np.mean or np.sum.\n    x : ndarray\n        A numpy array to apply the accumulator function.\n    *args : positional arguments\n        Positional arguments passed to the accumulator function after the\n        input x.\n    **kwargs : keyword arguments\n        Keyword arguments passed to the accumulator function.\n\n    Returns\n    -------\n    result\n        The output of the accumulator function passed to this function.\n    \"\"\"\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result\n\n\ndef _incremental_mean_and_var(\n    X, last_mean, last_variance, last_sample_count, sample_weight=None\n):\n    \"\"\"Calculate mean update and a Youngs and Cramer variance update.\n\n    If sample_weight is given, the weighted mean and variance is computed.\n\n    Update a given mean and (possibly) variance according to new data given\n    in X. last_mean is always required to compute the new mean.\n    If last_variance is None, no variance is computed and None return for\n    updated_variance.\n\n    From the paper \"Algorithms for computing the sample variance: analysis and\n    recommendations\", by Chan, Golub, and LeVeque.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data to use for variance update.\n\n    last_mean : array-like of shape (n_features,)\n\n    last_variance : array-like of shape (n_features,)\n\n    last_sample_count : array-like of shape (n_features,)\n        The number of samples encountered until now if sample_weight is None.\n        If sample_weight is not None, this is the sum of sample_weight\n        encountered.\n\n    sample_weight : array-like of shape (n_samples,) or None\n        Sample weights. If None, compute the unweighted mean/variance.\n\n    Returns\n    -------\n    updated_mean : ndarray of shape (n_features,)\n\n    updated_variance : ndarray of shape (n_features,)\n        None if last_variance was None.\n\n    updated_sample_count : ndarray of shape (n_features,)\n\n    Notes\n    -----\n    NaNs are ignored during the algorithm.\n\n    References\n    ----------\n    T. Chan, G. Golub, R. LeVeque. Algorithms for computing the sample\n        variance: recommendations, The American Statistician, Vol. 37, No. 3,\n        pp. 242-247\n\n    Also, see the sparse implementation of this in\n    `utils.sparsefuncs.incr_mean_variance_axis` and\n    `utils.sparsefuncs_fast.incr_mean_variance_axis0`\n    \"\"\"\n    # old = stats until now\n    # new = the current increment\n    # updated = the aggregated stats\n    last_sum = last_mean * last_sample_count\n    if sample_weight is not None:\n        if np_version >= parse_version(\"1.16.6\"):\n            # equivalent to np.nansum(X * sample_weight, axis=0)\n            # safer because np.float64(X*W) != np.float64(X)*np.float64(W)\n            # dtype arg of np.matmul only exists since version 1.16\n            new_sum = _safe_accumulator_op(\n                np.matmul, sample_weight, np.where(np.isnan(X), 0, X)\n            )\n        else:\n            new_sum = _safe_accumulator_op(\n                np.nansum, X * sample_weight[:, None], axis=0\n            )\n        new_sample_count = _safe_accumulator_op(\n            np.sum, sample_weight[:, None] * (~np.isnan(X)), axis=0\n        )\n    else:\n        new_sum = _safe_accumulator_op(np.nansum, X, axis=0)\n        new_sample_count = np.sum(~np.isnan(X), axis=0)\n\n    updated_sample_count = last_sample_count + new_sample_count\n\n    updated_mean = (last_sum + new_sum) / updated_sample_count\n\n    if last_variance is None:\n        updated_variance = None\n    else:\n        T = new_sum / new_sample_count\n        if sample_weight is not None:\n            if np_version >= parse_version(\"1.16.6\"):\n                # equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\n                # safer because np.float64(X*W) != np.float64(X)*np.float64(W)\n                # dtype arg of np.matmul only exists since version 1.16\n                new_unnormalized_variance = _safe_accumulator_op(\n                    np.matmul, sample_weight, np.where(np.isnan(X), 0, (X - T) ** 2)\n                )\n                correction = _safe_accumulator_op(\n                    np.matmul, sample_weight, np.where(np.isnan(X), 0, X - T)\n                )\n            else:\n                new_unnormalized_variance = _safe_accumulator_op(\n                    np.nansum, (X - T) ** 2 * sample_weight[:, None], axis=0\n                )\n                correction = _safe_accumulator_op(\n                    np.nansum, (X - T) * sample_weight[:, None], axis=0\n                )\n        else:\n            new_unnormalized_variance = _safe_accumulator_op(\n                np.nansum, (X - T) ** 2, axis=0\n            )\n            correction = _safe_accumulator_op(np.nansum, X - T, axis=0)\n\n        # correction term of the corrected 2 pass algorithm.\n        # See \"Algorithms for computing the sample variance: analysis\n        # and recommendations\", by Chan, Golub, and LeVeque.\n        new_unnormalized_variance -= correction ** 2 / new_sample_count\n\n        last_unnormalized_variance = last_variance * last_sample_count\n\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            last_over_new_count = last_sample_count / new_sample_count\n            updated_unnormalized_variance = (\n                last_unnormalized_variance\n                + new_unnormalized_variance\n                + last_over_new_count\n                / updated_sample_count\n                * (last_sum / last_over_new_count - new_sum) ** 2\n            )\n\n        zeros = last_sample_count == 0\n        updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]\n        updated_variance = updated_unnormalized_variance / updated_sample_count\n\n    return updated_mean, updated_variance, updated_sample_count\n\n\ndef _deterministic_vector_sign_flip(u):\n    \"\"\"Modify the sign of vectors for reproducibility.\n\n    Flips the sign of elements of all the vectors (rows of u) such that\n    the absolute maximum element of each vector is positive.\n\n    Parameters\n    ----------\n    u : ndarray\n        Array with vectors as its rows.\n\n    Returns\n    -------\n    u_flipped : ndarray with same shape as u\n        Array with the sign flipped vectors as its rows.\n    \"\"\"\n    max_abs_rows = np.argmax(np.abs(u), axis=1)\n    signs = np.sign(u[range(u.shape[0]), max_abs_rows])\n    u *= signs[:, np.newaxis]\n    return u\n\n\ndef stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):\n    \"\"\"Use high precision for cumsum and check that final value matches sum.\n\n    Parameters\n    ----------\n    arr : array-like\n        To be cumulatively summed as flat.\n    axis : int, default=None\n        Axis along which the cumulative sum is computed.\n        The default (None) is to compute the cumsum over the flattened array.\n    rtol : float, default=1e-05\n        Relative tolerance, see ``np.allclose``.\n    atol : float, default=1e-08\n        Absolute tolerance, see ``np.allclose``.\n    \"\"\"\n    out = np.cumsum(arr, axis=axis, dtype=np.float64)\n    expected = np.sum(arr, axis=axis, dtype=np.float64)\n    if not np.all(\n        np.isclose(\n            out.take(-1, axis=axis), expected, rtol=rtol, atol=atol, equal_nan=True\n        )\n    ):\n        warnings.warn(\n            \"cumsum was found to be unstable: \"\n            \"its last element does not correspond to sum\",\n            RuntimeWarning,\n        )\n    return out\n",1082],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py":["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.  abstractmethod() may be used to declare\n    abstract methods for properties and descriptors.\n\n    Usage:\n\n        class C(metaclass=ABCMeta):\n            @abstractmethod\n            def my_abstract_method(self, ...):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractclassmethod(classmethod):\n    \"\"\"A decorator indicating abstract classmethods.\n\n    Deprecated, use 'classmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @classmethod\n            @abstractmethod\n            def my_abstract_classmethod(cls, ...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractstaticmethod(staticmethod):\n    \"\"\"A decorator indicating abstract staticmethods.\n\n    Deprecated, use 'staticmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @staticmethod\n            @abstractmethod\n            def my_abstract_staticmethod(...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Deprecated, use 'property' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @property\n            @abstractmethod\n            def my_abstract_property(self):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n\ntry:\n    from _abc import (get_cache_token, _abc_init, _abc_register,\n                      _abc_instancecheck, _abc_subclasscheck, _get_dump,\n                      _reset_registry, _reset_caches)\nexcept ImportError:\n    from _py_abc import ABCMeta, get_cache_token\n    ABCMeta.__module__ = 'abc'\nelse:\n    class ABCMeta(type):\n        \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n        Use this metaclass to create an ABC.  An ABC can be subclassed\n        directly, and then acts as a mix-in class.  You can also register\n        unrelated concrete classes (even built-in classes) and unrelated\n        ABCs as 'virtual subclasses' -- these and their descendants will\n        be considered subclasses of the registering ABC by the built-in\n        issubclass() function, but the registering ABC won't show up in\n        their MRO (Method Resolution Order) nor will method\n        implementations defined by the registering ABC be callable (not\n        even via super()).\n        \"\"\"\n        def __new__(mcls, name, bases, namespace, **kwargs):\n            cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n            _abc_init(cls)\n            return cls\n\n        def register(cls, subclass):\n            \"\"\"Register a virtual subclass of an ABC.\n\n            Returns the subclass, to allow usage as a class decorator.\n            \"\"\"\n            return _abc_register(cls, subclass)\n\n        def __instancecheck__(cls, instance):\n            \"\"\"Override for isinstance(instance, cls).\"\"\"\n            return _abc_instancecheck(cls, instance)\n\n        def __subclasscheck__(cls, subclass):\n            \"\"\"Override for issubclass(subclass, cls).\"\"\"\n            return _abc_subclasscheck(cls, subclass)\n\n        def _dump_registry(cls, file=None):\n            \"\"\"Debug helper to print the ABC registry.\"\"\"\n            print(f\"Class: {cls.__module__}.{cls.__qualname__}\", file=file)\n            print(f\"Inv. counter: {get_cache_token()}\", file=file)\n            (_abc_registry, _abc_cache, _abc_negative_cache,\n             _abc_negative_cache_version) = _get_dump(cls)\n            print(f\"_abc_registry: {_abc_registry!r}\", file=file)\n            print(f\"_abc_cache: {_abc_cache!r}\", file=file)\n            print(f\"_abc_negative_cache: {_abc_negative_cache!r}\", file=file)\n            print(f\"_abc_negative_cache_version: {_abc_negative_cache_version!r}\",\n                  file=file)\n\n        def _abc_registry_clear(cls):\n            \"\"\"Clear the registry (for debugging or testing).\"\"\"\n            _reset_registry(cls)\n\n        def _abc_caches_clear(cls):\n            \"\"\"Clear the caches (for debugging or testing).\"\"\"\n            _reset_caches(cls)\n\n\nclass ABC(metaclass=ABCMeta):\n    \"\"\"Helper class that provides a standard way to create an ABC using\n    inheritance.\n    \"\"\"\n    __slots__ = ()\n",150],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/_utils.py":["#===============================================================================\n# Copyright 2014-2021 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#===============================================================================\n\nimport numpy as np\n\nfrom daal4py import _get__daal_link_version__ as dv\nfrom sklearn import __version__ as sklearn_version\nfrom distutils.version import LooseVersion\n\n\ndef set_idp_sklearn_verbose():\n    import logging\n    import warnings\n    import os\n    import sys\n    logLevel = os.environ.get(\"IDP_SKLEARN_VERBOSE\")\n    try:\n        if logLevel is not None:\n            logging.basicConfig(\n                stream=sys.stdout,\n                format='%(levelname)s: %(message)s', level=logLevel.upper())\n    except Exception:\n        warnings.warn('Unknown level \"{}\" for logging.\\n'\n                      'Please, use one of \"CRITICAL\", \"ERROR\", '\n                      '\"WARNING\", \"INFO\", \"DEBUG\".'.format(logLevel))\n\n\ndef daal_check_version(rule):\n    # First item is major version - 2021,\n    # second is minor+patch - 0110,\n    # third item is status - B\n    target = (int(dv()[0:4]), dv()[10:11], int(dv()[4:8]))\n    if not isinstance(rule[0], type(target)):\n        if rule > target:\n            return False\n    else:\n        for rule_item in rule:\n            if rule_item > target:\n                return False\n            if rule_item[0] == target[0]:\n                break\n    return True\n\n\ndef sklearn_check_version(ver):\n    return bool(LooseVersion(sklearn_version) >= LooseVersion(ver))\n\n\ndef get_daal_version():\n    return (int(dv()[0:4]), dv()[10:11], int(dv()[4:8]))\n\n\ndef parse_dtype(dt):\n    if dt == np.double:\n        return \"double\"\n    elif dt == np.single:\n        return \"float\"\n    raise ValueError(f\"Input array has unexpected dtype = {dt}\")\n\n\ndef getFPType(X):\n    try:\n        from pandas import DataFrame\n        from pandas.core.dtypes.cast import find_common_type\n        if isinstance(X, DataFrame):\n            dt = find_common_type(X.dtypes)\n            return parse_dtype(dt)\n    except ImportError:\n        pass\n\n    dt = getattr(X, 'dtype', None)\n    return parse_dtype(dt)\n\n\ndef make2d(X):\n    if np.isscalar(X):\n        X = np.asarray(X)[np.newaxis, np.newaxis]\n    elif isinstance(X, np.ndarray) and X.ndim == 1:\n        X = X.reshape((X.size, 1))\n    return X\n\n\ndef get_patch_message(s):\n    import sys\n    if s == \"daal\":\n        message = \"running accelerated version on \"\n        if 'daal4py.oneapi' in sys.modules:\n            from daal4py.oneapi import _get_device_name_sycl_ctxt\n            dev = _get_device_name_sycl_ctxt()\n            if dev == 'cpu' or dev == 'host' or dev is None:\n                message += 'CPU'\n            elif dev == 'gpu':\n                message += 'GPU'\n            else:\n                raise ValueError(f\"Unexpected device name {dev}.\"\n                                 \" Supported types are host, cpu and gpu\")\n        else:\n            message += 'CPU'\n\n    elif s == \"sklearn\":\n        message = \"fallback to original Scikit-learn\"\n    elif s == \"sklearn_after_daal\":\n        message = \"failed to run accelerated version, fallback to original Scikit-learn\"\n    else:\n        raise ValueError(\n            f\"Invalid input - expected one of 'daal','sklearn',\"\n            f\" 'sklearn_after_daal', got {s}\")\n    return message\n\n\ndef is_in_sycl_ctxt():\n    try:\n        from daal4py.oneapi import is_in_sycl_ctxt as is_in_ctx\n        return is_in_ctx()\n    except ModuleNotFoundError:\n        return False\n\n\ndef is_DataFrame(X):\n    try:\n        from pandas import DataFrame\n        return isinstance(X, DataFrame)\n    except ImportError:\n        return False\n\n\ndef get_dtype(X):\n    try:\n        from pandas.core.dtypes.cast import find_common_type\n        return find_common_type(X.dtypes) if is_DataFrame(X) else X.dtype\n    except ImportError:\n        return getattr(X, \"dtype\", None)\n\n\ndef get_number_of_types(dataframe):\n    dtypes = getattr(dataframe, \"dtypes\", None)\n    if dtypes is None or isinstance(dtypes, np.dtype):\n        return 1\n    return len(set(dtypes))\n",152],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py":["# Copyright 2001-2019 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2019 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n\nfrom string import Template\nfrom string import Formatter as StrFormatter\n\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',\n           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',\n           'lastResort', 'raiseExceptions']\n\nimport threading\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# The following module attributes are no longer updated.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = True\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = True\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = True\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = True\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'FATAL': FATAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual or numeric representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    If a string representation of the level is passed in, the corresponding\n    numeric value is returned.\n\n    If no matching numeric or string value is passed in, the string\n    'Level %s' % level is returned.\n    \"\"\"\n    # See Issues #22386, #27937 and #29220 for why it's this way\n    result = _levelToName.get(level)\n    if result is not None:\n        return result\n    result = _nameToLevel.get(level)\n    if result is not None:\n        return result\n    return \"Level %s\" % level\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\nif hasattr(sys, '_getframe'):\n    currentframe = lambda: sys._getframe(3)\nelse: #pragma: no cover\n    def currentframe():\n        \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n        try:\n            raise Exception\n        except Exception:\n            return sys.exc_info()[2].tb_frame.f_back\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame, by skipping frames whose filename is that of this\n# module's source. It therefore should contain the filename of this module's\n# source file.\n#\n# Ordinarily we would use __file__ for this, but frozen modules don't always\n# have __file__ set, for some reason (see Issue #21736). Thus, we get the\n# filename from a handy code object from a function defined in this module.\n# (There's no particular reason for picking addLevelName.)\n#\n\n_srcfile = os.path.normcase(addLevelName.__code__.co_filename)\n\n# _srcfile is only used in conjunction with sys._getframe().\n# To provide compatibility with older versions of Python, set _srcfile\n# to None if _getframe() is not available; this value will prevent\n# findCaller() from being called. You can also do this if you want to avoid\n# the overhead of fetching caller information, even when _getframe() is\n# available.\n#if not hasattr(sys, '_getframe'):\n#    _srcfile = None\n\n\ndef _checkLevel(level):\n    if isinstance(level, int):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\" % level)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\n_lock = threading.RLock()\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n\n# Prevent a held logging lock from blocking a child from logging.\n\nif not hasattr(os, 'register_at_fork'):  # Windows and friends.\n    def _register_at_fork_reinit_lock(instance):\n        pass  # no-op when os.register_at_fork does not exist.\nelse:\n    # A collection of instances with a _at_fork_reinit method (logging.Handler)\n    # to be called in the child after forking.  The weakref avoids us keeping\n    # discarded Handler instances alive.\n    _at_fork_reinit_lock_weakset = weakref.WeakSet()\n\n    def _register_at_fork_reinit_lock(instance):\n        _acquireLock()\n        try:\n            _at_fork_reinit_lock_weakset.add(instance)\n        finally:\n            _releaseLock()\n\n    def _after_at_fork_child_reinit_locks():\n        for handler in _at_fork_reinit_lock_weakset:\n            handler._at_fork_reinit()\n\n        # _acquireLock() was called in the parent before forking.\n        # The lock is reinitialized to unlocked state.\n        _lock._at_fork_reinit()\n\n    os.register_at_fork(before=_acquireLock,\n                        after_in_child=_after_at_fork_child_reinit_locks,\n                        after_in_parent=_releaseLock)\n\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None, sinfo=None, **kwargs):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warning('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.abc.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.stack_info = sinfo\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = (ct - int(ct)) * 1000\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads:\n            self.thread = threading.get_ident()\n            self.threadName = threading.current_thread().name\n        else: # pragma: no cover\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing: # pragma: no cover\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except Exception: #pragma: no cover\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __repr__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n#\n#   Determine which class to use when instantiating log records.\n#\n_logRecordFactory = LogRecord\n\ndef setLogRecordFactory(factory):\n    \"\"\"\n    Set the factory to be used when instantiating a log record.\n\n    :param factory: A callable which will be called to instantiate\n    a log record.\n    \"\"\"\n    global _logRecordFactory\n    _logRecordFactory = factory\n\ndef getLogRecordFactory():\n    \"\"\"\n    Return the factory to be used when instantiating a log record.\n    \"\"\"\n\n    return _logRecordFactory\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = _logRecordFactory(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n_str_formatter = StrFormatter()\ndel StrFormatter\n\n\nclass PercentStyle(object):\n\n    default_format = '%(message)s'\n    asctime_format = '%(asctime)s'\n    asctime_search = '%(asctime)'\n    validation_pattern = re.compile(r'%\\(\\w+\\)[#0+ -]*(\\*|\\d+)?(\\.(\\*|\\d+))?[diouxefgcrsa%]', re.I)\n\n    def __init__(self, fmt):\n        self._fmt = fmt or self.default_format\n\n    def usesTime(self):\n        return self._fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it matches the correct style\"\"\"\n        if not self.validation_pattern.search(self._fmt):\n            raise ValueError(\"Invalid format '%s' for '%s' style\" % (self._fmt, self.default_format[0]))\n\n    def _format(self, record):\n        return self._fmt % record.__dict__\n\n    def format(self, record):\n        try:\n            return self._format(record)\n        except KeyError as e:\n            raise ValueError('Formatting field not found in record: %s' % e)\n\n\nclass StrFormatStyle(PercentStyle):\n    default_format = '{message}'\n    asctime_format = '{asctime}'\n    asctime_search = '{asctime'\n\n    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\\d+|{\\w+})?[,_]?(\\.(\\d+|{\\w+}))?[bcdefgnosx%]?$', re.I)\n    field_spec = re.compile(r'^(\\d+|\\w+)(\\.\\w+|\\[[^]]+\\])*$')\n\n    def _format(self, record):\n        return self._fmt.format(**record.__dict__)\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it is the correct string formatting style\"\"\"\n        fields = set()\n        try:\n            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):\n                if fieldname:\n                    if not self.field_spec.match(fieldname):\n                        raise ValueError('invalid field name/expression: %r' % fieldname)\n                    fields.add(fieldname)\n                if conversion and conversion not in 'rsa':\n                    raise ValueError('invalid conversion: %r' % conversion)\n                if spec and not self.fmt_spec.match(spec):\n                    raise ValueError('bad specifier: %r' % spec)\n        except ValueError as e:\n            raise ValueError('invalid format: %s' % e)\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n\nclass StringTemplateStyle(PercentStyle):\n    default_format = '${message}'\n    asctime_format = '${asctime}'\n    asctime_search = '${asctime}'\n\n    def __init__(self, fmt):\n        self._fmt = fmt or self.default_format\n        self._tpl = Template(self._fmt)\n\n    def usesTime(self):\n        fmt = self._fmt\n        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_format) >= 0\n\n    def validate(self):\n        pattern = Template.pattern\n        fields = set()\n        for m in pattern.finditer(self._fmt):\n            d = m.groupdict()\n            if d['named']:\n                fields.add(d['named'])\n            elif d['braced']:\n                fields.add(d['braced'])\n            elif m.group(0) == '$':\n                raise ValueError('invalid format: bare \\'$\\' not allowed')\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n    def _format(self, record):\n        return self._tpl.substitute(**record.__dict__)\n\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\n_STYLES = {\n    '%': (PercentStyle, BASIC_FORMAT),\n    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),\n    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),\n}\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None, style='%', validate=True):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument. If datefmt is omitted, you get an\n        ISO8601-like (or RFC 3339-like) format.\n\n        Use a style parameter of '%', '{' or '$' to specify that you want to\n        use one of %-formatting, :meth:`str.format` (``{}``) formatting or\n        :class:`string.Template` formatting in your format string.\n\n        .. versionchanged:: 3.2\n           Added the ``style`` parameter.\n        \"\"\"\n        if style not in _STYLES:\n            raise ValueError('Style must be one of: %s' % ','.join(\n                             _STYLES.keys()))\n        self._style = _STYLES[style][0](fmt)\n        if validate:\n            self._style.validate()\n\n        self._fmt = self._style._fmt\n        self.datefmt = datefmt\n\n    default_time_format = '%Y-%m-%d %H:%M:%S'\n    default_msec_format = '%s,%03d'\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.\n        The resulting string is returned. This function uses a user-configurable\n        function to convert the creation time to a tuple. By default,\n        time.localtime() is used; to change this for a particular formatter\n        instance, set the 'converter' attribute to a function with the same\n        signature as time.localtime() or time.gmtime(). To change it for all\n        formatters, for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            s = time.strftime(self.default_time_format, ct)\n            if self.default_msec_format:\n                s = self.default_msec_format % (s, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = io.StringIO()\n        tb = ei[2]\n        # See issues #9427, #1553375. Commented out for now.\n        #if getattr(self, 'fullstack', False):\n        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)\n        traceback.print_exception(ei[0], ei[1], tb, None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._style.usesTime()\n\n    def formatMessage(self, record):\n        return self._style.format(record)\n\n    def formatStack(self, stack_info):\n        \"\"\"\n        This method is provided as an extension point for specialized\n        formatting of stack information.\n\n        The input data is a string as returned from a call to\n        :func:`traceback.print_stack`, but with the last trailing newline\n        removed.\n\n        The base implementation just returns the value passed in.\n        \"\"\"\n        return stack_info\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Returns True if the record should be logged, or False otherwise.\n        If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return True\n        elif self.name == record.name:\n            return True\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return False\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n\n        .. versionchanged:: 3.2\n\n           Allow filters to be just callables.\n        \"\"\"\n        rv = True\n        for f in self.filters:\n            if hasattr(f, 'filter'):\n                result = f.filter(record)\n            else:\n                result = f(record) # assume callable - will raise if not\n            if not result:\n                rv = False\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            if wr in handlers:\n                handlers.remove(wr)\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        self.lock = threading.RLock()\n        _register_at_fork_reinit_lock(self)\n\n    def _at_fork_reinit(self):\n        self.lock._at_fork_reinit()\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            t, v, tb = sys.exc_info()\n            try:\n                sys.stderr.write('--- Logging error ---\\n')\n                traceback.print_exception(t, v, tb, None, sys.stderr)\n                sys.stderr.write('Call stack:\\n')\n                # Walk the stack frame up until we're out of logging,\n                # so as to print the calling context.\n                frame = tb.tb_frame\n                while (frame and os.path.dirname(frame.f_code.co_filename) ==\n                       __path__[0]):\n                    frame = frame.f_back\n                if frame:\n                    traceback.print_stack(frame, file=sys.stderr)\n                else:\n                    # couldn't find the right stack frame, for some reason\n                    sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                     record.filename, record.lineno))\n                # Issue 18671: output logging message and arguments\n                try:\n                    sys.stderr.write('Message: %r\\n'\n                                     'Arguments: %s\\n' % (record.msg,\n                                                          record.args))\n                except RecursionError:  # See issue 36272\n                    raise\n                except Exception:\n                    sys.stderr.write('Unable to print the message and arguments'\n                                     ' - possible formatting error.\\nUse the'\n                                     ' traceback above to help find the error.\\n'\n                                    )\n            except OSError: #pragma: no cover\n                pass    # see issue 5971\n            finally:\n                del t, v, tb\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s (%s)>' % (self.__class__.__name__, level)\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    terminator = '\\n'\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            # issue 35046: merged two stream.writes into one.\n            stream.write(msg + self.terminator)\n            self.flush()\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n    def setStream(self, stream):\n        \"\"\"\n        Sets the StreamHandler's stream to the specified value,\n        if it is different.\n\n        Returns the old stream, if the stream was changed, or None\n        if it wasn't.\n        \"\"\"\n        if stream is self.stream:\n            result = None\n        else:\n            result = self.stream\n            self.acquire()\n            try:\n                self.flush()\n                self.stream = stream\n            finally:\n                self.release()\n        return result\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        name = getattr(self.stream, 'name', '')\n        #  bpo-36015: name can be an int\n        name = str(name)\n        if name:\n            name += ' '\n        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)\n\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=False, errors=None):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        # Issue #27493: add support for Path objects to be passed in\n        filename = os.fspath(filename)\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        self.errors = errors\n        self.delay = delay\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        return open(self.baseFilename, self.mode, encoding=self.encoding,\n                    errors=self.errors)\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n        \"\"\"\n        if self.stream is None:\n            self.stream = self._open()\n        StreamHandler.emit(self, record)\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)\n\n\nclass _StderrHandler(StreamHandler):\n    \"\"\"\n    This class is like a StreamHandler using sys.stderr, but always uses\n    whatever sys.stderr is currently set to rather than the value of\n    sys.stderr at handler construction time.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initialize the handler.\n        \"\"\"\n        Handler.__init__(self, level)\n\n    @property\n    def stream(self):\n        return sys.stderr\n\n\n_defaultLastResort = _StderrHandler(WARNING)\nlastResort = _defaultLastResort\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        if alogger not in self.loggerMap:\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = False\n        self.loggerDict = {}\n        self.loggerClass = None\n        self.logRecordFactory = None\n\n    @property\n    def disable(self):\n        return self._disable\n\n    @disable.setter\n    def disable(self, value):\n        self._disable = _checkLevel(value)\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, str):\n            raise TypeError('A logger name must be a string')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def setLogRecordFactory(self, factory):\n        \"\"\"\n        Set the factory to be used when instantiating a log record with this\n        Manager.\n        \"\"\"\n        self.logRecordFactory = factory\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n    def _clear_cache(self):\n        \"\"\"\n        Clear the cache for all loggers in loggerDict\n        Called when level changes are made\n        \"\"\"\n\n        _acquireLock()\n        for logger in self.loggerDict.values():\n            if isinstance(logger, Logger):\n                logger._cache.clear()\n        self.root._cache.clear()\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = True\n        self.handlers = []\n        self.disabled = False\n        self._cache = {}\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n        self.manager._clear_cache()\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        self.error(msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    fatal = critical\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self, stack_info=False, stacklevel=1):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        orig_f = f\n        while f and stacklevel > 1:\n            f = f.f_back\n            stacklevel -= 1\n        if not f:\n            f = orig_f\n        rv = \"(unknown file)\", 0, \"(unknown function)\", None\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            if filename == _srcfile:\n                f = f.f_back\n                continue\n            sinfo = None\n            if stack_info:\n                sio = io.StringIO()\n                sio.write('Stack (most recent call last):\\n')\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == '\\n':\n                    sinfo = sinfo[:-1]\n                sio.close()\n            rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\n            break\n        return rv\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n                             sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n             stacklevel=1):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        sinfo = None\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n            except ValueError: # pragma: no cover\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else: # pragma: no cover\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if isinstance(exc_info, BaseException):\n                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)\n            elif not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args,\n                                 exc_info, func, extra, sinfo)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if this logger has any handlers configured.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. Return True if a handler was found, else False.\n        Stop searching up the hierarchy whenever a logger with the \"propagate\"\n        attribute set to zero is found - that will be the last logger which\n        is checked for the existence of handlers.\n        \"\"\"\n        c = self\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0):\n            if lastResort:\n                if record.levelno >= lastResort.level:\n                    lastResort.handle(record)\n            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:\n                sys.stderr.write(\"No handlers could be found for logger\"\n                                 \" \\\"%s\\\"\\n\" % self.name)\n                self.manager.emittedNoHandlerWarning = True\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.disabled:\n            return False\n\n        try:\n            return self._cache[level]\n        except KeyError:\n            _acquireLock()\n            try:\n                if self.manager.disable >= level:\n                    is_enabled = self._cache[level] = False\n                else:\n                    is_enabled = self._cache[level] = (\n                        level >= self.getEffectiveLevel()\n                    )\n            finally:\n                _releaseLock()\n            return is_enabled\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\n    def __repr__(self):\n        level = getLevelName(self.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)\n\n    def __reduce__(self):\n        # In general, only the root logger will not be accessible via its name.\n        # However, the root logger's class has its own __reduce__ method.\n        if getLogger(self.name) is not self:\n            import pickle\n            raise pickle.PicklingError('logger cannot be pickled')\n        return getLogger, (self.name,)\n\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n    def __reduce__(self):\n        return getLogger, ()\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger.\n        \"\"\"\n        self.log(DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger.\n        \"\"\"\n        self.log(INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger.\n        \"\"\"\n        self.log(WARNING, msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger.\n        \"\"\"\n        self.log(CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the specified level on the underlying logger.\n        \"\"\"\n        self.logger.setLevel(level)\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for the underlying logger.\n        \"\"\"\n        return self.logger.getEffectiveLevel()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if the underlying logger has any handlers.\n        \"\"\"\n        return self.logger.hasHandlers()\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False):\n        \"\"\"\n        Low-level log implementation, proxied to allow nested logger adapters.\n        \"\"\"\n        return self.logger._log(\n            level,\n            msg,\n            args,\n            exc_info=exc_info,\n            extra=extra,\n            stack_info=stack_info,\n        )\n\n    @property\n    def manager(self):\n        return self.logger.manager\n\n    @manager.setter\n    def manager(self, value):\n        self.logger.manager = value\n\n    @property\n    def name(self):\n        return self.logger.name\n\n    def __repr__(self):\n        logger = self.logger\n        level = getLevelName(logger.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured, unless the keyword argument *force* is set to ``True``.\n    It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    style     If a format string is specified, use this to specify the\n              type of format string (possible values '%', '{', '$', for\n              %-formatting, :meth:`str.format` and :class:`string.Template`\n              - defaults to '%').\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n    handlers  If specified, this should be an iterable of already created\n              handlers, which will be added to the root handler. Any handler\n              in the list which does not have a formatter assigned will be\n              assigned the formatter created in this function.\n    force     If this keyword  is specified as true, any existing handlers\n              attached to the root logger are removed and closed, before\n              carrying out the configuration as specified by the other\n              arguments.\n    encoding  If specified together with a filename, this encoding is passed to\n              the created FileHandler, causing it to be used when the file is\n              opened.\n    errors    If specified together with a filename, this value is passed to the\n              created FileHandler, causing it to be used when the file is\n              opened in text mode. If not specified, the default value is\n              `backslashreplace`.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n\n    .. versionchanged:: 3.2\n       Added the ``style`` parameter.\n\n    .. versionchanged:: 3.3\n       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for\n       incompatible arguments (e.g. ``handlers`` specified together with\n       ``filename``/``filemode``, or ``filename``/``filemode`` specified\n       together with ``stream``, or ``handlers`` specified together with\n       ``stream``.\n\n    .. versionchanged:: 3.8\n       Added the ``force`` parameter.\n\n    .. versionchanged:: 3.9\n       Added the ``encoding`` and ``errors`` parameters.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        force = kwargs.pop('force', False)\n        encoding = kwargs.pop('encoding', None)\n        errors = kwargs.pop('errors', 'backslashreplace')\n        if force:\n            for h in root.handlers[:]:\n                root.removeHandler(h)\n                h.close()\n        if len(root.handlers) == 0:\n            handlers = kwargs.pop(\"handlers\", None)\n            if handlers is None:\n                if \"stream\" in kwargs and \"filename\" in kwargs:\n                    raise ValueError(\"'stream' and 'filename' should not be \"\n                                     \"specified together\")\n            else:\n                if \"stream\" in kwargs or \"filename\" in kwargs:\n                    raise ValueError(\"'stream' or 'filename' should not be \"\n                                     \"specified together with 'handlers'\")\n            if handlers is None:\n                filename = kwargs.pop(\"filename\", None)\n                mode = kwargs.pop(\"filemode\", 'a')\n                if filename:\n                    if 'b'in mode:\n                        errors = None\n                    h = FileHandler(filename, mode,\n                                    encoding=encoding, errors=errors)\n                else:\n                    stream = kwargs.pop(\"stream\", None)\n                    h = StreamHandler(stream)\n                handlers = [h]\n            dfs = kwargs.pop(\"datefmt\", None)\n            style = kwargs.pop(\"style\", '%')\n            if style not in _STYLES:\n                raise ValueError('Style must be one of: %s' % ','.join(\n                                 _STYLES.keys()))\n            fs = kwargs.pop(\"format\", _STYLES[style][1])\n            fmt = Formatter(fs, dfs, style)\n            for h in handlers:\n                if h.formatter is None:\n                    h.setFormatter(fmt)\n                root.addHandler(h)\n            level = kwargs.pop(\"level\", None)\n            if level is not None:\n                root.setLevel(level)\n            if kwargs:\n                keys = ', '.join(kwargs.keys())\n                raise ValueError('Unrecognised argument(s): %s' % keys)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if not name or isinstance(name, str) and name == root.name:\n        return root\n    return Logger.manager.getLogger(name)\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger. If the logger\n    has no handlers, call basicConfig() to add a console handler with a\n    pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\nfatal = critical\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, exc_info=True, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger, with exception\n    information. If the logger has no handlers, basicConfig() is called to add\n    a console handler with a pre-defined format.\n    \"\"\"\n    error(msg, *args, exc_info=exc_info, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\ndef warn(msg, *args, **kwargs):\n    warnings.warn(\"The 'warn' function is deprecated, \"\n        \"use 'warning' instead\", DeprecationWarning, 2)\n    warning(msg, *args, **kwargs)\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger. If\n    the logger has no handlers, call basicConfig() to add a console handler\n    with a pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level=CRITICAL):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n    root.manager._clear_cache()\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (OSError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except: # ignore everything, as we're shutting down\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def emit(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def createLock(self):\n        self.lock = None\n\n    def _at_fork_reinit(self):\n        pass\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        logger.warning(\"%s\", s)\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n",2220],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/inspect.py":["\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargvalues(), getcallargs() - get info about function arguments\n    getfullargspec() - same, with support for Python 3 features\n    formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\n    signature() - get a Signature object for the callable\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = ('Ka-Ping Yee <ping@lfw.org>',\n              'Yury Selivanov <yselivanov@sprymix.com>')\n\nimport abc\nimport ast\nimport dis\nimport collections.abc\nimport enum\nimport importlib.machinery\nimport itertools\nimport linecache\nimport os\nimport re\nimport sys\nimport tokenize\nimport token\nimport types\nimport warnings\nimport functools\nimport builtins\nfrom operator import attrgetter\nfrom collections import namedtuple, OrderedDict\n\n# Create constants for the compiler flags in Include/code.h\n# We try to get them from dis to avoid duplication\nmod_dict = globals()\nfor k, v in dis.COMPILER_FLAG_NAMES.items():\n    mod_dict[\"CO_\" + v] = k\n\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __cached__      pathname to byte compiled file\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, type)\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        __func__        function object containing implementation of method\n        __self__        instance to which this method is bound\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    __func__ attribute (etc) when an object passes ismethod().\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__get__\") and not hasattr(tp, \"__set__\")\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have a __set__ or a __delete__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__set__\") or hasattr(tp, \"__delete__\")\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        __code__        code object containing compiled function bytecode\n        __defaults__    tuple of any default values for arguments\n        __globals__     global namespace in which this function was defined\n        __annotations__ dict of parameter annotations\n        __kwdefaults__  dict of keyword only parameters with defaults\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef _has_code_flag(f, flag):\n    \"\"\"Return true if ``f`` is a function (or a method or functools.partial\n    wrapper wrapping a function) whose code object has the given ``flag``\n    set in its flags.\"\"\"\n    while ismethod(f):\n        f = f.__func__\n    f = functools._unwrap_partial(f)\n    if not isfunction(f):\n        return False\n    return bool(f.__code__.co_flags & flag)\n\ndef isgeneratorfunction(obj):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provide the same attributes as functions.\n    See help(isfunction) for a list of attributes.\"\"\"\n    return _has_code_flag(obj, CO_GENERATOR)\n\ndef iscoroutinefunction(obj):\n    \"\"\"Return true if the object is a coroutine function.\n\n    Coroutine functions are defined with \"async def\" syntax.\n    \"\"\"\n    return _has_code_flag(obj, CO_COROUTINE)\n\ndef isasyncgenfunction(obj):\n    \"\"\"Return true if the object is an asynchronous generator function.\n\n    Asynchronous generator functions are defined with \"async def\"\n    syntax and have \"yield\" expressions in their body.\n    \"\"\"\n    return _has_code_flag(obj, CO_ASYNC_GENERATOR)\n\ndef isasyncgen(object):\n    \"\"\"Return true if the object is an asynchronous generator.\"\"\"\n    return isinstance(object, types.AsyncGeneratorType)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef iscoroutine(object):\n    \"\"\"Return true if the object is a coroutine.\"\"\"\n    return isinstance(object, types.CoroutineType)\n\ndef isawaitable(object):\n    \"\"\"Return true if object can be passed to an ``await`` expression.\"\"\"\n    return (isinstance(object, types.CoroutineType) or\n            isinstance(object, types.GeneratorType) and\n                bool(object.gi_code.co_flags & CO_ITERABLE_COROUTINE) or\n            isinstance(object, collections.abc.Awaitable))\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount         number of arguments (not including *, ** args\n                            or keyword only arguments)\n        co_code             string of raw compiled bytecode\n        co_cellvars         tuple of names of cell variables\n        co_consts           tuple of constants used in the bytecode\n        co_filename         name of file in which this code object was created\n        co_firstlineno      number of first line in Python source code\n        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n                            | 16=nested | 32=generator | 64=nofree | 128=coroutine\n                            | 256=iterable_coroutine | 512=async_generator\n        co_freevars         tuple of names of free variables\n        co_posonlyargcount  number of positional only arguments\n        co_kwonlyargcount   number of keyword only arguments (not including ** arg)\n        co_lnotab           encoded mapping of line numbers to bytecode indices\n        co_name             name with which this code object was defined\n        co_names            tuple of names of local variables\n        co_nlocals          number of local variables\n        co_stacksize        virtual machine stack space required\n        co_varnames         tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    if not isinstance(object, type):\n        return False\n    if object.__flags__ & TPFLAGS_IS_ABSTRACT:\n        return True\n    if not issubclass(type(object), abc.ABCMeta):\n        return False\n    if hasattr(object, '__abstractmethods__'):\n        # It looks like ABCMeta.__new__ has finished running;\n        # TPFLAGS_IS_ABSTRACT should have been accurate.\n        return False\n    # It looks like ABCMeta.__new__ has not finished running yet; we're\n    # probably in __init_subclass__. We'll look for abstractmethods manually.\n    for name, value in object.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            return True\n    for base in object.__bases__:\n        for name in getattr(base, \"__abstractmethods__\", ()):\n            value = getattr(object, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                return True\n    return False\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    if isclass(object):\n        mro = (object,) + getmro(object)\n    else:\n        mro = ()\n    results = []\n    processed = set()\n    names = dir(object)\n    # :dd any DynamicClassAttributes to the list of names if object is a class;\n    # this may result in duplicate entries if, for example, a virtual\n    # attribute with the same name as a DynamicClassAttribute exists\n    try:\n        for base in object.__bases__:\n            for k, v in base.__dict__.items():\n                if isinstance(v, types.DynamicClassAttribute):\n                    names.append(k)\n    except AttributeError:\n        pass\n    for key in names:\n        # First try to get the value via getattr.  Some descriptors don't\n        # like calling their __get__ (see bug #1785), so fall back to\n        # looking in the __dict__.\n        try:\n            value = getattr(object, key)\n            # handle the duplicate key\n            if key in processed:\n                raise AttributeError\n        except AttributeError:\n            for base in mro:\n                if key in base.__dict__:\n                    value = base.__dict__[key]\n                    break\n            else:\n                # could be a (currently) missing slot member, or a buggy\n                # __dir__; discard and move on\n                continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n        processed.add(key)\n    results.sort(key=lambda pair: pair[0])\n    return results\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method or descriptor\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained by calling getattr; if this fails, or if the\n           resulting object does not live anywhere in the class' mro (including\n           metaclasses) then the object is looked up in the defining class's\n           dict (found by walking the mro).\n\n    If one of the items in dir(cls) is stored in the metaclass it will now\n    be discovered and not have None be listed as the class in which it was\n    defined.  Any items whose home class cannot be discovered are skipped.\n    \"\"\"\n\n    mro = getmro(cls)\n    metamro = getmro(type(cls)) # for attributes stored in the metaclass\n    metamro = tuple(cls for cls in metamro if cls not in (type, object))\n    class_bases = (cls,) + mro\n    all_bases = class_bases + metamro\n    names = dir(cls)\n    # :dd any DynamicClassAttributes to the list of names;\n    # this may result in duplicate entries if, for example, a virtual\n    # attribute with the same name as a DynamicClassAttribute exists.\n    for base in mro:\n        for k, v in base.__dict__.items():\n            if isinstance(v, types.DynamicClassAttribute):\n                names.append(k)\n    result = []\n    processed = set()\n\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Normal objects will be looked up with both getattr and directly in\n        # its class' dict (in case getattr fails [bug #1785], and also to look\n        # for a docstring).\n        # For DynamicClassAttributes on the second pass we only look in the\n        # class's dict.\n        #\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        homecls = None\n        get_obj = None\n        dict_obj = None\n        if name not in processed:\n            try:\n                if name == '__dict__':\n                    raise Exception(\"__dict__ is special, don't want the proxy\")\n                get_obj = getattr(cls, name)\n            except Exception as exc:\n                pass\n            else:\n                homecls = getattr(get_obj, \"__objclass__\", homecls)\n                if homecls not in class_bases:\n                    # if the resulting object does not live somewhere in the\n                    # mro, drop it and search the mro manually\n                    homecls = None\n                    last_cls = None\n                    # first look in the classes\n                    for srch_cls in class_bases:\n                        srch_obj = getattr(srch_cls, name, None)\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    # then check the metaclasses\n                    for srch_cls in metamro:\n                        try:\n                            srch_obj = srch_cls.__getattr__(cls, name)\n                        except AttributeError:\n                            continue\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    if last_cls is not None:\n                        homecls = last_cls\n        for base in all_bases:\n            if name in base.__dict__:\n                dict_obj = base.__dict__[name]\n                if homecls not in metamro:\n                    homecls = base\n                break\n        if homecls is None:\n            # unable to locate the attribute anywhere, most likely due to\n            # buggy custom __dir__; discard and move on\n            continue\n        obj = get_obj if get_obj is not None else dict_obj\n        # Classify the object or its descriptor.\n        if isinstance(dict_obj, (staticmethod, types.BuiltinMethodType)):\n            kind = \"static method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, (classmethod, types.ClassMethodDescriptorType)):\n            kind = \"class method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, property):\n            kind = \"property\"\n            obj = dict_obj\n        elif isroutine(obj):\n            kind = \"method\"\n        else:\n            kind = \"data\"\n        result.append(Attribute(name, kind, homecls, obj))\n        processed.add(name)\n    return result\n\n# ----------------------------------------------------------- class helpers\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    return cls.__mro__\n\n# -------------------------------------------------------- function helpers\n\ndef unwrap(func, *, stop=None):\n    \"\"\"Get the object wrapped by *func*.\n\n   Follows the chain of :attr:`__wrapped__` attributes returning the last\n   object in the chain.\n\n   *stop* is an optional callback accepting an object in the wrapper chain\n   as its sole argument that allows the unwrapping to be terminated early if\n   the callback returns a true value. If the callback never returns a true\n   value, the last object in the chain is returned as usual. For example,\n   :func:`signature` uses this to stop unwrapping if any object in the\n   chain has a ``__signature__`` attribute defined.\n\n   :exc:`ValueError` is raised if a cycle is encountered.\n\n    \"\"\"\n    if stop is None:\n        def _is_wrapper(f):\n            return hasattr(f, '__wrapped__')\n    else:\n        def _is_wrapper(f):\n            return hasattr(f, '__wrapped__') and not stop(f)\n    f = func  # remember the original func for error reporting\n    # Memoise by id to tolerate non-hashable objects, but store objects to\n    # ensure they aren't destroyed, which would allow their IDs to be reused.\n    memo = {id(f): f}\n    recursion_limit = sys.getrecursionlimit()\n    while _is_wrapper(func):\n        func = func.__wrapped__\n        id_func = id(func)\n        if (id_func in memo) or (len(memo) >= recursion_limit):\n            raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n        memo[id_func] = func\n    return func\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = line.expandtabs()\n    return len(expline) - len(expline.lstrip())\n\ndef _findclass(func):\n    cls = sys.modules.get(func.__module__)\n    if cls is None:\n        return None\n    for name in func.__qualname__.split('.')[:-1]:\n        cls = getattr(cls, name)\n    if not isclass(cls):\n        return None\n    return cls\n\ndef _finddoc(obj):\n    if isclass(obj):\n        for base in obj.__mro__:\n            if base is not object:\n                try:\n                    doc = base.__doc__\n                except AttributeError:\n                    continue\n                if doc is not None:\n                    return doc\n        return None\n\n    if ismethod(obj):\n        name = obj.__func__.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            getattr(getattr(self, name, None), '__func__') is obj.__func__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    elif isfunction(obj):\n        name = obj.__name__\n        cls = _findclass(obj)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif isbuiltin(obj):\n        name = obj.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            self.__qualname__ + '.' + name == obj.__qualname__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    # Should be tested before isdatadescriptor().\n    elif isinstance(obj, property):\n        func = obj.fget\n        name = func.__name__\n        cls = _findclass(func)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif ismethoddescriptor(obj) or isdatadescriptor(obj):\n        name = obj.__name__\n        cls = obj.__objclass__\n        if getattr(cls, name) is not obj:\n            return None\n        if ismemberdescriptor(obj):\n            slots = getattr(cls, '__slots__', None)\n            if isinstance(slots, dict) and name in slots:\n                return slots[name]\n    else:\n        return None\n    for base in cls.__mro__:\n        try:\n            doc = getattr(base, name).__doc__\n        except AttributeError:\n            continue\n        if doc is not None:\n            return doc\n    return None\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if doc is None:\n        try:\n            doc = _finddoc(object)\n        except (AttributeError, TypeError):\n            return None\n    if not isinstance(doc, str):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = doc.expandtabs().split('\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxsize\n        for line in lines[1:]:\n            content = len(line.lstrip())\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxsize:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return '\\n'.join(lines)\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if getattr(object, '__file__', None):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        if hasattr(object, '__module__'):\n            module = sys.modules.get(object.__module__)\n            if getattr(module, '__file__', None):\n                return module.__file__\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('module, class, method, function, traceback, frame, or '\n                    'code object was expected, got {}'.format(\n                    type(object).__name__))\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    fname = os.path.basename(path)\n    # Check for paths that look like an actual module file\n    suffixes = [(-len(suffix), suffix)\n                    for suffix in importlib.machinery.all_suffixes()]\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix in suffixes:\n        if fname.endswith(suffix):\n            return fname[:neglen]\n    return None\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]\n    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]\n    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n        filename = (os.path.splitext(filename)[0] +\n                    importlib.machinery.SOURCE_SUFFIXES[0])\n    elif any(filename.endswith(s) for s in\n                 importlib.machinery.EXTENSION_SUFFIXES):\n        return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n        return filename\n    # or it is in the linecache\n    if filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.copy().items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['builtins']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\n\nclass ClassFoundException(Exception):\n    pass\n\n\nclass _ClassFinder(ast.NodeVisitor):\n\n    def __init__(self, qualname):\n        self.stack = []\n        self.qualname = qualname\n\n    def visit_FunctionDef(self, node):\n        self.stack.append(node.name)\n        self.stack.append('<locals>')\n        self.generic_visit(node)\n        self.stack.pop()\n        self.stack.pop()\n\n    visit_AsyncFunctionDef = visit_FunctionDef\n\n    def visit_ClassDef(self, node):\n        self.stack.append(node.name)\n        if self.qualname == '.'.join(self.stack):\n            # Return the decorator for the class if present\n            if node.decorator_list:\n                line_number = node.decorator_list[0].lineno\n            else:\n                line_number = node.lineno\n\n            # decrement by one since lines starts with indexing by zero\n            line_number -= 1\n            raise ClassFoundException(line_number)\n        self.generic_visit(node)\n        self.stack.pop()\n\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An OSError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getsourcefile(object)\n    if file:\n        # Invalidate cache if needed.\n        linecache.checkcache(file)\n    else:\n        file = getfile(object)\n        # Allow filenames in form of \"<something>\" to pass through.\n        # `doctest` monkeypatches `linecache` module to enable\n        # inspection, so let `linecache.getlines` to be called.\n        if not (file.startswith('<') and file.endswith('>')):\n            raise OSError('source code not available')\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise OSError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        qualname = object.__qualname__\n        source = ''.join(lines)\n        tree = ast.parse(source)\n        class_finder = _ClassFinder(qualname)\n        try:\n            class_finder.visit(tree)\n        except ClassFoundException as e:\n            line_number = e.args[0]\n            return lines, line_number\n        else:\n            raise OSError('could not find class definition')\n\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise OSError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            try:\n                line = lines[lnum]\n            except IndexError:\n                raise OSError('lineno is out of bounds')\n            if pat.match(line):\n                break\n            lnum = lnum - 1\n        return lines, lnum\n    raise OSError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (OSError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and lines[start].strip() in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(lines[end].expandtabs())\n                end = end + 1\n            return ''.join(comments)\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and lines[end].lstrip()[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [lines[end].expandtabs().lstrip()]\n            if end > 0:\n                end = end - 1\n                comment = lines[end].expandtabs().lstrip()\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = lines[end].expandtabs().lstrip()\n            while comments and comments[0].strip() == '#':\n                comments[:1] = []\n            while comments and comments[-1].strip() == '#':\n                comments[-1:] = []\n            return ''.join(comments)\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.indecorator = False\n        self.decoratorhasargs = False\n        self.last = 1\n        self.body_col0 = None\n\n    def tokeneater(self, type, token, srowcol, erowcol, line):\n        if not self.started and not self.indecorator:\n            # skip any decorators\n            if token == \"@\":\n                self.indecorator = True\n            # look for the first \"def\", \"class\" or \"lambda\"\n            elif token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif token == \"(\":\n            if self.indecorator:\n                self.decoratorhasargs = True\n        elif token == \")\":\n            if self.indecorator:\n                self.indecorator = False\n                self.decoratorhasargs = False\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srowcol[0]\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n            # hitting a NEWLINE when in a decorator without args\n            # ends the decorator\n            if self.indecorator and not self.decoratorhasargs:\n                self.indecorator = False\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            if self.body_col0 is None and self.started:\n                self.body_col0 = erowcol[1]\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif type == tokenize.COMMENT:\n            if self.body_col0 is not None and srowcol[1] >= self.body_col0:\n                # Include comments if indented at least as much as the block\n                self.last = srowcol[0]\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokens = tokenize.generate_tokens(iter(lines).__next__)\n        for _token in tokens:\n            blockfinder.tokeneater(*_token)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An OSError is\n    raised if the source code cannot be retrieved.\"\"\"\n    object = unwrap(object)\n    lines, lnum = findsource(object)\n\n    if istraceback(object):\n        object = object.tb_frame\n\n    # for module or frame that corresponds to module, return all source lines\n    if (ismodule(object) or\n        (isframe(object) and object.f_code.co_name == \"<module>\")):\n        return lines, 0\n    else:\n        return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    OSError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return ''.join(lines)\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=False):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if parent not in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args, varargs, varkw')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where\n    'args' is the list of argument names. Keyword-only arguments are\n    appended. 'varargs' and 'varkw' are the names of the * and **\n    arguments or None.\"\"\"\n    if not iscode(co):\n        raise TypeError('{!r} is not a code object'.format(co))\n\n    names = co.co_varnames\n    nargs = co.co_argcount\n    nkwargs = co.co_kwonlyargcount\n    args = list(names[:nargs])\n    kwonlyargs = list(names[nargs:nargs+nkwargs])\n    step = 0\n\n    nargs += nkwargs\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args + kwonlyargs, varargs, varkw)\n\nArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n\ndef getargspec(func):\n    \"\"\"Get the names and default values of a function's parameters.\n\n    A tuple of four things is returned: (args, varargs, keywords, defaults).\n    'args' is a list of the argument names, including keyword-only argument names.\n    'varargs' and 'keywords' are the names of the * and ** parameters or None.\n    'defaults' is an n-tuple of the default values of the last n parameters.\n\n    This function is deprecated, as it does not support annotations or\n    keyword-only parameters and will raise ValueError if either is present\n    on the supplied callable.\n\n    For a more structured introspection API, use inspect.signature() instead.\n\n    Alternatively, use getfullargspec() for an API with a similar namedtuple\n    based interface, but full support for annotations and keyword-only\n    parameters.\n\n    Deprecated since Python 3.5, use `inspect.getfullargspec()`.\n    \"\"\"\n    warnings.warn(\"inspect.getargspec() is deprecated since Python 3.0, \"\n                  \"use inspect.signature() or inspect.getfullargspec()\",\n                  DeprecationWarning, stacklevel=2)\n    args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = \\\n        getfullargspec(func)\n    if kwonlyargs or ann:\n        raise ValueError(\"Function has keyword-only parameters or annotations\"\n                         \", use inspect.signature() API which can support them\")\n    return ArgSpec(args, varargs, varkw, defaults)\n\nFullArgSpec = namedtuple('FullArgSpec',\n    'args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations')\n\ndef getfullargspec(func):\n    \"\"\"Get the names and default values of a callable object's parameters.\n\n    A tuple of seven things is returned:\n    (args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations).\n    'args' is a list of the parameter names.\n    'varargs' and 'varkw' are the names of the * and ** parameters or None.\n    'defaults' is an n-tuple of the default values of the last n parameters.\n    'kwonlyargs' is a list of keyword-only parameter names.\n    'kwonlydefaults' is a dictionary mapping names from kwonlyargs to defaults.\n    'annotations' is a dictionary mapping parameter names to annotations.\n\n    Notable differences from inspect.signature():\n      - the \"self\" parameter is always reported, even for bound methods\n      - wrapper chains defined by __wrapped__ *not* unwrapped automatically\n    \"\"\"\n    try:\n        # Re: `skip_bound_arg=False`\n        #\n        # There is a notable difference in behaviour between getfullargspec\n        # and Signature: the former always returns 'self' parameter for bound\n        # methods, whereas the Signature always shows the actual calling\n        # signature of the passed object.\n        #\n        # To simulate this behaviour, we \"unbind\" bound methods, to trick\n        # inspect.signature to always return their first parameter (\"self\",\n        # usually)\n\n        # Re: `follow_wrapper_chains=False`\n        #\n        # getfullargspec() historically ignored __wrapped__ attributes,\n        # so we ensure that remains the case in 3.3+\n\n        sig = _signature_from_callable(func,\n                                       follow_wrapper_chains=False,\n                                       skip_bound_arg=False,\n                                       sigcls=Signature)\n    except Exception as ex:\n        # Most of the times 'signature' will raise ValueError.\n        # But, it can also raise AttributeError, and, maybe something\n        # else. So to be fully backwards compatible, we catch all\n        # possible exceptions here, and reraise a TypeError.\n        raise TypeError('unsupported callable') from ex\n\n    args = []\n    varargs = None\n    varkw = None\n    posonlyargs = []\n    kwonlyargs = []\n    annotations = {}\n    defaults = ()\n    kwdefaults = {}\n\n    if sig.return_annotation is not sig.empty:\n        annotations['return'] = sig.return_annotation\n\n    for param in sig.parameters.values():\n        kind = param.kind\n        name = param.name\n\n        if kind is _POSITIONAL_ONLY:\n            posonlyargs.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _POSITIONAL_OR_KEYWORD:\n            args.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _VAR_POSITIONAL:\n            varargs = name\n        elif kind is _KEYWORD_ONLY:\n            kwonlyargs.append(name)\n            if param.default is not param.empty:\n                kwdefaults[name] = param.default\n        elif kind is _VAR_KEYWORD:\n            varkw = name\n\n        if param.annotation is not param.empty:\n            annotations[name] = param.annotation\n\n    if not kwdefaults:\n        # compatibility with 'func.__kwdefaults__'\n        kwdefaults = None\n\n    if not defaults:\n        # compatibility with 'func.__defaults__'\n        defaults = None\n\n    return FullArgSpec(posonlyargs + args, varargs, varkw, defaults,\n                       kwonlyargs, kwdefaults, annotations)\n\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names.\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef formatannotation(annotation, base_module=None):\n    if getattr(annotation, '__module__', None) == 'typing':\n        return repr(annotation).replace('typing.', '')\n    if isinstance(annotation, type):\n        if annotation.__module__ in ('builtins', base_module):\n            return annotation.__qualname__\n        return annotation.__module__+'.'+annotation.__qualname__\n    return repr(annotation)\n\ndef formatannotationrelativeto(object):\n    module = getattr(object, '__module__', None)\n    def _formatannotation(annotation):\n        return formatannotation(annotation, module)\n    return _formatannotation\n\ndef formatargspec(args, varargs=None, varkw=None, defaults=None,\n                  kwonlyargs=(), kwonlydefaults={}, annotations={},\n                  formatarg=str,\n                  formatvarargs=lambda name: '*' + name,\n                  formatvarkw=lambda name: '**' + name,\n                  formatvalue=lambda value: '=' + repr(value),\n                  formatreturns=lambda text: ' -> ' + text,\n                  formatannotation=formatannotation):\n    \"\"\"Format an argument spec from the values returned by getfullargspec.\n\n    The first seven arguments are (args, varargs, varkw, defaults,\n    kwonlyargs, kwonlydefaults, annotations).  The other five arguments\n    are the corresponding optional formatting functions that are called to\n    turn names and values into strings.  The last argument is an optional\n    function to format the sequence of arguments.\n\n    Deprecated since Python 3.5: use the `signature` function and `Signature`\n    objects.\n    \"\"\"\n\n    from warnings import warn\n\n    warn(\"`formatargspec` is deprecated since Python 3.5. Use `signature` and \"\n         \"the `Signature` object directly\",\n         DeprecationWarning,\n         stacklevel=2)\n\n    def formatargandannotation(arg):\n        result = formatarg(arg)\n        if arg in annotations:\n            result += ': ' + formatannotation(annotations[arg])\n        return result\n    specs = []\n    if defaults:\n        firstdefault = len(args) - len(defaults)\n    for i, arg in enumerate(args):\n        spec = formatargandannotation(arg)\n        if defaults and i >= firstdefault:\n            spec = spec + formatvalue(defaults[i - firstdefault])\n        specs.append(spec)\n    if varargs is not None:\n        specs.append(formatvarargs(formatargandannotation(varargs)))\n    else:\n        if kwonlyargs:\n            specs.append('*')\n    if kwonlyargs:\n        for kwonlyarg in kwonlyargs:\n            spec = formatargandannotation(kwonlyarg)\n            if kwonlydefaults and kwonlyarg in kwonlydefaults:\n                spec += formatvalue(kwonlydefaults[kwonlyarg])\n            specs.append(spec)\n    if varkw is not None:\n        specs.append(formatvarkw(formatargandannotation(varkw)))\n    result = '(' + ', '.join(specs) + ')'\n    if 'return' in annotations:\n        result += formatreturns(formatannotation(annotations['return']))\n    return result\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value)):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(convert(args[i]))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + ', '.join(specs) + ')'\n\ndef _missing_arguments(f_name, argnames, pos, values):\n    names = [repr(name) for name in argnames if name not in values]\n    missing = len(names)\n    if missing == 1:\n        s = names[0]\n    elif missing == 2:\n        s = \"{} and {}\".format(*names)\n    else:\n        tail = \", {} and {}\".format(*names[-2:])\n        del names[-2:]\n        s = \", \".join(names) + tail\n    raise TypeError(\"%s() missing %i required %s argument%s: %s\" %\n                    (f_name, missing,\n                      \"positional\" if pos else \"keyword-only\",\n                      \"\" if missing == 1 else \"s\", s))\n\ndef _too_many(f_name, args, kwonly, varargs, defcount, given, values):\n    atleast = len(args) - defcount\n    kwonly_given = len([arg for arg in kwonly if arg in values])\n    if varargs:\n        plural = atleast != 1\n        sig = \"at least %d\" % (atleast,)\n    elif defcount:\n        plural = True\n        sig = \"from %d to %d\" % (atleast, len(args))\n    else:\n        plural = len(args) != 1\n        sig = str(len(args))\n    kwonly_sig = \"\"\n    if kwonly_given:\n        msg = \" positional argument%s (and %d keyword-only argument%s)\"\n        kwonly_sig = (msg % (\"s\" if given != 1 else \"\", kwonly_given,\n                             \"s\" if kwonly_given != 1 else \"\"))\n    raise TypeError(\"%s() takes %s positional argument%s but %d%s %s given\" %\n            (f_name, sig, \"s\" if plural else \"\", given, kwonly_sig,\n             \"was\" if given == 1 and not kwonly_given else \"were\"))\n\ndef getcallargs(func, /, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    spec = getfullargspec(func)\n    args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = spec\n    f_name = func.__name__\n    arg2value = {}\n\n\n    if ismethod(func) and func.__self__ is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.__self__,) + positional\n    num_pos = len(positional)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n\n    n = min(num_pos, num_args)\n    for i in range(n):\n        arg2value[args[i]] = positional[i]\n    if varargs:\n        arg2value[varargs] = tuple(positional[n:])\n    possible_kwargs = set(args + kwonlyargs)\n    if varkw:\n        arg2value[varkw] = {}\n    for kw, value in named.items():\n        if kw not in possible_kwargs:\n            if not varkw:\n                raise TypeError(\"%s() got an unexpected keyword argument %r\" %\n                                (f_name, kw))\n            arg2value[varkw][kw] = value\n            continue\n        if kw in arg2value:\n            raise TypeError(\"%s() got multiple values for argument %r\" %\n                            (f_name, kw))\n        arg2value[kw] = value\n    if num_pos > num_args and not varargs:\n        _too_many(f_name, args, kwonlyargs, varargs, num_defaults,\n                   num_pos, arg2value)\n    if num_pos < num_args:\n        req = args[:num_args - num_defaults]\n        for arg in req:\n            if arg not in arg2value:\n                _missing_arguments(f_name, req, True, arg2value)\n        for i, arg in enumerate(args[num_args - num_defaults:]):\n            if arg not in arg2value:\n                arg2value[arg] = defaults[i]\n    missing = 0\n    for kwarg in kwonlyargs:\n        if kwarg not in arg2value:\n            if kwonlydefaults and kwarg in kwonlydefaults:\n                arg2value[kwarg] = kwonlydefaults[kwarg]\n            else:\n                missing += 1\n    if missing:\n        _missing_arguments(f_name, kwonlyargs, False, arg2value)\n    return arg2value\n\nClosureVars = namedtuple('ClosureVars', 'nonlocals globals builtins unbound')\n\ndef getclosurevars(func):\n    \"\"\"\n    Get the mapping of free variables to their current values.\n\n    Returns a named tuple of dicts mapping the current nonlocal, global\n    and builtin references as seen by the body of the function. A final\n    set of unbound names that could not be resolved is also provided.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.__func__\n\n    if not isfunction(func):\n        raise TypeError(\"{!r} is not a Python function\".format(func))\n\n    code = func.__code__\n    # Nonlocal references are named in co_freevars and resolved\n    # by looking them up in __closure__ by positional index\n    if func.__closure__ is None:\n        nonlocal_vars = {}\n    else:\n        nonlocal_vars = {\n            var : cell.cell_contents\n            for var, cell in zip(code.co_freevars, func.__closure__)\n       }\n\n    # Global and builtin references are named in co_names and resolved\n    # by looking them up in __globals__ or __builtins__\n    global_ns = func.__globals__\n    builtin_ns = global_ns.get(\"__builtins__\", builtins.__dict__)\n    if ismodule(builtin_ns):\n        builtin_ns = builtin_ns.__dict__\n    global_vars = {}\n    builtin_vars = {}\n    unbound_names = set()\n    for name in code.co_names:\n        if name in (\"None\", \"True\", \"False\"):\n            # Because these used to be builtins instead of keywords, they\n            # may still show up as name references. We ignore them.\n            continue\n        try:\n            global_vars[name] = global_ns[name]\n        except KeyError:\n            try:\n                builtin_vars[name] = builtin_ns[name]\n            except KeyError:\n                unbound_names.add(name)\n\n    return ClosureVars(nonlocal_vars, global_vars,\n                       builtin_vars, unbound_names)\n\n# -------------------------------------------------- stack frame extraction\n\nTraceback = namedtuple('Traceback', 'filename lineno function code_context index')\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except OSError:\n            lines = index = None\n        else:\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\nFrameInfo = namedtuple('FrameInfo', ('frame',) + Traceback._fields)\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        frameinfo = (frame,) + getframeinfo(frame, context)\n        framelist.append(FrameInfo(*frameinfo))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n        framelist.append(FrameInfo(*frameinfo))\n        tb = tb.tb_next\n    return framelist\n\ndef currentframe():\n    \"\"\"Return the frame of the caller or None if this is not possible.\"\"\"\n    return sys._getframe(1) if hasattr(sys, \"_getframe\") else None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n\n\n# ------------------------------------------------ static version of getattr\n\n_sentinel = object()\n\ndef _static_getmro(klass):\n    return type.__dict__['__mro__'].__get__(klass)\n\ndef _check_instance(obj, attr):\n    instance_dict = {}\n    try:\n        instance_dict = object.__getattribute__(obj, \"__dict__\")\n    except AttributeError:\n        pass\n    return dict.get(instance_dict, attr, _sentinel)\n\n\ndef _check_class(klass, attr):\n    for entry in _static_getmro(klass):\n        if _shadowed_dict(type(entry)) is _sentinel:\n            try:\n                return entry.__dict__[attr]\n            except KeyError:\n                pass\n    return _sentinel\n\ndef _is_type(obj):\n    try:\n        _static_getmro(obj)\n    except TypeError:\n        return False\n    return True\n\ndef _shadowed_dict(klass):\n    dict_attr = type.__dict__[\"__dict__\"]\n    for entry in _static_getmro(klass):\n        try:\n            class_dict = dict_attr.__get__(entry)[\"__dict__\"]\n        except KeyError:\n            pass\n        else:\n            if not (type(class_dict) is types.GetSetDescriptorType and\n                    class_dict.__name__ == \"__dict__\" and\n                    class_dict.__objclass__ is entry):\n                return class_dict\n    return _sentinel\n\ndef getattr_static(obj, attr, default=_sentinel):\n    \"\"\"Retrieve attributes without triggering dynamic lookup via the\n       descriptor protocol,  __getattr__ or __getattribute__.\n\n       Note: this function may not be able to retrieve all attributes\n       that getattr can fetch (like dynamically created attributes)\n       and may find attributes that getattr can't (like descriptors\n       that raise AttributeError). It can also return descriptor objects\n       instead of instance members in some cases. See the\n       documentation for details.\n    \"\"\"\n    instance_result = _sentinel\n    if not _is_type(obj):\n        klass = type(obj)\n        dict_attr = _shadowed_dict(klass)\n        if (dict_attr is _sentinel or\n            type(dict_attr) is types.MemberDescriptorType):\n            instance_result = _check_instance(obj, attr)\n    else:\n        klass = obj\n\n    klass_result = _check_class(klass, attr)\n\n    if instance_result is not _sentinel and klass_result is not _sentinel:\n        if (_check_class(type(klass_result), '__get__') is not _sentinel and\n            _check_class(type(klass_result), '__set__') is not _sentinel):\n            return klass_result\n\n    if instance_result is not _sentinel:\n        return instance_result\n    if klass_result is not _sentinel:\n        return klass_result\n\n    if obj is klass:\n        # for types we check the metaclass too\n        for entry in _static_getmro(type(klass)):\n            if _shadowed_dict(type(entry)) is _sentinel:\n                try:\n                    return entry.__dict__[attr]\n                except KeyError:\n                    pass\n    if default is not _sentinel:\n        return default\n    raise AttributeError(attr)\n\n\n# ------------------------------------------------ generator introspection\n\nGEN_CREATED = 'GEN_CREATED'\nGEN_RUNNING = 'GEN_RUNNING'\nGEN_SUSPENDED = 'GEN_SUSPENDED'\nGEN_CLOSED = 'GEN_CLOSED'\n\ndef getgeneratorstate(generator):\n    \"\"\"Get current state of a generator-iterator.\n\n    Possible states are:\n      GEN_CREATED: Waiting to start execution.\n      GEN_RUNNING: Currently being executed by the interpreter.\n      GEN_SUSPENDED: Currently suspended at a yield expression.\n      GEN_CLOSED: Execution has completed.\n    \"\"\"\n    if generator.gi_running:\n        return GEN_RUNNING\n    if generator.gi_frame is None:\n        return GEN_CLOSED\n    if generator.gi_frame.f_lasti == -1:\n        return GEN_CREATED\n    return GEN_SUSPENDED\n\n\ndef getgeneratorlocals(generator):\n    \"\"\"\n    Get the mapping of generator local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n\n    if not isgenerator(generator):\n        raise TypeError(\"{!r} is not a Python generator\".format(generator))\n\n    frame = getattr(generator, \"gi_frame\", None)\n    if frame is not None:\n        return generator.gi_frame.f_locals\n    else:\n        return {}\n\n\n# ------------------------------------------------ coroutine introspection\n\nCORO_CREATED = 'CORO_CREATED'\nCORO_RUNNING = 'CORO_RUNNING'\nCORO_SUSPENDED = 'CORO_SUSPENDED'\nCORO_CLOSED = 'CORO_CLOSED'\n\ndef getcoroutinestate(coroutine):\n    \"\"\"Get current state of a coroutine object.\n\n    Possible states are:\n      CORO_CREATED: Waiting to start execution.\n      CORO_RUNNING: Currently being executed by the interpreter.\n      CORO_SUSPENDED: Currently suspended at an await expression.\n      CORO_CLOSED: Execution has completed.\n    \"\"\"\n    if coroutine.cr_running:\n        return CORO_RUNNING\n    if coroutine.cr_frame is None:\n        return CORO_CLOSED\n    if coroutine.cr_frame.f_lasti == -1:\n        return CORO_CREATED\n    return CORO_SUSPENDED\n\n\ndef getcoroutinelocals(coroutine):\n    \"\"\"\n    Get the mapping of coroutine local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n    frame = getattr(coroutine, \"cr_frame\", None)\n    if frame is not None:\n        return frame.f_locals\n    else:\n        return {}\n\n\n###############################################################################\n### Function Signature Object (PEP 362)\n###############################################################################\n\n\n_WrapperDescriptor = type(type.__call__)\n_MethodWrapper = type(all.__call__)\n_ClassMethodWrapper = type(int.__dict__['from_bytes'])\n\n_NonUserDefinedCallables = (_WrapperDescriptor,\n                            _MethodWrapper,\n                            _ClassMethodWrapper,\n                            types.BuiltinFunctionType)\n\n\ndef _signature_get_user_defined_method(cls, method_name):\n    \"\"\"Private helper. Checks if ``cls`` has an attribute\n    named ``method_name`` and returns it only if it is a\n    pure python function.\n    \"\"\"\n    try:\n        meth = getattr(cls, method_name)\n    except AttributeError:\n        return\n    else:\n        if not isinstance(meth, _NonUserDefinedCallables):\n            # Once '__signature__' will be added to 'C'-level\n            # callables, this check won't be necessary\n            return meth\n\n\ndef _signature_get_partial(wrapped_sig, partial, extra_args=()):\n    \"\"\"Private helper to calculate how 'wrapped_sig' signature will\n    look like after applying a 'functools.partial' object (or alike)\n    on it.\n    \"\"\"\n\n    old_params = wrapped_sig.parameters\n    new_params = OrderedDict(old_params.items())\n\n    partial_args = partial.args or ()\n    partial_keywords = partial.keywords or {}\n\n    if extra_args:\n        partial_args = extra_args + partial_args\n\n    try:\n        ba = wrapped_sig.bind_partial(*partial_args, **partial_keywords)\n    except TypeError as ex:\n        msg = 'partial object {!r} has incorrect arguments'.format(partial)\n        raise ValueError(msg) from ex\n\n\n    transform_to_kwonly = False\n    for param_name, param in old_params.items():\n        try:\n            arg_value = ba.arguments[param_name]\n        except KeyError:\n            pass\n        else:\n            if param.kind is _POSITIONAL_ONLY:\n                # If positional-only parameter is bound by partial,\n                # it effectively disappears from the signature\n                new_params.pop(param_name)\n                continue\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                if param_name in partial_keywords:\n                    # This means that this parameter, and all parameters\n                    # after it should be keyword-only (and var-positional\n                    # should be removed). Here's why. Consider the following\n                    # function:\n                    #     foo(a, b, *args, c):\n                    #         pass\n                    #\n                    # \"partial(foo, a='spam')\" will have the following\n                    # signature: \"(*, a='spam', b, c)\". Because attempting\n                    # to call that partial with \"(10, 20)\" arguments will\n                    # raise a TypeError, saying that \"a\" argument received\n                    # multiple values.\n                    transform_to_kwonly = True\n                    # Set the new default value\n                    new_params[param_name] = param.replace(default=arg_value)\n                else:\n                    # was passed as a positional argument\n                    new_params.pop(param.name)\n                    continue\n\n            if param.kind is _KEYWORD_ONLY:\n                # Set the new default value\n                new_params[param_name] = param.replace(default=arg_value)\n\n        if transform_to_kwonly:\n            assert param.kind is not _POSITIONAL_ONLY\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                new_param = new_params[param_name].replace(kind=_KEYWORD_ONLY)\n                new_params[param_name] = new_param\n                new_params.move_to_end(param_name)\n            elif param.kind in (_KEYWORD_ONLY, _VAR_KEYWORD):\n                new_params.move_to_end(param_name)\n            elif param.kind is _VAR_POSITIONAL:\n                new_params.pop(param.name)\n\n    return wrapped_sig.replace(parameters=new_params.values())\n\n\ndef _signature_bound_method(sig):\n    \"\"\"Private helper to transform signatures for unbound\n    functions to bound methods.\n    \"\"\"\n\n    params = tuple(sig.parameters.values())\n\n    if not params or params[0].kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n        raise ValueError('invalid method signature')\n\n    kind = params[0].kind\n    if kind in (_POSITIONAL_OR_KEYWORD, _POSITIONAL_ONLY):\n        # Drop first parameter:\n        # '(p1, p2[, ...])' -> '(p2[, ...])'\n        params = params[1:]\n    else:\n        if kind is not _VAR_POSITIONAL:\n            # Unless we add a new parameter type we never\n            # get here\n            raise ValueError('invalid argument type')\n        # It's a var-positional parameter.\n        # Do nothing. '(*args[, ...])' -> '(*args[, ...])'\n\n    return sig.replace(parameters=params)\n\n\ndef _signature_is_builtin(obj):\n    \"\"\"Private helper to test if `obj` is a callable that might\n    support Argument Clinic's __text_signature__ protocol.\n    \"\"\"\n    return (isbuiltin(obj) or\n            ismethoddescriptor(obj) or\n            isinstance(obj, _NonUserDefinedCallables) or\n            # Can't test 'isinstance(type)' here, as it would\n            # also be True for regular python classes\n            obj in (type, object))\n\n\ndef _signature_is_functionlike(obj):\n    \"\"\"Private helper to test if `obj` is a duck type of FunctionType.\n    A good example of such objects are functions compiled with\n    Cython, which have all attributes that a pure Python function\n    would have, but have their code statically compiled.\n    \"\"\"\n\n    if not callable(obj) or isclass(obj):\n        # All function-like objects are obviously callables,\n        # and not classes.\n        return False\n\n    name = getattr(obj, '__name__', None)\n    code = getattr(obj, '__code__', None)\n    defaults = getattr(obj, '__defaults__', _void) # Important to use _void ...\n    kwdefaults = getattr(obj, '__kwdefaults__', _void) # ... and not None here\n    annotations = getattr(obj, '__annotations__', None)\n\n    return (isinstance(code, types.CodeType) and\n            isinstance(name, str) and\n            (defaults is None or isinstance(defaults, tuple)) and\n            (kwdefaults is None or isinstance(kwdefaults, dict)) and\n            isinstance(annotations, dict))\n\n\ndef _signature_get_bound_param(spec):\n    \"\"\" Private helper to get first parameter name from a\n    __text_signature__ of a builtin method, which should\n    be in the following format: '($param1, ...)'.\n    Assumptions are that the first argument won't have\n    a default value or an annotation.\n    \"\"\"\n\n    assert spec.startswith('($')\n\n    pos = spec.find(',')\n    if pos == -1:\n        pos = spec.find(')')\n\n    cpos = spec.find(':')\n    assert cpos == -1 or cpos > pos\n\n    cpos = spec.find('=')\n    assert cpos == -1 or cpos > pos\n\n    return spec[2:pos]\n\n\ndef _signature_strip_non_python_syntax(signature):\n    \"\"\"\n    Private helper function. Takes a signature in Argument Clinic's\n    extended signature format.\n\n    Returns a tuple of three things:\n      * that signature re-rendered in standard Python syntax,\n      * the index of the \"self\" parameter (generally 0), or None if\n        the function does not have a \"self\" parameter, and\n      * the index of the last \"positional only\" parameter,\n        or None if the signature has no positional-only parameters.\n    \"\"\"\n\n    if not signature:\n        return signature, None, None\n\n    self_parameter = None\n    last_positional_only = None\n\n    lines = [l.encode('ascii') for l in signature.split('\\n')]\n    generator = iter(lines).__next__\n    token_stream = tokenize.tokenize(generator)\n\n    delayed_comma = False\n    skip_next_comma = False\n    text = []\n    add = text.append\n\n    current_parameter = 0\n    OP = token.OP\n    ERRORTOKEN = token.ERRORTOKEN\n\n    # token stream always starts with ENCODING token, skip it\n    t = next(token_stream)\n    assert t.type == tokenize.ENCODING\n\n    for t in token_stream:\n        type, string = t.type, t.string\n\n        if type == OP:\n            if string == ',':\n                if skip_next_comma:\n                    skip_next_comma = False\n                else:\n                    assert not delayed_comma\n                    delayed_comma = True\n                    current_parameter += 1\n                continue\n\n            if string == '/':\n                assert not skip_next_comma\n                assert last_positional_only is None\n                skip_next_comma = True\n                last_positional_only = current_parameter - 1\n                continue\n\n        if (type == ERRORTOKEN) and (string == '$'):\n            assert self_parameter is None\n            self_parameter = current_parameter\n            continue\n\n        if delayed_comma:\n            delayed_comma = False\n            if not ((type == OP) and (string == ')')):\n                add(', ')\n        add(string)\n        if (string == ','):\n            add(' ')\n    clean_signature = ''.join(text)\n    return clean_signature, self_parameter, last_positional_only\n\n\ndef _signature_fromstr(cls, obj, s, skip_bound_arg=True):\n    \"\"\"Private helper to parse content of '__text_signature__'\n    and return a Signature based on it.\n    \"\"\"\n    # Lazy import ast because it's relatively heavy and\n    # it's not used for other than this function.\n    import ast\n\n    Parameter = cls._parameter_cls\n\n    clean_signature, self_parameter, last_positional_only = \\\n        _signature_strip_non_python_syntax(s)\n\n    program = \"def foo\" + clean_signature + \": pass\"\n\n    try:\n        module = ast.parse(program)\n    except SyntaxError:\n        module = None\n\n    if not isinstance(module, ast.Module):\n        raise ValueError(\"{!r} builtin has invalid signature\".format(obj))\n\n    f = module.body[0]\n\n    parameters = []\n    empty = Parameter.empty\n    invalid = object()\n\n    module = None\n    module_dict = {}\n    module_name = getattr(obj, '__module__', None)\n    if module_name:\n        module = sys.modules.get(module_name, None)\n        if module:\n            module_dict = module.__dict__\n    sys_module_dict = sys.modules.copy()\n\n    def parse_name(node):\n        assert isinstance(node, ast.arg)\n        if node.annotation is not None:\n            raise ValueError(\"Annotations are not currently supported\")\n        return node.arg\n\n    def wrap_value(s):\n        try:\n            value = eval(s, module_dict)\n        except NameError:\n            try:\n                value = eval(s, sys_module_dict)\n            except NameError:\n                raise RuntimeError()\n\n        if isinstance(value, (str, int, float, bytes, bool, type(None))):\n            return ast.Constant(value)\n        raise RuntimeError()\n\n    class RewriteSymbolics(ast.NodeTransformer):\n        def visit_Attribute(self, node):\n            a = []\n            n = node\n            while isinstance(n, ast.Attribute):\n                a.append(n.attr)\n                n = n.value\n            if not isinstance(n, ast.Name):\n                raise RuntimeError()\n            a.append(n.id)\n            value = \".\".join(reversed(a))\n            return wrap_value(value)\n\n        def visit_Name(self, node):\n            if not isinstance(node.ctx, ast.Load):\n                raise ValueError()\n            return wrap_value(node.id)\n\n    def p(name_node, default_node, default=empty):\n        name = parse_name(name_node)\n        if name is invalid:\n            return None\n        if default_node and default_node is not _empty:\n            try:\n                default_node = RewriteSymbolics().visit(default_node)\n                o = ast.literal_eval(default_node)\n            except ValueError:\n                o = invalid\n            if o is invalid:\n                return None\n            default = o if o is not invalid else default\n        parameters.append(Parameter(name, kind, default=default, annotation=empty))\n\n    # non-keyword-only parameters\n    args = reversed(f.args.args)\n    defaults = reversed(f.args.defaults)\n    iter = itertools.zip_longest(args, defaults, fillvalue=None)\n    if last_positional_only is not None:\n        kind = Parameter.POSITIONAL_ONLY\n    else:\n        kind = Parameter.POSITIONAL_OR_KEYWORD\n    for i, (name, default) in enumerate(reversed(list(iter))):\n        p(name, default)\n        if i == last_positional_only:\n            kind = Parameter.POSITIONAL_OR_KEYWORD\n\n    # *args\n    if f.args.vararg:\n        kind = Parameter.VAR_POSITIONAL\n        p(f.args.vararg, empty)\n\n    # keyword-only arguments\n    kind = Parameter.KEYWORD_ONLY\n    for name, default in zip(f.args.kwonlyargs, f.args.kw_defaults):\n        p(name, default)\n\n    # **kwargs\n    if f.args.kwarg:\n        kind = Parameter.VAR_KEYWORD\n        p(f.args.kwarg, empty)\n\n    if self_parameter is not None:\n        # Possibly strip the bound argument:\n        #    - We *always* strip first bound argument if\n        #      it is a module.\n        #    - We don't strip first bound argument if\n        #      skip_bound_arg is False.\n        assert parameters\n        _self = getattr(obj, '__self__', None)\n        self_isbound = _self is not None\n        self_ismodule = ismodule(_self)\n        if self_isbound and (self_ismodule or skip_bound_arg):\n            parameters.pop(0)\n        else:\n            # for builtins, self parameter is always positional-only!\n            p = parameters[0].replace(kind=Parameter.POSITIONAL_ONLY)\n            parameters[0] = p\n\n    return cls(parameters, return_annotation=cls.empty)\n\n\ndef _signature_from_builtin(cls, func, skip_bound_arg=True):\n    \"\"\"Private helper function to get signature for\n    builtin callables.\n    \"\"\"\n\n    if not _signature_is_builtin(func):\n        raise TypeError(\"{!r} is not a Python builtin \"\n                        \"function\".format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if not s:\n        raise ValueError(\"no signature found for builtin {!r}\".format(func))\n\n    return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n\ndef _signature_from_function(cls, func, skip_bound_arg=True):\n    \"\"\"Private helper: constructs Signature for the given python function.\"\"\"\n\n    is_duck_function = False\n    if not isfunction(func):\n        if _signature_is_functionlike(func):\n            is_duck_function = True\n        else:\n            # If it's not a pure Python function, and not a duck type\n            # of pure function:\n            raise TypeError('{!r} is not a Python function'.format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if s:\n        return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n    Parameter = cls._parameter_cls\n\n    # Parameter information.\n    func_code = func.__code__\n    pos_count = func_code.co_argcount\n    arg_names = func_code.co_varnames\n    posonly_count = func_code.co_posonlyargcount\n    positional = arg_names[:pos_count]\n    keyword_only_count = func_code.co_kwonlyargcount\n    keyword_only = arg_names[pos_count:pos_count + keyword_only_count]\n    annotations = func.__annotations__\n    defaults = func.__defaults__\n    kwdefaults = func.__kwdefaults__\n\n    if defaults:\n        pos_default_count = len(defaults)\n    else:\n        pos_default_count = 0\n\n    parameters = []\n\n    non_default_count = pos_count - pos_default_count\n    posonly_left = posonly_count\n\n    # Non-keyword-only parameters w/o defaults.\n    for name in positional[:non_default_count]:\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind))\n        if posonly_left:\n            posonly_left -= 1\n\n    # ... w/ defaults.\n    for offset, name in enumerate(positional[non_default_count:]):\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind,\n                                    default=defaults[offset]))\n        if posonly_left:\n            posonly_left -= 1\n\n    # *args\n    if func_code.co_flags & CO_VARARGS:\n        name = arg_names[pos_count + keyword_only_count]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_POSITIONAL))\n\n    # Keyword-only parameters.\n    for name in keyword_only:\n        default = _empty\n        if kwdefaults is not None:\n            default = kwdefaults.get(name, _empty)\n\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_KEYWORD_ONLY,\n                                    default=default))\n    # **kwargs\n    if func_code.co_flags & CO_VARKEYWORDS:\n        index = pos_count + keyword_only_count\n        if func_code.co_flags & CO_VARARGS:\n            index += 1\n\n        name = arg_names[index]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_KEYWORD))\n\n    # Is 'func' is a pure Python function - don't validate the\n    # parameters list (for correct order and defaults), it should be OK.\n    return cls(parameters,\n               return_annotation=annotations.get('return', _empty),\n               __validate_parameters__=is_duck_function)\n\n\ndef _signature_from_callable(obj, *,\n                             follow_wrapper_chains=True,\n                             skip_bound_arg=True,\n                             sigcls):\n\n    \"\"\"Private helper function to get signature for arbitrary\n    callable objects.\n    \"\"\"\n\n    if not callable(obj):\n        raise TypeError('{!r} is not a callable object'.format(obj))\n\n    if isinstance(obj, types.MethodType):\n        # In this case we skip the first parameter of the underlying\n        # function (usually `self` or `cls`).\n        sig = _signature_from_callable(\n            obj.__func__,\n            follow_wrapper_chains=follow_wrapper_chains,\n            skip_bound_arg=skip_bound_arg,\n            sigcls=sigcls)\n\n        if skip_bound_arg:\n            return _signature_bound_method(sig)\n        else:\n            return sig\n\n    # Was this function wrapped by a decorator?\n    if follow_wrapper_chains:\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\n        if isinstance(obj, types.MethodType):\n            # If the unwrapped object is a *method*, we might want to\n            # skip its first parameter (self).\n            # See test_signature_wrapped_bound_method for details.\n            return _signature_from_callable(\n                obj,\n                follow_wrapper_chains=follow_wrapper_chains,\n                skip_bound_arg=skip_bound_arg,\n                sigcls=sigcls)\n\n    try:\n        sig = obj.__signature__\n    except AttributeError:\n        pass\n    else:\n        if sig is not None:\n            if not isinstance(sig, Signature):\n                raise TypeError(\n                    'unexpected object {!r} in __signature__ '\n                    'attribute'.format(sig))\n            return sig\n\n    try:\n        partialmethod = obj._partialmethod\n    except AttributeError:\n        pass\n    else:\n        if isinstance(partialmethod, functools.partialmethod):\n            # Unbound partialmethod (see functools.partialmethod)\n            # This means, that we need to calculate the signature\n            # as if it's a regular partial object, but taking into\n            # account that the first positional argument\n            # (usually `self`, or `cls`) will not be passed\n            # automatically (as for boundmethods)\n\n            wrapped_sig = _signature_from_callable(\n                partialmethod.func,\n                follow_wrapper_chains=follow_wrapper_chains,\n                skip_bound_arg=skip_bound_arg,\n                sigcls=sigcls)\n\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\n            if first_wrapped_param.kind is Parameter.VAR_POSITIONAL:\n                # First argument of the wrapped callable is `*args`, as in\n                # `partialmethod(lambda *args)`.\n                return sig\n            else:\n                sig_params = tuple(sig.parameters.values())\n                assert (not sig_params or\n                        first_wrapped_param is not sig_params[0])\n                new_params = (first_wrapped_param,) + sig_params\n                return sig.replace(parameters=new_params)\n\n    if isfunction(obj) or _signature_is_functionlike(obj):\n        # If it's a pure Python function, or an object that is duck type\n        # of a Python function (Cython functions, for instance), then:\n        return _signature_from_function(sigcls, obj,\n                                        skip_bound_arg=skip_bound_arg)\n\n    if _signature_is_builtin(obj):\n        return _signature_from_builtin(sigcls, obj,\n                                       skip_bound_arg=skip_bound_arg)\n\n    if isinstance(obj, functools.partial):\n        wrapped_sig = _signature_from_callable(\n            obj.func,\n            follow_wrapper_chains=follow_wrapper_chains,\n            skip_bound_arg=skip_bound_arg,\n            sigcls=sigcls)\n        return _signature_get_partial(wrapped_sig, obj)\n\n    sig = None\n    if isinstance(obj, type):\n        # obj is a class or a metaclass\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = _signature_get_user_defined_method(type(obj), '__call__')\n        if call is not None:\n            sig = _signature_from_callable(\n                call,\n                follow_wrapper_chains=follow_wrapper_chains,\n                skip_bound_arg=skip_bound_arg,\n                sigcls=sigcls)\n        else:\n            # Now we check if the 'obj' class has a '__new__' method\n            new = _signature_get_user_defined_method(obj, '__new__')\n            if new is not None:\n                sig = _signature_from_callable(\n                    new,\n                    follow_wrapper_chains=follow_wrapper_chains,\n                    skip_bound_arg=skip_bound_arg,\n                    sigcls=sigcls)\n            else:\n                # Finally, we should have at least __init__ implemented\n                init = _signature_get_user_defined_method(obj, '__init__')\n                if init is not None:\n                    sig = _signature_from_callable(\n                        init,\n                        follow_wrapper_chains=follow_wrapper_chains,\n                        skip_bound_arg=skip_bound_arg,\n                        sigcls=sigcls)\n\n        if sig is None:\n            # At this point we know, that `obj` is a class, with no user-\n            # defined '__init__', '__new__', or class-level '__call__'\n\n            for base in obj.__mro__[:-1]:\n                # Since '__text_signature__' is implemented as a\n                # descriptor that extracts text signature from the\n                # class docstring, if 'obj' is derived from a builtin\n                # class, its own '__text_signature__' may be 'None'.\n                # Therefore, we go through the MRO (except the last\n                # class in there, which is 'object') to find the first\n                # class with non-empty text signature.\n                try:\n                    text_sig = base.__text_signature__\n                except AttributeError:\n                    pass\n                else:\n                    if text_sig:\n                        # If 'obj' class has a __text_signature__ attribute:\n                        # return a signature based on it\n                        return _signature_fromstr(sigcls, obj, text_sig)\n\n            # No '__text_signature__' was found for the 'obj' class.\n            # Last option is to check if its '__init__' is\n            # object.__init__ or type.__init__.\n            if type not in obj.__mro__:\n                # We have a class (not metaclass), but no user-defined\n                # __init__ or __new__ for it\n                if (obj.__init__ is object.__init__ and\n                    obj.__new__ is object.__new__):\n                    # Return a signature of 'object' builtin.\n                    return sigcls.from_callable(object)\n                else:\n                    raise ValueError(\n                        'no signature found for builtin type {!r}'.format(obj))\n\n    elif not isinstance(obj, _NonUserDefinedCallables):\n        # An object with __call__\n        # We also check that the 'obj' is not an instance of\n        # _WrapperDescriptor or _MethodWrapper to avoid\n        # infinite recursion (and even potential segfault)\n        call = _signature_get_user_defined_method(type(obj), '__call__')\n        if call is not None:\n            try:\n                sig = _signature_from_callable(\n                    call,\n                    follow_wrapper_chains=follow_wrapper_chains,\n                    skip_bound_arg=skip_bound_arg,\n                    sigcls=sigcls)\n            except ValueError as ex:\n                msg = 'no signature found for {!r}'.format(obj)\n                raise ValueError(msg) from ex\n\n    if sig is not None:\n        # For classes and objects we skip the first parameter of their\n        # __call__, __new__, or __init__ methods\n        if skip_bound_arg:\n            return _signature_bound_method(sig)\n        else:\n            return sig\n\n    if isinstance(obj, types.BuiltinFunctionType):\n        # Raise a nicer error message for builtins\n        msg = 'no signature found for builtin function {!r}'.format(obj)\n        raise ValueError(msg)\n\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))\n\n\nclass _void:\n    \"\"\"A private marker - used in Parameter & Signature.\"\"\"\n\n\nclass _empty:\n    \"\"\"Marker object for Signature.empty and Parameter.empty.\"\"\"\n\n\nclass _ParameterKind(enum.IntEnum):\n    POSITIONAL_ONLY = 0\n    POSITIONAL_OR_KEYWORD = 1\n    VAR_POSITIONAL = 2\n    KEYWORD_ONLY = 3\n    VAR_KEYWORD = 4\n\n    def __str__(self):\n        return self._name_\n\n    @property\n    def description(self):\n        return _PARAM_NAME_MAPPING[self]\n\n_POSITIONAL_ONLY         = _ParameterKind.POSITIONAL_ONLY\n_POSITIONAL_OR_KEYWORD   = _ParameterKind.POSITIONAL_OR_KEYWORD\n_VAR_POSITIONAL          = _ParameterKind.VAR_POSITIONAL\n_KEYWORD_ONLY            = _ParameterKind.KEYWORD_ONLY\n_VAR_KEYWORD             = _ParameterKind.VAR_KEYWORD\n\n_PARAM_NAME_MAPPING = {\n    _POSITIONAL_ONLY: 'positional-only',\n    _POSITIONAL_OR_KEYWORD: 'positional or keyword',\n    _VAR_POSITIONAL: 'variadic positional',\n    _KEYWORD_ONLY: 'keyword-only',\n    _VAR_KEYWORD: 'variadic keyword'\n}\n\n\nclass Parameter:\n    \"\"\"Represents a parameter in a function signature.\n\n    Has the following public attributes:\n\n    * name : str\n        The name of the parameter as a string.\n    * default : object\n        The default value for the parameter if specified.  If the\n        parameter has no default value, this attribute is set to\n        `Parameter.empty`.\n    * annotation\n        The annotation for the parameter if specified.  If the\n        parameter has no annotation, this attribute is set to\n        `Parameter.empty`.\n    * kind : str\n        Describes how argument values are bound to the parameter.\n        Possible values: `Parameter.POSITIONAL_ONLY`,\n        `Parameter.POSITIONAL_OR_KEYWORD`, `Parameter.VAR_POSITIONAL`,\n        `Parameter.KEYWORD_ONLY`, `Parameter.VAR_KEYWORD`.\n    \"\"\"\n\n    __slots__ = ('_name', '_kind', '_default', '_annotation')\n\n    POSITIONAL_ONLY         = _POSITIONAL_ONLY\n    POSITIONAL_OR_KEYWORD   = _POSITIONAL_OR_KEYWORD\n    VAR_POSITIONAL          = _VAR_POSITIONAL\n    KEYWORD_ONLY            = _KEYWORD_ONLY\n    VAR_KEYWORD             = _VAR_KEYWORD\n\n    empty = _empty\n\n    def __init__(self, name, kind, *, default=_empty, annotation=_empty):\n        try:\n            self._kind = _ParameterKind(kind)\n        except ValueError:\n            raise ValueError(f'value {kind!r} is not a valid Parameter.kind')\n        if default is not _empty:\n            if self._kind in (_VAR_POSITIONAL, _VAR_KEYWORD):\n                msg = '{} parameters cannot have default values'\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n        self._default = default\n        self._annotation = annotation\n\n        if name is _empty:\n            raise ValueError('name is a required attribute for Parameter')\n\n        if not isinstance(name, str):\n            msg = 'name must be a str, not a {}'.format(type(name).__name__)\n            raise TypeError(msg)\n\n        if name[0] == '.' and name[1:].isdigit():\n            # These are implicit arguments generated by comprehensions. In\n            # order to provide a friendlier interface to users, we recast\n            # their name as \"implicitN\" and treat them as positional-only.\n            # See issue 19611.\n            if self._kind != _POSITIONAL_OR_KEYWORD:\n                msg = (\n                    'implicit arguments must be passed as '\n                    'positional or keyword arguments, not {}'\n                )\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n            self._kind = _POSITIONAL_ONLY\n            name = 'implicit{}'.format(name[1:])\n\n        if not name.isidentifier():\n            raise ValueError('{!r} is not a valid parameter name'.format(name))\n\n        self._name = name\n\n    def __reduce__(self):\n        return (type(self),\n                (self._name, self._kind),\n                {'_default': self._default,\n                 '_annotation': self._annotation})\n\n    def __setstate__(self, state):\n        self._default = state['_default']\n        self._annotation = state['_annotation']\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def default(self):\n        return self._default\n\n    @property\n    def annotation(self):\n        return self._annotation\n\n    @property\n    def kind(self):\n        return self._kind\n\n    def replace(self, *, name=_void, kind=_void,\n                annotation=_void, default=_void):\n        \"\"\"Creates a customized copy of the Parameter.\"\"\"\n\n        if name is _void:\n            name = self._name\n\n        if kind is _void:\n            kind = self._kind\n\n        if annotation is _void:\n            annotation = self._annotation\n\n        if default is _void:\n            default = self._default\n\n        return type(self)(name, kind, default=default, annotation=annotation)\n\n    def __str__(self):\n        kind = self.kind\n        formatted = self._name\n\n        # Add annotation and default value\n        if self._annotation is not _empty:\n            formatted = '{}: {}'.format(formatted,\n                                       formatannotation(self._annotation))\n\n        if self._default is not _empty:\n            if self._annotation is not _empty:\n                formatted = '{} = {}'.format(formatted, repr(self._default))\n            else:\n                formatted = '{}={}'.format(formatted, repr(self._default))\n\n        if kind == _VAR_POSITIONAL:\n            formatted = '*' + formatted\n        elif kind == _VAR_KEYWORD:\n            formatted = '**' + formatted\n\n        return formatted\n\n    def __repr__(self):\n        return '<{} \"{}\">'.format(self.__class__.__name__, self)\n\n    def __hash__(self):\n        return hash((self.name, self.kind, self.annotation, self.default))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Parameter):\n            return NotImplemented\n        return (self._name == other._name and\n                self._kind == other._kind and\n                self._default == other._default and\n                self._annotation == other._annotation)\n\n\nclass BoundArguments:\n    \"\"\"Result of `Signature.bind` call.  Holds the mapping of arguments\n    to the function's parameters.\n\n    Has the following public attributes:\n\n    * arguments : dict\n        An ordered mutable mapping of parameters' names to arguments' values.\n        Does not contain arguments' default values.\n    * signature : Signature\n        The Signature object that created this instance.\n    * args : tuple\n        Tuple of positional arguments values.\n    * kwargs : dict\n        Dict of keyword arguments values.\n    \"\"\"\n\n    __slots__ = ('arguments', '_signature', '__weakref__')\n\n    def __init__(self, signature, arguments):\n        self.arguments = arguments\n        self._signature = signature\n\n    @property\n    def signature(self):\n        return self._signature\n\n    @property\n    def args(self):\n        args = []\n        for param_name, param in self._signature.parameters.items():\n            if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                break\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                # We're done here. Other arguments\n                # will be mapped in 'BoundArguments.kwargs'\n                break\n            else:\n                if param.kind == _VAR_POSITIONAL:\n                    # *args\n                    args.extend(arg)\n                else:\n                    # plain argument\n                    args.append(arg)\n\n        return tuple(args)\n\n    @property\n    def kwargs(self):\n        kwargs = {}\n        kwargs_started = False\n        for param_name, param in self._signature.parameters.items():\n            if not kwargs_started:\n                if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                    kwargs_started = True\n                else:\n                    if param_name not in self.arguments:\n                        kwargs_started = True\n                        continue\n\n            if not kwargs_started:\n                continue\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                pass\n            else:\n                if param.kind == _VAR_KEYWORD:\n                    # **kwargs\n                    kwargs.update(arg)\n                else:\n                    # plain keyword argument\n                    kwargs[param_name] = arg\n\n        return kwargs\n\n    def apply_defaults(self):\n        \"\"\"Set default values for missing arguments.\n\n        For variable-positional arguments (*args) the default is an\n        empty tuple.\n\n        For variable-keyword arguments (**kwargs) the default is an\n        empty dict.\n        \"\"\"\n        arguments = self.arguments\n        new_arguments = []\n        for name, param in self._signature.parameters.items():\n            try:\n                new_arguments.append((name, arguments[name]))\n            except KeyError:\n                if param.default is not _empty:\n                    val = param.default\n                elif param.kind is _VAR_POSITIONAL:\n                    val = ()\n                elif param.kind is _VAR_KEYWORD:\n                    val = {}\n                else:\n                    # This BoundArguments was likely produced by\n                    # Signature.bind_partial().\n                    continue\n                new_arguments.append((name, val))\n        self.arguments = dict(new_arguments)\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, BoundArguments):\n            return NotImplemented\n        return (self.signature == other.signature and\n                self.arguments == other.arguments)\n\n    def __setstate__(self, state):\n        self._signature = state['_signature']\n        self.arguments = state['arguments']\n\n    def __getstate__(self):\n        return {'_signature': self._signature, 'arguments': self.arguments}\n\n    def __repr__(self):\n        args = []\n        for arg, value in self.arguments.items():\n            args.append('{}={!r}'.format(arg, value))\n        return '<{} ({})>'.format(self.__class__.__name__, ', '.join(args))\n\n\nclass Signature:\n    \"\"\"A Signature object represents the overall signature of a function.\n    It stores a Parameter object for each parameter accepted by the\n    function, as well as information specific to the function itself.\n\n    A Signature object has the following public attributes and methods:\n\n    * parameters : OrderedDict\n        An ordered mapping of parameters' names to the corresponding\n        Parameter objects (keyword-only arguments are in the same order\n        as listed in `code.co_varnames`).\n    * return_annotation : object\n        The annotation for the return type of the function if specified.\n        If the function has no annotation for its return type, this\n        attribute is set to `Signature.empty`.\n    * bind(*args, **kwargs) -> BoundArguments\n        Creates a mapping from positional and keyword arguments to\n        parameters.\n    * bind_partial(*args, **kwargs) -> BoundArguments\n        Creates a partial mapping from positional and keyword arguments\n        to parameters (simulating 'functools.partial' behavior.)\n    \"\"\"\n\n    __slots__ = ('_return_annotation', '_parameters')\n\n    _parameter_cls = Parameter\n    _bound_arguments_cls = BoundArguments\n\n    empty = _empty\n\n    def __init__(self, parameters=None, *, return_annotation=_empty,\n                 __validate_parameters__=True):\n        \"\"\"Constructs Signature from the given list of Parameter\n        objects and 'return_annotation'.  All arguments are optional.\n        \"\"\"\n\n        if parameters is None:\n            params = OrderedDict()\n        else:\n            if __validate_parameters__:\n                params = OrderedDict()\n                top_kind = _POSITIONAL_ONLY\n                kind_defaults = False\n\n                for param in parameters:\n                    kind = param.kind\n                    name = param.name\n\n                    if kind < top_kind:\n                        msg = (\n                            'wrong parameter order: {} parameter before {} '\n                            'parameter'\n                        )\n                        msg = msg.format(top_kind.description,\n                                         kind.description)\n                        raise ValueError(msg)\n                    elif kind > top_kind:\n                        kind_defaults = False\n                        top_kind = kind\n\n                    if kind in (_POSITIONAL_ONLY, _POSITIONAL_OR_KEYWORD):\n                        if param.default is _empty:\n                            if kind_defaults:\n                                # No default for this parameter, but the\n                                # previous parameter of the same kind had\n                                # a default\n                                msg = 'non-default argument follows default ' \\\n                                      'argument'\n                                raise ValueError(msg)\n                        else:\n                            # There is a default for this parameter.\n                            kind_defaults = True\n\n                    if name in params:\n                        msg = 'duplicate parameter name: {!r}'.format(name)\n                        raise ValueError(msg)\n\n                    params[name] = param\n            else:\n                params = OrderedDict((param.name, param) for param in parameters)\n\n        self._parameters = types.MappingProxyType(params)\n        self._return_annotation = return_annotation\n\n    @classmethod\n    def from_function(cls, func):\n        \"\"\"Constructs Signature for the given python function.\n\n        Deprecated since Python 3.5, use `Signature.from_callable()`.\n        \"\"\"\n\n        warnings.warn(\"inspect.Signature.from_function() is deprecated since \"\n                      \"Python 3.5, use Signature.from_callable()\",\n                      DeprecationWarning, stacklevel=2)\n        return _signature_from_function(cls, func)\n\n    @classmethod\n    def from_builtin(cls, func):\n        \"\"\"Constructs Signature for the given builtin function.\n\n        Deprecated since Python 3.5, use `Signature.from_callable()`.\n        \"\"\"\n\n        warnings.warn(\"inspect.Signature.from_builtin() is deprecated since \"\n                      \"Python 3.5, use Signature.from_callable()\",\n                      DeprecationWarning, stacklevel=2)\n        return _signature_from_builtin(cls, func)\n\n    @classmethod\n    def from_callable(cls, obj, *, follow_wrapped=True):\n        \"\"\"Constructs Signature for the given callable object.\"\"\"\n        return _signature_from_callable(obj, sigcls=cls,\n                                        follow_wrapper_chains=follow_wrapped)\n\n    @property\n    def parameters(self):\n        return self._parameters\n\n    @property\n    def return_annotation(self):\n        return self._return_annotation\n\n    def replace(self, *, parameters=_void, return_annotation=_void):\n        \"\"\"Creates a customized copy of the Signature.\n        Pass 'parameters' and/or 'return_annotation' arguments\n        to override them in the new copy.\n        \"\"\"\n\n        if parameters is _void:\n            parameters = self.parameters.values()\n\n        if return_annotation is _void:\n            return_annotation = self._return_annotation\n\n        return type(self)(parameters,\n                          return_annotation=return_annotation)\n\n    def _hash_basis(self):\n        params = tuple(param for param in self.parameters.values()\n                             if param.kind != _KEYWORD_ONLY)\n\n        kwo_params = {param.name: param for param in self.parameters.values()\n                                        if param.kind == _KEYWORD_ONLY}\n\n        return params, kwo_params, self.return_annotation\n\n    def __hash__(self):\n        params, kwo_params, return_annotation = self._hash_basis()\n        kwo_params = frozenset(kwo_params.values())\n        return hash((params, kwo_params, return_annotation))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Signature):\n            return NotImplemented\n        return self._hash_basis() == other._hash_basis()\n\n    def _bind(self, args, kwargs, *, partial=False):\n        \"\"\"Private method. Don't use directly.\"\"\"\n\n        arguments = {}\n\n        parameters = iter(self.parameters.values())\n        parameters_ex = ()\n        arg_vals = iter(args)\n\n        while True:\n            # Let's iterate through the positional arguments and corresponding\n            # parameters\n            try:\n                arg_val = next(arg_vals)\n            except StopIteration:\n                # No more positional arguments\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    # No more parameters. That's it. Just need to check that\n                    # we have no `kwargs` after this while loop\n                    break\n                else:\n                    if param.kind == _VAR_POSITIONAL:\n                        # That's OK, just empty *args.  Let's start parsing\n                        # kwargs\n                        break\n                    elif param.name in kwargs:\n                        if param.kind == _POSITIONAL_ONLY:\n                            msg = '{arg!r} parameter is positional only, ' \\\n                                  'but was passed as a keyword'\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg) from None\n                        parameters_ex = (param,)\n                        break\n                    elif (param.kind == _VAR_KEYWORD or\n                                                param.default is not _empty):\n                        # That's fine too - we have a default value for this\n                        # parameter.  So, lets start parsing `kwargs`, starting\n                        # with the current parameter\n                        parameters_ex = (param,)\n                        break\n                    else:\n                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,\n                        # not in `kwargs`\n                        if partial:\n                            parameters_ex = (param,)\n                            break\n                        else:\n                            msg = 'missing a required argument: {arg!r}'\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg) from None\n            else:\n                # We have a positional argument to process\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    raise TypeError('too many positional arguments') from None\n                else:\n                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                        # Looks like we have no parameter for this positional\n                        # argument\n                        raise TypeError(\n                            'too many positional arguments') from None\n\n                    if param.kind == _VAR_POSITIONAL:\n                        # We have an '*args'-like argument, let's fill it with\n                        # all positional arguments we have left and move on to\n                        # the next phase\n                        values = [arg_val]\n                        values.extend(arg_vals)\n                        arguments[param.name] = tuple(values)\n                        break\n\n                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:\n                        raise TypeError(\n                            'multiple values for argument {arg!r}'.format(\n                                arg=param.name)) from None\n\n                    arguments[param.name] = arg_val\n\n        # Now, we iterate through the remaining parameters to process\n        # keyword arguments\n        kwargs_param = None\n        for param in itertools.chain(parameters_ex, parameters):\n            if param.kind == _VAR_KEYWORD:\n                # Memorize that we have a '**kwargs'-like parameter\n                kwargs_param = param\n                continue\n\n            if param.kind == _VAR_POSITIONAL:\n                # Named arguments don't refer to '*args'-like parameters.\n                # We only arrive here if the positional arguments ended\n                # before reaching the last parameter before *args.\n                continue\n\n            param_name = param.name\n            try:\n                arg_val = kwargs.pop(param_name)\n            except KeyError:\n                # We have no value for this parameter.  It's fine though,\n                # if it has a default value, or it is an '*args'-like\n                # parameter, left alone by the processing of positional\n                # arguments.\n                if (not partial and param.kind != _VAR_POSITIONAL and\n                                                    param.default is _empty):\n                    raise TypeError('missing a required argument: {arg!r}'. \\\n                                    format(arg=param_name)) from None\n\n            else:\n                if param.kind == _POSITIONAL_ONLY:\n                    # This should never happen in case of a properly built\n                    # Signature object (but let's have this check here\n                    # to ensure correct behaviour just in case)\n                    raise TypeError('{arg!r} parameter is positional only, '\n                                    'but was passed as a keyword'. \\\n                                    format(arg=param.name))\n\n                arguments[param_name] = arg_val\n\n        if kwargs:\n            if kwargs_param is not None:\n                # Process our '**kwargs'-like parameter\n                arguments[kwargs_param.name] = kwargs\n            else:\n                raise TypeError(\n                    'got an unexpected keyword argument {arg!r}'.format(\n                        arg=next(iter(kwargs))))\n\n        return self._bound_arguments_cls(self, arguments)\n\n    def bind(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that maps the passed `args`\n        and `kwargs` to the function's signature.  Raises `TypeError`\n        if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs)\n\n    def bind_partial(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that partially maps the\n        passed `args` and `kwargs` to the function's signature.\n        Raises `TypeError` if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs, partial=True)\n\n    def __reduce__(self):\n        return (type(self),\n                (tuple(self._parameters.values()),),\n                {'_return_annotation': self._return_annotation})\n\n    def __setstate__(self, state):\n        self._return_annotation = state['_return_annotation']\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__, self)\n\n    def __str__(self):\n        result = []\n        render_pos_only_separator = False\n        render_kw_only_separator = True\n        for param in self.parameters.values():\n            formatted = str(param)\n\n            kind = param.kind\n\n            if kind == _POSITIONAL_ONLY:\n                render_pos_only_separator = True\n            elif render_pos_only_separator:\n                # It's not a positional-only parameter, and the flag\n                # is set to 'True' (there were pos-only params before.)\n                result.append('/')\n                render_pos_only_separator = False\n\n            if kind == _VAR_POSITIONAL:\n                # OK, we have an '*args'-like parameter, so we won't need\n                # a '*' to separate keyword-only arguments\n                render_kw_only_separator = False\n            elif kind == _KEYWORD_ONLY and render_kw_only_separator:\n                # We have a keyword-only parameter to render and we haven't\n                # rendered an '*args'-like parameter before, so add a '*'\n                # separator to the parameters list (\"foo(arg1, *, arg2)\" case)\n                result.append('*')\n                # This condition should be only triggered once, so\n                # reset the flag\n                render_kw_only_separator = False\n\n            result.append(formatted)\n\n        if render_pos_only_separator:\n            # There were only positional-only parameters, hence the\n            # flag was not reset to 'False'\n            result.append('/')\n\n        rendered = '({})'.format(', '.join(result))\n\n        if self.return_annotation is not _empty:\n            anno = formatannotation(self.return_annotation)\n            rendered += ' -> {}'.format(anno)\n\n        return rendered\n\n\ndef signature(obj, *, follow_wrapped=True):\n    \"\"\"Get a signature object for the passed callable.\"\"\"\n    return Signature.from_callable(obj, follow_wrapped=follow_wrapped)\n\n\ndef _main():\n    \"\"\" Logic for inspecting an object given at command line \"\"\"\n    import argparse\n    import importlib\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'object',\n         help=\"The object to be analysed. \"\n              \"It supports the 'module:qualname' syntax\")\n    parser.add_argument(\n        '-d', '--details', action='store_true',\n        help='Display info about the module rather than its source code')\n\n    args = parser.parse_args()\n\n    target = args.object\n    mod_name, has_attrs, attrs = target.partition(\":\")\n    try:\n        obj = module = importlib.import_module(mod_name)\n    except Exception as exc:\n        msg = \"Failed to import {} ({}: {})\".format(mod_name,\n                                                    type(exc).__name__,\n                                                    exc)\n        print(msg, file=sys.stderr)\n        sys.exit(2)\n\n    if has_attrs:\n        parts = attrs.split(\".\")\n        obj = module\n        for part in parts:\n            obj = getattr(obj, part)\n\n    if module.__name__ in sys.builtin_module_names:\n        print(\"Can't get info for builtin modules.\", file=sys.stderr)\n        sys.exit(1)\n\n    if args.details:\n        print('Target: {}'.format(target))\n        print('Origin: {}'.format(getsourcefile(module)))\n        print('Cached: {}'.format(module.__cached__))\n        if obj is module:\n            print('Loader: {}'.format(repr(module.__loader__)))\n            if hasattr(module, '__path__'):\n                print('Submodule search path: {}'.format(module.__path__))\n        else:\n            try:\n                __, lineno = findsource(obj)\n            except Exception:\n                pass\n            else:\n                print('Line: {}'.format(lineno))\n\n        print('\\n')\n    else:\n        print(getsource(obj))\n\n\nif __name__ == \"__main__\":\n    _main()\n",3192],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py":["\"\"\"Base classes for all estimators.\"\"\"\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n# License: BSD 3 clause\n\nimport copy\nimport warnings\nfrom collections import defaultdict\nimport platform\nimport inspect\nimport re\n\nimport numpy as np\n\nfrom . import __version__\nfrom ._config import get_config\nfrom .utils import _IS_32BIT\nfrom .utils._tags import (\n    _DEFAULT_TAGS,\n    _safe_tags,\n)\nfrom .utils.validation import check_X_y\nfrom .utils.validation import check_array\nfrom .utils.validation import _check_y\nfrom .utils.validation import _num_features\nfrom .utils._estimator_html_repr import estimator_html_repr\n\n\ndef clone(estimator, *, safe=True):\n    \"\"\"Constructs a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It yields a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    yield different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators.\n\n    \"\"\"\n    estimator_type = type(estimator)\n    # XXX: not handling dictionaries\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, \"get_params\") or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            if isinstance(estimator, type):\n                raise TypeError(\n                    \"Cannot clone object. \"\n                    + \"You should provide an instance of \"\n                    + \"scikit-learn estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n    return new_object\n\n\ndef _pprint(params, offset=0, printer=repr):\n    \"\"\"Pretty print the dictionary 'params'\n\n    Parameters\n    ----------\n    params : dict\n        The dictionary to pretty print\n\n    offset : int, default=0\n        The offset in characters to add at the begin of each line.\n\n    printer : callable, default=repr\n        The function to convert entries to strings, typically\n        the builtin str or repr\n\n    \"\"\"\n    # Do a multi-line justified repr:\n    options = np.get_printoptions()\n    np.set_printoptions(precision=5, threshold=64, edgeitems=2)\n    params_list = list()\n    this_line_length = offset\n    line_sep = \",\\n\" + (1 + offset // 2) * \" \"\n    for i, (k, v) in enumerate(sorted(params.items())):\n        if type(v) is float:\n            # use str for representing floating point numbers\n            # this way we get consistent representation across\n            # architectures and versions.\n            this_repr = \"%s=%s\" % (k, str(v))\n        else:\n            # use repr of the rest\n            this_repr = \"%s=%s\" % (k, printer(v))\n        if len(this_repr) > 500:\n            this_repr = this_repr[:300] + \"...\" + this_repr[-100:]\n        if i > 0:\n            if this_line_length + len(this_repr) >= 75 or \"\\n\" in this_repr:\n                params_list.append(line_sep)\n                this_line_length = len(line_sep)\n            else:\n                params_list.append(\", \")\n                this_line_length += 2\n        params_list.append(this_repr)\n        this_line_length += len(this_repr)\n\n    np.set_printoptions(**options)\n    lines = \"\".join(params_list)\n    # Strip trailing space to avoid nightmare in doctests\n    lines = \"\\n\".join(l.rstrip(\" \") for l in lines.split(\"\\n\"))\n    return lines\n\n\nclass BaseEstimator:\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n    \"\"\"\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [\n            p\n            for p in init_signature.parameters.values()\n            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n        ]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\n                    \"scikit-learn estimators should always \"\n                    \"specify their parameters in the signature\"\n                    \" of their __init__ (no varargs).\"\n                    \" %s with constructor %s doesn't \"\n                    \" follow this convention.\" % (cls, init_signature)\n                )\n        # Extract and sort argument names excluding 'self'\n        return sorted([p.name for p in parameters])\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        out = dict()\n        for key in self._get_param_names():\n            value = getattr(self, key)\n            if deep and hasattr(value, \"get_params\"):\n                deep_items = value.get_params().items()\n                out.update((key + \"__\" + k, val) for k, val in deep_items)\n            out[key] = value\n        return out\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.\n\n        Returns\n        -------\n        self : estimator instance\n            Estimator instance.\n        \"\"\"\n        if not params:\n            # Simple optimization to gain speed (inspect is slow)\n            return self\n        valid_params = self.get_params(deep=True)\n\n        nested_params = defaultdict(dict)  # grouped by prefix\n        for key, value in params.items():\n            key, delim, sub_key = key.partition(\"__\")\n            if key not in valid_params:\n                raise ValueError(\n                    \"Invalid parameter %s for estimator %s. \"\n                    \"Check the list of available parameters \"\n                    \"with `estimator.get_params().keys()`.\" % (key, self)\n                )\n\n            if delim:\n                nested_params[key][sub_key] = value\n            else:\n                setattr(self, key, value)\n                valid_params[key] = value\n\n        for key, sub_params in nested_params.items():\n            valid_params[key].set_params(**sub_params)\n\n        return self\n\n    def __repr__(self, N_CHAR_MAX=700):\n        # N_CHAR_MAX is the (approximate) maximum number of non-blank\n        # characters to render. We pass it as an optional parameter to ease\n        # the tests.\n\n        from .utils._pprint import _EstimatorPrettyPrinter\n\n        N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n\n        # use ellipsis for sequences with a lot of elements\n        pp = _EstimatorPrettyPrinter(\n            compact=True,\n            indent=1,\n            indent_at_name=True,\n            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW,\n        )\n\n        repr_ = pp.pformat(self)\n\n        # Use bruteforce ellipsis when there are a lot of non-blank characters\n        n_nonblank = len(\"\".join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n            regex = r\"^(\\s*\\S){%d}\" % lim\n            # The regex '^(\\s*\\S){%d}' % n\n            # matches from the start of the string until the nth non-blank\n            # character:\n            # - ^ matches the start of string\n            # - (pattern){n} matches n repetitions of pattern\n            # - \\s*\\S matches a non-blank char following zero or more blanks\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n\n            if \"\\n\" in repr_[left_lim:-right_lim]:\n                # The left side and right side aren't on the same line.\n                # To avoid weird cuts, e.g.:\n                # categoric...ore',\n                # we need to start the right side with an appropriate newline\n                # character so that it renders properly as:\n                # categoric...\n                # handle_unknown='ignore',\n                # so we add [^\\n]*\\n which matches until the next \\n\n                regex += r\"[^\\n]*\\n\"\n                right_lim = re.match(regex, repr_[::-1]).end()\n\n            ellipsis = \"...\"\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                # Only add ellipsis if it results in a shorter repr\n                repr_ = repr_[:left_lim] + \"...\" + repr_[-right_lim:]\n\n        return repr_\n\n    def __getstate__(self):\n        try:\n            state = super().__getstate__()\n        except AttributeError:\n            state = self.__dict__.copy()\n\n        if type(self).__module__.startswith(\"sklearn.\"):\n            return dict(state.items(), _sklearn_version=__version__)\n        else:\n            return state\n\n    def __setstate__(self, state):\n        if type(self).__module__.startswith(\"sklearn.\"):\n            pickle_version = state.pop(\"_sklearn_version\", \"pre-0.18\")\n            if pickle_version != __version__:\n                warnings.warn(\n                    \"Trying to unpickle estimator {0} from version {1} when \"\n                    \"using version {2}. This might lead to breaking code or \"\n                    \"invalid results. Use at your own risk.\".format(\n                        self.__class__.__name__, pickle_version, __version__\n                    ),\n                    UserWarning,\n                )\n        try:\n            super().__setstate__(state)\n        except AttributeError:\n            self.__dict__.update(state)\n\n    def _more_tags(self):\n        return _DEFAULT_TAGS\n\n    def _get_tags(self):\n        collected_tags = {}\n        for base_class in reversed(inspect.getmro(self.__class__)):\n            if hasattr(base_class, \"_more_tags\"):\n                # need the if because mixins might not have _more_tags\n                # but might do redundant work in estimators\n                # (i.e. calling more tags on BaseEstimator multiple times)\n                more_tags = base_class._more_tags(self)\n                collected_tags.update(more_tags)\n        return collected_tags\n\n    def _check_n_features(self, X, reset):\n        \"\"\"Set the `n_features_in_` attribute, or check against it.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n        reset : bool\n            If True, the `n_features_in_` attribute is set to `X.shape[1]`.\n            If False and the attribute exists, then check that it is equal to\n            `X.shape[1]`. If False and the attribute does *not* exist, then\n            the check is skipped.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n        \"\"\"\n        try:\n            n_features = _num_features(X)\n        except TypeError as e:\n            if not reset and hasattr(self, \"n_features_in_\"):\n                raise ValueError(\n                    \"X does not contain any features, but \"\n                    f\"{self.__class__.__name__} is expecting \"\n                    f\"{self.n_features_in_} features\"\n                ) from e\n            # If the number of features is not defined and reset=True,\n            # then we skip this check\n            return\n\n        if reset:\n            self.n_features_in_ = n_features\n            return\n\n        if not hasattr(self, \"n_features_in_\"):\n            # Skip this check if the expected number of expected input features\n            # was not recorded by calling fit first. This is typically the case\n            # for stateless transformers.\n            return\n\n        if n_features != self.n_features_in_:\n            raise ValueError(\n                f\"X has {n_features} features, but {self.__class__.__name__} \"\n                f\"is expecting {self.n_features_in_} features as input.\"\n            )\n\n    def _validate_data(\n        self,\n        X=\"no_validation\",\n        y=\"no_validation\",\n        reset=True,\n        validate_separately=False,\n        **check_params,\n    ):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape \\\n                (n_samples, n_features), default='no validation'\n            The input samples.\n            If `'no_validation'`, no validation is performed on `X`. This is\n            useful for meta-estimator which can delegate input validation to\n            their underlying estimator(s). In that case `y` must be passed and\n            the only accepted `check_params` are `multi_output` and\n            `y_numeric`.\n\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set. In that case\n              `X` must be passed.\n            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n              checked with either `check_array` or `check_X_y` depending on\n              `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if both `X` and `y` are\n            validated.\n        \"\"\"\n        if y is None and self._get_tags()[\"requires_y\"]:\n            raise ValueError(\n                f\"This {self.__class__.__name__} estimator \"\n                \"requires y to be passed, but the target y is None.\"\n            )\n\n        no_val_X = isinstance(X, str) and X == \"no_validation\"\n        no_val_y = y is None or isinstance(y, str) and y == \"no_validation\"\n\n        if no_val_X and no_val_y:\n            raise ValueError(\"Validation should be done on X, y or both.\")\n        elif not no_val_X and no_val_y:\n            X = check_array(X, **check_params)\n            out = X\n        elif no_val_X and not no_val_y:\n            y = _check_y(y, **check_params)\n            out = y\n        else:\n            if validate_separately:\n                # We need this because some estimators validate X and y\n                # separately, and in general, separately calling check_array()\n                # on X and y isn't equivalent to just calling check_X_y()\n                # :(\n                check_X_params, check_y_params = validate_separately\n                X = check_array(X, **check_X_params)\n                y = check_array(y, **check_y_params)\n            else:\n                X, y = check_X_y(X, y, **check_params)\n            out = X, y\n\n        if not no_val_X and check_params.get(\"ensure_2d\", True):\n            self._check_n_features(X, reset=reset)\n\n        return out\n\n    @property\n    def _repr_html_(self):\n        \"\"\"HTML representation of estimator.\n\n        This is redundant with the logic of `_repr_mimebundle_`. The latter\n        should be favorted in the long term, `_repr_html_` is only\n        implemented for consumers who do not interpret `_repr_mimbundle_`.\n        \"\"\"\n        if get_config()[\"display\"] != \"diagram\":\n            raise AttributeError(\n                \"_repr_html_ is only defined when the \"\n                \"'display' configuration option is set to \"\n                \"'diagram'\"\n            )\n        return self._repr_html_inner\n\n    def _repr_html_inner(self):\n        \"\"\"This function is returned by the @property `_repr_html_` to make\n        `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending\n        on `get_config()[\"display\"]`.\n        \"\"\"\n        return estimator_html_repr(self)\n\n    def _repr_mimebundle_(self, **kwargs):\n        \"\"\"Mime bundle used by jupyter kernels to display estimator\"\"\"\n        output = {\"text/plain\": repr(self)}\n        if get_config()[\"display\"] == \"diagram\":\n            output[\"text/html\"] = estimator_html_repr(self)\n        return output\n\n\nclass ClassifierMixin:\n    \"\"\"Mixin class for all classifiers in scikit-learn.\"\"\"\n\n    _estimator_type = \"classifier\"\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` wrt. `y`.\n        \"\"\"\n        from .metrics import accuracy_score\n\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {\"requires_y\": True}\n\n\nclass RegressorMixin:\n    \"\"\"Mixin class for all regression estimators in scikit-learn.\"\"\"\n\n    _estimator_type = \"regressor\"\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the coefficient of determination of the prediction.\n\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\\\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).\n        \"\"\"\n\n        from .metrics import r2_score\n\n        y_pred = self.predict(X)\n        return r2_score(y, y_pred, sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {\"requires_y\": True}\n\n\nclass ClusterMixin:\n    \"\"\"Mixin class for all cluster estimators in scikit-learn.\"\"\"\n\n    _estimator_type = \"clusterer\"\n\n    def fit_predict(self, X, y=None):\n        \"\"\"\n        Perform clustering on `X` and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,), dtype=np.int64\n            Cluster labels.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n        self.fit(X)\n        return self.labels_\n\n    def _more_tags(self):\n        return {\"preserves_dtype\": []}\n\n\nclass BiclusterMixin:\n    \"\"\"Mixin class for all bicluster estimators in scikit-learn.\"\"\"\n\n    @property\n    def biclusters_(self):\n        \"\"\"Convenient way to get row and column indicators together.\n\n        Returns the ``rows_`` and ``columns_`` members.\n        \"\"\"\n        return self.rows_, self.columns_\n\n    def get_indices(self, i):\n        \"\"\"Row and column indices of the `i`'th bicluster.\n\n        Only works if ``rows_`` and ``columns_`` attributes exist.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n\n        Returns\n        -------\n        row_ind : ndarray, dtype=np.intp\n            Indices of rows in the dataset that belong to the bicluster.\n        col_ind : ndarray, dtype=np.intp\n            Indices of columns in the dataset that belong to the bicluster.\n\n        \"\"\"\n        rows = self.rows_[i]\n        columns = self.columns_[i]\n        return np.nonzero(rows)[0], np.nonzero(columns)[0]\n\n    def get_shape(self, i):\n        \"\"\"Shape of the `i`'th bicluster.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n\n        Returns\n        -------\n        n_rows : int\n            Number of rows in the bicluster.\n\n        n_cols : int\n            Number of columns in the bicluster.\n        \"\"\"\n        indices = self.get_indices(i)\n        return tuple(len(i) for i in indices)\n\n    def get_submatrix(self, i, data):\n        \"\"\"Return the submatrix corresponding to bicluster `i`.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n        data : array-like of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        submatrix : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.\n\n        Notes\n        -----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.\n        \"\"\"\n        from .utils.validation import check_array\n\n        data = check_array(data, accept_sparse=\"csr\")\n        row_ind, col_ind = self.get_indices(i)\n        return data[row_ind[:, np.newaxis], col_ind]\n\n\nclass TransformerMixin:\n    \"\"\"Mixin class for all transformers in scikit-learn.\"\"\"\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n        if y is None:\n            # fit method of arity 1 (unsupervised transformation)\n            return self.fit(X, **fit_params).transform(X)\n        else:\n            # fit method of arity 2 (supervised transformation)\n            return self.fit(X, y, **fit_params).transform(X)\n\n\nclass DensityMixin:\n    \"\"\"Mixin class for all density estimators in scikit-learn.\"\"\"\n\n    _estimator_type = \"DensityEstimator\"\n\n    def score(self, X, y=None):\n        \"\"\"Return the score of the model on the data `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        score : float\n        \"\"\"\n        pass\n\n\nclass OutlierMixin:\n    \"\"\"Mixin class for all outlier detection estimators in scikit-learn.\"\"\"\n\n    _estimator_type = \"outlier_detector\"\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Perform fit on X and returns labels for X.\n\n        Returns -1 for outliers and 1 for inliers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features)\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            1 for inliers, -1 for outliers.\n        \"\"\"\n        # override for transductive outlier detectors like LocalOulierFactor\n        return self.fit(X).predict(X)\n\n\nclass MetaEstimatorMixin:\n    _required_parameters = [\"estimator\"]\n    \"\"\"Mixin class for all meta estimators in scikit-learn.\"\"\"\n\n\nclass MultiOutputMixin:\n    \"\"\"Mixin to mark estimators that support multioutput.\"\"\"\n\n    def _more_tags(self):\n        return {\"multioutput\": True}\n\n\nclass _UnstableArchMixin:\n    \"\"\"Mark estimators that are non-determinstic on 32bit or PowerPC\"\"\"\n\n    def _more_tags(self):\n        return {\n            \"non_deterministic\": (\n                _IS_32BIT or platform.machine().startswith((\"ppc\", \"powerpc\"))\n            )\n        }\n\n\ndef is_classifier(estimator):\n    \"\"\"Return True if the given estimator is (probably) a classifier.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a classifier and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"classifier\"\n\n\ndef is_regressor(estimator):\n    \"\"\"Return True if the given estimator is (probably) a regressor.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a regressor and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"regressor\"\n\n\ndef is_outlier_detector(estimator):\n    \"\"\"Return True if the given estimator is (probably) an outlier detector.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is an outlier detector and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"outlier_detector\"\n\n\ndef _is_pairwise(estimator):\n    \"\"\"Returns True if estimator is pairwise.\n\n    - If the `_pairwise` attribute and the tag are present and consistent,\n      then use the value and not issue a warning.\n    - If the `_pairwise` attribute and the tag are present and not\n      consistent, use the `_pairwise` value and issue a deprecation\n      warning.\n    - If only the `_pairwise` attribute is present and it is not False,\n      issue a deprecation warning and use the `_pairwise` value.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if the estimator is pairwise and False otherwise.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n        has_pairwise_attribute = hasattr(estimator, \"_pairwise\")\n        pairwise_attribute = getattr(estimator, \"_pairwise\", False)\n    pairwise_tag = _safe_tags(estimator, key=\"pairwise\")\n\n    if has_pairwise_attribute:\n        if pairwise_attribute != pairwise_tag:\n            warnings.warn(\n                \"_pairwise was deprecated in 0.24 and will be removed in 1.1 \"\n                \"(renaming of 0.26). Set the estimator tags of your estimator \"\n                \"instead\",\n                FutureWarning,\n            )\n        return pairwise_attribute\n\n    # use pairwise tag when the attribute is not present\n    return pairwise_tag\n",916],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/linear_model/_base.py":["\"\"\"\nGeneralized Linear Models.\n\"\"\"\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# Fabian Pedregosa <fabian.pedregosa@inria.fr>\n# Olivier Grisel <olivier.grisel@ensta.org>\n#         Vincent Michel <vincent.michel@inria.fr>\n#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#         Lars Buitinck\n#         Maryan Morel <maryan.morel@polytechnique.edu>\n#         Giorgio Patrini <giorgio.patrini@anu.edu.au>\n#         Maria Telenczuk <https://github.com/maikia>\n# License: BSD 3 clause\n\nfrom abc import ABCMeta, abstractmethod\nimport numbers\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy import linalg\nfrom scipy import optimize\nfrom scipy import sparse\nfrom scipy.special import expit\nfrom joblib import Parallel\n\nfrom ..base import BaseEstimator, ClassifierMixin, RegressorMixin, MultiOutputMixin\nfrom ..preprocessing._data import _is_constant_feature\nfrom ..utils import check_array\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils import check_random_state\nfrom ..utils.extmath import safe_sparse_dot\nfrom ..utils.extmath import _incremental_mean_and_var\nfrom ..utils.sparsefuncs import mean_variance_axis, inplace_column_scale\nfrom ..utils.fixes import sparse_lsqr\nfrom ..utils._seq_dataset import ArrayDataset32, CSRDataset32\nfrom ..utils._seq_dataset import ArrayDataset64, CSRDataset64\nfrom ..utils.validation import check_is_fitted, _check_sample_weight\nfrom ..utils.fixes import delayed\n\n# TODO: bayesian_ridge_regression and bayesian_regression_ard\n# should be squashed into its respective objects.\n\nSPARSE_INTERCEPT_DECAY = 0.01\n# For sparse data intercept updates are scaled by this decay factor to avoid\n# intercept oscillation.\n\n\n# FIXME in 1.2: parameter 'normalize' should be removed from linear models\n# in cases where now normalize=False. The default value of 'normalize' should\n# be changed to False in linear models where now normalize=True\ndef _deprecate_normalize(normalize, default, estimator_name):\n    \"\"\"Normalize is to be deprecated from linear models and a use of\n    a pipeline with a StandardScaler is to be recommended instead.\n    Here the appropriate message is selected to be displayed to the user\n    depending on the default normalize value (as it varies between the linear\n    models and normalize value selected by the user).\n\n    Parameters\n    ----------\n    normalize : bool,\n        normalize value passed by the user\n\n    default : bool,\n        default normalize value used by the estimator\n\n    estimator_name : string,\n        name of the linear estimator which calls this function.\n        The name will be used for writing the deprecation warnings\n\n    Returns\n    -------\n    normalize : bool,\n        normalize value which should further be used by the estimator at this\n        stage of the depreciation process\n\n    Notes\n    -----\n    This function should be updated in 1.2 depending on the value of\n    `normalize`:\n    - True, warning: `normalize` was deprecated in 1.2 and will be removed in\n      1.4. Suggest to use pipeline instead.\n    - False, `normalize` was deprecated in 1.2 and it will be removed in 1.4.\n      Leave normalize to its default value.\n    - `deprecated` - this should only be possible with default == False as from\n      1.2 `normalize` in all the linear models should be either removed or the\n      default should be set to False.\n    This function should be completely removed in 1.4.\n    \"\"\"\n\n    if normalize not in [True, False, \"deprecated\"]:\n        raise ValueError(\n            \"Leave 'normalize' to its default value or set it to True or False\"\n        )\n\n    if normalize == \"deprecated\":\n        _normalize = default\n    else:\n        _normalize = normalize\n\n    pipeline_msg = (\n        \"If you wish to scale the data, use Pipeline with a StandardScaler \"\n        \"in a preprocessing stage. To reproduce the previous behavior:\\n\\n\"\n        \"from sklearn.pipeline import make_pipeline\\n\\n\"\n        \"model = make_pipeline(StandardScaler(with_mean=False), \"\n        f\"{estimator_name}())\\n\\n\"\n        \"If you wish to pass a sample_weight parameter, you need to pass it \"\n        \"as a fit parameter to each step of the pipeline as follows:\\n\\n\"\n        \"kwargs = {s[0] + '__sample_weight': sample_weight for s \"\n        \"in model.steps}\\n\"\n        \"model.fit(X, y, **kwargs)\\n\\n\"\n    )\n\n    if estimator_name == \"Ridge\" or estimator_name == \"RidgeClassifier\":\n        alpha_msg = \"Set parameter alpha to: original_alpha * n_samples. \"\n    elif \"Lasso\" in estimator_name:\n        alpha_msg = \"Set parameter alpha to: original_alpha * np.sqrt(n_samples). \"\n    elif \"ElasticNet\" in estimator_name:\n        alpha_msg = (\n            \"Set parameter alpha to original_alpha * np.sqrt(n_samples) if \"\n            \"l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is \"\n            \"0. For other values of l1_ratio, no analytic formula is \"\n            \"available.\"\n        )\n    elif estimator_name == \"RidgeCV\" or estimator_name == \"RidgeClassifierCV\":\n        alpha_msg = \"Set parameter alphas to: original_alphas * n_samples. \"\n    else:\n        alpha_msg = \"\"\n\n    if default and normalize == \"deprecated\":\n        warnings.warn(\n            \"The default of 'normalize' will be set to False in version 1.2 \"\n            \"and deprecated in version 1.4.\\n\"\n            + pipeline_msg\n            + alpha_msg,\n            FutureWarning,\n        )\n    elif normalize != \"deprecated\" and normalize and not default:\n        warnings.warn(\n            \"'normalize' was deprecated in version 1.0 and will be removed in 1.2.\\n\"\n            + pipeline_msg\n            + alpha_msg,\n            FutureWarning,\n        )\n    elif not normalize and not default:\n        warnings.warn(\n            \"'normalize' was deprecated in version 1.0 and will be \"\n            \"removed in 1.2. \"\n            \"Please leave the normalize parameter to its default value to \"\n            \"silence this warning. The default behavior of this estimator \"\n            \"is to not do any normalization. If normalization is needed \"\n            \"please use sklearn.preprocessing.StandardScaler instead.\",\n            FutureWarning,\n        )\n\n    return _normalize\n\n\ndef make_dataset(X, y, sample_weight, random_state=None):\n    \"\"\"Create ``Dataset`` abstraction for sparse and dense inputs.\n\n    This also returns the ``intercept_decay`` which is different\n    for sparse datasets.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Training data\n\n    y : array-like, shape (n_samples, )\n        Target values.\n\n    sample_weight : numpy array of shape (n_samples,)\n        The weight of each sample\n\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for dataset shuffling and noise.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    dataset\n        The ``Dataset`` abstraction\n    intercept_decay\n        The intercept decay\n    \"\"\"\n\n    rng = check_random_state(random_state)\n    # seed should never be 0 in SequentialDataset64\n    seed = rng.randint(1, np.iinfo(np.int32).max)\n\n    if X.dtype == np.float32:\n        CSRData = CSRDataset32\n        ArrayData = ArrayDataset32\n    else:\n        CSRData = CSRDataset64\n        ArrayData = ArrayDataset64\n\n    if sp.issparse(X):\n        dataset = CSRData(X.data, X.indptr, X.indices, y, sample_weight, seed=seed)\n        intercept_decay = SPARSE_INTERCEPT_DECAY\n    else:\n        X = np.ascontiguousarray(X)\n        dataset = ArrayData(X, y, sample_weight, seed=seed)\n        intercept_decay = 1.0\n\n    return dataset, intercept_decay\n\n\ndef _preprocess_data(\n    X,\n    y,\n    fit_intercept,\n    normalize=False,\n    copy=True,\n    sample_weight=None,\n    return_mean=False,\n    check_input=True,\n):\n    \"\"\"Center and scale data.\n\n    Centers data to have mean zero along axis 0. If fit_intercept=False or if\n    the X is a sparse matrix, no centering is done, but normalization can still\n    be applied. The function returns the statistics necessary to reconstruct\n    the input data, which are X_offset, y_offset, X_scale, such that the output\n\n        X = (X - X_offset) / X_scale\n\n    X_scale is the L2 norm of X - X_offset. If sample_weight is not None,\n    then the weighted mean of X and y is zero, and not the mean itself. If\n    return_mean=True, the mean, eventually weighted, is returned, independently\n    of whether X was centered (option used for optimization with sparse data in\n    coordinate_descend).\n\n    This is here because nearly all linear models will want their data to be\n    centered. This function also systematically makes y consistent with X.dtype\n    \"\"\"\n    if isinstance(sample_weight, numbers.Number):\n        sample_weight = None\n    if sample_weight is not None:\n        sample_weight = np.asarray(sample_weight)\n\n    if check_input:\n        X = check_array(X, copy=copy, accept_sparse=[\"csr\", \"csc\"], dtype=FLOAT_DTYPES)\n    elif copy:\n        if sp.issparse(X):\n            X = X.copy()\n        else:\n            X = X.copy(order=\"K\")\n\n    y = np.asarray(y, dtype=X.dtype)\n\n    if fit_intercept:\n        if sp.issparse(X):\n            X_offset, X_var = mean_variance_axis(X, axis=0, weights=sample_weight)\n            if not return_mean:\n                X_offset[:] = X.dtype.type(0)\n        else:\n            if normalize:\n                X_offset, X_var, _ = _incremental_mean_and_var(\n                    X,\n                    last_mean=0.0,\n                    last_variance=0.0,\n                    last_sample_count=0.0,\n                    sample_weight=sample_weight,\n                )\n            else:\n                X_offset = np.average(X, axis=0, weights=sample_weight)\n\n            X_offset = X_offset.astype(X.dtype, copy=False)\n            X -= X_offset\n\n        if normalize:\n            X_var = X_var.astype(X.dtype, copy=False)\n            # Detect constant features on the computed variance, before taking\n            # the np.sqrt. Otherwise constant features cannot be detected with\n            # sample weights.\n            constant_mask = _is_constant_feature(X_var, X_offset, X.shape[0])\n            if sample_weight is None:\n                X_var *= X.shape[0]\n            else:\n                X_var *= sample_weight.sum()\n            X_scale = np.sqrt(X_var, out=X_var)\n            X_scale[constant_mask] = 1.0\n            if sp.issparse(X):\n                inplace_column_scale(X, 1.0 / X_scale)\n            else:\n                X /= X_scale\n        else:\n            X_scale = np.ones(X.shape[1], dtype=X.dtype)\n\n        y_offset = np.average(y, axis=0, weights=sample_weight)\n        y = y - y_offset\n    else:\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\n        if y.ndim == 1:\n            y_offset = X.dtype.type(0)\n        else:\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\n\n    return X, y, X_offset, y_offset, X_scale\n\n\n# TODO: _rescale_data should be factored into _preprocess_data.\n# Currently, the fact that sag implements its own way to deal with\n# sample_weight makes the refactoring tricky.\n\n\ndef _rescale_data(X, y, sample_weight):\n    \"\"\"Rescale data sample-wise by square root of sample_weight.\n\n    For many linear models, this enables easy support for sample_weight.\n\n    Returns\n    -------\n    X_rescaled : {array-like, sparse matrix}\n\n    y_rescaled : {array-like, sparse matrix}\n    \"\"\"\n    n_samples = X.shape[0]\n    sample_weight = np.asarray(sample_weight)\n    if sample_weight.ndim == 0:\n        sample_weight = np.full(n_samples, sample_weight, dtype=sample_weight.dtype)\n    sample_weight = np.sqrt(sample_weight)\n    sw_matrix = sparse.dia_matrix((sample_weight, 0), shape=(n_samples, n_samples))\n    X = safe_sparse_dot(sw_matrix, X)\n    y = safe_sparse_dot(sw_matrix, y)\n    return X, y\n\n\nclass LinearModel(BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Base class for Linear Models\"\"\"\n\n    @abstractmethod\n    def fit(self, X, y):\n        \"\"\"Fit model.\"\"\"\n\n    def _decision_function(self, X):\n        check_is_fitted(self)\n\n        X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n        return safe_sparse_dot(X, self.coef_.T, dense_output=True) + self.intercept_\n\n    def predict(self, X):\n        \"\"\"\n        Predict using the linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array, shape (n_samples,)\n            Returns predicted values.\n        \"\"\"\n        return self._decision_function(X)\n\n    _preprocess_data = staticmethod(_preprocess_data)\n\n    def _set_intercept(self, X_offset, y_offset, X_scale):\n        \"\"\"Set the intercept_\"\"\"\n        if self.fit_intercept:\n            self.coef_ = self.coef_ / X_scale\n            self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n        else:\n            self.intercept_ = 0.0\n\n    def _more_tags(self):\n        return {\"requires_y\": True}\n\n\n# XXX Should this derive from LinearModel? It should be a mixin, not an ABC.\n# Maybe the n_features checking can be moved to LinearModel.\nclass LinearClassifierMixin(ClassifierMixin):\n    \"\"\"Mixin for linear classifiers.\n\n    Handles prediction for sparse and dense X.\n    \"\"\"\n\n    def decision_function(self, X):\n        \"\"\"\n        Predict confidence scores for samples.\n\n        The confidence score for a sample is proportional to the signed\n        distance of that sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n            Confidence scores per (sample, class) combination. In the binary\n            case, confidence score for self.classes_[1] where >0 means this\n            class would be predicted.\n        \"\"\"\n        check_is_fitted(self)\n\n        X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n        scores = safe_sparse_dot(X, self.coef_.T, dense_output=True) + self.intercept_\n        return scores.ravel() if scores.shape[1] == 1 else scores\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for samples in X.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array, shape [n_samples]\n            Predicted class label per sample.\n        \"\"\"\n        scores = self.decision_function(X)\n        if len(scores.shape) == 1:\n            indices = (scores > 0).astype(int)\n        else:\n            indices = scores.argmax(axis=1)\n        return self.classes_[indices]\n\n    def _predict_proba_lr(self, X):\n        \"\"\"Probability estimation for OvR logistic regression.\n\n        Positive class probabilities are computed as\n        1. / (1. + np.exp(-self.decision_function(X)));\n        multiclass is handled by normalizing that over all classes.\n        \"\"\"\n        prob = self.decision_function(X)\n        expit(prob, out=prob)\n        if prob.ndim == 1:\n            return np.vstack([1 - prob, prob]).T\n        else:\n            # OvR normalization, like LibLinear's predict_probability\n            prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n            return prob\n\n\nclass SparseCoefMixin:\n    \"\"\"Mixin for converting coef_ to and from CSR format.\n\n    L1-regularizing estimators should inherit this.\n    \"\"\"\n\n    def densify(self):\n        \"\"\"\n        Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before densifying.\"\n        check_is_fitted(self, msg=msg)\n        if sp.issparse(self.coef_):\n            self.coef_ = self.coef_.toarray()\n        return self\n\n    def sparsify(self):\n        \"\"\"\n        Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before sparsifying.\"\n        check_is_fitted(self, msg=msg)\n        self.coef_ = sp.csr_matrix(self.coef_)\n        return self\n\n\nclass LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):\n    \"\"\"\n    Ordinary least squares Linear Regression.\n\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n    to minimize the residual sum of squares between the observed targets in\n    the dataset, and the targets predicted by the linear approximation.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : bool, default=False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n        .. deprecated:: 1.0\n           `normalize` was deprecated in version 1.0 and will be\n           removed in 1.2.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This will only provide\n        speedup for n_targets > 1 and sufficient large problems.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive : bool, default=False\n        When set to ``True``, forces the coefficients to be positive. This\n        option is only supported for dense arrays.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features, ) or (n_targets, n_features)\n        Estimated coefficients for the linear regression problem.\n        If multiple targets are passed during the fit (y 2D), this\n        is a 2D array of shape (n_targets, n_features), while if only\n        one target is passed, this is a 1D array of length n_features.\n\n    rank_ : int\n        Rank of matrix `X`. Only available when `X` is dense.\n\n    singular_ : array of shape (min(X, y),)\n        Singular values of `X`. Only available when `X` is dense.\n\n    intercept_ : float or array of shape (n_targets,)\n        Independent term in the linear model. Set to 0.0 if\n        `fit_intercept = False`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    See Also\n    --------\n    Ridge : Ridge regression addresses some of the\n        problems of Ordinary Least Squares by imposing a penalty on the\n        size of the coefficients with l2 regularization.\n    Lasso : The Lasso is a linear model that estimates\n        sparse coefficients with l1 regularization.\n    ElasticNet : Elastic-Net is a linear regression\n        model trained with both l1 and l2 -norm regularization of the\n        coefficients.\n\n    Notes\n    -----\n    From the implementation point of view, this is just plain Ordinary\n    Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n    (scipy.optimize.nnls) wrapped as a predictor object.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    >>> # y = 1 * x_0 + 2 * x_1 + 3\n    >>> y = np.dot(X, np.array([1, 2])) + 3\n    >>> reg = LinearRegression().fit(X, y)\n    >>> reg.score(X, y)\n    1.0\n    >>> reg.coef_\n    array([1., 2.])\n    >>> reg.intercept_\n    3.0...\n    >>> reg.predict(np.array([[3, 5]]))\n    array([16.])\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        fit_intercept=True,\n        normalize=\"deprecated\",\n        copy_X=True,\n        n_jobs=None,\n        positive=False,\n    ):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n        self.positive = positive\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"\n        Fit linear model.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values. Will be cast to X's dtype if necessary.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample.\n\n            .. versionadded:: 0.17\n               parameter *sample_weight* support to LinearRegression.\n\n        Returns\n        -------\n        self : object\n            Fitted Estimator.\n        \"\"\"\n\n        _normalize = _deprecate_normalize(\n            self.normalize, default=False, estimator_name=self.__class__.__name__\n        )\n\n        n_jobs_ = self.n_jobs\n\n        accept_sparse = False if self.positive else [\"csr\", \"csc\", \"coo\"]\n\n        X, y = self._validate_data(\n            X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True\n        )\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n            X,\n            y,\n            fit_intercept=self.fit_intercept,\n            normalize=_normalize,\n            copy=self.copy_X,\n            sample_weight=sample_weight,\n            return_mean=True,\n        )\n\n        if sample_weight is not None:\n            # Sample weight can be implemented via a simple rescaling.\n            X, y = _rescale_data(X, y, sample_weight)\n\n        if self.positive:\n            if y.ndim < 2:\n                self.coef_, self._residues = optimize.nnls(X, y)\n            else:\n                # scipy.optimize.nnls cannot handle y with shape (M, K)\n                outs = Parallel(n_jobs=n_jobs_)(\n                    delayed(optimize.nnls)(X, y[:, j]) for j in range(y.shape[1])\n                )\n                self.coef_, self._residues = map(np.vstack, zip(*outs))\n        elif sp.issparse(X):\n            X_offset_scale = X_offset / X_scale\n\n            def matvec(b):\n                return X.dot(b) - b.dot(X_offset_scale)\n\n            def rmatvec(b):\n                return X.T.dot(b) - X_offset_scale * np.sum(b)\n\n            X_centered = sparse.linalg.LinearOperator(\n                shape=X.shape, matvec=matvec, rmatvec=rmatvec\n            )\n\n            if y.ndim < 2:\n                out = sparse_lsqr(X_centered, y)\n                self.coef_ = out[0]\n                self._residues = out[3]\n            else:\n                # sparse_lstsq cannot handle y with shape (M, K)\n                outs = Parallel(n_jobs=n_jobs_)(\n                    delayed(sparse_lsqr)(X_centered, y[:, j].ravel())\n                    for j in range(y.shape[1])\n                )\n                self.coef_ = np.vstack([out[0] for out in outs])\n                self._residues = np.vstack([out[3] for out in outs])\n        else:\n            self.coef_, self._residues, self.rank_, self.singular_ = linalg.lstsq(X, y)\n            self.coef_ = self.coef_.T\n\n        if y.ndim == 1:\n            self.coef_ = np.ravel(self.coef_)\n        self._set_intercept(X_offset, y_offset, X_scale)\n        return self\n\n\ndef _check_precomputed_gram_matrix(\n    X, precompute, X_offset, X_scale, rtol=1e-7, atol=1e-5\n):\n    \"\"\"Computes a single element of the gram matrix and compares it to\n    the corresponding element of the user supplied gram matrix.\n\n    If the values do not match a ValueError will be thrown.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Data array.\n\n    precompute : array-like of shape (n_features, n_features)\n        User-supplied gram matrix.\n\n    X_offset : ndarray of shape (n_features,)\n        Array of feature means used to center design matrix.\n\n    X_scale : ndarray of shape (n_features,)\n        Array of feature scale factors used to normalize design matrix.\n\n    rtol : float, default=1e-7\n        Relative tolerance; see numpy.allclose.\n\n    atol : float, default=1e-5\n        absolute tolerance; see :func`numpy.allclose`. Note that the default\n        here is more tolerant than the default for\n        :func:`numpy.testing.assert_allclose`, where `atol=0`.\n\n    Raises\n    ------\n    ValueError\n        Raised when the provided Gram matrix is not consistent.\n    \"\"\"\n\n    n_features = X.shape[1]\n    f1 = n_features // 2\n    f2 = min(f1 + 1, n_features - 1)\n\n    v1 = (X[:, f1] - X_offset[f1]) * X_scale[f1]\n    v2 = (X[:, f2] - X_offset[f2]) * X_scale[f2]\n\n    expected = np.dot(v1, v2)\n    actual = precompute[f1, f2]\n\n    if not np.isclose(expected, actual, rtol=rtol, atol=atol):\n        raise ValueError(\n            \"Gram matrix passed in via 'precompute' parameter \"\n            \"did not pass validation when a single element was \"\n            \"checked - please check that it was computed \"\n            f\"properly. For element ({f1},{f2}) we computed \"\n            f\"{expected} but the user-supplied value was \"\n            f\"{actual}.\"\n        )\n\n\ndef _pre_fit(\n    X,\n    y,\n    Xy,\n    precompute,\n    normalize,\n    fit_intercept,\n    copy,\n    check_input=True,\n    sample_weight=None,\n):\n    \"\"\"Aux function used at beginning of fit in linear models\n\n    Parameters\n    ----------\n    order : 'F', 'C' or None, default=None\n        Whether X and y will be forced to be fortran or c-style. Only relevant\n        if sample_weight is not None.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    if sparse.isspmatrix(X):\n        # copy is not needed here as X is not modified inplace when X is sparse\n        precompute = False\n        X, y, X_offset, y_offset, X_scale = _preprocess_data(\n            X,\n            y,\n            fit_intercept=fit_intercept,\n            normalize=normalize,\n            copy=False,\n            return_mean=True,\n            check_input=check_input,\n        )\n    else:\n        # copy was done in fit if necessary\n        X, y, X_offset, y_offset, X_scale = _preprocess_data(\n            X,\n            y,\n            fit_intercept=fit_intercept,\n            normalize=normalize,\n            copy=copy,\n            check_input=check_input,\n            sample_weight=sample_weight,\n        )\n    if sample_weight is not None:\n        X, y = _rescale_data(X, y, sample_weight=sample_weight)\n\n    # FIXME: 'normalize' to be removed in 1.2\n    if hasattr(precompute, \"__array__\"):\n        if (\n            fit_intercept\n            and not np.allclose(X_offset, np.zeros(n_features))\n            or normalize\n            and not np.allclose(X_scale, np.ones(n_features))\n        ):\n            warnings.warn(\n                \"Gram matrix was provided but X was centered to fit \"\n                \"intercept, or X was normalized : recomputing Gram matrix.\",\n                UserWarning,\n            )\n            # recompute Gram\n            precompute = \"auto\"\n            Xy = None\n        elif check_input:\n            # If we're going to use the user's precomputed gram matrix, we\n            # do a quick check to make sure its not totally bogus.\n            _check_precomputed_gram_matrix(X, precompute, X_offset, X_scale)\n\n    # precompute if n_samples > n_features\n    if isinstance(precompute, str) and precompute == \"auto\":\n        precompute = n_samples > n_features\n\n    if precompute is True:\n        # make sure that the 'precompute' array is contiguous.\n        precompute = np.empty(shape=(n_features, n_features), dtype=X.dtype, order=\"C\")\n        np.dot(X.T, X, out=precompute)\n\n    if not hasattr(precompute, \"__array__\"):\n        Xy = None  # cannot use Xy if precompute is not Gram\n\n    if hasattr(precompute, \"__array__\") and Xy is None:\n        common_dtype = np.find_common_type([X.dtype, y.dtype], [])\n        if y.ndim == 1:\n            # Xy is 1d, make sure it is contiguous.\n            Xy = np.empty(shape=n_features, dtype=common_dtype, order=\"C\")\n            np.dot(X.T, y, out=Xy)\n        else:\n            # Make sure that Xy is always F contiguous even if X or y are not\n            # contiguous: the goal is to make it fast to extract the data for a\n            # specific target.\n            n_targets = y.shape[1]\n            Xy = np.empty(shape=(n_features, n_targets), dtype=common_dtype, order=\"F\")\n            np.dot(y.T, X, out=Xy.T)\n\n    return X, y, X_offset, y_offset, X_scale, precompute, Xy\n",871],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/linear_model/_ridge_0_22.py":["#===============================================================================\n# Copyright 2014-2021 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#===============================================================================\n\nimport numpy as np\nfrom scipy import sparse as sp\nfrom sklearn.utils import check_array, check_X_y\nfrom sklearn.linear_model._ridge import _BaseRidge\nfrom sklearn.linear_model._ridge import Ridge as Ridge_original\n\nimport daal4py\nfrom .._utils import make2d, getFPType, get_patch_message, sklearn_check_version\nimport logging\n\n\ndef _daal4py_fit(self, X, y_):\n    X = make2d(X)\n    y = make2d(y_)\n\n    _fptype = getFPType(X)\n\n    ridge_params = np.asarray(self.alpha, dtype=X.dtype)\n    if ridge_params.size != 1 and ridge_params.size != y.shape[1]:\n        raise ValueError(\"alpha length is wrong\")\n    ridge_params = ridge_params.reshape((1, -1))\n\n    ridge_alg = daal4py.ridge_regression_training(\n        fptype=_fptype,\n        method='defaultDense',\n        interceptFlag=(self.fit_intercept is True),\n        ridgeParameters=ridge_params\n    )\n    try:\n        ridge_res = ridge_alg.compute(X, y)\n    except RuntimeError:\n        return None\n\n    ridge_model = ridge_res.model\n    self.daal_model_ = ridge_model\n    coefs = ridge_model.Beta\n\n    self.intercept_ = coefs[:, 0].copy(order='C')\n    self.coef_ = coefs[:, 1:].copy(order='C')\n\n    if self.coef_.shape[0] == 1 and y_.ndim == 1:\n        self.coef_ = np.ravel(self.coef_)\n        self.intercept_ = self.intercept_[0]\n\n    return self\n\n\ndef _daal4py_predict(self, X):\n    X = make2d(X)\n    _fptype = getFPType(self.coef_)\n\n    ridge_palg = daal4py.ridge_regression_prediction(\n        fptype=_fptype,\n        method='defaultDense'\n    )\n    if self.n_features_in_ != X.shape[1]:\n        raise ValueError(\n            (f'X has {X.shape[1]} features, '\n             f'but Ridge is expecting {self.n_features_in_} features as input'))\n    ridge_res = ridge_palg.compute(X, self.daal_model_)\n\n    res = ridge_res.prediction\n\n    if res.shape[1] == 1 and self.coef_.ndim == 1:\n        res = np.ravel(res)\n    return res\n\n\ndef _fit_ridge(self, X, y, sample_weight=None):\n    \"\"\"Fit Ridge regression model\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n        Training data\n\n    y : array-like, shape = [n_samples] or [n_samples, n_targets]\n        Target values\n\n    sample_weight : float or numpy array of shape [n_samples]\n        Individual weights for each sample\n\n    Returns\n    -------\n    self : returns an instance of self.\n    \"\"\"\n    if sklearn_check_version('1.0'):\n        from sklearn.linear_model._base import _deprecate_normalize\n        self._normalize = _deprecate_normalize(\n            self.normalize, default=False,\n            estimator_name=self.__class__.__name__\n        )\n\n    X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=[np.float64, np.float32],\n                     multi_output=True, y_numeric=True)\n    self.n_features_in_ = X.shape[1]\n    self.sample_weight_ = sample_weight\n    self.fit_shape_good_for_daal_ = True if X.shape[0] >= X.shape[1] else False\n    if not self.solver == 'auto' or \\\n            sp.issparse(X) or \\\n            not self.fit_shape_good_for_daal_ or \\\n            not (X.dtype == np.float64 or X.dtype == np.float32) or \\\n            sample_weight is not None:\n        if hasattr(self, 'daal_model_'):\n            del self.daal_model_\n        logging.info(\"sklearn.linear_model.Ridge.fit: \" + get_patch_message(\"sklearn\"))\n        return super(Ridge, self).fit(X, y, sample_weight=sample_weight)\n    self.n_iter_ = None\n    logging.info(\"sklearn.linear_model.Ridge.fit: \" + get_patch_message(\"daal\"))\n    res = _daal4py_fit(self, X, y)\n    if res is None:\n        logging.info(\n            \"sklearn.linear_model.Ridge.fit: \" + get_patch_message(\"sklearn_after_daal\"))\n        if hasattr(self, 'daal_model_'):\n            del self.daal_model_\n        return super(Ridge, self).fit(X, y, sample_weight=sample_weight)\n    return res\n\n\ndef _predict_ridge(self, X):\n    \"\"\"Predict using the linear model\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n        Samples.\n\n    Returns\n    -------\n    C : array, shape = (n_samples,)\n        Returns predicted values.\n    \"\"\"\n    X = check_array(\n        X, accept_sparse=['csr', 'csc', 'coo'], dtype=[np.float64, np.float32])\n    good_shape_for_daal = \\\n        True if X.ndim <= 1 else True if X.shape[0] >= X.shape[1] else False\n\n    if not self.solver == 'auto' or \\\n            not hasattr(self, 'daal_model_') or \\\n            sp.issparse(X) or \\\n            not good_shape_for_daal or \\\n            (hasattr(self, 'sample_weight_') and self.sample_weight_ is not None):\n        logging.info(\n            \"sklearn.linear_model.Ridge.predict: \" + get_patch_message(\"sklearn\"))\n        return self._decision_function(X)\n    logging.info(\"sklearn.linear_model.Ridge.predict: \" + get_patch_message(\"daal\"))\n    return _daal4py_predict(self, X)\n\n\nclass Ridge(Ridge_original, _BaseRidge):\n    __doc__ = Ridge_original.__doc__\n\n    if sklearn_check_version('1.0'):\n        def __init__(self, alpha=1.0, fit_intercept=True, normalize='deprecated',\n                     copy_X=True, max_iter=None, tol=1e-3, solver=\"auto\",\n                     random_state=None):\n            self.alpha = alpha\n            self.fit_intercept = fit_intercept\n            self.normalize = normalize\n            self.copy_X = copy_X\n            self.max_iter = max_iter\n            self.tol = tol\n            self.solver = solver\n            self.random_state = random_state\n    else:\n        def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n                     copy_X=True, max_iter=None, tol=1e-3, solver=\"auto\",\n                     random_state=None):\n            self.alpha = alpha\n            self.fit_intercept = fit_intercept\n            self.normalize = normalize\n            self.copy_X = copy_X\n            self.max_iter = max_iter\n            self.tol = tol\n            self.solver = solver\n            self.random_state = random_state\n\n    def fit(self, X, y, sample_weight=None):\n        return _fit_ridge(self, X, y, sample_weight=sample_weight)\n\n    def predict(self, X):\n        return _predict_ridge(self, X)\n",198]},"functions":{"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py",1205],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:437)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",437],"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:458)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",458],"_add_filter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:181)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",181],"simplefilter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:165)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",165],"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:477)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",477],"_ensure_no_complex_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:475)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",475],"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py",16],"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py",24],"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:284)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py",284],"issubdtype (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:358)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py",358],"_sum_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2118)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",2118],"<dictcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:70)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",70],"_wrapreduction (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:69)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",69],"sum (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2123)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",2123],"_safe_accumulator_op (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:869)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py",869],"_assert_all_finite (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:89)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",89],"__subclasscheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:121)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py",121],"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:117)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py",117],"_num_samples (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:252)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",252],"check_array (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:485)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",485],"get_patch_message (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/_utils.py:96)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/_utils.py",96],"isEnabledFor (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py:1689)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py",1689],"info (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py:1436)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py",1436],"info (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py:2089)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/logging/__init__.py",2089],"isclass (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/inspect.py:73)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/inspect.py",73],"<listcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:1199)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",1199],"check_is_fitted (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:1138)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",1138],"_num_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:199)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",199],"_check_n_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:349)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py",349],"_validate_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:395)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py",395],"safe_sparse_dot (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:120)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py",120],"_decision_function (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/linear_model/_base.py:342)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/linear_model/_base.py",342],"_predict_ridge (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/linear_model/_ridge_0_22.py:136)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/linear_model/_ridge_0_22.py",136],"predict (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/linear_model/_ridge_0_22.py:197)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/daal4py/sklearn/linear_model/_ridge_0_22.py",197]}}}