{"traceEvents":[{"ph":"M","pid":26795,"tid":26795,"name":"process_name","args":{"name":"MainProcess"}},{"ph":"M","pid":26795,"tid":11117594,"name":"thread_name","args":{"name":"MainThread"}},{"pid":26795,"tid":11117594,"ts":1296152293336.02,"dur":4.98,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293346.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293347.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293347.04,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293348.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293349.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293350.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293350.04,"dur":0.96,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293368.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293368.0,"dur":0.06,"name":"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293371.0,"dur":2.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:437)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293392.0,"dur":1.0,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293374.0,"dur":19.02,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:458)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293395.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293396.02,"dur":0.98,"name":"list.remove","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293399.0,"dur":0.02,"name":"list.insert","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293400.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293396.0,"dur":4.04,"name":"_add_filter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:181)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293394.0,"dur":6.06,"name":"simplefilter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:165)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293423.0,"dur":1.0,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293422.0,"dur":2.02,"name":"asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:23)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293425.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293424.04,"dur":1.0,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:477)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293427.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293427.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293426.0,"dur":2.0,"name":"_ensure_no_complex_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:452)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293452.02,"dur":0.98,"name":"str.rpartition","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293452.0,"dur":1.02,"name":"parent (<frozen importlib._bootstrap>:398)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293473.0,"dur":1.0,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293459.0,"dur":15.02,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293475.0,"dur":0.02,"name":"dict.copy","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293458.0,"dur":17.04,"name":"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293477.0,"dur":0.02,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293476.0,"dur":1.04,"name":"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293481.02,"dur":0.98,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293481.0,"dur":18.0,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:285)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293500.02,"dur":0.98,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293500.0,"dur":1.02,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:285)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293501.04,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293480.0,"dur":21.08,"name":"issubdtype (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:359)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293503.02,"dur":0.98,"name":"_sum_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2106)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293505.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293528.0,"dur":0.02,"name":"dict.items","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293529.0,"dur":1.0,"name":"<dictcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:71)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293531.0,"dur":26.0,"name":"numpy.ufunc.reduce","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293507.0,"dur":50.02,"name":"_wrapreduction (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:70)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293505.0,"dur":52.04,"name":"sum (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2111)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293504.02,"dur":56.98,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293503.0,"dur":58.02,"name":"sum (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293479.0,"dur":82.04,"name":"_safe_accumulator_op (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:838)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293429.0,"dur":138.0,"name":"_assert_all_finite (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:83)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293571.0,"dur":1.0,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293572.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293573.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293573.04,"dur":0.96,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293579.02,"dur":0.98,"name":"_abc._abc_subclasscheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293579.0,"dur":1.02,"name":"__subclasscheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:100)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293576.02,"dur":4.02,"name":"_abc._abc_instancecheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293576.0,"dur":4.06,"name":"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:96)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293575.0,"dur":6.0,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293568.0,"dur":13.02,"name":"_num_samples (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:242)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293345.0,"dur":257.0,"name":"check_array (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:459)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293603.0,"dur":0.02,"name":"dict.get","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293606.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293607.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293607.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293608.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293604.02,"dur":4.02,"name":"_num_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:185)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293608.06,"dur":0.94,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293604.0,"dur":5.02,"name":"_check_n_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:334)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293336.0,"dur":273.04,"name":"_validate_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:379)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293635.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293634.0,"dur":1.04,"name":"isclass (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/inspect.py:73)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293635.06,"dur":0.94,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293636.02,"dur":0.98,"name":"builtins.vars","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293638.02,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293639.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293639.04,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293640.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293640.04,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293640.08,"dur":0.92,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293641.02,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293641.06,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293642.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293642.04,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293642.08,"dur":0.92,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293643.02,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293643.06,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293644.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293644.04,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293645.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293645.04,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293645.08,"dur":0.92,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293646.02,"dur":1.98,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293648.02,"dur":0.02,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293649.0,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293649.04,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293649.08,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293649.12,"dur":0.88,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293650.02,"dur":0.02,"name":"str.endswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293638.0,"dur":12.06,"name":"<listcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:1095)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293612.0,"dur":39.0,"name":"check_is_fitted (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:1036)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293653.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293654.02,"dur":0.98,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293655.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293655.06,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293656.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293656.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293657.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293657.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293673.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293672.0,"dur":1.04,"name":"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293674.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:437)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293677.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293675.02,"dur":2.02,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:458)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293678.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293679.02,"dur":0.98,"name":"list.remove","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293699.0,"dur":0.02,"name":"list.insert","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293699.04,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293679.0,"dur":20.08,"name":"_add_filter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:181)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293678.0,"dur":21.1,"name":"simplefilter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:165)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293700.02,"dur":0.98,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293700.0,"dur":1.02,"name":"asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:23)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293702.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293701.04,"dur":1.0,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:477)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293703.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293704.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293703.0,"dur":1.04,"name":"_ensure_no_complex_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:452)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293706.0,"dur":0.02,"name":"str.rpartition","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293705.02,"dur":1.98,"name":"parent (<frozen importlib._bootstrap>:398)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293710.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293710.0,"dur":0.06,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293711.0,"dur":0.02,"name":"dict.copy","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293709.0,"dur":2.04,"name":"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293733.02,"dur":0.02,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293733.0,"dur":0.06,"name":"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293736.0,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293735.02,"dur":1.98,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:285)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293738.02,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293738.0,"dur":0.06,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:285)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293740.0,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293735.0,"dur":5.04,"name":"issubdtype (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:359)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293763.02,"dur":0.02,"name":"_sum_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2106)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293764.04,"dur":0.96,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293766.02,"dur":0.02,"name":"dict.items","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293767.0,"dur":0.02,"name":"<dictcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:71)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293768.0,"dur":20.0,"name":"numpy.ufunc.reduce","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293766.0,"dur":22.02,"name":"_wrapreduction (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:70)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293764.02,"dur":24.98,"name":"sum (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2111)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293764.0,"dur":25.02,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293763.0,"dur":26.04,"name":"sum (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293734.0,"dur":55.06,"name":"_safe_accumulator_op (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:838)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293705.0,"dur":87.0,"name":"_assert_all_finite (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:83)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293794.0,"dur":1.0,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293795.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293795.06,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293796.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293798.02,"dur":0.02,"name":"_abc._abc_subclasscheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293798.0,"dur":1.0,"name":"__subclasscheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:100)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293797.02,"dur":2.0,"name":"_abc._abc_instancecheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293797.0,"dur":2.04,"name":"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:96)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293796.04,"dur":3.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293793.0,"dur":6.08,"name":"_num_samples (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:242)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293654.0,"dur":146.0,"name":"check_array (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:459)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293800.02,"dur":0.98,"name":"dict.get","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293803.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293803.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293803.08,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293804.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293801.04,"dur":3.0,"name":"_num_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:185)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293804.06,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293801.02,"dur":3.98,"name":"_check_n_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:334)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293652.0,"dur":153.02,"name":"_validate_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:379)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293809.0,"dur":1.0,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293838.0,"dur":2.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:34)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293812.0,"dur":29.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:280)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293841.02,"dur":0.98,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293842.02,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293807.02,"dur":35.04,"name":"get_active_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:76)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293845.0,"dur":1.0,"name":"current_process (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/process.py:37)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293847.0,"dur":0.02,"name":"dict.get","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293846.02,"dur":1.98,"name":"daemon (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/process.py:198)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293850.0,"dur":0.02,"name":"_thread.get_ident","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293849.0,"dur":1.04,"name":"current_thread (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:1318)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293851.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293848.02,"dur":3.02,"name":"in_main_thread (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:185)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293843.0,"dur":8.06,"name":"effective_n_jobs (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:501)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293807.0,"dur":46.0,"name":"effective_n_jobs (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:385)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293855.02,"dur":0.98,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293855.0,"dur":1.02,"name":"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293873.0,"dur":6.0,"name":"re.Pattern.search","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293880.0,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293881.0,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293881.04,"dur":0.96,"name":"str.split","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293883.0,"dur":1.0,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:294)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293885.0,"dur":0.02,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:294)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293885.04,"dur":0.02,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:294)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293885.08,"dur":0.92,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:294)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293887.0,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293887.04,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293888.0,"dur":0.02,"name":"_parse_letter_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:416)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293888.04,"dur":0.96,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293889.02,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293889.06,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293889.1,"dur":0.9,"name":"_parse_letter_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:416)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293890.02,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293890.06,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293890.1,"dur":0.9,"name":"_parse_letter_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:416)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293891.02,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293891.06,"dur":0.94,"name":"_parse_local_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:455)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293894.02,"dur":0.98,"name":"type.__new__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293894.0,"dur":1.02,"name":"<lambda> (<string>:1)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293901.0,"dur":0.02,"name":"<lambda> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:482)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293897.0,"dur":33.0,"name":"_cmpkey (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:467)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293872.0,"dur":58.02,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:284)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293857.0,"dur":74.0,"name":"parse (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:65)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293932.02,"dur":2.98,"name":"re.Pattern.search","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293935.02,"dur":0.98,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293936.02,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293936.06,"dur":0.94,"name":"str.split","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293937.02,"dur":0.98,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:294)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293953.0,"dur":0.02,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:294)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293953.04,"dur":0.02,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:294)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293954.0,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293954.04,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293955.0,"dur":0.02,"name":"_parse_letter_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:416)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293955.04,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293955.08,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293955.12,"dur":0.88,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293956.02,"dur":0.02,"name":"_parse_letter_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:416)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293956.06,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293956.1,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293957.0,"dur":0.02,"name":"_parse_letter_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:416)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293959.0,"dur":0.02,"name":"re.Match.group","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293959.04,"dur":0.02,"name":"_parse_local_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:455)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293960.02,"dur":0.02,"name":"type.__new__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293960.0,"dur":1.0,"name":"<lambda> (<string>:1)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293979.0,"dur":1.0,"name":"<lambda> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:482)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293961.02,"dur":19.0,"name":"_cmpkey (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:467)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293932.0,"dur":49.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:284)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293931.02,"dur":50.0,"name":"parse (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:65)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293982.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293982.0,"dur":0.06,"name":"__lt__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:92)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293986.0,"dur":1.0,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293988.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:34)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293987.02,"dur":2.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:280)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293990.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293990.04,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293991.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:34)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293985.02,"dur":7.0,"name":"get_active_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:76)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293996.0,"dur":1.0,"name":"_init (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:206)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293998.0,"dur":1.0,"name":"_thread.allocate_lock","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294000.0,"dur":4.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:228)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294005.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:228)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294007.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:228)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293995.0,"dur":14.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:34)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294010.02,"dur":16.98,"name":"posix.urandom","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294029.0,"dur":0.02,"name":"list.count","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294030.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294031.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294032.0,"dur":1.0,"name":"type.from_bytes","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294028.0,"dur":8.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py:138)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294010.0,"dur":27.0,"name":"uuid4 (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py:713)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294037.02,"dur":1.98,"name":"hex (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py:333)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294040.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294041.0,"dur":4.0,"name":"memstr_to_bytes (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/disk.py:42)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294045.02,"dur":0.98,"name":"builtins.max","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294047.0,"dur":1.0,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294049.0,"dur":1.0,"name":"get_context (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/context.py:233)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294052.0,"dur":1.0,"name":"RLock (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:82)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293985.0,"dur":69.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:637)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294062.0,"dur":0.02,"name":"effective_n_jobs (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:227)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294063.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:34)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294065.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:609)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294061.0,"dur":5.02,"name":"configure (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:389)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294069.02,"dur":0.98,"name":"effective_n_jobs (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:200)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294069.0,"dur":1.02,"name":"configure (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:70)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294067.0,"dur":3.04,"name":"_initialize_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:730)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294058.0,"dur":13.0,"name":"_initialize_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:730)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294072.02,"dur":0.98,"name":"_abc._abc_instancecheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294072.0,"dur":3.0,"name":"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:96)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294071.02,"dur":4.0,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294076.0,"dur":1.0,"name":"_print (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:862)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294077.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294078.0,"dur":0.02,"name":"start_call (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:80)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294078.04,"dur":0.02,"name":"builtins.iter","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294080.0,"dur":0.02,"name":"time.time","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294083.0,"dur":0.02,"name":"compute_batch_size (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:89)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294086.0,"dur":0.02,"name":"_thread.lock.__enter__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294085.02,"dur":1.02,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:256)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294087.02,"dur":0.98,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294087.0,"dur":1.02,"name":"_qsize (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:209)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294089.0,"dur":0.02,"name":"_thread.lock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294088.04,"dur":1.0,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:259)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294085.0,"dur":5.0,"name":"get (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:154)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294092.02,"dur":1.98,"name":"gen_even_slices (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/__init__.py:726)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294097.0,"dur":1.0,"name":"wraps (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/functools.py:65)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294100.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294101.0,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294101.04,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294101.08,"dur":0.92,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294102.02,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294102.06,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294102.1,"dur":0.9,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294103.02,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294103.06,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294104.0,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294104.04,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294105.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294105.04,"dur":0.96,"name":"dict.update","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294099.0,"dur":7.02,"name":"update_wrapper (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/functools.py:35)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294095.0,"dur":11.04,"name":"delayed (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py:188)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294111.04,"dur":0.96,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294111.02,"dur":1.0,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294112.04,"dur":0.02,"name":"dict.copy","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294111.0,"dur":2.0,"name":"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294113.04,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294114.0,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294114.04,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294114.08,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294114.12,"dur":0.88,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294115.02,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294115.06,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294115.1,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294115.14,"dur":0.86,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294116.02,"dur":0.02,"name":"builtins.setattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294118.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294118.04,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294118.08,"dur":0.92,"name":"dict.update","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294113.02,"dur":6.0,"name":"update_wrapper (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/functools.py:35)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294110.0,"dur":9.04,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py:198)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294109.0,"dur":10.06,"name":"delayed_function (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py:190)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294092.0,"dur":28.0,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:716)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294120.02,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294121.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294121.04,"dur":0.96,"name":"builtins.max","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294122.02,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294124.02,"dur":0.98,"name":"str.rpartition","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294124.0,"dur":1.02,"name":"parent (<frozen importlib._bootstrap>:398)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294129.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294131.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:34)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294130.0,"dur":3.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:280)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294133.02,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294133.06,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294128.0,"dur":6.0,"name":"get_active_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:76)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294123.0,"dur":11.02,"name":"get_nested_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:213)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294136.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294136.04,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294135.0,"dur":2.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:245)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294138.02,"dur":0.98,"name":"_thread.lock.__enter__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294138.0,"dur":1.02,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:256)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294140.0,"dur":0.02,"name":"collections.deque.append","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294139.04,"dur":1.0,"name":"_put (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:213)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294142.02,"dur":0.98,"name":"_thread.lock.acquire","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294142.0,"dur":1.02,"name":"_is_owned (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:271)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294141.0,"dur":3.0,"name":"notify (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:351)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294145.02,"dur":0.02,"name":"_thread.lock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294145.0,"dur":0.06,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:259)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294137.02,"dur":8.06,"name":"put (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:122)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294147.0,"dur":0.02,"name":"_thread.lock.__enter__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294146.02,"dur":1.02,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:256)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294147.08,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294147.06,"dur":0.94,"name":"_qsize (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:209)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294148.04,"dur":0.96,"name":"collections.deque.popleft","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294148.02,"dur":1.0,"name":"_get (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:217)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294149.08,"dur":0.92,"name":"_thread.lock.acquire","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294149.06,"dur":0.96,"name":"_is_owned (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:271)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294149.04,"dur":1.0,"name":"notify (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:351)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294151.0,"dur":0.02,"name":"_thread.lock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294150.06,"dur":0.98,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:259)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294146.0,"dur":5.06,"name":"get (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:154)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294152.02,"dur":0.98,"name":"__len__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:275)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294152.0,"dur":1.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294156.0,"dur":0.02,"name":"__len__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:275)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294155.02,"dur":1.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294157.0,"dur":0.02,"name":"time.time","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294157.06,"dur":0.02,"name":"__len__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:275)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294157.04,"dur":0.96,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294158.02,"dur":0.98,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:345)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294159.02,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294163.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294164.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294163.0,"dur":2.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:184)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294166.0,"dur":0.02,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:216)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294173.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294171.0,"dur":3.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:86)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294169.0,"dur":5.02,"name":"helper (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:242)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294176.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294176.02,"dur":0.98,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294177.02,"dur":0.02,"name":"dict.copy","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294176.0,"dur":1.06,"name":"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294178.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294178.02,"dur":0.98,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294178.0,"dur":1.02,"name":"set_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:42)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294175.02,"dur":4.02,"name":"config_context (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:99)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294175.0,"dur":4.06,"name":"builtins.next","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294174.04,"dur":5.96,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:112)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294183.02,"dur":0.98,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294184.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294184.06,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294185.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294185.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294186.0,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294186.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294187.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294187.0,"dur":0.06,"name":"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294188.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:437)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294190.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294189.02,"dur":1.98,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:458)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294191.04,"dur":0.96,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294192.04,"dur":0.96,"name":"list.remove","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294194.0,"dur":0.02,"name":"list.insert","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294194.04,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294192.02,"dur":2.06,"name":"_add_filter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:181)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294191.02,"dur":3.98,"name":"simplefilter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:165)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294197.02,"dur":1.98,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294197.0,"dur":2.02,"name":"asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:23)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294200.0,"dur":0.02,"name":"_warnings._filters_mutated","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294199.04,"dur":1.0,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:477)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294201.02,"dur":1.98,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294203.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294201.0,"dur":2.06,"name":"_ensure_no_complex_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:452)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294205.02,"dur":0.02,"name":"str.rpartition","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294205.0,"dur":1.0,"name":"parent (<frozen importlib._bootstrap>:398)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294208.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294208.02,"dur":0.98,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294209.02,"dur":0.02,"name":"dict.copy","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294208.0,"dur":1.06,"name":"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294210.02,"dur":0.02,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294210.0,"dur":0.06,"name":"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294212.04,"dur":0.96,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294212.02,"dur":1.0,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:285)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294214.02,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294214.0,"dur":0.06,"name":"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:285)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294215.0,"dur":0.02,"name":"builtins.issubclass","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294212.0,"dur":3.04,"name":"issubdtype (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:359)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294216.02,"dur":0.02,"name":"_sum_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2106)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294217.04,"dur":0.96,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294219.02,"dur":0.02,"name":"dict.items","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294220.0,"dur":1.0,"name":"<dictcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:71)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294222.0,"dur":6.0,"name":"numpy.ufunc.reduce","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294219.0,"dur":9.02,"name":"_wrapreduction (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:70)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294217.02,"dur":11.02,"name":"sum (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2111)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294217.0,"dur":12.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294216.0,"dur":13.02,"name":"sum (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294211.0,"dur":18.04,"name":"_safe_accumulator_op (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:838)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294204.0,"dur":29.0,"name":"_assert_all_finite (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:83)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294235.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294236.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294236.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294237.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294252.04,"dur":0.96,"name":"_abc._abc_subclasscheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294252.02,"dur":1.0,"name":"__subclasscheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:100)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294252.0,"dur":1.04,"name":"_abc._abc_instancecheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294251.02,"dur":2.04,"name":"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:96)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294251.0,"dur":3.0,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294234.0,"dur":20.02,"name":"_num_samples (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:242)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294183.0,"dur":71.04,"name":"check_array (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:459)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294287.02,"dur":0.98,"name":"numpy.empty","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294290.0,"dur":1.0,"name":"copyto (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:1054)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294291.02,"dur":2.98,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294289.0,"dur":5.02,"name":"copyto (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294287.0,"dur":7.04,"name":"full (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py:288)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294181.0,"dur":127.0,"name":"sklearn.neighbors._kd_tree.KDTree.query","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294180.02,"dur":128.98,"name":"_tree_query_parallel_helper (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:537)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294311.04,"dur":0.96,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294311.02,"dur":4.98,"name":"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294311.0,"dur":5.02,"name":"set_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:42)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294310.02,"dur":6.98,"name":"config_context (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:99)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294310.0,"dur":7.02,"name":"builtins.next","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294309.02,"dur":8.98,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:121)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294168.0,"dur":151.0,"name":"__call__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py:203)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294167.0,"dur":152.02,"name":"<listcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:262)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294321.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294320.02,"dur":1.98,"name":"unregister (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:222)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294320.0,"dur":2.02,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:219)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294161.02,"dur":161.02,"name":"__call__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:258)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294161.0,"dur":162.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:569)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294325.0,"dur":0.02,"name":"time.time","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294326.0,"dur":0.02,"name":"batch_completed (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:93)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294327.0,"dur":0.02,"name":"print_progress (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:875)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294328.0,"dur":0.02,"name":"_thread.RLock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294324.0,"dur":4.04,"name":"__call__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:350)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294160.0,"dur":169.0,"name":"apply_async (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:206)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294329.02,"dur":0.02,"name":"list.insert","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294329.06,"dur":0.94,"name":"_thread.RLock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294155.0,"dur":175.02,"name":"_dispatch (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:759)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294330.04,"dur":0.02,"name":"_thread.RLock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294082.0,"dur":248.08,"name":"dispatch_one_batch (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:796)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294333.0,"dur":0.02,"name":"compute_batch_size (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:89)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294334.04,"dur":0.96,"name":"_thread.lock.__enter__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294334.02,"dur":1.0,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:256)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294335.06,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294335.04,"dur":0.96,"name":"_qsize (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:209)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294337.0,"dur":0.02,"name":"_thread.lock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294336.02,"dur":1.02,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:259)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294334.0,"dur":3.06,"name":"get (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:154)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294339.02,"dur":0.02,"name":"gen_even_slices (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/__init__.py:726)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294339.0,"dur":1.0,"name":"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:716)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294341.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294341.04,"dur":0.96,"name":"_thread.RLock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294332.0,"dur":10.02,"name":"dispatch_one_batch (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:796)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294345.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294344.0,"dur":1.04,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:86)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294343.0,"dur":2.06,"name":"helper (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:242)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294346.04,"dur":0.96,"name":"retrieval_context (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:137)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294346.02,"dur":1.0,"name":"builtins.next","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294346.0,"dur":1.04,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:112)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294348.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294348.04,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294349.0,"dur":0.02,"name":"list.pop","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294349.04,"dur":0.96,"name":"_thread.RLock.__exit__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294350.02,"dur":1.98,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294352.02,"dur":0.98,"name":"get (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:574)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294353.02,"dur":0.02,"name":"list.extend","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294353.06,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294347.06,"dur":6.04,"name":"retrieve (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:918)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294354.04,"dur":0.96,"name":"retrieval_context (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:137)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294354.02,"dur":1.0,"name":"builtins.next","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294354.0,"dur":1.04,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:121)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294356.0,"dur":0.02,"name":"time.time","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294356.04,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294357.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294358.02,"dur":0.98,"name":"str.startswith","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294358.0,"dur":1.02,"name":"_squeeze_time (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/logger.py:23)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294357.04,"dur":5.96,"name":"short_format_time (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/logger.py:39)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294382.0,"dur":1.0,"name":"_print (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:862)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294383.02,"dur":0.98,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294384.02,"dur":0.02,"name":"stop_call (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:83)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294385.02,"dur":0.02,"name":"terminate (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:86)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294385.0,"dur":0.06,"name":"_terminate_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:755)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294057.0,"dur":329.0,"name":"__call__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:958)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294391.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294390.02,"dur":1.02,"name":"_arrays_for_stack_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:208)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294390.0,"dur":1.06,"name":"_vhstack_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:219)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294394.0,"dur":0.02,"name":"_atleast_2d_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:78)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294395.02,"dur":0.98,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294395.0,"dur":1.02,"name":"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294397.0,"dur":0.02,"name":"list.append","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294397.04,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294394.06,"dur":3.02,"name":"atleast_2d (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:82)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294394.04,"dur":3.96,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294393.0,"dur":5.02,"name":"atleast_2d (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294398.04,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294401.0,"dur":0.02,"name":"concatenate (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:143)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294401.04,"dur":3.96,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294400.0,"dur":5.02,"name":"concatenate (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294392.02,"dur":13.02,"name":"vstack (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:223)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294392.0,"dur":14.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294389.0,"dur":17.02,"name":"vstack (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294407.02,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294407.0,"dur":0.06,"name":"_arrays_for_stack_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:208)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294406.06,"dur":1.02,"name":"_vhstack_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:219)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294408.04,"dur":0.02,"name":"_atleast_2d_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:78)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294427.0,"dur":0.02,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294409.02,"dur":18.02,"name":"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294428.0,"dur":0.02,"name":"list.append","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294428.04,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294409.0,"dur":19.08,"name":"atleast_2d (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:82)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294408.08,"dur":21.92,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294408.02,"dur":22.0,"name":"atleast_2d (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294430.04,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294431.02,"dur":0.02,"name":"concatenate (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:143)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294431.06,"dur":1.94,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294431.0,"dur":2.02,"name":"concatenate (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294408.0,"dur":25.04,"name":"vstack (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:223)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294407.1,"dur":25.96,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294406.04,"dur":27.04,"name":"vstack (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293611.0,"dur":822.1,"name":"kneighbors (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:592)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294435.0,"dur":1.0,"name":"numpy.ndarray.reshape","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294436.02,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294439.0,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294439.04,"dur":0.02,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294439.08,"dur":0.92,"name":"builtins.hasattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294440.02,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294442.02,"dur":0.98,"name":"_abc._abc_subclasscheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294442.0,"dur":1.02,"name":"__subclasscheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:100)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294441.04,"dur":2.0,"name":"_abc._abc_instancecheck","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294441.02,"dur":2.04,"name":"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:96)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294441.0,"dur":2.08,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294437.0,"dur":6.1,"name":"_num_samples (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:242)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294444.0,"dur":1.0,"name":"_get_weights (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:69)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294446.0,"dur":1.0,"name":"numpy.empty","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294459.0,"dur":0.02,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294458.02,"dur":1.02,"name":"asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:23)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294458.0,"dur":2.0,"name":"_chk_asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py:244)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294463.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:429)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294466.02,"dur":0.98,"name":"numpy.geterrobj","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294467.04,"dur":0.96,"name":"numpy.geterrobj","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294467.02,"dur":2.98,"name":"geterr (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:132)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294472.0,"dur":1.0,"name":"numpy.seterrobj","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294466.0,"dur":7.02,"name":"seterr (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:32)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294465.0,"dur":9.0,"name":"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:433)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294475.0,"dur":0.02,"name":"_sum_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2106)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294476.02,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294477.02,"dur":0.98,"name":"dict.items","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294478.02,"dur":0.02,"name":"<dictcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:71)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294479.0,"dur":6.0,"name":"numpy.ufunc.reduce","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294477.0,"dur":8.02,"name":"_wrapreduction (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:70)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294476.0,"dur":10.0,"name":"sum (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2111)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294475.04,"dur":10.98,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294474.02,"dur":12.02,"name":"sum (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294492.0,"dur":0.02,"name":"numpy.geterrobj","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294492.06,"dur":0.02,"name":"numpy.geterrobj","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294492.04,"dur":1.96,"name":"geterr (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:132)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294495.0,"dur":2.0,"name":"numpy.seterrobj","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294491.0,"dur":7.0,"name":"seterr (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:32)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294490.0,"dur":8.02,"name":"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:438)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294461.0,"dur":38.0,"name":"_contains_nan (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py:215)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294504.0,"dur":0.02,"name":"_transpose_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:598)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294506.0,"dur":0.02,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294506.04,"dur":0.96,"name":"numpy.ndarray.transpose","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294505.02,"dur":2.98,"name":"_wrapfunc (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:52)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294505.0,"dur":3.02,"name":"transpose (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:602)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294504.04,"dur":4.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294503.0,"dur":5.06,"name":"transpose (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294512.0,"dur":0.02,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294512.04,"dur":0.02,"name":"builtins.isinstance","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294513.0,"dur":2.0,"name":"numpy.zeros","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294516.0,"dur":0.02,"name":"_zeros_like_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py:71)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294519.0,"dur":0.02,"name":"empty_like (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:75)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294520.0,"dur":3.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294518.0,"dur":5.02,"name":"empty_like (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294524.0,"dur":1.0,"name":"numpy.zeros","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294526.0,"dur":0.02,"name":"copyto (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:1054)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294527.0,"dur":1.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294525.02,"dur":3.0,"name":"copyto (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294517.0,"dur":11.04,"name":"zeros_like (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py:75)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294516.04,"dur":12.96,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294515.02,"dur":14.0,"name":"zeros_like (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294530.02,"dur":0.02,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294541.0,"dur":1.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py:20)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294542.04,"dur":4.96,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294542.02,"dur":5.0,"name":"asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:23)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294548.0,"dur":1.0,"name":"_maybe_view_as_subclass (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py:25)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294530.0,"dur":20.0,"name":"as_strided (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py:38)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294511.0,"dur":43.0,"name":"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py:647)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294555.0,"dur":1.0,"name":"numpy.empty","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294557.0,"dur":0.02,"name":"numpy.empty","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294558.0,"dur":0.02,"name":"__iter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py:655)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294559.02,"dur":0.98,"name":"builtins.next","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294559.0,"dur":1.02,"name":"__next__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py:674)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294564.0,"dur":0.02,"name":"_unique_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py:133)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294566.02,"dur":0.02,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294566.0,"dur":0.06,"name":"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294567.04,"dur":0.96,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294567.02,"dur":1.0,"name":"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294568.04,"dur":0.96,"name":"numpy.ndarray.flatten","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294570.0,"dur":1.0,"name":"numpy.ndarray.sort","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294572.0,"dur":1.0,"name":"numpy.empty","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294582.0,"dur":0.02,"name":"_nonzero_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1823)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294583.04,"dur":0.96,"name":"builtins.getattr","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294586.0,"dur":1.0,"name":"numpy.ndarray.nonzero","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294583.02,"dur":4.98,"name":"_wrapfunc (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:52)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294583.0,"dur":5.02,"name":"nonzero (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1827)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294582.04,"dur":6.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294581.0,"dur":7.06,"name":"nonzero (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294589.02,"dur":0.02,"name":"concatenate (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:143)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294590.0,"dur":3.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294589.0,"dur":4.02,"name":"concatenate (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294595.0,"dur":0.02,"name":"_diff_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/function_base.py:1149)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294597.02,"dur":0.02,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294597.0,"dur":0.06,"name":"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294598.0,"dur":0.02,"name":"numpy.core._multiarray_umath.normalize_axis_index","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294599.0,"dur":0.02,"name":"list.append","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294599.04,"dur":0.96,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294596.02,"dur":7.98,"name":"diff (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/function_base.py:1153)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294596.0,"dur":9.0,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294594.0,"dur":11.02,"name":"diff (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294567.0,"dur":38.04,"name":"_unique1d (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py:310)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294606.02,"dur":0.98,"name":"builtins.len","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294606.0,"dur":1.02,"name":"_unpack_tuple (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py:125)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294565.0,"dur":42.04,"name":"unique (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py:138)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294564.04,"dur":43.02,"name":"numpy.core._multiarray_umath.implement_array_function","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294562.0,"dur":45.08,"name":"unique (<__array_function__ internals>:2)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294608.0,"dur":1.0,"name":"numpy.ndarray.argmax","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294612.0,"dur":5.0,"name":"numpy.ufunc.reduce","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294611.02,"dur":6.0,"name":"_amax (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_methods.py:37)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294611.0,"dur":7.0,"name":"numpy.ndarray.max","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294561.0,"dur":57.02,"name":"_mode1D (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py:558)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294620.0,"dur":0.02,"name":"builtins.next","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294619.0,"dur":1.04,"name":"__next__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py:674)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294621.0,"dur":1.0,"name":"numpy.ndarray.reshape","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294622.02,"dur":0.98,"name":"numpy.ndarray.reshape","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294625.0,"dur":0.02,"name":"type.__new__","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294624.0,"dur":1.04,"name":"<lambda> (<string>:1)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294457.0,"dur":168.06,"name":"mode (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py:485)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294628.0,"dur":1.0,"name":"numpy.ndarray.ravel","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294630.0,"dur":1.0,"name":"numpy.array","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294629.02,"dur":2.0,"name":"asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:23)","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294631.04,"dur":1.96,"name":"numpy.ndarray.take","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152294634.0,"dur":0.02,"name":"numpy.ndarray.ravel","ph":"X","cat":"FEE"},{"pid":26795,"tid":11117594,"ts":1296152293333.0,"dur":1302.0,"name":"predict (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:179)","ph":"X","cat":"FEE"}],"viztracer_metadata":{"version":"0.13.3"},"displayTimeUnit":"us","file_info":{"files":{"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py":["\"\"\"Base class for sparse matrices\"\"\"\nimport numpy as np\n\nfrom .sputils import (isdense, isscalarlike, isintlike,\n                      get_sum_dtype, validateaxis, check_reshape_kwargs,\n                      check_shape, asmatrix)\n\n__all__ = ['spmatrix', 'isspmatrix', 'issparse',\n           'SparseWarning', 'SparseEfficiencyWarning']\n\n\nclass SparseWarning(Warning):\n    pass\n\n\nclass SparseFormatWarning(SparseWarning):\n    pass\n\n\nclass SparseEfficiencyWarning(SparseWarning):\n    pass\n\n\n# The formats that we might potentially understand.\n_formats = {'csc': [0, \"Compressed Sparse Column\"],\n            'csr': [1, \"Compressed Sparse Row\"],\n            'dok': [2, \"Dictionary Of Keys\"],\n            'lil': [3, \"List of Lists\"],\n            'dod': [4, \"Dictionary of Dictionaries\"],\n            'sss': [5, \"Symmetric Sparse Skyline\"],\n            'coo': [6, \"COOrdinate\"],\n            'lba': [7, \"Linpack BAnded\"],\n            'egd': [8, \"Ellpack-itpack Generalized Diagonal\"],\n            'dia': [9, \"DIAgonal\"],\n            'bsr': [10, \"Block Sparse Row\"],\n            'msr': [11, \"Modified compressed Sparse Row\"],\n            'bsc': [12, \"Block Sparse Column\"],\n            'msc': [13, \"Modified compressed Sparse Column\"],\n            'ssk': [14, \"Symmetric SKyline\"],\n            'nsk': [15, \"Nonsymmetric SKyline\"],\n            'jad': [16, \"JAgged Diagonal\"],\n            'uss': [17, \"Unsymmetric Sparse Skyline\"],\n            'vbr': [18, \"Variable Block Row\"],\n            'und': [19, \"Undefined\"]\n            }\n\n\n# These univariate ufuncs preserve zeros.\n_ufuncs_with_fixed_point_at_zero = frozenset([\n        np.sin, np.tan, np.arcsin, np.arctan, np.sinh, np.tanh, np.arcsinh,\n        np.arctanh, np.rint, np.sign, np.expm1, np.log1p, np.deg2rad,\n        np.rad2deg, np.floor, np.ceil, np.trunc, np.sqrt])\n\n\nMAXPRINT = 50\n\n\nclass spmatrix(object):\n    \"\"\" This class provides a base class for all sparse matrices.  It\n    cannot be instantiated.  Most of the work is provided by subclasses.\n    \"\"\"\n\n    __array_priority__ = 10.1\n    ndim = 2\n\n    def __init__(self, maxprint=MAXPRINT):\n        self._shape = None\n        if self.__class__.__name__ == 'spmatrix':\n            raise ValueError(\"This class is not intended\"\n                             \" to be instantiated directly.\")\n        self.maxprint = maxprint\n\n    def set_shape(self, shape):\n        \"\"\"See `reshape`.\"\"\"\n        # Make sure copy is False since this is in place\n        # Make sure format is unchanged because we are doing a __dict__ swap\n        new_matrix = self.reshape(shape, copy=False).asformat(self.format)\n        self.__dict__ = new_matrix.__dict__\n\n    def get_shape(self):\n        \"\"\"Get shape of a matrix.\"\"\"\n        return self._shape\n\n    shape = property(fget=get_shape, fset=set_shape)\n\n    def reshape(self, *args, **kwargs):\n        \"\"\"reshape(self, shape, order='C', copy=False)\n\n        Gives a new shape to a sparse matrix without changing its data.\n\n        Parameters\n        ----------\n        shape : length-2 tuple of ints\n            The new shape should be compatible with the original shape.\n        order : {'C', 'F'}, optional\n            Read the elements using this index order. 'C' means to read and\n            write the elements using C-like index order; e.g., read entire first\n            row, then second row, etc. 'F' means to read and write the elements\n            using Fortran-like index order; e.g., read entire first column, then\n            second column, etc.\n        copy : bool, optional\n            Indicates whether or not attributes of self should be copied\n            whenever possible. The degree to which attributes are copied varies\n            depending on the type of sparse matrix being used.\n\n        Returns\n        -------\n        reshaped_matrix : sparse matrix\n            A sparse matrix with the given `shape`, not necessarily of the same\n            format as the current object.\n\n        See Also\n        --------\n        numpy.matrix.reshape : NumPy's implementation of 'reshape' for\n                               matrices\n        \"\"\"\n        # If the shape already matches, don't bother doing an actual reshape\n        # Otherwise, the default is to convert to COO and use its reshape\n        shape = check_shape(args, self.shape)\n        order, copy = check_reshape_kwargs(kwargs)\n        if shape == self.shape:\n            if copy:\n                return self.copy()\n            else:\n                return self\n\n        return self.tocoo(copy=copy).reshape(shape, order=order, copy=False)\n\n    def resize(self, shape):\n        \"\"\"Resize the matrix in-place to dimensions given by ``shape``\n\n        Any elements that lie within the new shape will remain at the same\n        indices, while non-zero elements lying outside the new shape are\n        removed.\n\n        Parameters\n        ----------\n        shape : (int, int)\n            number of rows and columns in the new matrix\n\n        Notes\n        -----\n        The semantics are not identical to `numpy.ndarray.resize` or\n        `numpy.resize`. Here, the same data will be maintained at each index\n        before and after reshape, if that index is within the new bounds. In\n        numpy, resizing maintains contiguity of the array, moving elements\n        around in the logical matrix but not within a flattened representation.\n\n        We give no guarantees about whether the underlying data attributes\n        (arrays, etc.) will be modified in place or replaced with new objects.\n        \"\"\"\n        # As an inplace operation, this requires implementation in each format.\n        raise NotImplementedError(\n            '{}.resize is not implemented'.format(type(self).__name__))\n\n    def astype(self, dtype, casting='unsafe', copy=True):\n        \"\"\"Cast the matrix elements to a specified type.\n\n        Parameters\n        ----------\n        dtype : string or numpy dtype\n            Typecode or data-type to which to cast the data.\n        casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n            Controls what kind of data casting may occur.\n            Defaults to 'unsafe' for backwards compatibility.\n            'no' means the data types should not be cast at all.\n            'equiv' means only byte-order changes are allowed.\n            'safe' means only casts which can preserve values are allowed.\n            'same_kind' means only safe casts or casts within a kind,\n            like float64 to float32, are allowed.\n            'unsafe' means any data conversions may be done.\n        copy : bool, optional\n            If `copy` is `False`, the result might share some memory with this\n            matrix. If `copy` is `True`, it is guaranteed that the result and\n            this matrix do not share any memory.\n        \"\"\"\n\n        dtype = np.dtype(dtype)\n        if self.dtype != dtype:\n            return self.tocsr().astype(\n                dtype, casting=casting, copy=copy).asformat(self.format)\n        elif copy:\n            return self.copy()\n        else:\n            return self\n\n    def asfptype(self):\n        \"\"\"Upcast matrix to a floating point format (if necessary)\"\"\"\n\n        fp_types = ['f', 'd', 'F', 'D']\n\n        if self.dtype.char in fp_types:\n            return self\n        else:\n            for fp_type in fp_types:\n                if self.dtype <= np.dtype(fp_type):\n                    return self.astype(fp_type)\n\n            raise TypeError('cannot upcast [%s] to a floating '\n                            'point format' % self.dtype.name)\n\n    def __iter__(self):\n        for r in range(self.shape[0]):\n            yield self[r, :]\n\n    def getmaxprint(self):\n        \"\"\"Maximum number of elements to display when printed.\"\"\"\n        return self.maxprint\n\n    def count_nonzero(self):\n        \"\"\"Number of non-zero entries, equivalent to\n\n        np.count_nonzero(a.toarray())\n\n        Unlike getnnz() and the nnz property, which return the number of stored\n        entries (the length of the data attribute), this method counts the\n        actual number of non-zero entries in data.\n        \"\"\"\n        raise NotImplementedError(\"count_nonzero not implemented for %s.\" %\n                                  self.__class__.__name__)\n\n    def getnnz(self, axis=None):\n        \"\"\"Number of stored values, including explicit zeros.\n\n        Parameters\n        ----------\n        axis : None, 0, or 1\n            Select between the number of values across the whole matrix, in\n            each column, or in each row.\n\n        See also\n        --------\n        count_nonzero : Number of non-zero entries\n        \"\"\"\n        raise NotImplementedError(\"getnnz not implemented for %s.\" %\n                                  self.__class__.__name__)\n\n    @property\n    def nnz(self):\n        \"\"\"Number of stored values, including explicit zeros.\n\n        See also\n        --------\n        count_nonzero : Number of non-zero entries\n        \"\"\"\n        return self.getnnz()\n\n    def getformat(self):\n        \"\"\"Format of a matrix representation as a string.\"\"\"\n        return getattr(self, 'format', 'und')\n\n    def __repr__(self):\n        _, format_name = _formats[self.getformat()]\n        return \"<%dx%d sparse matrix of type '%s'\\n\" \\\n               \"\\twith %d stored elements in %s format>\" % \\\n               (self.shape + (self.dtype.type, self.nnz, format_name))\n\n    def __str__(self):\n        maxprint = self.getmaxprint()\n\n        A = self.tocoo()\n\n        # helper function, outputs \"(i,j)  v\"\n        def tostr(row, col, data):\n            triples = zip(list(zip(row, col)), data)\n            return '\\n'.join([('  %s\\t%s' % t) for t in triples])\n\n        if self.nnz > maxprint:\n            half = maxprint // 2\n            out = tostr(A.row[:half], A.col[:half], A.data[:half])\n            out += \"\\n  :\\t:\\n\"\n            half = maxprint - maxprint//2\n            out += tostr(A.row[-half:], A.col[-half:], A.data[-half:])\n        else:\n            out = tostr(A.row, A.col, A.data)\n\n        return out\n\n    def __bool__(self):  # Simple -- other ideas?\n        if self.shape == (1, 1):\n            return self.nnz != 0\n        else:\n            raise ValueError(\"The truth value of an array with more than one \"\n                             \"element is ambiguous. Use a.any() or a.all().\")\n    __nonzero__ = __bool__\n\n    # What should len(sparse) return? For consistency with dense matrices,\n    # perhaps it should be the number of rows?  But for some uses the number of\n    # non-zeros is more important.  For now, raise an exception!\n    def __len__(self):\n        raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n                        \" or shape[0]\")\n\n    def asformat(self, format, copy=False):\n        \"\"\"Return this matrix in the passed format.\n\n        Parameters\n        ----------\n        format : {str, None}\n            The desired matrix format (\"csr\", \"csc\", \"lil\", \"dok\", \"array\", ...)\n            or None for no conversion.\n        copy : bool, optional\n            If True, the result is guaranteed to not share data with self.\n\n        Returns\n        -------\n        A : This matrix in the passed format.\n        \"\"\"\n        if format is None or format == self.format:\n            if copy:\n                return self.copy()\n            else:\n                return self\n        else:\n            try:\n                convert_method = getattr(self, 'to' + format)\n            except AttributeError as e:\n                raise ValueError('Format {} is unknown.'.format(format)) from e\n\n            # Forward the copy kwarg, if it's accepted.\n            try:\n                return convert_method(copy=copy)\n            except TypeError:\n                return convert_method()\n\n    ###################################################################\n    #  NOTE: All arithmetic operations use csr_matrix by default.\n    # Therefore a new sparse matrix format just needs to define a\n    # .tocsr() method to provide arithmetic support. Any of these\n    # methods can be overridden for efficiency.\n    ####################################################################\n\n    def multiply(self, other):\n        \"\"\"Point-wise multiplication by another matrix\n        \"\"\"\n        return self.tocsr().multiply(other)\n\n    def maximum(self, other):\n        \"\"\"Element-wise maximum between this and another matrix.\"\"\"\n        return self.tocsr().maximum(other)\n\n    def minimum(self, other):\n        \"\"\"Element-wise minimum between this and another matrix.\"\"\"\n        return self.tocsr().minimum(other)\n\n    def dot(self, other):\n        \"\"\"Ordinary dot product\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from scipy.sparse import csr_matrix\n        >>> A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n        >>> v = np.array([1, 0, -1])\n        >>> A.dot(v)\n        array([ 1, -3, -1], dtype=int64)\n\n        \"\"\"\n        return self * other\n\n    def power(self, n, dtype=None):\n        \"\"\"Element-wise power.\"\"\"\n        return self.tocsr().power(n, dtype=dtype)\n\n    def __eq__(self, other):\n        return self.tocsr().__eq__(other)\n\n    def __ne__(self, other):\n        return self.tocsr().__ne__(other)\n\n    def __lt__(self, other):\n        return self.tocsr().__lt__(other)\n\n    def __gt__(self, other):\n        return self.tocsr().__gt__(other)\n\n    def __le__(self, other):\n        return self.tocsr().__le__(other)\n\n    def __ge__(self, other):\n        return self.tocsr().__ge__(other)\n\n    def __abs__(self):\n        return abs(self.tocsr())\n\n    def __round__(self, ndigits=0):\n        return round(self.tocsr(), ndigits=ndigits)\n\n    def _add_sparse(self, other):\n        return self.tocsr()._add_sparse(other)\n\n    def _add_dense(self, other):\n        return self.tocoo()._add_dense(other)\n\n    def _sub_sparse(self, other):\n        return self.tocsr()._sub_sparse(other)\n\n    def _sub_dense(self, other):\n        return self.todense() - other\n\n    def _rsub_dense(self, other):\n        # note: this can't be replaced by other + (-self) for unsigned types\n        return other - self.todense()\n\n    def __add__(self, other):  # self + other\n        if isscalarlike(other):\n            if other == 0:\n                return self.copy()\n            # Now we would add this scalar to every element.\n            raise NotImplementedError('adding a nonzero scalar to a '\n                                      'sparse matrix is not supported')\n        elif isspmatrix(other):\n            if other.shape != self.shape:\n                raise ValueError(\"inconsistent shapes\")\n            return self._add_sparse(other)\n        elif isdense(other):\n            other = np.broadcast_to(other, self.shape)\n            return self._add_dense(other)\n        else:\n            return NotImplemented\n\n    def __radd__(self,other):  # other + self\n        return self.__add__(other)\n\n    def __sub__(self, other):  # self - other\n        if isscalarlike(other):\n            if other == 0:\n                return self.copy()\n            raise NotImplementedError('subtracting a nonzero scalar from a '\n                                      'sparse matrix is not supported')\n        elif isspmatrix(other):\n            if other.shape != self.shape:\n                raise ValueError(\"inconsistent shapes\")\n            return self._sub_sparse(other)\n        elif isdense(other):\n            other = np.broadcast_to(other, self.shape)\n            return self._sub_dense(other)\n        else:\n            return NotImplemented\n\n    def __rsub__(self,other):  # other - self\n        if isscalarlike(other):\n            if other == 0:\n                return -self.copy()\n            raise NotImplementedError('subtracting a sparse matrix from a '\n                                      'nonzero scalar is not supported')\n        elif isdense(other):\n            other = np.broadcast_to(other, self.shape)\n            return self._rsub_dense(other)\n        else:\n            return NotImplemented\n\n    def __mul__(self, other):\n        \"\"\"interpret other and call one of the following\n\n        self._mul_scalar()\n        self._mul_vector()\n        self._mul_multivector()\n        self._mul_sparse_matrix()\n        \"\"\"\n\n        M, N = self.shape\n\n        if other.__class__ is np.ndarray:\n            # Fast path for the most common case\n            if other.shape == (N,):\n                return self._mul_vector(other)\n            elif other.shape == (N, 1):\n                return self._mul_vector(other.ravel()).reshape(M, 1)\n            elif other.ndim == 2 and other.shape[0] == N:\n                return self._mul_multivector(other)\n\n        if isscalarlike(other):\n            # scalar value\n            return self._mul_scalar(other)\n\n        if issparse(other):\n            if self.shape[1] != other.shape[0]:\n                raise ValueError('dimension mismatch')\n            return self._mul_sparse_matrix(other)\n\n        # If it's a list or whatever, treat it like a matrix\n        other_a = np.asanyarray(other)\n\n        if other_a.ndim == 0 and other_a.dtype == np.object_:\n            # Not interpretable as an array; return NotImplemented so that\n            # other's __rmul__ can kick in if that's implemented.\n            return NotImplemented\n\n        try:\n            other.shape\n        except AttributeError:\n            other = other_a\n\n        if other.ndim == 1 or other.ndim == 2 and other.shape[1] == 1:\n            # dense row or column vector\n            if other.shape != (N,) and other.shape != (N, 1):\n                raise ValueError('dimension mismatch')\n\n            result = self._mul_vector(np.ravel(other))\n\n            if isinstance(other, np.matrix):\n                result = asmatrix(result)\n\n            if other.ndim == 2 and other.shape[1] == 1:\n                # If 'other' was an (nx1) column vector, reshape the result\n                result = result.reshape(-1, 1)\n\n            return result\n\n        elif other.ndim == 2:\n            ##\n            # dense 2D array or matrix (\"multivector\")\n\n            if other.shape[0] != self.shape[1]:\n                raise ValueError('dimension mismatch')\n\n            result = self._mul_multivector(np.asarray(other))\n\n            if isinstance(other, np.matrix):\n                result = asmatrix(result)\n\n            return result\n\n        else:\n            raise ValueError('could not interpret dimensions')\n\n    # by default, use CSR for __mul__ handlers\n    def _mul_scalar(self, other):\n        return self.tocsr()._mul_scalar(other)\n\n    def _mul_vector(self, other):\n        return self.tocsr()._mul_vector(other)\n\n    def _mul_multivector(self, other):\n        return self.tocsr()._mul_multivector(other)\n\n    def _mul_sparse_matrix(self, other):\n        return self.tocsr()._mul_sparse_matrix(other)\n\n    def __rmul__(self, other):  # other * self\n        if isscalarlike(other):\n            return self.__mul__(other)\n        else:\n            # Don't use asarray unless we have to\n            try:\n                tr = other.transpose()\n            except AttributeError:\n                tr = np.asarray(other).transpose()\n            return (self.transpose() * tr).transpose()\n\n    #######################\n    # matmul (@) operator #\n    #######################\n\n    def __matmul__(self, other):\n        if isscalarlike(other):\n            raise ValueError(\"Scalar operands are not allowed, \"\n                             \"use '*' instead\")\n        return self.__mul__(other)\n\n    def __rmatmul__(self, other):\n        if isscalarlike(other):\n            raise ValueError(\"Scalar operands are not allowed, \"\n                             \"use '*' instead\")\n        return self.__rmul__(other)\n\n    ####################\n    # Other Arithmetic #\n    ####################\n\n    def _divide(self, other, true_divide=False, rdivide=False):\n        if isscalarlike(other):\n            if rdivide:\n                if true_divide:\n                    return np.true_divide(other, self.todense())\n                else:\n                    return np.divide(other, self.todense())\n\n            if true_divide and np.can_cast(self.dtype, np.float_):\n                return self.astype(np.float_)._mul_scalar(1./other)\n            else:\n                r = self._mul_scalar(1./other)\n\n                scalar_dtype = np.asarray(other).dtype\n                if (np.issubdtype(self.dtype, np.integer) and\n                        np.issubdtype(scalar_dtype, np.integer)):\n                    return r.astype(self.dtype)\n                else:\n                    return r\n\n        elif isdense(other):\n            if not rdivide:\n                if true_divide:\n                    return np.true_divide(self.todense(), other)\n                else:\n                    return np.divide(self.todense(), other)\n            else:\n                if true_divide:\n                    return np.true_divide(other, self.todense())\n                else:\n                    return np.divide(other, self.todense())\n        elif isspmatrix(other):\n            if rdivide:\n                return other._divide(self, true_divide, rdivide=False)\n\n            self_csr = self.tocsr()\n            if true_divide and np.can_cast(self.dtype, np.float_):\n                return self_csr.astype(np.float_)._divide_sparse(other)\n            else:\n                return self_csr._divide_sparse(other)\n        else:\n            return NotImplemented\n\n    def __truediv__(self, other):\n        return self._divide(other, true_divide=True)\n\n    def __div__(self, other):\n        # Always do true division\n        return self._divide(other, true_divide=True)\n\n    def __rtruediv__(self, other):\n        # Implementing this as the inverse would be too magical -- bail out\n        return NotImplemented\n\n    def __rdiv__(self, other):\n        # Implementing this as the inverse would be too magical -- bail out\n        return NotImplemented\n\n    def __neg__(self):\n        return -self.tocsr()\n\n    def __iadd__(self, other):\n        return NotImplemented\n\n    def __isub__(self, other):\n        return NotImplemented\n\n    def __imul__(self, other):\n        return NotImplemented\n\n    def __idiv__(self, other):\n        return self.__itruediv__(other)\n\n    def __itruediv__(self, other):\n        return NotImplemented\n\n    def __pow__(self, other):\n        if self.shape[0] != self.shape[1]:\n            raise TypeError('matrix is not square')\n\n        if isintlike(other):\n            other = int(other)\n            if other < 0:\n                raise ValueError('exponent must be >= 0')\n\n            if other == 0:\n                from .construct import eye\n                return eye(self.shape[0], dtype=self.dtype)\n            elif other == 1:\n                return self.copy()\n            else:\n                tmp = self.__pow__(other//2)\n                if (other % 2):\n                    return self * tmp * tmp\n                else:\n                    return tmp * tmp\n        elif isscalarlike(other):\n            raise ValueError('exponent must be an integer')\n        else:\n            return NotImplemented\n\n    def __getattr__(self, attr):\n        if attr == 'A':\n            return self.toarray()\n        elif attr == 'T':\n            return self.transpose()\n        elif attr == 'H':\n            return self.getH()\n        elif attr == 'real':\n            return self._real()\n        elif attr == 'imag':\n            return self._imag()\n        elif attr == 'size':\n            return self.getnnz()\n        else:\n            raise AttributeError(attr + \" not found\")\n\n    def transpose(self, axes=None, copy=False):\n        \"\"\"\n        Reverses the dimensions of the sparse matrix.\n\n        Parameters\n        ----------\n        axes : None, optional\n            This argument is in the signature *solely* for NumPy\n            compatibility reasons. Do not pass in anything except\n            for the default value.\n        copy : bool, optional\n            Indicates whether or not attributes of `self` should be\n            copied whenever possible. The degree to which attributes\n            are copied varies depending on the type of sparse matrix\n            being used.\n\n        Returns\n        -------\n        p : `self` with the dimensions reversed.\n\n        See Also\n        --------\n        numpy.matrix.transpose : NumPy's implementation of 'transpose'\n                                 for matrices\n        \"\"\"\n        return self.tocsr(copy=copy).transpose(axes=axes, copy=False)\n\n    def conj(self, copy=True):\n        \"\"\"Element-wise complex conjugation.\n\n        If the matrix is of non-complex data type and `copy` is False,\n        this method does nothing and the data is not copied.\n\n        Parameters\n        ----------\n        copy : bool, optional\n            If True, the result is guaranteed to not share data with self.\n\n        Returns\n        -------\n        A : The element-wise complex conjugate.\n\n        \"\"\"\n        if np.issubdtype(self.dtype, np.complexfloating):\n            return self.tocsr(copy=copy).conj(copy=False)\n        elif copy:\n            return self.copy()\n        else:\n            return self\n\n    def conjugate(self, copy=True):\n        return self.conj(copy=copy)\n\n    conjugate.__doc__ = conj.__doc__\n\n    # Renamed conjtranspose() -> getH() for compatibility with dense matrices\n    def getH(self):\n        \"\"\"Return the Hermitian transpose of this matrix.\n\n        See Also\n        --------\n        numpy.matrix.getH : NumPy's implementation of `getH` for matrices\n        \"\"\"\n        return self.transpose().conj()\n\n    def _real(self):\n        return self.tocsr()._real()\n\n    def _imag(self):\n        return self.tocsr()._imag()\n\n    def nonzero(self):\n        \"\"\"nonzero indices\n\n        Returns a tuple of arrays (row,col) containing the indices\n        of the non-zero elements of the matrix.\n\n        Examples\n        --------\n        >>> from scipy.sparse import csr_matrix\n        >>> A = csr_matrix([[1,2,0],[0,0,3],[4,0,5]])\n        >>> A.nonzero()\n        (array([0, 0, 1, 2, 2]), array([0, 1, 2, 0, 2]))\n\n        \"\"\"\n\n        # convert to COOrdinate format\n        A = self.tocoo()\n        nz_mask = A.data != 0\n        return (A.row[nz_mask], A.col[nz_mask])\n\n    def getcol(self, j):\n        \"\"\"Returns a copy of column j of the matrix, as an (m x 1) sparse\n        matrix (column vector).\n        \"\"\"\n        # Spmatrix subclasses should override this method for efficiency.\n        # Post-multiply by a (n x 1) column vector 'a' containing all zeros\n        # except for a_j = 1\n        from .csc import csc_matrix\n        n = self.shape[1]\n        if j < 0:\n            j += n\n        if j < 0 or j >= n:\n            raise IndexError(\"index out of bounds\")\n        col_selector = csc_matrix(([1], [[j], [0]]),\n                                  shape=(n, 1), dtype=self.dtype)\n        return self * col_selector\n\n    def getrow(self, i):\n        \"\"\"Returns a copy of row i of the matrix, as a (1 x n) sparse\n        matrix (row vector).\n        \"\"\"\n        # Spmatrix subclasses should override this method for efficiency.\n        # Pre-multiply by a (1 x m) row vector 'a' containing all zeros\n        # except for a_i = 1\n        from .csr import csr_matrix\n        m = self.shape[0]\n        if i < 0:\n            i += m\n        if i < 0 or i >= m:\n            raise IndexError(\"index out of bounds\")\n        row_selector = csr_matrix(([1], [[0], [i]]),\n                                  shape=(1, m), dtype=self.dtype)\n        return row_selector * self\n\n    # The following dunder methods cannot be implemented.\n    #\n    # def __array__(self):\n    #     # Sparse matrices rely on NumPy wrapping them in object arrays under\n    #     # the hood to make unary ufuncs work on them. So we cannot raise\n    #     # TypeError here - which would be handy to not give users object\n    #     # arrays they probably don't want (they're looking for `.toarray()`).\n    #     #\n    #     # Conversion with `toarray()` would also break things because of the\n    #     # behavior discussed above, plus we want to avoid densification by\n    #     # accident because that can too easily blow up memory.\n    #\n    # def __array_ufunc__(self):\n    #     # We cannot implement __array_ufunc__ due to mismatching semantics.\n    #     # See gh-7707 and gh-7349 for details.\n    #\n    # def __array_function__(self):\n    #     # We cannot implement __array_function__ due to mismatching semantics.\n    #     # See gh-10362 for details.\n\n    def todense(self, order=None, out=None):\n        \"\"\"\n        Return a dense matrix representation of this matrix.\n\n        Parameters\n        ----------\n        order : {'C', 'F'}, optional\n            Whether to store multi-dimensional data in C (row-major)\n            or Fortran (column-major) order in memory. The default\n            is 'None', indicating the NumPy default of C-ordered.\n            Cannot be specified in conjunction with the `out`\n            argument.\n\n        out : ndarray, 2-D, optional\n            If specified, uses this array (or `numpy.matrix`) as the\n            output buffer instead of allocating a new array to\n            return. The provided array must have the same shape and\n            dtype as the sparse matrix on which you are calling the\n            method.\n\n        Returns\n        -------\n        arr : numpy.matrix, 2-D\n            A NumPy matrix object with the same shape and containing\n            the same data represented by the sparse matrix, with the\n            requested memory order. If `out` was passed and was an\n            array (rather than a `numpy.matrix`), it will be filled\n            with the appropriate values and returned wrapped in a\n            `numpy.matrix` object that shares the same memory.\n        \"\"\"\n        return asmatrix(self.toarray(order=order, out=out))\n\n    def toarray(self, order=None, out=None):\n        \"\"\"\n        Return a dense ndarray representation of this matrix.\n\n        Parameters\n        ----------\n        order : {'C', 'F'}, optional\n            Whether to store multidimensional data in C (row-major)\n            or Fortran (column-major) order in memory. The default\n            is 'None', indicating the NumPy default of C-ordered.\n            Cannot be specified in conjunction with the `out`\n            argument.\n\n        out : ndarray, 2-D, optional\n            If specified, uses this array as the output buffer\n            instead of allocating a new array to return. The provided\n            array must have the same shape and dtype as the sparse\n            matrix on which you are calling the method. For most\n            sparse types, `out` is required to be memory contiguous\n            (either C or Fortran ordered).\n\n        Returns\n        -------\n        arr : ndarray, 2-D\n            An array with the same shape and containing the same\n            data represented by the sparse matrix, with the requested\n            memory order. If `out` was passed, the same object is\n            returned after being modified in-place to contain the\n            appropriate values.\n        \"\"\"\n        return self.tocoo(copy=False).toarray(order=order, out=out)\n\n    # Any sparse matrix format deriving from spmatrix must define one of\n    # tocsr or tocoo. The other conversion methods may be implemented for\n    # efficiency, but are not required.\n    def tocsr(self, copy=False):\n        \"\"\"Convert this matrix to Compressed Sparse Row format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant csr_matrix.\n        \"\"\"\n        return self.tocoo(copy=copy).tocsr(copy=False)\n\n    def todok(self, copy=False):\n        \"\"\"Convert this matrix to Dictionary Of Keys format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant dok_matrix.\n        \"\"\"\n        return self.tocoo(copy=copy).todok(copy=False)\n\n    def tocoo(self, copy=False):\n        \"\"\"Convert this matrix to COOrdinate format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant coo_matrix.\n        \"\"\"\n        return self.tocsr(copy=False).tocoo(copy=copy)\n\n    def tolil(self, copy=False):\n        \"\"\"Convert this matrix to List of Lists format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant lil_matrix.\n        \"\"\"\n        return self.tocsr(copy=False).tolil(copy=copy)\n\n    def todia(self, copy=False):\n        \"\"\"Convert this matrix to sparse DIAgonal format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant dia_matrix.\n        \"\"\"\n        return self.tocoo(copy=copy).todia(copy=False)\n\n    def tobsr(self, blocksize=None, copy=False):\n        \"\"\"Convert this matrix to Block Sparse Row format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant bsr_matrix.\n\n        When blocksize=(R, C) is provided, it will be used for construction of\n        the bsr_matrix.\n        \"\"\"\n        return self.tocsr(copy=False).tobsr(blocksize=blocksize, copy=copy)\n\n    def tocsc(self, copy=False):\n        \"\"\"Convert this matrix to Compressed Sparse Column format.\n\n        With copy=False, the data/indices may be shared between this matrix and\n        the resultant csc_matrix.\n        \"\"\"\n        return self.tocsr(copy=copy).tocsc(copy=False)\n\n    def copy(self):\n        \"\"\"Returns a copy of this matrix.\n\n        No data/indices will be shared between the returned value and current\n        matrix.\n        \"\"\"\n        return self.__class__(self, copy=True)\n\n    def sum(self, axis=None, dtype=None, out=None):\n        \"\"\"\n        Sum the matrix elements over a given axis.\n\n        Parameters\n        ----------\n        axis : {-2, -1, 0, 1, None} optional\n            Axis along which the sum is computed. The default is to\n            compute the sum of all the matrix elements, returning a scalar\n            (i.e., `axis` = `None`).\n        dtype : dtype, optional\n            The type of the returned matrix and of the accumulator in which\n            the elements are summed.  The dtype of `a` is used by default\n            unless `a` has an integer dtype of less precision than the default\n            platform integer.  In that case, if `a` is signed then the platform\n            integer is used while if `a` is unsigned then an unsigned integer\n            of the same precision as the platform integer is used.\n\n            .. versionadded:: 0.18.0\n\n        out : np.matrix, optional\n            Alternative output matrix in which to place the result. It must\n            have the same shape as the expected output, but the type of the\n            output values will be cast if necessary.\n\n            .. versionadded:: 0.18.0\n\n        Returns\n        -------\n        sum_along_axis : np.matrix\n            A matrix with the same shape as `self`, with the specified\n            axis removed.\n\n        See Also\n        --------\n        numpy.matrix.sum : NumPy's implementation of 'sum' for matrices\n\n        \"\"\"\n        validateaxis(axis)\n\n        # We use multiplication by a matrix of ones to achieve this.\n        # For some sparse matrix formats more efficient methods are\n        # possible -- these should override this function.\n        m, n = self.shape\n\n        # Mimic numpy's casting.\n        res_dtype = get_sum_dtype(self.dtype)\n\n        if axis is None:\n            # sum over rows and columns\n            return (self * asmatrix(np.ones(\n                (n, 1), dtype=res_dtype))).sum(\n                dtype=dtype, out=out)\n\n        if axis < 0:\n            axis += 2\n\n        # axis = 0 or 1 now\n        if axis == 0:\n            # sum over columns\n            ret = asmatrix(np.ones(\n                (1, m), dtype=res_dtype)) * self\n        else:\n            # sum over rows\n            ret = self * asmatrix(\n                np.ones((n, 1), dtype=res_dtype))\n\n        if out is not None and out.shape != ret.shape:\n            raise ValueError(\"dimensions do not match\")\n\n        return ret.sum(axis=(), dtype=dtype, out=out)\n\n    def mean(self, axis=None, dtype=None, out=None):\n        \"\"\"\n        Compute the arithmetic mean along the specified axis.\n\n        Returns the average of the matrix elements. The average is taken\n        over all elements in the matrix by default, otherwise over the\n        specified axis. `float64` intermediate and return values are used\n        for integer inputs.\n\n        Parameters\n        ----------\n        axis : {-2, -1, 0, 1, None} optional\n            Axis along which the mean is computed. The default is to compute\n            the mean of all elements in the matrix (i.e., `axis` = `None`).\n        dtype : data-type, optional\n            Type to use in computing the mean. For integer inputs, the default\n            is `float64`; for floating point inputs, it is the same as the\n            input dtype.\n\n            .. versionadded:: 0.18.0\n\n        out : np.matrix, optional\n            Alternative output matrix in which to place the result. It must\n            have the same shape as the expected output, but the type of the\n            output values will be cast if necessary.\n\n            .. versionadded:: 0.18.0\n\n        Returns\n        -------\n        m : np.matrix\n\n        See Also\n        --------\n        numpy.matrix.mean : NumPy's implementation of 'mean' for matrices\n\n        \"\"\"\n        def _is_integral(dtype):\n            return (np.issubdtype(dtype, np.integer) or\n                    np.issubdtype(dtype, np.bool_))\n\n        validateaxis(axis)\n\n        res_dtype = self.dtype.type\n        integral = _is_integral(self.dtype)\n\n        # output dtype\n        if dtype is None:\n            if integral:\n                res_dtype = np.float64\n        else:\n            res_dtype = np.dtype(dtype).type\n\n        # intermediate dtype for summation\n        inter_dtype = np.float64 if integral else res_dtype\n        inter_self = self.astype(inter_dtype)\n\n        if axis is None:\n            return (inter_self / np.array(\n                self.shape[0] * self.shape[1]))\\\n                .sum(dtype=res_dtype, out=out)\n\n        if axis < 0:\n            axis += 2\n\n        # axis = 0 or 1 now\n        if axis == 0:\n            return (inter_self * (1.0 / self.shape[0])).sum(\n                axis=0, dtype=res_dtype, out=out)\n        else:\n            return (inter_self * (1.0 / self.shape[1])).sum(\n                axis=1, dtype=res_dtype, out=out)\n\n    def diagonal(self, k=0):\n        \"\"\"Returns the kth diagonal of the matrix.\n\n        Parameters\n        ----------\n        k : int, optional\n            Which diagonal to get, corresponding to elements a[i, i+k].\n            Default: 0 (the main diagonal).\n\n            .. versionadded:: 1.0\n\n        See also\n        --------\n        numpy.diagonal : Equivalent numpy function.\n\n        Examples\n        --------\n        >>> from scipy.sparse import csr_matrix\n        >>> A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n        >>> A.diagonal()\n        array([1, 0, 5])\n        >>> A.diagonal(k=1)\n        array([2, 3])\n        \"\"\"\n        return self.tocsr().diagonal(k=k)\n\n    def setdiag(self, values, k=0):\n        \"\"\"\n        Set diagonal or off-diagonal elements of the array.\n\n        Parameters\n        ----------\n        values : array_like\n            New values of the diagonal elements.\n\n            Values may have any length. If the diagonal is longer than values,\n            then the remaining diagonal entries will not be set. If values are\n            longer than the diagonal, then the remaining values are ignored.\n\n            If a scalar value is given, all of the diagonal is set to it.\n\n        k : int, optional\n            Which off-diagonal to set, corresponding to elements a[i,i+k].\n            Default: 0 (the main diagonal).\n\n        \"\"\"\n        M, N = self.shape\n        if (k > 0 and k >= N) or (k < 0 and -k >= M):\n            raise ValueError(\"k exceeds matrix dimensions\")\n        self._setdiag(np.asarray(values), k)\n\n    def _setdiag(self, values, k):\n        M, N = self.shape\n        if k < 0:\n            if values.ndim == 0:\n                # broadcast\n                max_index = min(M+k, N)\n                for i in range(max_index):\n                    self[i - k, i] = values\n            else:\n                max_index = min(M+k, N, len(values))\n                if max_index <= 0:\n                    return\n                for i, v in enumerate(values[:max_index]):\n                    self[i - k, i] = v\n        else:\n            if values.ndim == 0:\n                # broadcast\n                max_index = min(M, N-k)\n                for i in range(max_index):\n                    self[i, i + k] = values\n            else:\n                max_index = min(M, N-k, len(values))\n                if max_index <= 0:\n                    return\n                for i, v in enumerate(values[:max_index]):\n                    self[i, i + k] = v\n\n    def _process_toarray_args(self, order, out):\n        if out is not None:\n            if order is not None:\n                raise ValueError('order cannot be specified if out '\n                                 'is not None')\n            if out.shape != self.shape or out.dtype != self.dtype:\n                raise ValueError('out array must be same dtype and shape as '\n                                 'sparse matrix')\n            out[...] = 0.\n            return out\n        else:\n            return np.zeros(self.shape, dtype=self.dtype, order=order)\n\n\ndef isspmatrix(x):\n    \"\"\"Is x of a sparse matrix type?\n\n    Parameters\n    ----------\n    x\n        object to check for being a sparse matrix\n\n    Returns\n    -------\n    bool\n        True if x is a sparse matrix, False otherwise\n\n    Notes\n    -----\n    issparse and isspmatrix are aliases for the same function.\n\n    Examples\n    --------\n    >>> from scipy.sparse import csr_matrix, isspmatrix\n    >>> isspmatrix(csr_matrix([[5]]))\n    True\n\n    >>> from scipy.sparse import isspmatrix\n    >>> isspmatrix(5)\n    False\n    \"\"\"\n    return isinstance(x, spmatrix)\n\n\nissparse = isspmatrix\n",1235],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py":["\"\"\"Python part of the warnings subsystem.\"\"\"\n\nimport sys\n\n\n__all__ = [\"warn\", \"warn_explicit\", \"showwarning\",\n           \"formatwarning\", \"filterwarnings\", \"simplefilter\",\n           \"resetwarnings\", \"catch_warnings\"]\n\ndef showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"Hook to write a warning to a file; replace if you like.\"\"\"\n    msg = WarningMessage(message, category, filename, lineno, file, line)\n    _showwarnmsg_impl(msg)\n\ndef formatwarning(message, category, filename, lineno, line=None):\n    \"\"\"Function to format a warning the standard way.\"\"\"\n    msg = WarningMessage(message, category, filename, lineno, None, line)\n    return _formatwarnmsg_impl(msg)\n\ndef _showwarnmsg_impl(msg):\n    file = msg.file\n    if file is None:\n        file = sys.stderr\n        if file is None:\n            # sys.stderr is None when run with pythonw.exe:\n            # warnings get lost\n            return\n    text = _formatwarnmsg(msg)\n    try:\n        file.write(text)\n    except OSError:\n        # the file (probably stderr) is invalid - this warning gets lost.\n        pass\n\ndef _formatwarnmsg_impl(msg):\n    category = msg.category.__name__\n    s =  f\"{msg.filename}:{msg.lineno}: {category}: {msg.message}\\n\"\n\n    if msg.line is None:\n        try:\n            import linecache\n            line = linecache.getline(msg.filename, msg.lineno)\n        except Exception:\n            # When a warning is logged during Python shutdown, linecache\n            # and the import machinery don't work anymore\n            line = None\n            linecache = None\n    else:\n        line = msg.line\n    if line:\n        line = line.strip()\n        s += \"  %s\\n\" % line\n\n    if msg.source is not None:\n        try:\n            import tracemalloc\n        # Logging a warning should not raise a new exception:\n        # catch Exception, not only ImportError and RecursionError.\n        except Exception:\n            # don't suggest to enable tracemalloc if it's not available\n            tracing = True\n            tb = None\n        else:\n            tracing = tracemalloc.is_tracing()\n            try:\n                tb = tracemalloc.get_object_traceback(msg.source)\n            except Exception:\n                # When a warning is logged during Python shutdown, tracemalloc\n                # and the import machinery don't work anymore\n                tb = None\n\n        if tb is not None:\n            s += 'Object allocated at (most recent call last):\\n'\n            for frame in tb:\n                s += ('  File \"%s\", lineno %s\\n'\n                      % (frame.filename, frame.lineno))\n\n                try:\n                    if linecache is not None:\n                        line = linecache.getline(frame.filename, frame.lineno)\n                    else:\n                        line = None\n                except Exception:\n                    line = None\n                if line:\n                    line = line.strip()\n                    s += '    %s\\n' % line\n        elif not tracing:\n            s += (f'{category}: Enable tracemalloc to get the object '\n                  f'allocation traceback\\n')\n    return s\n\n# Keep a reference to check if the function was replaced\n_showwarning_orig = showwarning\n\ndef _showwarnmsg(msg):\n    \"\"\"Hook to write a warning to a file; replace if you like.\"\"\"\n    try:\n        sw = showwarning\n    except NameError:\n        pass\n    else:\n        if sw is not _showwarning_orig:\n            # warnings.showwarning() was replaced\n            if not callable(sw):\n                raise TypeError(\"warnings.showwarning() must be set to a \"\n                                \"function or method\")\n\n            sw(msg.message, msg.category, msg.filename, msg.lineno,\n               msg.file, msg.line)\n            return\n    _showwarnmsg_impl(msg)\n\n# Keep a reference to check if the function was replaced\n_formatwarning_orig = formatwarning\n\ndef _formatwarnmsg(msg):\n    \"\"\"Function to format a warning the standard way.\"\"\"\n    try:\n        fw = formatwarning\n    except NameError:\n        pass\n    else:\n        if fw is not _formatwarning_orig:\n            # warnings.formatwarning() was replaced\n            return fw(msg.message, msg.category,\n                      msg.filename, msg.lineno, msg.line)\n    return _formatwarnmsg_impl(msg)\n\ndef filterwarnings(action, message=\"\", category=Warning, module=\"\", lineno=0,\n                   append=False):\n    \"\"\"Insert an entry into the list of warnings filters (at the front).\n\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'message' -- a regex that the warning message must match\n    'category' -- a class that the warning must be a subclass of\n    'module' -- a regex that the module name must match\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(message, str), \"message must be a string\"\n    assert isinstance(category, type), \"category must be a class\"\n    assert issubclass(category, Warning), \"category must be a Warning subclass\"\n    assert isinstance(module, str), \"module must be a string\"\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n\n    if message or module:\n        import re\n\n    if message:\n        message = re.compile(message, re.I)\n    else:\n        message = None\n    if module:\n        module = re.compile(module)\n    else:\n        module = None\n\n    _add_filter(action, message, category, module, lineno, append=append)\n\ndef simplefilter(action, category=Warning, lineno=0, append=False):\n    \"\"\"Insert a simple entry into the list of warnings filters (at the front).\n\n    A simple filter matches all modules and messages.\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'category' -- a class that the warning must be a subclass of\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    _add_filter(action, None, category, None, lineno, append=append)\n\ndef _add_filter(*item, append):\n    # Remove possible duplicate filters, so new one will be placed\n    # in correct place. If append=True and duplicate exists, do nothing.\n    if not append:\n        try:\n            filters.remove(item)\n        except ValueError:\n            pass\n        filters.insert(0, item)\n    else:\n        if item not in filters:\n            filters.append(item)\n    _filters_mutated()\n\ndef resetwarnings():\n    \"\"\"Clear the list of warning filters, so that no filters are active.\"\"\"\n    filters[:] = []\n    _filters_mutated()\n\nclass _OptionError(Exception):\n    \"\"\"Exception used by option processing helpers.\"\"\"\n    pass\n\n# Helper to process -W options passed via sys.warnoptions\ndef _processoptions(args):\n    for arg in args:\n        try:\n            _setoption(arg)\n        except _OptionError as msg:\n            print(\"Invalid -W option ignored:\", msg, file=sys.stderr)\n\n# Helper for _processoptions()\ndef _setoption(arg):\n    parts = arg.split(':')\n    if len(parts) > 5:\n        raise _OptionError(\"too many fields (max 5): %r\" % (arg,))\n    while len(parts) < 5:\n        parts.append('')\n    action, message, category, module, lineno = [s.strip()\n                                                 for s in parts]\n    action = _getaction(action)\n    category = _getcategory(category)\n    if message or module:\n        import re\n    if message:\n        message = re.escape(message)\n    if module:\n        module = re.escape(module) + r'\\Z'\n    if lineno:\n        try:\n            lineno = int(lineno)\n            if lineno < 0:\n                raise ValueError\n        except (ValueError, OverflowError):\n            raise _OptionError(\"invalid lineno %r\" % (lineno,)) from None\n    else:\n        lineno = 0\n    filterwarnings(action, message, category, module, lineno)\n\n# Helper for _setoption()\ndef _getaction(action):\n    if not action:\n        return \"default\"\n    if action == \"all\": return \"always\" # Alias\n    for a in ('default', 'always', 'ignore', 'module', 'once', 'error'):\n        if a.startswith(action):\n            return a\n    raise _OptionError(\"invalid action: %r\" % (action,))\n\n# Helper for _setoption()\ndef _getcategory(category):\n    if not category:\n        return Warning\n    if '.' not in category:\n        import builtins as m\n        klass = category\n    else:\n        module, _, klass = category.rpartition('.')\n        try:\n            m = __import__(module, None, None, [klass])\n        except ImportError:\n            raise _OptionError(\"invalid module name: %r\" % (module,)) from None\n    try:\n        cat = getattr(m, klass)\n    except AttributeError:\n        raise _OptionError(\"unknown warning category: %r\" % (category,)) from None\n    if not issubclass(cat, Warning):\n        raise _OptionError(\"invalid warning category: %r\" % (category,))\n    return cat\n\n\ndef _is_internal_frame(frame):\n    \"\"\"Signal whether the frame is an internal CPython implementation detail.\"\"\"\n    filename = frame.f_code.co_filename\n    return 'importlib' in filename and '_bootstrap' in filename\n\n\ndef _next_external_frame(frame):\n    \"\"\"Find the next frame that doesn't involve CPython internals.\"\"\"\n    frame = frame.f_back\n    while frame is not None and _is_internal_frame(frame):\n        frame = frame.f_back\n    return frame\n\n\n# Code typically replaced by _warnings\ndef warn(message, category=None, stacklevel=1, source=None):\n    \"\"\"Issue a warning, or maybe ignore it or raise an exception.\"\"\"\n    # Check if message is already a Warning object\n    if isinstance(message, Warning):\n        category = message.__class__\n    # Check category argument\n    if category is None:\n        category = UserWarning\n    if not (isinstance(category, type) and issubclass(category, Warning)):\n        raise TypeError(\"category must be a Warning subclass, \"\n                        \"not '{:s}'\".format(type(category).__name__))\n    # Get context information\n    try:\n        if stacklevel <= 1 or _is_internal_frame(sys._getframe(1)):\n            # If frame is too small to care or if the warning originated in\n            # internal code, then do not try to hide any frames.\n            frame = sys._getframe(stacklevel)\n        else:\n            frame = sys._getframe(1)\n            # Look for one frame less since the above line starts us off.\n            for x in range(stacklevel-1):\n                frame = _next_external_frame(frame)\n                if frame is None:\n                    raise ValueError\n    except ValueError:\n        globals = sys.__dict__\n        filename = \"sys\"\n        lineno = 1\n    else:\n        globals = frame.f_globals\n        filename = frame.f_code.co_filename\n        lineno = frame.f_lineno\n    if '__name__' in globals:\n        module = globals['__name__']\n    else:\n        module = \"<string>\"\n    registry = globals.setdefault(\"__warningregistry__\", {})\n    warn_explicit(message, category, filename, lineno, module, registry,\n                  globals, source)\n\ndef warn_explicit(message, category, filename, lineno,\n                  module=None, registry=None, module_globals=None,\n                  source=None):\n    lineno = int(lineno)\n    if module is None:\n        module = filename or \"<unknown>\"\n        if module[-3:].lower() == \".py\":\n            module = module[:-3] # XXX What about leading pathname?\n    if registry is None:\n        registry = {}\n    if registry.get('version', 0) != _filters_version:\n        registry.clear()\n        registry['version'] = _filters_version\n    if isinstance(message, Warning):\n        text = str(message)\n        category = message.__class__\n    else:\n        text = message\n        message = category(message)\n    key = (text, category, lineno)\n    # Quick test for common case\n    if registry.get(key):\n        return\n    # Search the filters\n    for item in filters:\n        action, msg, cat, mod, ln = item\n        if ((msg is None or msg.match(text)) and\n            issubclass(category, cat) and\n            (mod is None or mod.match(module)) and\n            (ln == 0 or lineno == ln)):\n            break\n    else:\n        action = defaultaction\n    # Early exit actions\n    if action == \"ignore\":\n        return\n\n    # Prime the linecache for formatting, in case the\n    # \"file\" is actually in a zipfile or something.\n    import linecache\n    linecache.getlines(filename, module_globals)\n\n    if action == \"error\":\n        raise message\n    # Other actions\n    if action == \"once\":\n        registry[key] = 1\n        oncekey = (text, category)\n        if onceregistry.get(oncekey):\n            return\n        onceregistry[oncekey] = 1\n    elif action == \"always\":\n        pass\n    elif action == \"module\":\n        registry[key] = 1\n        altkey = (text, category, 0)\n        if registry.get(altkey):\n            return\n        registry[altkey] = 1\n    elif action == \"default\":\n        registry[key] = 1\n    else:\n        # Unrecognized actions are errors\n        raise RuntimeError(\n              \"Unrecognized action (%r) in warnings.filters:\\n %s\" %\n              (action, item))\n    # Print message and context\n    msg = WarningMessage(message, category, filename, lineno, source)\n    _showwarnmsg(msg)\n\n\nclass WarningMessage(object):\n\n    _WARNING_DETAILS = (\"message\", \"category\", \"filename\", \"lineno\", \"file\",\n                        \"line\", \"source\")\n\n    def __init__(self, message, category, filename, lineno, file=None,\n                 line=None, source=None):\n        self.message = message\n        self.category = category\n        self.filename = filename\n        self.lineno = lineno\n        self.file = file\n        self.line = line\n        self.source = source\n        self._category_name = category.__name__ if category else None\n\n    def __str__(self):\n        return (\"{message : %r, category : %r, filename : %r, lineno : %s, \"\n                    \"line : %r}\" % (self.message, self._category_name,\n                                    self.filename, self.lineno, self.line))\n\n\nclass catch_warnings(object):\n\n    \"\"\"A context manager that copies and restores the warnings filter upon\n    exiting the context.\n\n    The 'record' argument specifies whether warnings should be captured by a\n    custom implementation of warnings.showwarning() and be appended to a list\n    returned by the context manager. Otherwise None is returned by the context\n    manager. The objects appended to the list are arguments whose attributes\n    mirror the arguments to showwarning().\n\n    The 'module' argument is to specify an alternative module to the module\n    named 'warnings' and imported under that name. This argument is only useful\n    when testing the warnings module itself.\n\n    \"\"\"\n\n    def __init__(self, *, record=False, module=None):\n        \"\"\"Specify whether to record warnings and if an alternative module\n        should be used other than sys.modules['warnings'].\n\n        For compatibility with Python 3.0, please consider all arguments to be\n        keyword-only.\n\n        \"\"\"\n        self._record = record\n        self._module = sys.modules['warnings'] if module is None else module\n        self._entered = False\n\n    def __repr__(self):\n        args = []\n        if self._record:\n            args.append(\"record=True\")\n        if self._module is not sys.modules['warnings']:\n            args.append(\"module=%r\" % self._module)\n        name = type(self).__name__\n        return \"%s(%s)\" % (name, \", \".join(args))\n\n    def __enter__(self):\n        if self._entered:\n            raise RuntimeError(\"Cannot enter %r twice\" % self)\n        self._entered = True\n        self._filters = self._module.filters\n        self._module.filters = self._filters[:]\n        self._module._filters_mutated()\n        self._showwarning = self._module.showwarning\n        self._showwarnmsg_impl = self._module._showwarnmsg_impl\n        if self._record:\n            log = []\n            self._module._showwarnmsg_impl = log.append\n            # Reset showwarning() to the default implementation to make sure\n            # that _showwarnmsg() calls _showwarnmsg_impl()\n            self._module.showwarning = self._module._showwarning_orig\n            return log\n        else:\n            return None\n\n    def __exit__(self, *exc_info):\n        if not self._entered:\n            raise RuntimeError(\"Cannot exit %r without entering first\" % self)\n        self._module.filters = self._filters\n        self._module._filters_mutated()\n        self._module.showwarning = self._showwarning\n        self._module._showwarnmsg_impl = self._showwarnmsg_impl\n\n\n# Private utility function called by _PyErr_WarnUnawaitedCoroutine\ndef _warn_unawaited_coroutine(coro):\n    msg_lines = [\n        f\"coroutine '{coro.__qualname__}' was never awaited\\n\"\n    ]\n    if coro.cr_origin is not None:\n        import linecache, traceback\n        def extract():\n            for filename, lineno, funcname in reversed(coro.cr_origin):\n                line = linecache.getline(filename, lineno)\n                yield (filename, lineno, funcname, line)\n        msg_lines.append(\"Coroutine created at (most recent call last)\\n\")\n        msg_lines += traceback.format_list(list(extract()))\n    msg = \"\".join(msg_lines).rstrip(\"\\n\")\n    # Passing source= here means that if the user happens to have tracemalloc\n    # enabled and tracking where the coroutine was created, the warning will\n    # contain that traceback. This does mean that if they have *both*\n    # coroutine origin tracking *and* tracemalloc enabled, they'll get two\n    # partially-redundant tracebacks. If we wanted to be clever we could\n    # probably detect this case and avoid it, but for now we don't bother.\n    warn(msg, category=RuntimeWarning, stacklevel=2, source=coro)\n\n\n# filters contains a sequence of filter 5-tuples\n# The components of the 5-tuple are:\n# - an action: error, ignore, always, default, module, or once\n# - a compiled regex that must match the warning message\n# - a class representing the warning category\n# - a compiled regex that must match the module that is being warned\n# - a line number for the line being warning, or 0 to mean any line\n# If either if the compiled regexs are None, match anything.\ntry:\n    from _warnings import (filters, _defaultaction, _onceregistry,\n                           warn, warn_explicit, _filters_mutated)\n    defaultaction = _defaultaction\n    onceregistry = _onceregistry\n    _warnings_defaults = True\nexcept ImportError:\n    filters = []\n    defaultaction = \"default\"\n    onceregistry = {}\n\n    _filters_version = 1\n\n    def _filters_mutated():\n        global _filters_version\n        _filters_version += 1\n\n    _warnings_defaults = False\n\n\n# Module initialization\n_processoptions(sys.warnoptions)\nif not _warnings_defaults:\n    # Several warning categories are ignored by default in regular builds\n    if not hasattr(sys, 'gettotalrefcount'):\n        filterwarnings(\"default\", category=DeprecationWarning,\n                       module=\"__main__\", append=1)\n        simplefilter(\"ignore\", category=DeprecationWarning, append=1)\n        simplefilter(\"ignore\", category=PendingDeprecationWarning, append=1)\n        simplefilter(\"ignore\", category=ImportWarning, append=1)\n        simplefilter(\"ignore\", category=ResourceWarning, append=1)\n\ndel _warnings_defaults\n",549],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py":["\"\"\"\nFunctions in the ``as*array`` family that promote array-likes into arrays.\n\n`require` fits this category despite its name not matching this pattern.\n\"\"\"\nfrom .overrides import (\n    array_function_dispatch,\n    set_array_function_like_doc,\n    set_module,\n)\nfrom .multiarray import array\n\n\n__all__ = [\n    \"asarray\", \"asanyarray\", \"ascontiguousarray\", \"asfortranarray\", \"require\",\n]\n\n\ndef _asarray_dispatcher(a, dtype=None, order=None, *, like=None):\n    return (like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef asarray(a, dtype=None, order=None, *, like=None):\n    \"\"\"Convert the input to an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data, in any form that can be converted to an array.  This\n        includes lists, lists of tuples, tuples, tuples of tuples, tuples\n        of lists and ndarrays.\n    dtype : data-type, optional\n        By default, the data-type is inferred from the input data.\n    order : {'C', 'F', 'A', 'K'}, optional\n        Memory layout.  'A' and 'K' depend on the order of input array a.\n        'C' row-major (C-style), \n        'F' column-major (Fortran-style) memory representation.\n        'A' (any) means 'F' if `a` is Fortran contiguous, 'C' otherwise\n        'K' (keep) preserve input order\n        Defaults to 'C'.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array interpretation of `a`.  No copy is performed if the input\n        is already an ndarray with matching dtype and order.  If `a` is a\n        subclass of ndarray, a base class ndarray is returned.\n\n    See Also\n    --------\n    asanyarray : Similar function which passes through subclasses.\n    ascontiguousarray : Convert input to a contiguous array.\n    asfarray : Convert input to a floating point ndarray.\n    asfortranarray : Convert input to an ndarray with column-major\n                     memory order.\n    asarray_chkfinite : Similar function which checks input for NaNs and Infs.\n    fromiter : Create an array from an iterator.\n    fromfunction : Construct an array by executing a function on grid\n                   positions.\n\n    Examples\n    --------\n    Convert a list into an array:\n\n    >>> a = [1, 2]\n    >>> np.asarray(a)\n    array([1, 2])\n\n    Existing arrays are not copied:\n\n    >>> a = np.array([1, 2])\n    >>> np.asarray(a) is a\n    True\n\n    If `dtype` is set, array is copied only if dtype does not match:\n\n    >>> a = np.array([1, 2], dtype=np.float32)\n    >>> np.asarray(a, dtype=np.float32) is a\n    True\n    >>> np.asarray(a, dtype=np.float64) is a\n    False\n\n    Contrary to `asanyarray`, ndarray subclasses are not passed through:\n\n    >>> issubclass(np.recarray, np.ndarray)\n    True\n    >>> a = np.array([(1.0, 2), (3.0, 4)], dtype='f4,i4').view(np.recarray)\n    >>> np.asarray(a) is a\n    False\n    >>> np.asanyarray(a) is a\n    True\n\n    \"\"\"\n    if like is not None:\n        return _asarray_with_like(a, dtype=dtype, order=order, like=like)\n\n    return array(a, dtype, copy=False, order=order)\n\n\n_asarray_with_like = array_function_dispatch(\n    _asarray_dispatcher\n)(asarray)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef asanyarray(a, dtype=None, order=None, *, like=None):\n    \"\"\"Convert the input to an ndarray, but pass ndarray subclasses through.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data, in any form that can be converted to an array.  This\n        includes scalars, lists, lists of tuples, tuples, tuples of tuples,\n        tuples of lists, and ndarrays.\n    dtype : data-type, optional\n        By default, the data-type is inferred from the input data.\n    order : {'C', 'F', 'A', 'K'}, optional\n        Memory layout.  'A' and 'K' depend on the order of input array a.\n        'C' row-major (C-style), \n        'F' column-major (Fortran-style) memory representation.\n        'A' (any) means 'F' if `a` is Fortran contiguous, 'C' otherwise\n        'K' (keep) preserve input order\n        Defaults to 'C'.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray or an ndarray subclass\n        Array interpretation of `a`.  If `a` is an ndarray or a subclass\n        of ndarray, it is returned as-is and no copy is performed.\n\n    See Also\n    --------\n    asarray : Similar function which always returns ndarrays.\n    ascontiguousarray : Convert input to a contiguous array.\n    asfarray : Convert input to a floating point ndarray.\n    asfortranarray : Convert input to an ndarray with column-major\n                     memory order.\n    asarray_chkfinite : Similar function which checks input for NaNs and\n                        Infs.\n    fromiter : Create an array from an iterator.\n    fromfunction : Construct an array by executing a function on grid\n                   positions.\n\n    Examples\n    --------\n    Convert a list into an array:\n\n    >>> a = [1, 2]\n    >>> np.asanyarray(a)\n    array([1, 2])\n\n    Instances of `ndarray` subclasses are passed through as-is:\n\n    >>> a = np.array([(1.0, 2), (3.0, 4)], dtype='f4,i4').view(np.recarray)\n    >>> np.asanyarray(a) is a\n    True\n\n    \"\"\"\n    if like is not None:\n        return _asanyarray_with_like(a, dtype=dtype, order=order, like=like)\n\n    return array(a, dtype, copy=False, order=order, subok=True)\n\n\n_asanyarray_with_like = array_function_dispatch(\n    _asarray_dispatcher\n)(asanyarray)\n\n\ndef _asarray_contiguous_fortran_dispatcher(a, dtype=None, *, like=None):\n    return (like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef ascontiguousarray(a, dtype=None, *, like=None):\n    \"\"\"\n    Return a contiguous array (ndim >= 1) in memory (C order).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    dtype : str or dtype object, optional\n        Data-type of returned array.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Contiguous array of same shape and content as `a`, with type `dtype`\n        if specified.\n\n    See Also\n    --------\n    asfortranarray : Convert input to an ndarray with column-major\n                     memory order.\n    require : Return an ndarray that satisfies requirements.\n    ndarray.flags : Information about the memory layout of the array.\n\n    Examples\n    --------\n    >>> x = np.arange(6).reshape(2,3)\n    >>> np.ascontiguousarray(x, dtype=np.float32)\n    array([[0., 1., 2.],\n           [3., 4., 5.]], dtype=float32)\n    >>> x.flags['C_CONTIGUOUS']\n    True\n\n    Note: This function returns an array with at least one-dimension (1-d) \n    so it will not preserve 0-d arrays.  \n\n    \"\"\"\n    if like is not None:\n        return _ascontiguousarray_with_like(a, dtype=dtype, like=like)\n\n    return array(a, dtype, copy=False, order='C', ndmin=1)\n\n\n_ascontiguousarray_with_like = array_function_dispatch(\n    _asarray_contiguous_fortran_dispatcher\n)(ascontiguousarray)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef asfortranarray(a, dtype=None, *, like=None):\n    \"\"\"\n    Return an array (ndim >= 1) laid out in Fortran order in memory.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    dtype : str or dtype object, optional\n        By default, the data-type is inferred from the input data.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        The input `a` in Fortran, or column-major, order.\n\n    See Also\n    --------\n    ascontiguousarray : Convert input to a contiguous (C order) array.\n    asanyarray : Convert input to an ndarray with either row or\n        column-major memory order.\n    require : Return an ndarray that satisfies requirements.\n    ndarray.flags : Information about the memory layout of the array.\n\n    Examples\n    --------\n    >>> x = np.arange(6).reshape(2,3)\n    >>> y = np.asfortranarray(x)\n    >>> x.flags['F_CONTIGUOUS']\n    False\n    >>> y.flags['F_CONTIGUOUS']\n    True\n\n    Note: This function returns an array with at least one-dimension (1-d) \n    so it will not preserve 0-d arrays.  \n\n    \"\"\"\n    if like is not None:\n        return _asfortranarray_with_like(a, dtype=dtype, like=like)\n\n    return array(a, dtype, copy=False, order='F', ndmin=1)\n\n\n_asfortranarray_with_like = array_function_dispatch(\n    _asarray_contiguous_fortran_dispatcher\n)(asfortranarray)\n\n\ndef _require_dispatcher(a, dtype=None, requirements=None, *, like=None):\n    return (like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef require(a, dtype=None, requirements=None, *, like=None):\n    \"\"\"\n    Return an ndarray of the provided type that satisfies requirements.\n\n    This function is useful to be sure that an array with the correct flags\n    is returned for passing to compiled code (perhaps through ctypes).\n\n    Parameters\n    ----------\n    a : array_like\n       The object to be converted to a type-and-requirement-satisfying array.\n    dtype : data-type\n       The required data-type. If None preserve the current dtype. If your\n       application requires the data to be in native byteorder, include\n       a byteorder specification as a part of the dtype specification.\n    requirements : str or list of str\n       The requirements list can be any of the following\n\n       * 'F_CONTIGUOUS' ('F') - ensure a Fortran-contiguous array\n       * 'C_CONTIGUOUS' ('C') - ensure a C-contiguous array\n       * 'ALIGNED' ('A')      - ensure a data-type aligned array\n       * 'WRITEABLE' ('W')    - ensure a writable array\n       * 'OWNDATA' ('O')      - ensure an array that owns its own data\n       * 'ENSUREARRAY', ('E') - ensure a base array, instead of a subclass\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array with specified requirements and type if given.\n\n    See Also\n    --------\n    asarray : Convert input to an ndarray.\n    asanyarray : Convert to an ndarray, but pass through ndarray subclasses.\n    ascontiguousarray : Convert input to a contiguous array.\n    asfortranarray : Convert input to an ndarray with column-major\n                     memory order.\n    ndarray.flags : Information about the memory layout of the array.\n\n    Notes\n    -----\n    The returned array will be guaranteed to have the listed requirements\n    by making a copy if needed.\n\n    Examples\n    --------\n    >>> x = np.arange(6).reshape(2,3)\n    >>> x.flags\n      C_CONTIGUOUS : True\n      F_CONTIGUOUS : False\n      OWNDATA : False\n      WRITEABLE : True\n      ALIGNED : True\n      WRITEBACKIFCOPY : False\n      UPDATEIFCOPY : False\n\n    >>> y = np.require(x, dtype=np.float32, requirements=['A', 'O', 'W', 'F'])\n    >>> y.flags\n      C_CONTIGUOUS : False\n      F_CONTIGUOUS : True\n      OWNDATA : True\n      WRITEABLE : True\n      ALIGNED : True\n      WRITEBACKIFCOPY : False\n      UPDATEIFCOPY : False\n\n    \"\"\"\n    if like is not None:\n        return _require_with_like(\n            a,\n            dtype=dtype,\n            requirements=requirements,\n            like=like,\n        )\n\n    possible_flags = {'C': 'C', 'C_CONTIGUOUS': 'C', 'CONTIGUOUS': 'C',\n                      'F': 'F', 'F_CONTIGUOUS': 'F', 'FORTRAN': 'F',\n                      'A': 'A', 'ALIGNED': 'A',\n                      'W': 'W', 'WRITEABLE': 'W',\n                      'O': 'O', 'OWNDATA': 'O',\n                      'E': 'E', 'ENSUREARRAY': 'E'}\n    if not requirements:\n        return asanyarray(a, dtype=dtype)\n    else:\n        requirements = {possible_flags[x.upper()] for x in requirements}\n\n    if 'E' in requirements:\n        requirements.remove('E')\n        subok = False\n    else:\n        subok = True\n\n    order = 'A'\n    if requirements >= {'C', 'F'}:\n        raise ValueError('Cannot specify both \"C\" and \"F\" order')\n    elif 'F' in requirements:\n        order = 'F'\n        requirements.remove('F')\n    elif 'C' in requirements:\n        order = 'C'\n        requirements.remove('C')\n\n    arr = array(a, dtype=dtype, order=order, copy=False, subok=subok)\n\n    for prop in requirements:\n        if not arr.flags[prop]:\n            arr = arr.copy(order)\n            break\n    return arr\n\n\n_require_with_like = array_function_dispatch(\n    _require_dispatcher\n)(require)\n",411],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py":["\"\"\"Utilities for input validation\"\"\"\n\n# Authors: Olivier Grisel\n#          Gael Varoquaux\n#          Andreas Mueller\n#          Lars Buitinck\n#          Alexandre Gramfort\n#          Nicolas Tresegnie\n#          Sylvain Marie\n# License: BSD 3 clause\n\nfrom functools import wraps\nimport warnings\nimport numbers\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom inspect import signature, isclass, Parameter\n\n# mypy error: Module 'numpy.core.numeric' has no attribute 'ComplexWarning'\nfrom numpy.core.numeric import ComplexWarning  # type: ignore\nimport joblib\n\nfrom contextlib import suppress\n\nfrom .fixes import _object_dtype_isnan, parse_version\nfrom .. import get_config as _get_config\nfrom ..exceptions import PositiveSpectrumWarning\nfrom ..exceptions import NotFittedError\nfrom ..exceptions import DataConversionWarning\n\nFLOAT_DTYPES = (np.float64, np.float32, np.float16)\n\n\ndef _deprecate_positional_args(func=None, *, version=\"1.1 (renaming of 0.26)\"):\n    \"\"\"Decorator for methods that issues warnings for positional arguments.\n\n    Using the keyword-only argument syntax in pep 3102, arguments after the\n    * will issue a warning when passed as a positional argument.\n\n    Parameters\n    ----------\n    func : callable, default=None\n        Function to check arguments on.\n    version : callable, default=\"1.1 (renaming of 0.26)\"\n        The version when positional arguments will result in error.\n    \"\"\"\n    def _inner_deprecate_positional_args(f):\n        sig = signature(f)\n        kwonly_args = []\n        all_args = []\n\n        for name, param in sig.parameters.items():\n            if param.kind == Parameter.POSITIONAL_OR_KEYWORD:\n                all_args.append(name)\n            elif param.kind == Parameter.KEYWORD_ONLY:\n                kwonly_args.append(name)\n\n        @wraps(f)\n        def inner_f(*args, **kwargs):\n            extra_args = len(args) - len(all_args)\n            if extra_args <= 0:\n                return f(*args, **kwargs)\n\n            # extra_args > 0\n            args_msg = ['{}={}'.format(name, arg)\n                        for name, arg in zip(kwonly_args[:extra_args],\n                                             args[-extra_args:])]\n            args_msg = \", \".join(args_msg)\n            warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n                          f\"{version} passing these as positional arguments \"\n                          \"will result in an error\", FutureWarning)\n            kwargs.update(zip(sig.parameters, args))\n            return f(**kwargs)\n        return inner_f\n\n    if func is not None:\n        return _inner_deprecate_positional_args(func)\n\n    return _inner_deprecate_positional_args\n\n\ndef _assert_all_finite(X, allow_nan=False, msg_dtype=None):\n    \"\"\"Like assert_all_finite, but only for ndarray.\"\"\"\n    # validation is also imported in extmath\n    from .extmath import _safe_accumulator_op\n\n    if _get_config()['assume_finite']:\n        return\n    X = np.asanyarray(X)\n    # First try an O(n) time, O(1) space solution for the common case that\n    # everything is finite; fall back to O(n) space np.isfinite to prevent\n    # false positives from overflow in sum method. The sum is also calculated\n    # safely to reduce dtype induced overflows.\n    is_float = X.dtype.kind in 'fc'\n    if is_float and (np.isfinite(_safe_accumulator_op(np.sum, X))):\n        pass\n    elif is_float:\n        msg_err = \"Input contains {} or a value too large for {!r}.\"\n        if (allow_nan and np.isinf(X).any() or\n                not allow_nan and not np.isfinite(X).all()):\n            type_err = 'infinity' if allow_nan else 'NaN, infinity'\n            raise ValueError(\n                    msg_err.format\n                    (type_err,\n                     msg_dtype if msg_dtype is not None else X.dtype)\n            )\n    # for object dtype data, we only check for NaNs (GH-13254)\n    elif X.dtype == np.dtype('object') and not allow_nan:\n        if _object_dtype_isnan(X).any():\n            raise ValueError(\"Input contains NaN\")\n\n\ndef assert_all_finite(X, *, allow_nan=False):\n    \"\"\"Throw a ValueError if X contains NaN or infinity.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix}\n\n    allow_nan : bool, default=False\n    \"\"\"\n    _assert_all_finite(X.data if sp.issparse(X) else X, allow_nan)\n\n\ndef as_float_array(X, *, copy=True, force_all_finite=True):\n    \"\"\"Converts an array-like to an array of floats.\n\n    The new dtype will be np.float32 or np.float64, depending on the original\n    type. The function can create a copy or modify the argument depending\n    on the argument copy.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n\n    copy : bool, default=True\n        If True, a copy of X will be created. If False, a copy may still be\n        returned if X's dtype is not a floating point type.\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in X. The\n        possibilities are:\n\n        - True: Force all values of X to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in X.\n        - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot\n          be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n    Returns\n    -------\n    XT : {ndarray, sparse matrix}\n        An array of type float.\n    \"\"\"\n    if isinstance(X, np.matrix) or (not isinstance(X, np.ndarray)\n                                    and not sp.issparse(X)):\n        return check_array(X, accept_sparse=['csr', 'csc', 'coo'],\n                           dtype=np.float64, copy=copy,\n                           force_all_finite=force_all_finite, ensure_2d=False)\n    elif sp.issparse(X) and X.dtype in [np.float32, np.float64]:\n        return X.copy() if copy else X\n    elif X.dtype in [np.float32, np.float64]:  # is numpy array\n        return X.copy('F' if X.flags['F_CONTIGUOUS'] else 'C') if copy else X\n    else:\n        if X.dtype.kind in 'uib' and X.dtype.itemsize <= 4:\n            return_dtype = np.float32\n        else:\n            return_dtype = np.float64\n        return X.astype(return_dtype)\n\n\ndef _is_arraylike(x):\n    \"\"\"Returns whether the input is array-like.\"\"\"\n    return (hasattr(x, '__len__') or\n            hasattr(x, 'shape') or\n            hasattr(x, '__array__'))\n\n\ndef _num_features(X):\n    \"\"\"Return the number of features in an array-like X.\n\n    This helper function tries hard to avoid to materialize an array version\n    of X unless necessary. For instance, if X is a list of lists,\n    this function will return the length of the first element, assuming\n    that subsequent elements are all lists of the same length without\n    checking.\n    Parameters\n    ----------\n    X : array-like\n        array-like to get the number of features.\n\n    Returns\n    -------\n    features : int\n        Number of features\n    \"\"\"\n    type_ = type(X)\n    if type_.__module__ == \"builtins\":\n        type_name = type_.__qualname__\n    else:\n        type_name = f\"{type_.__module__}.{type_.__qualname__}\"\n    message = (\n        \"Unable to find the number of features from X of type \"\n        f\"{type_name}\"\n    )\n    if not hasattr(X, '__len__') and not hasattr(X, 'shape'):\n        if not hasattr(X, '__array__'):\n            raise TypeError(message)\n        # Only convert X to a numpy array if there is no cheaper, heuristic\n        # option.\n        X = np.asarray(X)\n\n    if hasattr(X, 'shape'):\n        if not hasattr(X.shape, '__len__') or len(X.shape) <= 1:\n            message += f\" with shape {X.shape}\"\n            raise TypeError(message)\n        return X.shape[1]\n\n    first_sample = X[0]\n\n    # Do not consider an array-like of strings or dicts to be a 2D array\n    if isinstance(first_sample, (str, bytes, dict)):\n        message += (f\" where the samples are of type \"\n                    f\"{type(first_sample).__qualname__}\")\n        raise TypeError(message)\n\n    try:\n        # If X is a list of lists, for instance, we assume that all nested\n        # lists have the same length without checking or converting to\n        # a numpy array to keep this function call as cheap as possible.\n        return len(first_sample)\n    except Exception as err:\n        raise TypeError(message) from err\n\n\ndef _num_samples(x):\n    \"\"\"Return number of samples in array-like x.\"\"\"\n    message = 'Expected sequence or array-like, got %s' % type(x)\n    if hasattr(x, 'fit') and callable(x.fit):\n        # Don't get num_samples from an ensembles length!\n        raise TypeError(message)\n\n    if not hasattr(x, '__len__') and not hasattr(x, 'shape'):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(message)\n\n    if hasattr(x, 'shape') and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError(\"Singleton array %r cannot be considered\"\n                            \" a valid collection.\" % x)\n        # Check that shape is returning an integer or default to len\n        # Dask dataframes may not return numeric shape[0] value\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error\n\n\ndef check_memory(memory):\n    \"\"\"Check that ``memory`` is joblib.Memory-like.\n\n    joblib.Memory-like means that ``memory`` can be converted into a\n    joblib.Memory instance (typically a str denoting the ``location``)\n    or has the same interface (has a ``cache`` method).\n\n    Parameters\n    ----------\n    memory : None, str or object with the joblib.Memory interface\n\n    Returns\n    -------\n    memory : object with the joblib.Memory interface\n\n    Raises\n    ------\n    ValueError\n        If ``memory`` is not joblib.Memory-like.\n    \"\"\"\n\n    if memory is None or isinstance(memory, str):\n        if parse_version(joblib.__version__) < parse_version('0.12'):\n            memory = joblib.Memory(cachedir=memory, verbose=0)\n        else:\n            memory = joblib.Memory(location=memory, verbose=0)\n    elif not hasattr(memory, 'cache'):\n        raise ValueError(\"'memory' should be None, a string or have the same\"\n                         \" interface as joblib.Memory.\"\n                         \" Got memory='{}' instead.\".format(memory))\n    return memory\n\n\ndef check_consistent_length(*arrays):\n    \"\"\"Check that all arrays have consistent first dimensions.\n\n    Checks whether all objects in arrays have the same shape or length.\n\n    Parameters\n    ----------\n    *arrays : list or tuple of input objects.\n        Objects that will be checked for consistent length.\n    \"\"\"\n\n    lengths = [_num_samples(X) for X in arrays if X is not None]\n    uniques = np.unique(lengths)\n    if len(uniques) > 1:\n        raise ValueError(\"Found input variables with inconsistent numbers of\"\n                         \" samples: %r\" % [int(l) for l in lengths])\n\n\ndef _make_indexable(iterable):\n    \"\"\"Ensure iterable supports indexing or convert to an indexable variant.\n\n    Convert sparse matrices to csr and other non-indexable iterable to arrays.\n    Let `None` and indexable objects (e.g. pandas dataframes) pass unchanged.\n\n    Parameters\n    ----------\n    iterable : {list, dataframe, ndarray, sparse matrix} or None\n        Object to be converted to an indexable iterable.\n    \"\"\"\n    if sp.issparse(iterable):\n        return iterable.tocsr()\n    elif hasattr(iterable, \"__getitem__\") or hasattr(iterable, \"iloc\"):\n        return iterable\n    elif iterable is None:\n        return iterable\n    return np.array(iterable)\n\n\ndef indexable(*iterables):\n    \"\"\"Make arrays indexable for cross-validation.\n\n    Checks consistent length, passes through None, and ensures that everything\n    can be indexed by converting sparse matrices to csr and converting\n    non-interable objects to arrays.\n\n    Parameters\n    ----------\n    *iterables : {lists, dataframes, ndarrays, sparse matrices}\n        List of objects to ensure sliceability.\n    \"\"\"\n    result = [_make_indexable(X) for X in iterables]\n    check_consistent_length(*result)\n    return result\n\n\ndef _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy,\n                          force_all_finite, accept_large_sparse):\n    \"\"\"Convert a sparse matrix to a given format.\n\n    Checks the sparse format of spmatrix and converts if necessary.\n\n    Parameters\n    ----------\n    spmatrix : sparse matrix\n        Input to validate and convert.\n\n    accept_sparse : str, bool or list/tuple of str\n        String[s] representing allowed sparse matrix formats ('csc',\n        'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). If the input is sparse but\n        not in the allowed format, it will be converted to the first listed\n        format. True allows the input to be any format. False means\n        that a sparse matrix input will raise an error.\n\n    dtype : str, type or None\n        Data type of result. If None, the dtype of the input is preserved.\n\n    copy : bool\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : bool or 'allow-nan'\n        Whether to raise an error on np.inf, np.nan, pd.NA in X. The\n        possibilities are:\n\n        - True: Force all values of X to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in X.\n        - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot\n          be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n    Returns\n    -------\n    spmatrix_converted : sparse matrix.\n        Matrix that is ensured to have an allowed type.\n    \"\"\"\n    if dtype is None:\n        dtype = spmatrix.dtype\n\n    changed_format = False\n\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n\n    # Indices dtype validation\n    _check_large_sparse(spmatrix, accept_large_sparse)\n\n    if accept_sparse is False:\n        raise TypeError('A sparse matrix was passed, but dense '\n                        'data is required. Use X.toarray() to '\n                        'convert to a dense numpy array.')\n    elif isinstance(accept_sparse, (list, tuple)):\n        if len(accept_sparse) == 0:\n            raise ValueError(\"When providing 'accept_sparse' \"\n                             \"as a tuple or list, it must contain at \"\n                             \"least one string value.\")\n        # ensure correct sparse format\n        if spmatrix.format not in accept_sparse:\n            # create new with correct sparse\n            spmatrix = spmatrix.asformat(accept_sparse[0])\n            changed_format = True\n    elif accept_sparse is not True:\n        # any other type\n        raise ValueError(\"Parameter 'accept_sparse' should be a string, \"\n                         \"boolean or list of strings. You provided \"\n                         \"'accept_sparse={}'.\".format(accept_sparse))\n\n    if dtype != spmatrix.dtype:\n        # convert dtype\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and not changed_format:\n        # force copy\n        spmatrix = spmatrix.copy()\n\n    if force_all_finite:\n        if not hasattr(spmatrix, \"data\"):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\"\n                          % spmatrix.format, stacklevel=2)\n        else:\n            _assert_all_finite(spmatrix.data,\n                               allow_nan=force_all_finite == 'allow-nan')\n\n    return spmatrix\n\n\ndef _ensure_no_complex_data(array):\n    if hasattr(array, 'dtype') and array.dtype is not None \\\n            and hasattr(array.dtype, 'kind') and array.dtype.kind == \"c\":\n        raise ValueError(\"Complex data not supported\\n\"\n                         \"{}\\n\".format(array))\n\n\ndef check_array(array, accept_sparse=False, *, accept_large_sparse=True,\n                dtype=\"numeric\", order=None, copy=False, force_all_finite=True,\n                ensure_2d=True, allow_nd=False, ensure_min_samples=1,\n                ensure_min_features=1, estimator=None):\n\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is checked to be a non-empty 2D array containing\n    only finite values. If the dtype of the array is object, attempt\n    converting to float, raising on failure.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : str, bool or list/tuple of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse=False will cause it to be accepted\n        only if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n\n    order : {'F', 'C'} or None, default=None\n        Whether an array will be forced to be fortran or c-style.\n        When order is None (default), then if copy=False, nothing is ensured\n        about the memory layout of the output array; otherwise (copy=True)\n        the memory layout of the returned array is kept as close as possible\n        to the original array.\n\n    copy : bool, default=False\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n    ensure_2d : bool, default=True\n        Whether to raise a value error if array is not 2D.\n\n    allow_nd : bool, default=False\n        Whether to allow array.ndim > 2.\n\n    ensure_min_samples : int, default=1\n        Make sure that the array has a minimum number of samples in its first\n        axis (rows for a 2D array). Setting to 0 disables this check.\n\n    ensure_min_features : int, default=1\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when the input data has effectively 2\n        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n        disables this check.\n\n    estimator : str or estimator instance, default=None\n        If passed, include the name of the estimator in warning messages.\n\n    Returns\n    -------\n    array_converted : object\n        The converted and validated array.\n    \"\"\"\n    if isinstance(array, np.matrix):\n        warnings.warn(\n            \"np.matrix usage is deprecated in 1.0 and will raise a TypeError \"\n            \"in 1.2. Please convert to a numpy array with np.asarray. For \"\n            \"more information see: \"\n            \"https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\",  # noqa\n            FutureWarning)\n\n    # store reference to original array to check if copy is needed when\n    # function returns\n    array_orig = array\n\n    # store whether originally we wanted numeric dtype\n    dtype_numeric = isinstance(dtype, str) and dtype == \"numeric\"\n\n    dtype_orig = getattr(array, \"dtype\", None)\n    if not hasattr(dtype_orig, 'kind'):\n        # not a data type (e.g. a column named dtype in a pandas DataFrame)\n        dtype_orig = None\n\n    # check if the object contains several dtypes (typically a pandas\n    # DataFrame), and store them. If not, store None.\n    dtypes_orig = None\n    has_pd_integer_array = False\n    if hasattr(array, \"dtypes\") and hasattr(array.dtypes, '__array__'):\n        # throw warning if columns are sparse. If all columns are sparse, then\n        # array.sparse exists and sparsity will be perserved (later).\n        with suppress(ImportError):\n            from pandas.api.types import is_sparse\n            if (not hasattr(array, 'sparse') and\n                    array.dtypes.apply(is_sparse).any()):\n                warnings.warn(\n                    \"pandas.DataFrame with sparse columns found.\"\n                    \"It will be converted to a dense numpy array.\"\n                )\n\n        dtypes_orig = list(array.dtypes)\n        # pandas boolean dtype __array__ interface coerces bools to objects\n        for i, dtype_iter in enumerate(dtypes_orig):\n            if dtype_iter.kind == 'b':\n                dtypes_orig[i] = np.dtype(object)\n            elif dtype_iter.name.startswith((\"Int\", \"UInt\")):\n                # name looks like an Integer Extension Array, now check for\n                # the dtype\n                with suppress(ImportError):\n                    from pandas import (Int8Dtype, Int16Dtype,\n                                        Int32Dtype, Int64Dtype,\n                                        UInt8Dtype, UInt16Dtype,\n                                        UInt32Dtype, UInt64Dtype)\n                    if isinstance(dtype_iter, (Int8Dtype, Int16Dtype,\n                                               Int32Dtype, Int64Dtype,\n                                               UInt8Dtype, UInt16Dtype,\n                                               UInt32Dtype, UInt64Dtype)):\n                        has_pd_integer_array = True\n\n        if all(isinstance(dtype, np.dtype) for dtype in dtypes_orig):\n            dtype_orig = np.result_type(*dtypes_orig)\n\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == \"O\":\n            # if input is object, convert to float.\n            dtype = np.float64\n        else:\n            dtype = None\n\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            # no dtype conversion required\n            dtype = None\n        else:\n            # dtype conversion required. Let's select the first element of the\n            # list of accepted types.\n            dtype = dtype[0]\n\n    if has_pd_integer_array:\n        # If there are any pandas integer extension arrays,\n        array = array.astype(dtype)\n\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\"'\n                         '. Got {!r} instead'.format(force_all_finite))\n\n    if estimator is not None:\n        if isinstance(estimator, str):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = \"Estimator\"\n    context = \" by %s\" % estimator_name if estimator is not None else \"\"\n\n    # When all dataframe columns are sparse, convert to a sparse array\n    if hasattr(array, 'sparse') and array.ndim > 1:\n        # DataFrame.sparse only supports `to_coo`\n        array = array.sparse.to_coo()\n        if array.dtype == np.dtype('object'):\n            unique_dtypes = set(\n                [dt.subtype.name for dt in array_orig.dtypes]\n            )\n            if len(unique_dtypes) > 1:\n                raise ValueError(\n                    \"Pandas DataFrame with mixed sparse extension arrays \"\n                    \"generated a sparse matrix with object dtype which \"\n                    \"can not be converted to a scipy sparse matrix.\"\n                    \"Sparse extension arrays should all have the same \"\n                    \"numeric type.\")\n\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n                                      dtype=dtype, copy=copy,\n                                      force_all_finite=force_all_finite,\n                                      accept_large_sparse=accept_large_sparse)\n    else:\n        # If np.array(..) gives ComplexWarning, then we convert the warning\n        # to an error. This is needed because specifying a non complex\n        # dtype to the function converts complex to real dtype,\n        # thereby passing the test made in the lines following the scope\n        # of warnings context manager.\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                if dtype is not None and np.dtype(dtype).kind in 'iu':\n                    # Conversion float -> int should not contain NaN or\n                    # inf (numpy#14412). We cannot use casting='safe' because\n                    # then conversion float -> int would be disallowed.\n                    array = np.asarray(array, order=order)\n                    if array.dtype.kind == 'f':\n                        _assert_all_finite(array, allow_nan=False,\n                                           msg_dtype=dtype)\n                    array = array.astype(dtype, casting=\"unsafe\", copy=False)\n                else:\n                    array = np.asarray(array, order=order, dtype=dtype)\n            except ComplexWarning as complex_warning:\n                raise ValueError(\"Complex data not supported\\n\"\n                                 \"{}\\n\".format(array)) from complex_warning\n\n        # It is possible that the np.array(..) gave no warning. This happens\n        # when no dtype conversion happened, for example dtype = None. The\n        # result is that np.array(..) produces an array of complex dtype\n        # and we need to catch and raise exception for such cases.\n        _ensure_no_complex_data(array)\n\n        if ensure_2d:\n            # If input is scalar raise error\n            if array.ndim == 0:\n                raise ValueError(\n                    \"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n            # If input is 1D raise error\n            if array.ndim == 1:\n                raise ValueError(\n                    \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n\n        # make sure we actually converted to numeric:\n        if dtype_numeric and array.dtype.kind in \"OUSV\":\n            warnings.warn(\n                \"Arrays of bytes/strings is being converted to decimal \"\n                \"numbers if dtype='numeric'. This behavior is deprecated in \"\n                \"0.24 and will be removed in 1.1 (renaming of 0.26). Please \"\n                \"convert your data to numeric values explicitly instead.\",\n                FutureWarning, stacklevel=2\n            )\n            try:\n                array = array.astype(np.float64)\n            except ValueError as e:\n                raise ValueError(\n                    \"Unable to convert array of bytes/strings \"\n                    \"into decimal numbers with dtype='numeric'\") from e\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n                             % (array.ndim, estimator_name))\n\n        if force_all_finite:\n            _assert_all_finite(array,\n                               allow_nan=force_all_finite == 'allow-nan')\n\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n                             \" minimum of %d is required%s.\"\n                             % (n_samples, array.shape, ensure_min_samples,\n                                context))\n\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError(\"Found array with %d feature(s) (shape=%s) while\"\n                             \" a minimum of %d is required%s.\"\n                             % (n_features, array.shape, ensure_min_features,\n                                context))\n\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n\n    return array\n\n\ndef _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\n    \"\"\"\n    if not accept_large_sparse:\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = ['col', 'row']\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if (indices_datatype not in supported_indices):\n                raise ValueError(\"Only sparse matrices with 32-bit integer\"\n                                 \" indices are accepted. Got %s indices.\"\n                                 % indices_datatype)\n\n\ndef check_X_y(X, y, accept_sparse=False, *, accept_large_sparse=True,\n              dtype=\"numeric\", order=None, copy=False, force_all_finite=True,\n              ensure_2d=True, allow_nd=False, multi_output=False,\n              ensure_min_samples=1, ensure_min_features=1, y_numeric=False,\n              estimator=None):\n    \"\"\"Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X to be 2D and y 1D. By\n    default, X is checked to be non-empty and containing only finite values.\n    Standard input checks are also applied to y, such as checking that y\n    does not have np.nan or np.inf targets. For multi-label y, set\n    multi_output=True to allow 2D and sparse y. If the dtype of X is\n    object, attempt converting to float, raising on failure.\n\n    Parameters\n    ----------\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    y : {ndarray, list, sparse matrix}\n        Labels.\n\n    accept_sparse : str, bool or list of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse will cause it to be accepted only\n        if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n\n    order : {'F', 'C'}, default=None\n        Whether an array will be forced to be fortran or c-style.\n\n    copy : bool, default=False\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in X. This parameter\n        does not influence whether y can have np.inf, np.nan, pd.NA values.\n        The possibilities are:\n\n        - True: Force all values of X to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in X.\n        - 'allow-nan': accepts only np.nan or pd.NA values in X. Values cannot\n          be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n    ensure_2d : bool, default=True\n        Whether to raise a value error if X is not 2D.\n\n    allow_nd : bool, default=False\n        Whether to allow X.ndim > 2.\n\n    multi_output : bool, default=False\n        Whether to allow 2D y (array or sparse matrix). If false, y will be\n        validated as a vector. y cannot have np.nan or np.inf values if\n        multi_output=True.\n\n    ensure_min_samples : int, default=1\n        Make sure that X has a minimum number of samples in its first\n        axis (rows for a 2D array).\n\n    ensure_min_features : int, default=1\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when X has effectively 2 dimensions or\n        is originally 1D and ``ensure_2d`` is True. Setting to 0 disables\n        this check.\n\n    y_numeric : bool, default=False\n        Whether to ensure that y has a numeric type. If dtype of y is object,\n        it is converted to float64. Should only be used for regression\n        algorithms.\n\n    estimator : str or estimator instance, default=None\n        If passed, include the name of the estimator in warning messages.\n\n    Returns\n    -------\n    X_converted : object\n        The converted and validated X.\n\n    y_converted : object\n        The converted and validated y.\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    X = check_array(X, accept_sparse=accept_sparse,\n                    accept_large_sparse=accept_large_sparse,\n                    dtype=dtype, order=order, copy=copy,\n                    force_all_finite=force_all_finite,\n                    ensure_2d=ensure_2d, allow_nd=allow_nd,\n                    ensure_min_samples=ensure_min_samples,\n                    ensure_min_features=ensure_min_features,\n                    estimator=estimator)\n    if multi_output:\n        y = check_array(y, accept_sparse='csr', force_all_finite=True,\n                        ensure_2d=False, dtype=None)\n    else:\n        y = column_or_1d(y, warn=True)\n        _assert_all_finite(y)\n    if y_numeric and y.dtype.kind == 'O':\n        y = y.astype(np.float64)\n\n    check_consistent_length(X, y)\n\n    return X, y\n\n\ndef column_or_1d(y, *, warn=False):\n    \"\"\" Ravel column or 1d numpy array, else raises an error.\n\n    Parameters\n    ----------\n    y : array-like\n\n    warn : bool, default=False\n       To control display of warnings.\n\n    Returns\n    -------\n    y : ndarray\n\n    \"\"\"\n    y = np.asarray(y)\n    shape = np.shape(y)\n    if len(shape) == 1:\n        return np.ravel(y)\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn(\"A column-vector y was passed when a 1d array was\"\n                          \" expected. Please change the shape of y to \"\n                          \"(n_samples, ), for example using ravel().\",\n                          DataConversionWarning, stacklevel=2)\n        return np.ravel(y)\n\n    raise ValueError(\n        \"y should be a 1d array, \"\n        \"got an array of shape {} instead.\".format(shape))\n\n\ndef check_random_state(seed):\n    \"\"\"Turn seed into a np.random.RandomState instance\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'\n                     ' instance' % seed)\n\n\ndef has_fit_parameter(estimator, parameter):\n    \"\"\"Checks whether the estimator's fit method supports the given parameter.\n\n    Parameters\n    ----------\n    estimator : object\n        An estimator to inspect.\n\n    parameter : str\n        The searched parameter.\n\n    Returns\n    -------\n    is_parameter: bool\n        Whether the parameter was found to be a named parameter of the\n        estimator's fit method.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> has_fit_parameter(SVC(), \"sample_weight\")\n    True\n\n    \"\"\"\n    return parameter in signature(estimator.fit).parameters\n\n\ndef check_symmetric(array, *, tol=1E-10, raise_warning=True,\n                    raise_exception=False):\n    \"\"\"Make sure that array is 2D, square and symmetric.\n\n    If the array is not symmetric, then a symmetrized version is returned.\n    Optionally, a warning or exception is raised if the matrix is not\n    symmetric.\n\n    Parameters\n    ----------\n    array : {ndarray, sparse matrix}\n        Input object to check / convert. Must be two-dimensional and square,\n        otherwise a ValueError will be raised.\n\n    tol : float, default=1e-10\n        Absolute tolerance for equivalence of arrays. Default = 1E-10.\n\n    raise_warning : bool, default=True\n        If True then raise a warning if conversion is required.\n\n    raise_exception : bool, default=False\n        If True then raise an exception if array is not symmetric.\n\n    Returns\n    -------\n    array_sym : {ndarray, sparse matrix}\n        Symmetrized version of the input array, i.e. the average of array\n        and array.transpose(). If sparse, then duplicate entries are first\n        summed and zeros are eliminated.\n    \"\"\"\n    if (array.ndim != 2) or (array.shape[0] != array.shape[1]):\n        raise ValueError(\"array must be 2-dimensional and square. \"\n                         \"shape = {0}\".format(array.shape))\n\n    if sp.issparse(array):\n        diff = array - array.T\n        # only csr, csc, and coo have `data` attribute\n        if diff.format not in ['csr', 'csc', 'coo']:\n            diff = diff.tocsr()\n        symmetric = np.all(abs(diff.data) < tol)\n    else:\n        symmetric = np.allclose(array, array.T, atol=tol)\n\n    if not symmetric:\n        if raise_exception:\n            raise ValueError(\"Array must be symmetric\")\n        if raise_warning:\n            warnings.warn(\"Array is not symmetric, and will be converted \"\n                          \"to symmetric by average with its transpose.\",\n                          stacklevel=2)\n        if sp.issparse(array):\n            conversion = 'to' + array.format\n            array = getattr(0.5 * (array + array.T), conversion)()\n        else:\n            array = 0.5 * (array + array.T)\n\n    return array\n\n\ndef check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a NotFittedError with the given message.\n\n    This utility is meant to be used internally by estimators themselves,\n    typically in their own predict / transform methods.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    NotFittedError\n        If the attributes are not found.\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\"This %(name)s instance is not fitted yet. Call 'fit' with \"\n               \"appropriate arguments before using this estimator.\")\n\n    if not hasattr(estimator, 'fit'):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    if attributes is not None:\n        if not isinstance(attributes, (list, tuple)):\n            attributes = [attributes]\n        attrs = all_or_any([hasattr(estimator, attr) for attr in attributes])\n    else:\n        attrs = [v for v in vars(estimator)\n                 if v.endswith(\"_\") and not v.startswith(\"__\")]\n\n    if not attrs:\n        raise NotFittedError(msg % {'name': type(estimator).__name__})\n\n\ndef check_non_negative(X, whom):\n    \"\"\"\n    Check if there is any negative value in an array.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Input data.\n\n    whom : str\n        Who passed X to this function.\n    \"\"\"\n    # avoid X.min() on sparse matrix since it also sorts the indices\n    if sp.issparse(X):\n        if X.format in ['lil', 'dok']:\n            X = X.tocsr()\n        if X.data.size == 0:\n            X_min = 0\n        else:\n            X_min = X.data.min()\n    else:\n        X_min = X.min()\n\n    if X_min < 0:\n        raise ValueError(\"Negative values in data passed to %s\" % whom)\n\n\ndef check_scalar(x, name, target_type, *, min_val=None, max_val=None):\n    \"\"\"Validate scalar parameters type and value.\n\n    Parameters\n    ----------\n    x : object\n        The scalar parameter to validate.\n\n    name : str\n        The name of the parameter to be printed in error messages.\n\n    target_type : type or tuple\n        Acceptable data types for the parameter.\n\n    min_val : float or int, default=None\n        The minimum valid value the parameter can take. If None (default) it\n        is implied that the parameter does not have a lower bound.\n\n    max_val : float or int, default=None\n        The maximum valid value the parameter can take. If None (default) it\n        is implied that the parameter does not have an upper bound.\n\n    Raises\n    -------\n    TypeError\n        If the parameter's type does not match the desired type.\n\n    ValueError\n        If the parameter's value violates the given bounds.\n    \"\"\"\n\n    if not isinstance(x, target_type):\n        raise TypeError('`{}` must be an instance of {}, not {}.'\n                        .format(name, target_type, type(x)))\n\n    if min_val is not None and x < min_val:\n        raise ValueError('`{}`= {}, must be >= {}.'.format(name, x, min_val))\n\n    if max_val is not None and x > max_val:\n        raise ValueError('`{}`= {}, must be <= {}.'.format(name, x, max_val))\n\n\ndef _check_psd_eigenvalues(lambdas, enable_warnings=False):\n    \"\"\"Check the eigenvalues of a positive semidefinite (PSD) matrix.\n\n    Checks the provided array of PSD matrix eigenvalues for numerical or\n    conditioning issues and returns a fixed validated version. This method\n    should typically be used if the PSD matrix is user-provided (e.g. a\n    Gram matrix) or computed using a user-provided dissimilarity metric\n    (e.g. kernel function), or if the decomposition process uses approximation\n    methods (randomized SVD, etc.).\n\n    It checks for three things:\n\n    - that there are no significant imaginary parts in eigenvalues (more than\n      1e-5 times the maximum real part). If this check fails, it raises a\n      ``ValueError``. Otherwise all non-significant imaginary parts that may\n      remain are set to zero. This operation is traced with a\n      ``PositiveSpectrumWarning`` when ``enable_warnings=True``.\n\n    - that eigenvalues are not all negative. If this check fails, it raises a\n      ``ValueError``\n\n    - that there are no significant negative eigenvalues with absolute value\n      more than 1e-10 (1e-6) and more than 1e-5 (5e-3) times the largest\n      positive eigenvalue in double (simple) precision. If this check fails,\n      it raises a ``ValueError``. Otherwise all negative eigenvalues that may\n      remain are set to zero. This operation is traced with a\n      ``PositiveSpectrumWarning`` when ``enable_warnings=True``.\n\n    Finally, all the positive eigenvalues that are too small (with a value\n    smaller than the maximum eigenvalue multiplied by 1e-12 (2e-7)) are set to\n    zero. This operation is traced with a ``PositiveSpectrumWarning`` when\n    ``enable_warnings=True``.\n\n    Parameters\n    ----------\n    lambdas : array-like of shape (n_eigenvalues,)\n        Array of eigenvalues to check / fix.\n\n    enable_warnings : bool, default=False\n        When this is set to ``True``, a ``PositiveSpectrumWarning`` will be\n        raised when there are imaginary parts, negative eigenvalues, or\n        extremely small non-zero eigenvalues. Otherwise no warning will be\n        raised. In both cases, imaginary parts, negative eigenvalues, and\n        extremely small non-zero eigenvalues will be set to zero.\n\n    Returns\n    -------\n    lambdas_fixed : ndarray of shape (n_eigenvalues,)\n        A fixed validated copy of the array of eigenvalues.\n\n    Examples\n    --------\n    >>> _check_psd_eigenvalues([1, 2])      # nominal case\n    array([1, 2])\n    >>> _check_psd_eigenvalues([5, 5j])     # significant imag part\n    Traceback (most recent call last):\n        ...\n    ValueError: There are significant imaginary parts in eigenvalues (1\n        of the maximum real part). Either the matrix is not PSD, or there was\n        an issue while computing the eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, 5e-5j])  # insignificant imag part\n    array([5., 0.])\n    >>> _check_psd_eigenvalues([-5, -1])    # all negative\n    Traceback (most recent call last):\n        ...\n    ValueError: All eigenvalues are negative (maximum is -1). Either the\n        matrix is not PSD, or there was an issue while computing the\n        eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, -1])     # significant negative\n    Traceback (most recent call last):\n        ...\n    ValueError: There are significant negative eigenvalues (0.2 of the\n        maximum positive). Either the matrix is not PSD, or there was an issue\n        while computing the eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, -5e-5])  # insignificant negative\n    array([5., 0.])\n    >>> _check_psd_eigenvalues([5, 4e-12])  # bad conditioning (too small)\n    array([5., 0.])\n\n    \"\"\"\n\n    lambdas = np.array(lambdas)\n    is_double_precision = lambdas.dtype == np.float64\n\n    # note: the minimum value available is\n    #  - single-precision: np.finfo('float32').eps = 1.2e-07\n    #  - double-precision: np.finfo('float64').eps = 2.2e-16\n\n    # the various thresholds used for validation\n    # we may wish to change the value according to precision.\n    significant_imag_ratio = 1e-5\n    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3\n    significant_neg_value = 1e-10 if is_double_precision else 1e-6\n    small_pos_ratio = 1e-12 if is_double_precision else 2e-7\n\n    # Check that there are no significant imaginary parts\n    if not np.isreal(lambdas).all():\n        max_imag_abs = np.abs(np.imag(lambdas)).max()\n        max_real_abs = np.abs(np.real(lambdas)).max()\n        if max_imag_abs > significant_imag_ratio * max_real_abs:\n            raise ValueError(\n                \"There are significant imaginary parts in eigenvalues (%g \"\n                \"of the maximum real part). Either the matrix is not PSD, or \"\n                \"there was an issue while computing the eigendecomposition \"\n                \"of the matrix.\"\n                % (max_imag_abs / max_real_abs))\n\n        # warn about imaginary parts being removed\n        if enable_warnings:\n            warnings.warn(\"There are imaginary parts in eigenvalues (%g \"\n                          \"of the maximum real part). Either the matrix is not\"\n                          \" PSD, or there was an issue while computing the \"\n                          \"eigendecomposition of the matrix. Only the real \"\n                          \"parts will be kept.\"\n                          % (max_imag_abs / max_real_abs),\n                          PositiveSpectrumWarning)\n\n    # Remove all imaginary parts (even if zero)\n    lambdas = np.real(lambdas)\n\n    # Check that there are no significant negative eigenvalues\n    max_eig = lambdas.max()\n    if max_eig < 0:\n        raise ValueError(\"All eigenvalues are negative (maximum is %g). \"\n                         \"Either the matrix is not PSD, or there was an \"\n                         \"issue while computing the eigendecomposition of \"\n                         \"the matrix.\" % max_eig)\n\n    else:\n        min_eig = lambdas.min()\n        if (min_eig < -significant_neg_ratio * max_eig\n                and min_eig < -significant_neg_value):\n            raise ValueError(\"There are significant negative eigenvalues (%g\"\n                             \" of the maximum positive). Either the matrix is \"\n                             \"not PSD, or there was an issue while computing \"\n                             \"the eigendecomposition of the matrix.\"\n                             % (-min_eig / max_eig))\n        elif min_eig < 0:\n            # Remove all negative values and warn about it\n            if enable_warnings:\n                warnings.warn(\"There are negative eigenvalues (%g of the \"\n                              \"maximum positive). Either the matrix is not \"\n                              \"PSD, or there was an issue while computing the\"\n                              \" eigendecomposition of the matrix. Negative \"\n                              \"eigenvalues will be replaced with 0.\"\n                              % (-min_eig / max_eig),\n                              PositiveSpectrumWarning)\n            lambdas[lambdas < 0] = 0\n\n    # Check for conditioning (small positive non-zeros)\n    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)\n    if too_small_lambdas.any():\n        if enable_warnings:\n            warnings.warn(\"Badly conditioned PSD matrix spectrum: the largest \"\n                          \"eigenvalue is more than %g times the smallest. \"\n                          \"Small eigenvalues will be replaced with 0.\"\n                          \"\" % (1 / small_pos_ratio),\n                          PositiveSpectrumWarning)\n        lambdas[too_small_lambdas] = 0\n\n    return lambdas\n\n\ndef _check_sample_weight(sample_weight, X, dtype=None, copy=False):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n       Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    dtype : dtype, default=None\n       dtype of the validated `sample_weight`.\n       If None, and the input `sample_weight` is an array, the dtype of the\n       input is preserved; otherwise an array with the default numpy dtype\n       is be allocated.  If `dtype` is not one of `float32`, `float64`,\n       `None`, the output will be of dtype `float64`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n       Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    n_samples = _num_samples(X)\n\n    if dtype is not None and dtype not in [np.float32, np.float64]:\n        dtype = np.float64\n\n    if sample_weight is None:\n        sample_weight = np.ones(n_samples, dtype=dtype)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = np.full(n_samples, sample_weight, dtype=dtype)\n    else:\n        if dtype is None:\n            dtype = [np.float64, np.float32]\n        sample_weight = check_array(\n            sample_weight, accept_sparse=False, ensure_2d=False, dtype=dtype,\n            order=\"C\", copy=copy\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\"Sample weights must be 1D array or scalar\")\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\"sample_weight.shape == {}, expected {}!\"\n                             .format(sample_weight.shape, (n_samples,)))\n\n    return sample_weight\n\n\ndef _allclose_dense_sparse(x, y, rtol=1e-7, atol=1e-9):\n    \"\"\"Check allclose for sparse and dense data.\n\n    Both x and y need to be either sparse or dense, they\n    can't be mixed.\n\n    Parameters\n    ----------\n    x : {array-like, sparse matrix}\n        First array to compare.\n\n    y : {array-like, sparse matrix}\n        Second array to compare.\n\n    rtol : float, default=1e-7\n        Relative tolerance; see numpy.allclose.\n\n    atol : float, default=1e-9\n        absolute tolerance; see numpy.allclose. Note that the default here is\n        more tolerant than the default for numpy.testing.assert_allclose, where\n        atol=0.\n    \"\"\"\n    if sp.issparse(x) and sp.issparse(y):\n        x = x.tocsr()\n        y = y.tocsr()\n        x.sum_duplicates()\n        y.sum_duplicates()\n        return (np.array_equal(x.indices, y.indices) and\n                np.array_equal(x.indptr, y.indptr) and\n                np.allclose(x.data, y.data, rtol=rtol, atol=atol))\n    elif not sp.issparse(x) and not sp.issparse(y):\n        return np.allclose(x, y, rtol=rtol, atol=atol)\n    raise ValueError(\"Can only compare two sparse matrices, not a sparse \"\n                     \"matrix and an array\")\n\n\ndef _check_fit_params(X, fit_params, indices=None):\n    \"\"\"Check and validate the parameters passed during `fit`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data array.\n\n    fit_params : dict\n        Dictionary containing the parameters passed at fit.\n\n    indices : array-like of shape (n_samples,), default=None\n        Indices to be selected if the parameter has the same size as `X`.\n\n    Returns\n    -------\n    fit_params_validated : dict\n        Validated parameters. We ensure that the values support indexing.\n    \"\"\"\n    from . import _safe_indexing\n    fit_params_validated = {}\n    for param_key, param_value in fit_params.items():\n        if (not _is_arraylike(param_value) or\n                _num_samples(param_value) != _num_samples(X)):\n            # Non-indexable pass-through (for now for backward-compatibility).\n            # https://github.com/scikit-learn/scikit-learn/issues/15805\n            fit_params_validated[param_key] = param_value\n        else:\n            # Any other fit_params should support indexing\n            # (e.g. for cross-validation).\n            fit_params_validated[param_key] = _make_indexable(param_value)\n            fit_params_validated[param_key] = _safe_indexing(\n                fit_params_validated[param_key], indices\n            )\n\n    return fit_params_validated\n",1462],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py":["\"\"\"Global configuration state and functions for management\n\"\"\"\nimport os\nfrom contextlib import contextmanager as contextmanager\nimport threading\n\n_global_config = {\n    'assume_finite': bool(os.environ.get('SKLEARN_ASSUME_FINITE', False)),\n    'working_memory': int(os.environ.get('SKLEARN_WORKING_MEMORY', 1024)),\n    'print_changed_only': True,\n    'display': 'text',\n}\n_threadlocal = threading.local()\n\n\ndef _get_threadlocal_config():\n    \"\"\"Get a threadlocal **mutable** configuration. If the configuration\n    does not exist, copy the default global configuration.\"\"\"\n    if not hasattr(_threadlocal, 'global_config'):\n        _threadlocal.global_config = _global_config.copy()\n    return _threadlocal.global_config\n\n\ndef get_config():\n    \"\"\"Retrieve current values for configuration set by :func:`set_config`\n\n    Returns\n    -------\n    config : dict\n        Keys are parameter names that can be passed to :func:`set_config`.\n\n    See Also\n    --------\n    config_context : Context manager for global scikit-learn configuration.\n    set_config : Set global scikit-learn configuration.\n    \"\"\"\n    # Return a copy of the threadlocal configuration so that users will\n    # not be able to modify the configuration with the returned dict.\n    return _get_threadlocal_config().copy()\n\n\ndef set_config(assume_finite=None, working_memory=None,\n               print_changed_only=None, display=None):\n    \"\"\"Set global scikit-learn configuration\n\n    .. versionadded:: 0.19\n\n    Parameters\n    ----------\n    assume_finite : bool, default=None\n        If True, validation for finiteness will be skipped,\n        saving time, but leading to potential crashes. If\n        False, validation for finiteness will be performed,\n        avoiding error.  Global default: False.\n\n        .. versionadded:: 0.19\n\n    working_memory : int, default=None\n        If set, scikit-learn will attempt to limit the size of temporary arrays\n        to this number of MiB (per job when parallelised), often saving both\n        computation time and memory on expensive operations that can be\n        performed in chunks. Global default: 1024.\n\n        .. versionadded:: 0.20\n\n    print_changed_only : bool, default=None\n        If True, only the parameters that were set to non-default\n        values will be printed when printing an estimator. For example,\n        ``print(SVC())`` while True will only print 'SVC()' while the default\n        behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n        all the non-changed parameters.\n\n        .. versionadded:: 0.21\n\n    display : {'text', 'diagram'}, default=None\n        If 'diagram', estimators will be displayed as a diagram in a Jupyter\n        lab or notebook context. If 'text', estimators will be displayed as\n        text. Default is 'text'.\n\n        .. versionadded:: 0.23\n\n    See Also\n    --------\n    config_context : Context manager for global scikit-learn configuration.\n    get_config : Retrieve current values of the global configuration.\n    \"\"\"\n    local_config = _get_threadlocal_config()\n\n    if assume_finite is not None:\n        local_config['assume_finite'] = assume_finite\n    if working_memory is not None:\n        local_config['working_memory'] = working_memory\n    if print_changed_only is not None:\n        local_config['print_changed_only'] = print_changed_only\n    if display is not None:\n        local_config['display'] = display\n\n\n@contextmanager\ndef config_context(**new_config):\n    \"\"\"Context manager for global scikit-learn configuration\n\n    Parameters\n    ----------\n    assume_finite : bool, default=False\n        If True, validation for finiteness will be skipped,\n        saving time, but leading to potential crashes. If\n        False, validation for finiteness will be performed,\n        avoiding error.  Global default: False.\n\n    working_memory : int, default=1024\n        If set, scikit-learn will attempt to limit the size of temporary arrays\n        to this number of MiB (per job when parallelised), often saving both\n        computation time and memory on expensive operations that can be\n        performed in chunks. Global default: 1024.\n\n    print_changed_only : bool, default=True\n        If True, only the parameters that were set to non-default\n        values will be printed when printing an estimator. For example,\n        ``print(SVC())`` while True will only print 'SVC()', but would print\n        'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n        when False. Default is True.\n\n        .. versionchanged:: 0.23\n           Default changed from False to True.\n\n    display : {'text', 'diagram'}, default='text'\n        If 'diagram', estimators will be displayed as a diagram in a Jupyter\n        lab or notebook context. If 'text', estimators will be displayed as\n        text. Default is 'text'.\n\n        .. versionadded:: 0.23\n\n    Notes\n    -----\n    All settings, not just those presently modified, will be returned to\n    their previous values when the context manager is exited.\n\n    Examples\n    --------\n    >>> import sklearn\n    >>> from sklearn.utils.validation import assert_all_finite\n    >>> with sklearn.config_context(assume_finite=True):\n    ...     assert_all_finite([float('nan')])\n    >>> with sklearn.config_context(assume_finite=True):\n    ...     with sklearn.config_context(assume_finite=False):\n    ...         assert_all_finite([float('nan')])\n    Traceback (most recent call last):\n    ...\n    ValueError: Input contains NaN, ...\n\n    See Also\n    --------\n    set_config : Set global scikit-learn configuration.\n    get_config : Retrieve current values of the global configuration.\n    \"\"\"\n    old_config = get_config()\n    set_config(**new_config)\n\n    try:\n        yield\n    finally:\n        set_config(**old_config)\n",163],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py":["\"\"\"\nnumerictypes: Define the numeric type objects\n\nThis module is designed so \"from numerictypes import \\\\*\" is safe.\nExported symbols include:\n\n  Dictionary with all registered number types (including aliases):\n    typeDict\n\n  Type objects (not all will be available, depends on platform):\n      see variable sctypes for which ones you have\n\n    Bit-width names\n\n    int8 int16 int32 int64 int128\n    uint8 uint16 uint32 uint64 uint128\n    float16 float32 float64 float96 float128 float256\n    complex32 complex64 complex128 complex192 complex256 complex512\n    datetime64 timedelta64\n\n    c-based names\n\n    bool_\n\n    object_\n\n    void, str_, unicode_\n\n    byte, ubyte,\n    short, ushort\n    intc, uintc,\n    intp, uintp,\n    int_, uint,\n    longlong, ulonglong,\n\n    single, csingle,\n    float_, complex_,\n    longfloat, clongfloat,\n\n   As part of the type-hierarchy:    xx -- is bit-width\n\n   generic\n     +-> bool_                                  (kind=b)\n     +-> number\n     |   +-> integer\n     |   |   +-> signedinteger     (intxx)      (kind=i)\n     |   |   |     byte\n     |   |   |     short\n     |   |   |     intc\n     |   |   |     intp            int0\n     |   |   |     int_\n     |   |   |     longlong\n     |   |   \\\\-> unsignedinteger  (uintxx)     (kind=u)\n     |   |         ubyte\n     |   |         ushort\n     |   |         uintc\n     |   |         uintp           uint0\n     |   |         uint_\n     |   |         ulonglong\n     |   +-> inexact\n     |       +-> floating          (floatxx)    (kind=f)\n     |       |     half\n     |       |     single\n     |       |     float_          (double)\n     |       |     longfloat\n     |       \\\\-> complexfloating  (complexxx)  (kind=c)\n     |             csingle         (singlecomplex)\n     |             complex_        (cfloat, cdouble)\n     |             clongfloat      (longcomplex)\n     +-> flexible\n     |   +-> character\n     |   |     str_     (string_, bytes_)       (kind=S)    [Python 2]\n     |   |     unicode_                         (kind=U)    [Python 2]\n     |   |\n     |   |     bytes_   (string_)               (kind=S)    [Python 3]\n     |   |     str_     (unicode_)              (kind=U)    [Python 3]\n     |   |\n     |   \\\\-> void                              (kind=V)\n     \\\\-> object_ (not used much)               (kind=O)\n\n\"\"\"\nimport types as _types\nimport numbers\nimport warnings\n\nfrom numpy.core.multiarray import (\n        typeinfo, ndarray, array, empty, dtype, datetime_data,\n        datetime_as_string, busday_offset, busday_count, is_busday,\n        busdaycalendar\n        )\nfrom numpy.core.overrides import set_module\n\n# we add more at the bottom\n__all__ = ['sctypeDict', 'typeDict', 'sctypes',\n           'ScalarType', 'obj2sctype', 'cast', 'nbytes', 'sctype2char',\n           'maximum_sctype', 'issctype', 'typecodes', 'find_common_type',\n           'issubdtype', 'datetime_data', 'datetime_as_string',\n           'busday_offset', 'busday_count', 'is_busday', 'busdaycalendar',\n           ]\n\n# we don't need all these imports, but we need to keep them for compatibility\n# for users using np.core.numerictypes.UPPER_TABLE\nfrom ._string_helpers import (\n    english_lower, english_upper, english_capitalize, LOWER_TABLE, UPPER_TABLE\n)\n\nfrom ._type_aliases import (\n    sctypeDict,\n    allTypes,\n    bitname,\n    sctypes,\n    _concrete_types,\n    _concrete_typeinfo,\n    _bits_of,\n)\nfrom ._dtype import _kind_name\n\n# we don't export these for import *, but we do want them accessible\n# as numerictypes.bool, etc.\nfrom builtins import bool, int, float, complex, object, str, bytes\nfrom numpy.compat import long, unicode\n\n\n# We use this later\ngeneric = allTypes['generic']\n\ngenericTypeRank = ['bool', 'int8', 'uint8', 'int16', 'uint16',\n                   'int32', 'uint32', 'int64', 'uint64', 'int128',\n                   'uint128', 'float16',\n                   'float32', 'float64', 'float80', 'float96', 'float128',\n                   'float256',\n                   'complex32', 'complex64', 'complex128', 'complex160',\n                   'complex192', 'complex256', 'complex512', 'object']\n\n@set_module('numpy')\ndef maximum_sctype(t):\n    \"\"\"\n    Return the scalar type of highest precision of the same kind as the input.\n\n    Parameters\n    ----------\n    t : dtype or dtype specifier\n        The input data type. This can be a `dtype` object or an object that\n        is convertible to a `dtype`.\n\n    Returns\n    -------\n    out : dtype\n        The highest precision data type of the same kind (`dtype.kind`) as `t`.\n\n    See Also\n    --------\n    obj2sctype, mintypecode, sctype2char\n    dtype\n\n    Examples\n    --------\n    >>> np.maximum_sctype(int)\n    <class 'numpy.int64'>\n    >>> np.maximum_sctype(np.uint8)\n    <class 'numpy.uint64'>\n    >>> np.maximum_sctype(complex)\n    <class 'numpy.complex256'> # may vary\n\n    >>> np.maximum_sctype(str)\n    <class 'numpy.str_'>\n\n    >>> np.maximum_sctype('i2')\n    <class 'numpy.int64'>\n    >>> np.maximum_sctype('f4')\n    <class 'numpy.float128'> # may vary\n\n    \"\"\"\n    g = obj2sctype(t)\n    if g is None:\n        return t\n    t = g\n    base = _kind_name(dtype(t))\n    if base in sctypes:\n        return sctypes[base][-1]\n    else:\n        return t\n\n\n@set_module('numpy')\ndef issctype(rep):\n    \"\"\"\n    Determines whether the given object represents a scalar data-type.\n\n    Parameters\n    ----------\n    rep : any\n        If `rep` is an instance of a scalar dtype, True is returned. If not,\n        False is returned.\n\n    Returns\n    -------\n    out : bool\n        Boolean result of check whether `rep` is a scalar dtype.\n\n    See Also\n    --------\n    issubsctype, issubdtype, obj2sctype, sctype2char\n\n    Examples\n    --------\n    >>> np.issctype(np.int32)\n    True\n    >>> np.issctype(list)\n    False\n    >>> np.issctype(1.1)\n    False\n\n    Strings are also a scalar type:\n\n    >>> np.issctype(np.dtype('str'))\n    True\n\n    \"\"\"\n    if not isinstance(rep, (type, dtype)):\n        return False\n    try:\n        res = obj2sctype(rep)\n        if res and res != object_:\n            return True\n        return False\n    except Exception:\n        return False\n\n\n@set_module('numpy')\ndef obj2sctype(rep, default=None):\n    \"\"\"\n    Return the scalar dtype or NumPy equivalent of Python type of an object.\n\n    Parameters\n    ----------\n    rep : any\n        The object of which the type is returned.\n    default : any, optional\n        If given, this is returned for objects whose types can not be\n        determined. If not given, None is returned for those objects.\n\n    Returns\n    -------\n    dtype : dtype or Python type\n        The data type of `rep`.\n\n    See Also\n    --------\n    sctype2char, issctype, issubsctype, issubdtype, maximum_sctype\n\n    Examples\n    --------\n    >>> np.obj2sctype(np.int32)\n    <class 'numpy.int32'>\n    >>> np.obj2sctype(np.array([1., 2.]))\n    <class 'numpy.float64'>\n    >>> np.obj2sctype(np.array([1.j]))\n    <class 'numpy.complex128'>\n\n    >>> np.obj2sctype(dict)\n    <class 'numpy.object_'>\n    >>> np.obj2sctype('string')\n\n    >>> np.obj2sctype(1, default=list)\n    <class 'list'>\n\n    \"\"\"\n    # prevent abstract classes being upcast\n    if isinstance(rep, type) and issubclass(rep, generic):\n        return rep\n    # extract dtype from arrays\n    if isinstance(rep, ndarray):\n        return rep.dtype.type\n    # fall back on dtype to convert\n    try:\n        res = dtype(rep)\n    except Exception:\n        return default\n    else:\n        return res.type\n\n\n@set_module('numpy')\ndef issubclass_(arg1, arg2):\n    \"\"\"\n    Determine if a class is a subclass of a second class.\n\n    `issubclass_` is equivalent to the Python built-in ``issubclass``,\n    except that it returns False instead of raising a TypeError if one\n    of the arguments is not a class.\n\n    Parameters\n    ----------\n    arg1 : class\n        Input class. True is returned if `arg1` is a subclass of `arg2`.\n    arg2 : class or tuple of classes.\n        Input class. If a tuple of classes, True is returned if `arg1` is a\n        subclass of any of the tuple elements.\n\n    Returns\n    -------\n    out : bool\n        Whether `arg1` is a subclass of `arg2` or not.\n\n    See Also\n    --------\n    issubsctype, issubdtype, issctype\n\n    Examples\n    --------\n    >>> np.issubclass_(np.int32, int)\n    False\n    >>> np.issubclass_(np.int32, float)\n    False\n    >>> np.issubclass_(np.float64, float)\n    True\n\n    \"\"\"\n    try:\n        return issubclass(arg1, arg2)\n    except TypeError:\n        return False\n\n\n@set_module('numpy')\ndef issubsctype(arg1, arg2):\n    \"\"\"\n    Determine if the first argument is a subclass of the second argument.\n\n    Parameters\n    ----------\n    arg1, arg2 : dtype or dtype specifier\n        Data-types.\n\n    Returns\n    -------\n    out : bool\n        The result.\n\n    See Also\n    --------\n    issctype, issubdtype, obj2sctype\n\n    Examples\n    --------\n    >>> np.issubsctype('S8', str)\n    False\n    >>> np.issubsctype(np.array([1]), int)\n    True\n    >>> np.issubsctype(np.array([1]), float)\n    False\n\n    \"\"\"\n    return issubclass(obj2sctype(arg1), obj2sctype(arg2))\n\n\n@set_module('numpy')\ndef issubdtype(arg1, arg2):\n    r\"\"\"\n    Returns True if first argument is a typecode lower/equal in type hierarchy.\n\n    This is like the builtin :func:`issubclass`, but for `dtype`\\ s.\n\n    Parameters\n    ----------\n    arg1, arg2 : dtype_like\n        `dtype` or object coercible to one\n\n    Returns\n    -------\n    out : bool\n\n    See Also\n    --------\n    :ref:`arrays.scalars` : Overview of the numpy type hierarchy.\n    issubsctype, issubclass_\n\n    Examples\n    --------\n    `issubdtype` can be used to check the type of arrays:\n\n    >>> ints = np.array([1, 2, 3], dtype=np.int32)\n    >>> np.issubdtype(ints.dtype, np.integer)\n    True\n    >>> np.issubdtype(ints.dtype, np.floating)\n    False\n\n    >>> floats = np.array([1, 2, 3], dtype=np.float32)\n    >>> np.issubdtype(floats.dtype, np.integer)\n    False\n    >>> np.issubdtype(floats.dtype, np.floating)\n    True\n\n    Similar types of different sizes are not subdtypes of each other:\n\n    >>> np.issubdtype(np.float64, np.float32)\n    False\n    >>> np.issubdtype(np.float32, np.float64)\n    False\n\n    but both are subtypes of `floating`:\n\n    >>> np.issubdtype(np.float64, np.floating)\n    True\n    >>> np.issubdtype(np.float32, np.floating)\n    True\n\n    For convenience, dtype-like objects are allowed too:\n\n    >>> np.issubdtype('S1', np.string_)\n    True\n    >>> np.issubdtype('i4', np.signedinteger)\n    True\n\n    \"\"\"\n    if not issubclass_(arg1, generic):\n        arg1 = dtype(arg1).type\n    if not issubclass_(arg2, generic):\n        arg2 = dtype(arg2).type\n\n    return issubclass(arg1, arg2)\n\n\n# This dictionary allows look up based on any alias for an array data-type\nclass _typedict(dict):\n    \"\"\"\n    Base object for a dictionary for look-up with any alias for an array dtype.\n\n    Instances of `_typedict` can not be used as dictionaries directly,\n    first they have to be populated.\n\n    \"\"\"\n\n    def __getitem__(self, obj):\n        return dict.__getitem__(self, obj2sctype(obj))\n\nnbytes = _typedict()\n_alignment = _typedict()\n_maxvals = _typedict()\n_minvals = _typedict()\ndef _construct_lookups():\n    for name, info in _concrete_typeinfo.items():\n        obj = info.type\n        nbytes[obj] = info.bits // 8\n        _alignment[obj] = info.alignment\n        if len(info) > 5:\n            _maxvals[obj] = info.max\n            _minvals[obj] = info.min\n        else:\n            _maxvals[obj] = None\n            _minvals[obj] = None\n\n_construct_lookups()\n\n\n@set_module('numpy')\ndef sctype2char(sctype):\n    \"\"\"\n    Return the string representation of a scalar dtype.\n\n    Parameters\n    ----------\n    sctype : scalar dtype or object\n        If a scalar dtype, the corresponding string character is\n        returned. If an object, `sctype2char` tries to infer its scalar type\n        and then return the corresponding string character.\n\n    Returns\n    -------\n    typechar : str\n        The string character corresponding to the scalar type.\n\n    Raises\n    ------\n    ValueError\n        If `sctype` is an object for which the type can not be inferred.\n\n    See Also\n    --------\n    obj2sctype, issctype, issubsctype, mintypecode\n\n    Examples\n    --------\n    >>> for sctype in [np.int32, np.double, np.complex_, np.string_, np.ndarray]:\n    ...     print(np.sctype2char(sctype))\n    l # may vary\n    d\n    D\n    S\n    O\n\n    >>> x = np.array([1., 2-1.j])\n    >>> np.sctype2char(x)\n    'D'\n    >>> np.sctype2char(list)\n    'O'\n\n    \"\"\"\n    sctype = obj2sctype(sctype)\n    if sctype is None:\n        raise ValueError(\"unrecognized type\")\n    if sctype not in _concrete_types:\n        # for compatibility\n        raise KeyError(sctype)\n    return dtype(sctype).char\n\n# Create dictionary of casting functions that wrap sequences\n# indexed by type or type character\ncast = _typedict()\nfor key in _concrete_types:\n    cast[key] = lambda x, k=key: array(x, copy=False).astype(k)\n\ntry:\n    ScalarType = [_types.IntType, _types.FloatType, _types.ComplexType,\n                  _types.LongType, _types.BooleanType,\n                   _types.StringType, _types.UnicodeType, _types.BufferType]\nexcept AttributeError:\n    # Py3K\n    ScalarType = [int, float, complex, int, bool, bytes, str, memoryview]\n\nScalarType.extend(_concrete_types)\nScalarType = tuple(ScalarType)\n\n\n# Now add the types we've determined to this module\nfor key in allTypes:\n    globals()[key] = allTypes[key]\n    __all__.append(key)\n\ndel key\n\ntypecodes = {'Character':'c',\n             'Integer':'bhilqp',\n             'UnsignedInteger':'BHILQP',\n             'Float':'efdg',\n             'Complex':'FDG',\n             'AllInteger':'bBhHiIlLqQpP',\n             'AllFloat':'efdgFDG',\n             'Datetime': 'Mm',\n             'All':'?bhilqpBHILQPefdgFDGSUVOMm'}\n\n# backwards compatibility --- deprecated name\ntypeDict = sctypeDict\n\n# b -> boolean\n# u -> unsigned integer\n# i -> signed integer\n# f -> floating point\n# c -> complex\n# M -> datetime\n# m -> timedelta\n# S -> string\n# U -> Unicode string\n# V -> record\n# O -> Python object\n_kind_list = ['b', 'u', 'i', 'f', 'c', 'S', 'U', 'V', 'O', 'M', 'm']\n\n__test_types = '?'+typecodes['AllInteger'][:-2]+typecodes['AllFloat']+'O'\n__len_test_types = len(__test_types)\n\n# Keep incrementing until a common type both can be coerced to\n#  is found.  Otherwise, return None\ndef _find_common_coerce(a, b):\n    if a > b:\n        return a\n    try:\n        thisind = __test_types.index(a.char)\n    except ValueError:\n        return None\n    return _can_coerce_all([a, b], start=thisind)\n\n# Find a data-type that all data-types in a list can be coerced to\ndef _can_coerce_all(dtypelist, start=0):\n    N = len(dtypelist)\n    if N == 0:\n        return None\n    if N == 1:\n        return dtypelist[0]\n    thisind = start\n    while thisind < __len_test_types:\n        newdtype = dtype(__test_types[thisind])\n        numcoerce = len([x for x in dtypelist if newdtype >= x])\n        if numcoerce == N:\n            return newdtype\n        thisind += 1\n    return None\n\ndef _register_types():\n    numbers.Integral.register(integer)\n    numbers.Complex.register(inexact)\n    numbers.Real.register(floating)\n    numbers.Number.register(number)\n\n_register_types()\n\n\n@set_module('numpy')\ndef find_common_type(array_types, scalar_types):\n    \"\"\"\n    Determine common type following standard coercion rules.\n\n    Parameters\n    ----------\n    array_types : sequence\n        A list of dtypes or dtype convertible objects representing arrays.\n    scalar_types : sequence\n        A list of dtypes or dtype convertible objects representing scalars.\n\n    Returns\n    -------\n    datatype : dtype\n        The common data type, which is the maximum of `array_types` ignoring\n        `scalar_types`, unless the maximum of `scalar_types` is of a\n        different kind (`dtype.kind`). If the kind is not understood, then\n        None is returned.\n\n    See Also\n    --------\n    dtype, common_type, can_cast, mintypecode\n\n    Examples\n    --------\n    >>> np.find_common_type([], [np.int64, np.float32, complex])\n    dtype('complex128')\n    >>> np.find_common_type([np.int64, np.float32], [])\n    dtype('float64')\n\n    The standard casting rules ensure that a scalar cannot up-cast an\n    array unless the scalar is of a fundamentally different kind of data\n    (i.e. under a different hierarchy in the data type hierarchy) then\n    the array:\n\n    >>> np.find_common_type([np.float32], [np.int64, np.float64])\n    dtype('float32')\n\n    Complex is of a different type, so it up-casts the float in the\n    `array_types` argument:\n\n    >>> np.find_common_type([np.float32], [complex])\n    dtype('complex128')\n\n    Type specifier strings are convertible to dtypes and can therefore\n    be used instead of dtypes:\n\n    >>> np.find_common_type(['f4', 'f4', 'i4'], ['c8'])\n    dtype('complex128')\n\n    \"\"\"\n    array_types = [dtype(x) for x in array_types]\n    scalar_types = [dtype(x) for x in scalar_types]\n\n    maxa = _can_coerce_all(array_types)\n    maxsc = _can_coerce_all(scalar_types)\n\n    if maxa is None:\n        return maxsc\n\n    if maxsc is None:\n        return maxa\n\n    try:\n        index_a = _kind_list.index(maxa.kind)\n        index_sc = _kind_list.index(maxsc.kind)\n    except ValueError:\n        return None\n\n    if index_sc > index_a:\n        return _find_common_coerce(maxsc, maxa)\n    else:\n        return maxa\n",672],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py":["\"\"\"Module containing non-deprecated functions borrowed from Numeric.\n\n\"\"\"\nimport functools\nimport types\nimport warnings\n\nimport numpy as np\nfrom . import multiarray as mu\nfrom . import overrides\nfrom . import umath as um\nfrom . import numerictypes as nt\nfrom ._asarray import asarray, array, asanyarray\nfrom .multiarray import concatenate\nfrom . import _methods\n\n_dt_ = nt.sctype2char\n\n# functions that are methods\n__all__ = [\n    'alen', 'all', 'alltrue', 'amax', 'amin', 'any', 'argmax',\n    'argmin', 'argpartition', 'argsort', 'around', 'choose', 'clip',\n    'compress', 'cumprod', 'cumproduct', 'cumsum', 'diagonal', 'mean',\n    'ndim', 'nonzero', 'partition', 'prod', 'product', 'ptp', 'put',\n    'ravel', 'repeat', 'reshape', 'resize', 'round_',\n    'searchsorted', 'shape', 'size', 'sometrue', 'sort', 'squeeze',\n    'std', 'sum', 'swapaxes', 'take', 'trace', 'transpose', 'var',\n]\n\n_gentype = types.GeneratorType\n# save away Python sum\n_sum_ = sum\n\narray_function_dispatch = functools.partial(\n    overrides.array_function_dispatch, module='numpy')\n\n\n# functions that are now methods\ndef _wrapit(obj, method, *args, **kwds):\n    try:\n        wrap = obj.__array_wrap__\n    except AttributeError:\n        wrap = None\n    result = getattr(asarray(obj), method)(*args, **kwds)\n    if wrap:\n        if not isinstance(result, mu.ndarray):\n            result = asarray(result)\n        result = wrap(result)\n    return result\n\n\ndef _wrapfunc(obj, method, *args, **kwds):\n    bound = getattr(obj, method, None)\n    if bound is None:\n        return _wrapit(obj, method, *args, **kwds)\n\n    try:\n        return bound(*args, **kwds)\n    except TypeError:\n        # A TypeError occurs if the object does have such a method in its\n        # class, but its signature is not identical to that of NumPy's. This\n        # situation has occurred in the case of a downstream library like\n        # 'pandas'.\n        #\n        # Call _wrapit from within the except clause to ensure a potential\n        # exception has a traceback chain.\n        return _wrapit(obj, method, *args, **kwds)\n\n\ndef _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs):\n    passkwargs = {k: v for k, v in kwargs.items()\n                  if v is not np._NoValue}\n\n    if type(obj) is not mu.ndarray:\n        try:\n            reduction = getattr(obj, method)\n        except AttributeError:\n            pass\n        else:\n            # This branch is needed for reductions like any which don't\n            # support a dtype.\n            if dtype is not None:\n                return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\n            else:\n                return reduction(axis=axis, out=out, **passkwargs)\n\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n\n\ndef _take_dispatcher(a, indices, axis=None, out=None, mode=None):\n    return (a, out)\n\n\n@array_function_dispatch(_take_dispatcher)\ndef take(a, indices, axis=None, out=None, mode='raise'):\n    \"\"\"\n    Take elements from an array along an axis.\n\n    When axis is not None, this function does the same thing as \"fancy\"\n    indexing (indexing arrays using arrays); however, it can be easier to use\n    if you need elements along a given axis. A call such as\n    ``np.take(arr, indices, axis=3)`` is equivalent to\n    ``arr[:,:,:,indices,...]``.\n\n    Explained without fancy indexing, this is equivalent to the following use\n    of `ndindex`, which sets each of ``ii``, ``jj``, and ``kk`` to a tuple of\n    indices::\n\n        Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n        Nj = indices.shape\n        for ii in ndindex(Ni):\n            for jj in ndindex(Nj):\n                for kk in ndindex(Nk):\n                    out[ii + jj + kk] = a[ii + (indices[jj],) + kk]\n\n    Parameters\n    ----------\n    a : array_like (Ni..., M, Nk...)\n        The source array.\n    indices : array_like (Nj...)\n        The indices of the values to extract.\n\n        .. versionadded:: 1.8.0\n\n        Also allow scalars for indices.\n    axis : int, optional\n        The axis over which to select values. By default, the flattened\n        input array is used.\n    out : ndarray, optional (Ni..., Nj..., Nk...)\n        If provided, the result will be placed in this array. It should\n        be of the appropriate shape and dtype. Note that `out` is always\n        buffered if `mode='raise'`; use other modes for better performance.\n    mode : {'raise', 'wrap', 'clip'}, optional\n        Specifies how out-of-bounds indices will behave.\n\n        * 'raise' -- raise an error (default)\n        * 'wrap' -- wrap around\n        * 'clip' -- clip to the range\n\n        'clip' mode means that all indices that are too large are replaced\n        by the index that addresses the last element along that axis. Note\n        that this disables indexing with negative numbers.\n\n    Returns\n    -------\n    out : ndarray (Ni..., Nj..., Nk...)\n        The returned array has the same type as `a`.\n\n    See Also\n    --------\n    compress : Take elements using a boolean mask\n    ndarray.take : equivalent method\n    take_along_axis : Take elements by matching the array and the index arrays\n\n    Notes\n    -----\n\n    By eliminating the inner loop in the description above, and using `s_` to\n    build simple slice objects, `take` can be expressed  in terms of applying\n    fancy indexing to each 1-d slice::\n\n        Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n        for ii in ndindex(Ni):\n            for kk in ndindex(Nj):\n                out[ii + s_[...,] + kk] = a[ii + s_[:,] + kk][indices]\n\n    For this reason, it is equivalent to (but faster than) the following use\n    of `apply_along_axis`::\n\n        out = np.apply_along_axis(lambda a_1d: a_1d[indices], axis, a)\n\n    Examples\n    --------\n    >>> a = [4, 3, 5, 7, 6, 8]\n    >>> indices = [0, 1, 4]\n    >>> np.take(a, indices)\n    array([4, 3, 6])\n\n    In this example if `a` is an ndarray, \"fancy\" indexing can be used.\n\n    >>> a = np.array(a)\n    >>> a[indices]\n    array([4, 3, 6])\n\n    If `indices` is not one dimensional, the output also has these dimensions.\n\n    >>> np.take(a, [[0, 1], [2, 3]])\n    array([[4, 3],\n           [5, 7]])\n    \"\"\"\n    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)\n\n\ndef _reshape_dispatcher(a, newshape, order=None):\n    return (a,)\n\n\n# not deprecated --- copy if necessary, view otherwise\n@array_function_dispatch(_reshape_dispatcher)\ndef reshape(a, newshape, order='C'):\n    \"\"\"\n    Gives a new shape to an array without changing its data.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be reshaped.\n    newshape : int or tuple of ints\n        The new shape should be compatible with the original shape. If\n        an integer, then the result will be a 1-D array of that length.\n        One shape dimension can be -1. In this case, the value is\n        inferred from the length of the array and remaining dimensions.\n    order : {'C', 'F', 'A'}, optional\n        Read the elements of `a` using this index order, and place the\n        elements into the reshaped array using this index order.  'C'\n        means to read / write the elements using C-like index order,\n        with the last axis index changing fastest, back to the first\n        axis index changing slowest. 'F' means to read / write the\n        elements using Fortran-like index order, with the first index\n        changing fastest, and the last index changing slowest. Note that\n        the 'C' and 'F' options take no account of the memory layout of\n        the underlying array, and only refer to the order of indexing.\n        'A' means to read / write the elements in Fortran-like index\n        order if `a` is Fortran *contiguous* in memory, C-like order\n        otherwise.\n\n    Returns\n    -------\n    reshaped_array : ndarray\n        This will be a new view object if possible; otherwise, it will\n        be a copy.  Note there is no guarantee of the *memory layout* (C- or\n        Fortran- contiguous) of the returned array.\n\n    See Also\n    --------\n    ndarray.reshape : Equivalent method.\n\n    Notes\n    -----\n    It is not always possible to change the shape of an array without\n    copying the data. If you want an error to be raised when the data is copied,\n    you should assign the new shape to the shape attribute of the array::\n\n     >>> a = np.zeros((10, 2))\n\n     # A transpose makes the array non-contiguous\n     >>> b = a.T\n\n     # Taking a view makes it possible to modify the shape without modifying\n     # the initial object.\n     >>> c = b.view()\n     >>> c.shape = (20)\n     Traceback (most recent call last):\n        ...\n     AttributeError: Incompatible shape for in-place modification. Use\n     `.reshape()` to make a copy with the desired shape.\n\n    The `order` keyword gives the index ordering both for *fetching* the values\n    from `a`, and then *placing* the values into the output array.\n    For example, let's say you have an array:\n\n    >>> a = np.arange(6).reshape((3, 2))\n    >>> a\n    array([[0, 1],\n           [2, 3],\n           [4, 5]])\n\n    You can think of reshaping as first raveling the array (using the given\n    index order), then inserting the elements from the raveled array into the\n    new array using the same kind of index ordering as was used for the\n    raveling.\n\n    >>> np.reshape(a, (2, 3)) # C-like index ordering\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.reshape(np.ravel(a), (2, 3)) # equivalent to C ravel then C reshape\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.reshape(a, (2, 3), order='F') # Fortran-like index ordering\n    array([[0, 4, 3],\n           [2, 1, 5]])\n    >>> np.reshape(np.ravel(a, order='F'), (2, 3), order='F')\n    array([[0, 4, 3],\n           [2, 1, 5]])\n\n    Examples\n    --------\n    >>> a = np.array([[1,2,3], [4,5,6]])\n    >>> np.reshape(a, 6)\n    array([1, 2, 3, 4, 5, 6])\n    >>> np.reshape(a, 6, order='F')\n    array([1, 4, 2, 5, 3, 6])\n\n    >>> np.reshape(a, (3,-1))       # the unspecified value is inferred to be 2\n    array([[1, 2],\n           [3, 4],\n           [5, 6]])\n    \"\"\"\n    return _wrapfunc(a, 'reshape', newshape, order=order)\n\n\ndef _choose_dispatcher(a, choices, out=None, mode=None):\n    yield a\n    yield from choices\n    yield out\n\n\n@array_function_dispatch(_choose_dispatcher)\ndef choose(a, choices, out=None, mode='raise'):\n    \"\"\"\n    Construct an array from an index array and a set of arrays to choose from.\n\n    First of all, if confused or uncertain, definitely look at the Examples -\n    in its full generality, this function is less simple than it might\n    seem from the following code description (below ndi =\n    `numpy.lib.index_tricks`):\n\n    ``np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)])``.\n\n    But this omits some subtleties.  Here is a fully general summary:\n\n    Given an \"index\" array (`a`) of integers and a sequence of `n` arrays\n    (`choices`), `a` and each choice array are first broadcast, as necessary,\n    to arrays of a common shape; calling these *Ba* and *Bchoices[i], i =\n    0,...,n-1* we have that, necessarily, ``Ba.shape == Bchoices[i].shape``\n    for each `i`.  Then, a new array with shape ``Ba.shape`` is created as\n    follows:\n\n    * if ``mode=raise`` (the default), then, first of all, each element of\n      `a` (and thus `Ba`) must be in the range `[0, n-1]`; now, suppose that\n      `i` (in that range) is the value at the `(j0, j1, ..., jm)` position\n      in `Ba` - then the value at the same position in the new array is the\n      value in `Bchoices[i]` at that same position;\n\n    * if ``mode=wrap``, values in `a` (and thus `Ba`) may be any (signed)\n      integer; modular arithmetic is used to map integers outside the range\n      `[0, n-1]` back into that range; and then the new array is constructed\n      as above;\n\n    * if ``mode=clip``, values in `a` (and thus `Ba`) may be any (signed)\n      integer; negative integers are mapped to 0; values greater than `n-1`\n      are mapped to `n-1`; and then the new array is constructed as above.\n\n    Parameters\n    ----------\n    a : int array\n        This array must contain integers in `[0, n-1]`, where `n` is the number\n        of choices, unless ``mode=wrap`` or ``mode=clip``, in which cases any\n        integers are permissible.\n    choices : sequence of arrays\n        Choice arrays. `a` and all of the choices must be broadcastable to the\n        same shape.  If `choices` is itself an array (not recommended), then\n        its outermost dimension (i.e., the one corresponding to\n        ``choices.shape[0]``) is taken as defining the \"sequence\".\n    out : array, optional\n        If provided, the result will be inserted into this array. It should\n        be of the appropriate shape and dtype. Note that `out` is always\n        buffered if `mode='raise'`; use other modes for better performance.\n    mode : {'raise' (default), 'wrap', 'clip'}, optional\n        Specifies how indices outside `[0, n-1]` will be treated:\n\n          * 'raise' : an exception is raised\n          * 'wrap' : value becomes value mod `n`\n          * 'clip' : values < 0 are mapped to 0, values > n-1 are mapped to n-1\n\n    Returns\n    -------\n    merged_array : array\n        The merged result.\n\n    Raises\n    ------\n    ValueError: shape mismatch\n        If `a` and each choice array are not all broadcastable to the same\n        shape.\n\n    See Also\n    --------\n    ndarray.choose : equivalent method\n    numpy.take_along_axis : Preferable if `choices` is an array\n\n    Notes\n    -----\n    To reduce the chance of misinterpretation, even though the following\n    \"abuse\" is nominally supported, `choices` should neither be, nor be\n    thought of as, a single array, i.e., the outermost sequence-like container\n    should be either a list or a tuple.\n\n    Examples\n    --------\n\n    >>> choices = [[0, 1, 2, 3], [10, 11, 12, 13],\n    ...   [20, 21, 22, 23], [30, 31, 32, 33]]\n    >>> np.choose([2, 3, 1, 0], choices\n    ... # the first element of the result will be the first element of the\n    ... # third (2+1) \"array\" in choices, namely, 20; the second element\n    ... # will be the second element of the fourth (3+1) choice array, i.e.,\n    ... # 31, etc.\n    ... )\n    array([20, 31, 12,  3])\n    >>> np.choose([2, 4, 1, 0], choices, mode='clip') # 4 goes to 3 (4-1)\n    array([20, 31, 12,  3])\n    >>> # because there are 4 choice arrays\n    >>> np.choose([2, 4, 1, 0], choices, mode='wrap') # 4 goes to (4 mod 4)\n    array([20,  1, 12,  3])\n    >>> # i.e., 0\n\n    A couple examples illustrating how choose broadcasts:\n\n    >>> a = [[1, 0, 1], [0, 1, 0], [1, 0, 1]]\n    >>> choices = [-10, 10]\n    >>> np.choose(a, choices)\n    array([[ 10, -10,  10],\n           [-10,  10, -10],\n           [ 10, -10,  10]])\n\n    >>> # With thanks to Anne Archibald\n    >>> a = np.array([0, 1]).reshape((2,1,1))\n    >>> c1 = np.array([1, 2, 3]).reshape((1,3,1))\n    >>> c2 = np.array([-1, -2, -3, -4, -5]).reshape((1,1,5))\n    >>> np.choose(a, (c1, c2)) # result is 2x3x5, res[0,:,:]=c1, res[1,:,:]=c2\n    array([[[ 1,  1,  1,  1,  1],\n            [ 2,  2,  2,  2,  2],\n            [ 3,  3,  3,  3,  3]],\n           [[-1, -2, -3, -4, -5],\n            [-1, -2, -3, -4, -5],\n            [-1, -2, -3, -4, -5]]])\n\n    \"\"\"\n    return _wrapfunc(a, 'choose', choices, out=out, mode=mode)\n\n\ndef _repeat_dispatcher(a, repeats, axis=None):\n    return (a,)\n\n\n@array_function_dispatch(_repeat_dispatcher)\ndef repeat(a, repeats, axis=None):\n    \"\"\"\n    Repeat elements of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    repeats : int or array of ints\n        The number of repetitions for each element.  `repeats` is broadcasted\n        to fit the shape of the given axis.\n    axis : int, optional\n        The axis along which to repeat values.  By default, use the\n        flattened input array, and return a flat output array.\n\n    Returns\n    -------\n    repeated_array : ndarray\n        Output array which has the same shape as `a`, except along\n        the given axis.\n\n    See Also\n    --------\n    tile : Tile an array.\n    unique : Find the unique elements of an array.\n\n    Examples\n    --------\n    >>> np.repeat(3, 4)\n    array([3, 3, 3, 3])\n    >>> x = np.array([[1,2],[3,4]])\n    >>> np.repeat(x, 2)\n    array([1, 1, 2, 2, 3, 3, 4, 4])\n    >>> np.repeat(x, 3, axis=1)\n    array([[1, 1, 1, 2, 2, 2],\n           [3, 3, 3, 4, 4, 4]])\n    >>> np.repeat(x, [1, 2], axis=0)\n    array([[1, 2],\n           [3, 4],\n           [3, 4]])\n\n    \"\"\"\n    return _wrapfunc(a, 'repeat', repeats, axis=axis)\n\n\ndef _put_dispatcher(a, ind, v, mode=None):\n    return (a, ind, v)\n\n\n@array_function_dispatch(_put_dispatcher)\ndef put(a, ind, v, mode='raise'):\n    \"\"\"\n    Replaces specified elements of an array with given values.\n\n    The indexing works on the flattened target array. `put` is roughly\n    equivalent to:\n\n    ::\n\n        a.flat[ind] = v\n\n    Parameters\n    ----------\n    a : ndarray\n        Target array.\n    ind : array_like\n        Target indices, interpreted as integers.\n    v : array_like\n        Values to place in `a` at target indices. If `v` is shorter than\n        `ind` it will be repeated as necessary.\n    mode : {'raise', 'wrap', 'clip'}, optional\n        Specifies how out-of-bounds indices will behave.\n\n        * 'raise' -- raise an error (default)\n        * 'wrap' -- wrap around\n        * 'clip' -- clip to the range\n\n        'clip' mode means that all indices that are too large are replaced\n        by the index that addresses the last element along that axis. Note\n        that this disables indexing with negative numbers. In 'raise' mode,\n        if an exception occurs the target array may still be modified.\n\n    See Also\n    --------\n    putmask, place\n    put_along_axis : Put elements by matching the array and the index arrays\n\n    Examples\n    --------\n    >>> a = np.arange(5)\n    >>> np.put(a, [0, 2], [-44, -55])\n    >>> a\n    array([-44,   1, -55,   3,   4])\n\n    >>> a = np.arange(5)\n    >>> np.put(a, 22, -5, mode='clip')\n    >>> a\n    array([ 0,  1,  2,  3, -5])\n\n    \"\"\"\n    try:\n        put = a.put\n    except AttributeError as e:\n        raise TypeError(\"argument 1 must be numpy.ndarray, \"\n                        \"not {name}\".format(name=type(a).__name__)) from e\n\n    return put(ind, v, mode=mode)\n\n\ndef _swapaxes_dispatcher(a, axis1, axis2):\n    return (a,)\n\n\n@array_function_dispatch(_swapaxes_dispatcher)\ndef swapaxes(a, axis1, axis2):\n    \"\"\"\n    Interchange two axes of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis1 : int\n        First axis.\n    axis2 : int\n        Second axis.\n\n    Returns\n    -------\n    a_swapped : ndarray\n        For NumPy >= 1.10.0, if `a` is an ndarray, then a view of `a` is\n        returned; otherwise a new array is created. For earlier NumPy\n        versions a view of `a` is returned only if the order of the\n        axes is changed, otherwise the input array is returned.\n\n    Examples\n    --------\n    >>> x = np.array([[1,2,3]])\n    >>> np.swapaxes(x,0,1)\n    array([[1],\n           [2],\n           [3]])\n\n    >>> x = np.array([[[0,1],[2,3]],[[4,5],[6,7]]])\n    >>> x\n    array([[[0, 1],\n            [2, 3]],\n           [[4, 5],\n            [6, 7]]])\n\n    >>> np.swapaxes(x,0,2)\n    array([[[0, 4],\n            [2, 6]],\n           [[1, 5],\n            [3, 7]]])\n\n    \"\"\"\n    return _wrapfunc(a, 'swapaxes', axis1, axis2)\n\n\ndef _transpose_dispatcher(a, axes=None):\n    return (a,)\n\n\n@array_function_dispatch(_transpose_dispatcher)\ndef transpose(a, axes=None):\n    \"\"\"\n    Reverse or permute the axes of an array; returns the modified array.\n\n    For an array a with two axes, transpose(a) gives the matrix transpose.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axes : tuple or list of ints, optional\n        If specified, it must be a tuple or list which contains a permutation of\n        [0,1,..,N-1] where N is the number of axes of a.  The i'th axis of the\n        returned array will correspond to the axis numbered ``axes[i]`` of the\n        input.  If not specified, defaults to ``range(a.ndim)[::-1]``, which\n        reverses the order of the axes.\n\n    Returns\n    -------\n    p : ndarray\n        `a` with its axes permuted.  A view is returned whenever\n        possible.\n\n    See Also\n    --------\n    moveaxis\n    argsort\n\n    Notes\n    -----\n    Use `transpose(a, argsort(axes))` to invert the transposition of tensors\n    when using the `axes` keyword argument.\n\n    Transposing a 1-D array returns an unchanged view of the original array.\n\n    Examples\n    --------\n    >>> x = np.arange(4).reshape((2,2))\n    >>> x\n    array([[0, 1],\n           [2, 3]])\n\n    >>> np.transpose(x)\n    array([[0, 2],\n           [1, 3]])\n\n    >>> x = np.ones((1, 2, 3))\n    >>> np.transpose(x, (1, 0, 2)).shape\n    (2, 1, 3)\n\n    >>> x = np.ones((2, 3, 4, 5))\n    >>> np.transpose(x).shape\n    (5, 4, 3, 2)\n\n    \"\"\"\n    return _wrapfunc(a, 'transpose', axes)\n\n\ndef _partition_dispatcher(a, kth, axis=None, kind=None, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_partition_dispatcher)\ndef partition(a, kth, axis=-1, kind='introselect', order=None):\n    \"\"\"\n    Return a partitioned copy of an array.\n\n    Creates a copy of the array with its elements rearranged in such a\n    way that the value of the element in k-th position is in the\n    position it would be in a sorted array. All elements smaller than\n    the k-th element are moved before this element and all equal or\n    greater are moved behind it. The ordering of the elements in the two\n    partitions is undefined.\n\n    .. versionadded:: 1.8.0\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be sorted.\n    kth : int or sequence of ints\n        Element index to partition by. The k-th value of the element\n        will be in its final sorted position and all smaller elements\n        will be moved before it and all equal or greater elements behind\n        it. The order of all elements in the partitions is undefined. If\n        provided with a sequence of k-th it will partition all elements\n        indexed by k-th  of them into their sorted position at once.\n    axis : int or None, optional\n        Axis along which to sort. If None, the array is flattened before\n        sorting. The default is -1, which sorts along the last axis.\n    kind : {'introselect'}, optional\n        Selection algorithm. Default is 'introselect'.\n    order : str or list of str, optional\n        When `a` is an array with fields defined, this argument\n        specifies which fields to compare first, second, etc.  A single\n        field can be specified as a string.  Not all fields need be\n        specified, but unspecified fields will still be used, in the\n        order in which they come up in the dtype, to break ties.\n\n    Returns\n    -------\n    partitioned_array : ndarray\n        Array of the same type and shape as `a`.\n\n    See Also\n    --------\n    ndarray.partition : Method to sort an array in-place.\n    argpartition : Indirect partition.\n    sort : Full sorting\n\n    Notes\n    -----\n    The various selection algorithms are characterized by their average\n    speed, worst case performance, work space size, and whether they are\n    stable. A stable sort keeps items with the same key in the same\n    relative order. The available algorithms have the following\n    properties:\n\n    ================= ======= ============= ============ =======\n       kind            speed   worst case    work space  stable\n    ================= ======= ============= ============ =======\n    'introselect'        1        O(n)           0         no\n    ================= ======= ============= ============ =======\n\n    All the partition algorithms make temporary copies of the data when\n    partitioning along any but the last axis.  Consequently,\n    partitioning along the last axis is faster and uses less space than\n    partitioning along any other axis.\n\n    The sort order for complex numbers is lexicographic. If both the\n    real and imaginary parts are non-nan then the order is determined by\n    the real parts except when they are equal, in which case the order\n    is determined by the imaginary parts.\n\n    Examples\n    --------\n    >>> a = np.array([3, 4, 2, 1])\n    >>> np.partition(a, 3)\n    array([2, 1, 3, 4])\n\n    >>> np.partition(a, (1, 3))\n    array([1, 2, 3, 4])\n\n    \"\"\"\n    if axis is None:\n        # flatten returns (1, N) for np.matrix, so always use the last axis\n        a = asanyarray(a).flatten()\n        axis = -1\n    else:\n        a = asanyarray(a).copy(order=\"K\")\n    a.partition(kth, axis=axis, kind=kind, order=order)\n    return a\n\n\ndef _argpartition_dispatcher(a, kth, axis=None, kind=None, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_argpartition_dispatcher)\ndef argpartition(a, kth, axis=-1, kind='introselect', order=None):\n    \"\"\"\n    Perform an indirect partition along the given axis using the\n    algorithm specified by the `kind` keyword. It returns an array of\n    indices of the same shape as `a` that index data along the given\n    axis in partitioned order.\n\n    .. versionadded:: 1.8.0\n\n    Parameters\n    ----------\n    a : array_like\n        Array to sort.\n    kth : int or sequence of ints\n        Element index to partition by. The k-th element will be in its\n        final sorted position and all smaller elements will be moved\n        before it and all larger elements behind it. The order all\n        elements in the partitions is undefined. If provided with a\n        sequence of k-th it will partition all of them into their sorted\n        position at once.\n    axis : int or None, optional\n        Axis along which to sort. The default is -1 (the last axis). If\n        None, the flattened array is used.\n    kind : {'introselect'}, optional\n        Selection algorithm. Default is 'introselect'\n    order : str or list of str, optional\n        When `a` is an array with fields defined, this argument\n        specifies which fields to compare first, second, etc. A single\n        field can be specified as a string, and not all fields need be\n        specified, but unspecified fields will still be used, in the\n        order in which they come up in the dtype, to break ties.\n\n    Returns\n    -------\n    index_array : ndarray, int\n        Array of indices that partition `a` along the specified axis.\n        If `a` is one-dimensional, ``a[index_array]`` yields a partitioned `a`.\n        More generally, ``np.take_along_axis(a, index_array, axis=a)`` always\n        yields the partitioned `a`, irrespective of dimensionality.\n\n    See Also\n    --------\n    partition : Describes partition algorithms used.\n    ndarray.partition : Inplace partition.\n    argsort : Full indirect sort.\n    take_along_axis : Apply ``index_array`` from argpartition\n                      to an array as if by calling partition.\n\n    Notes\n    -----\n    See `partition` for notes on the different selection algorithms.\n\n    Examples\n    --------\n    One dimensional array:\n\n    >>> x = np.array([3, 4, 2, 1])\n    >>> x[np.argpartition(x, 3)]\n    array([2, 1, 3, 4])\n    >>> x[np.argpartition(x, (1, 3))]\n    array([1, 2, 3, 4])\n\n    >>> x = [3, 4, 2, 1]\n    >>> np.array(x)[np.argpartition(x, 3)]\n    array([2, 1, 3, 4])\n\n    Multi-dimensional array:\n\n    >>> x = np.array([[3, 4, 2], [1, 3, 1]])\n    >>> index_array = np.argpartition(x, kth=1, axis=-1)\n    >>> np.take_along_axis(x, index_array, axis=-1)  # same as np.partition(x, kth=1)\n    array([[2, 3, 4],\n           [1, 1, 3]])\n\n    \"\"\"\n    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)\n\n\ndef _sort_dispatcher(a, axis=None, kind=None, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_sort_dispatcher)\ndef sort(a, axis=-1, kind=None, order=None):\n    \"\"\"\n    Return a sorted copy of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be sorted.\n    axis : int or None, optional\n        Axis along which to sort. If None, the array is flattened before\n        sorting. The default is -1, which sorts along the last axis.\n    kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n        Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n        and 'mergesort' use timsort or radix sort under the covers and, in general,\n        the actual implementation will vary with data type. The 'mergesort' option\n        is retained for backwards compatibility.\n\n        .. versionchanged:: 1.15.0.\n           The 'stable' option was added.\n\n    order : str or list of str, optional\n        When `a` is an array with fields defined, this argument specifies\n        which fields to compare first, second, etc.  A single field can\n        be specified as a string, and not all fields need be specified,\n        but unspecified fields will still be used, in the order in which\n        they come up in the dtype, to break ties.\n\n    Returns\n    -------\n    sorted_array : ndarray\n        Array of the same type and shape as `a`.\n\n    See Also\n    --------\n    ndarray.sort : Method to sort an array in-place.\n    argsort : Indirect sort.\n    lexsort : Indirect stable sort on multiple keys.\n    searchsorted : Find elements in a sorted array.\n    partition : Partial sort.\n\n    Notes\n    -----\n    The various sorting algorithms are characterized by their average speed,\n    worst case performance, work space size, and whether they are stable. A\n    stable sort keeps items with the same key in the same relative\n    order. The four algorithms implemented in NumPy have the following\n    properties:\n\n    =========== ======= ============= ============ ========\n       kind      speed   worst case    work space   stable\n    =========== ======= ============= ============ ========\n    'quicksort'    1     O(n^2)            0          no\n    'heapsort'     3     O(n*log(n))       0          no\n    'mergesort'    2     O(n*log(n))      ~n/2        yes\n    'timsort'      2     O(n*log(n))      ~n/2        yes\n    =========== ======= ============= ============ ========\n\n    .. note:: The datatype determines which of 'mergesort' or 'timsort'\n       is actually used, even if 'mergesort' is specified. User selection\n       at a finer scale is not currently available.\n\n    All the sort algorithms make temporary copies of the data when\n    sorting along any but the last axis.  Consequently, sorting along\n    the last axis is faster and uses less space than sorting along\n    any other axis.\n\n    The sort order for complex numbers is lexicographic. If both the real\n    and imaginary parts are non-nan then the order is determined by the\n    real parts except when they are equal, in which case the order is\n    determined by the imaginary parts.\n\n    Previous to numpy 1.4.0 sorting real and complex arrays containing nan\n    values led to undefined behaviour. In numpy versions >= 1.4.0 nan\n    values are sorted to the end. The extended sort order is:\n\n      * Real: [R, nan]\n      * Complex: [R + Rj, R + nanj, nan + Rj, nan + nanj]\n\n    where R is a non-nan real value. Complex values with the same nan\n    placements are sorted according to the non-nan part if it exists.\n    Non-nan values are sorted as before.\n\n    .. versionadded:: 1.12.0\n\n    quicksort has been changed to `introsort <https://en.wikipedia.org/wiki/Introsort>`_.\n    When sorting does not make enough progress it switches to\n    `heapsort <https://en.wikipedia.org/wiki/Heapsort>`_.\n    This implementation makes quicksort O(n*log(n)) in the worst case.\n\n    'stable' automatically chooses the best stable sorting algorithm\n    for the data type being sorted.\n    It, along with 'mergesort' is currently mapped to\n    `timsort <https://en.wikipedia.org/wiki/Timsort>`_\n    or `radix sort <https://en.wikipedia.org/wiki/Radix_sort>`_\n    depending on the data type.\n    API forward compatibility currently limits the\n    ability to select the implementation and it is hardwired for the different\n    data types.\n\n    .. versionadded:: 1.17.0\n\n    Timsort is added for better performance on already or nearly\n    sorted data. On random data timsort is almost identical to\n    mergesort. It is now used for stable sort while quicksort is still the\n    default sort if none is chosen. For timsort details, refer to\n    `CPython listsort.txt <https://github.com/python/cpython/blob/3.7/Objects/listsort.txt>`_.\n    'mergesort' and 'stable' are mapped to radix sort for integer data types. Radix sort is an\n    O(n) sort instead of O(n log n).\n\n    .. versionchanged:: 1.18.0\n\n    NaT now sorts to the end of arrays for consistency with NaN.\n\n    Examples\n    --------\n    >>> a = np.array([[1,4],[3,1]])\n    >>> np.sort(a)                # sort along the last axis\n    array([[1, 4],\n           [1, 3]])\n    >>> np.sort(a, axis=None)     # sort the flattened array\n    array([1, 1, 3, 4])\n    >>> np.sort(a, axis=0)        # sort along the first axis\n    array([[1, 1],\n           [3, 4]])\n\n    Use the `order` keyword to specify a field to use when sorting a\n    structured array:\n\n    >>> dtype = [('name', 'S10'), ('height', float), ('age', int)]\n    >>> values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),\n    ...           ('Galahad', 1.7, 38)]\n    >>> a = np.array(values, dtype=dtype)       # create a structured array\n    >>> np.sort(a, order='height')                        # doctest: +SKIP\n    array([('Galahad', 1.7, 38), ('Arthur', 1.8, 41),\n           ('Lancelot', 1.8999999999999999, 38)],\n          dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n\n    Sort by age, then height if ages are equal:\n\n    >>> np.sort(a, order=['age', 'height'])               # doctest: +SKIP\n    array([('Galahad', 1.7, 38), ('Lancelot', 1.8999999999999999, 38),\n           ('Arthur', 1.8, 41)],\n          dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n\n    \"\"\"\n    if axis is None:\n        # flatten returns (1, N) for np.matrix, so always use the last axis\n        a = asanyarray(a).flatten()\n        axis = -1\n    else:\n        a = asanyarray(a).copy(order=\"K\")\n    a.sort(axis=axis, kind=kind, order=order)\n    return a\n\n\ndef _argsort_dispatcher(a, axis=None, kind=None, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_argsort_dispatcher)\ndef argsort(a, axis=-1, kind=None, order=None):\n    \"\"\"\n    Returns the indices that would sort an array.\n\n    Perform an indirect sort along the given axis using the algorithm specified\n    by the `kind` keyword. It returns an array of indices of the same shape as\n    `a` that index data along the given axis in sorted order.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to sort.\n    axis : int or None, optional\n        Axis along which to sort.  The default is -1 (the last axis). If None,\n        the flattened array is used.\n    kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n        Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n        and 'mergesort' use timsort under the covers and, in general, the\n        actual implementation will vary with data type. The 'mergesort' option\n        is retained for backwards compatibility.\n\n        .. versionchanged:: 1.15.0.\n           The 'stable' option was added.\n    order : str or list of str, optional\n        When `a` is an array with fields defined, this argument specifies\n        which fields to compare first, second, etc.  A single field can\n        be specified as a string, and not all fields need be specified,\n        but unspecified fields will still be used, in the order in which\n        they come up in the dtype, to break ties.\n\n    Returns\n    -------\n    index_array : ndarray, int\n        Array of indices that sort `a` along the specified `axis`.\n        If `a` is one-dimensional, ``a[index_array]`` yields a sorted `a`.\n        More generally, ``np.take_along_axis(a, index_array, axis=axis)``\n        always yields the sorted `a`, irrespective of dimensionality.\n\n    See Also\n    --------\n    sort : Describes sorting algorithms used.\n    lexsort : Indirect stable sort with multiple keys.\n    ndarray.sort : Inplace sort.\n    argpartition : Indirect partial sort.\n    take_along_axis : Apply ``index_array`` from argsort\n                      to an array as if by calling sort.\n\n    Notes\n    -----\n    See `sort` for notes on the different sorting algorithms.\n\n    As of NumPy 1.4.0 `argsort` works with real/complex arrays containing\n    nan values. The enhanced sort order is documented in `sort`.\n\n    Examples\n    --------\n    One dimensional array:\n\n    >>> x = np.array([3, 1, 2])\n    >>> np.argsort(x)\n    array([1, 2, 0])\n\n    Two-dimensional array:\n\n    >>> x = np.array([[0, 3], [2, 2]])\n    >>> x\n    array([[0, 3],\n           [2, 2]])\n\n    >>> ind = np.argsort(x, axis=0)  # sorts along first axis (down)\n    >>> ind\n    array([[0, 1],\n           [1, 0]])\n    >>> np.take_along_axis(x, ind, axis=0)  # same as np.sort(x, axis=0)\n    array([[0, 2],\n           [2, 3]])\n\n    >>> ind = np.argsort(x, axis=1)  # sorts along last axis (across)\n    >>> ind\n    array([[0, 1],\n           [0, 1]])\n    >>> np.take_along_axis(x, ind, axis=1)  # same as np.sort(x, axis=1)\n    array([[0, 3],\n           [2, 2]])\n\n    Indices of the sorted elements of a N-dimensional array:\n\n    >>> ind = np.unravel_index(np.argsort(x, axis=None), x.shape)\n    >>> ind\n    (array([0, 1, 1, 0]), array([0, 0, 1, 1]))\n    >>> x[ind]  # same as np.sort(x, axis=None)\n    array([0, 2, 2, 3])\n\n    Sorting with keys:\n\n    >>> x = np.array([(1, 0), (0, 1)], dtype=[('x', '<i4'), ('y', '<i4')])\n    >>> x\n    array([(1, 0), (0, 1)],\n          dtype=[('x', '<i4'), ('y', '<i4')])\n\n    >>> np.argsort(x, order=('x','y'))\n    array([1, 0])\n\n    >>> np.argsort(x, order=('y','x'))\n    array([0, 1])\n\n    \"\"\"\n    return _wrapfunc(a, 'argsort', axis=axis, kind=kind, order=order)\n\n\ndef _argmax_dispatcher(a, axis=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_argmax_dispatcher)\ndef argmax(a, axis=None, out=None):\n    \"\"\"\n    Returns the indices of the maximum values along an axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int, optional\n        By default, the index is into the flattened array, otherwise\n        along the specified axis.\n    out : array, optional\n        If provided, the result will be inserted into this array. It should\n        be of the appropriate shape and dtype.\n\n    Returns\n    -------\n    index_array : ndarray of ints\n        Array of indices into the array. It has the same shape as `a.shape`\n        with the dimension along `axis` removed.\n\n    See Also\n    --------\n    ndarray.argmax, argmin\n    amax : The maximum value along a given axis.\n    unravel_index : Convert a flat index into an index tuple.\n    take_along_axis : Apply ``np.expand_dims(index_array, axis)``\n                      from argmax to an array as if by calling max.\n\n    Notes\n    -----\n    In case of multiple occurrences of the maximum values, the indices\n    corresponding to the first occurrence are returned.\n\n    Examples\n    --------\n    >>> a = np.arange(6).reshape(2,3) + 10\n    >>> a\n    array([[10, 11, 12],\n           [13, 14, 15]])\n    >>> np.argmax(a)\n    5\n    >>> np.argmax(a, axis=0)\n    array([1, 1, 1])\n    >>> np.argmax(a, axis=1)\n    array([2, 2])\n\n    Indexes of the maximal elements of a N-dimensional array:\n\n    >>> ind = np.unravel_index(np.argmax(a, axis=None), a.shape)\n    >>> ind\n    (1, 2)\n    >>> a[ind]\n    15\n\n    >>> b = np.arange(6)\n    >>> b[1] = 5\n    >>> b\n    array([0, 5, 2, 3, 4, 5])\n    >>> np.argmax(b)  # Only the first occurrence is returned.\n    1\n\n    >>> x = np.array([[4,2,3], [1,0,3]])\n    >>> index_array = np.argmax(x, axis=-1)\n    >>> # Same as np.max(x, axis=-1, keepdims=True)\n    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\n    array([[4],\n           [3]])\n    >>> # Same as np.max(x, axis=-1)\n    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\n    array([4, 3])\n\n    \"\"\"\n    return _wrapfunc(a, 'argmax', axis=axis, out=out)\n\n\ndef _argmin_dispatcher(a, axis=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_argmin_dispatcher)\ndef argmin(a, axis=None, out=None):\n    \"\"\"\n    Returns the indices of the minimum values along an axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int, optional\n        By default, the index is into the flattened array, otherwise\n        along the specified axis.\n    out : array, optional\n        If provided, the result will be inserted into this array. It should\n        be of the appropriate shape and dtype.\n\n    Returns\n    -------\n    index_array : ndarray of ints\n        Array of indices into the array. It has the same shape as `a.shape`\n        with the dimension along `axis` removed.\n\n    See Also\n    --------\n    ndarray.argmin, argmax\n    amin : The minimum value along a given axis.\n    unravel_index : Convert a flat index into an index tuple.\n    take_along_axis : Apply ``np.expand_dims(index_array, axis)``\n                      from argmin to an array as if by calling min.\n\n    Notes\n    -----\n    In case of multiple occurrences of the minimum values, the indices\n    corresponding to the first occurrence are returned.\n\n    Examples\n    --------\n    >>> a = np.arange(6).reshape(2,3) + 10\n    >>> a\n    array([[10, 11, 12],\n           [13, 14, 15]])\n    >>> np.argmin(a)\n    0\n    >>> np.argmin(a, axis=0)\n    array([0, 0, 0])\n    >>> np.argmin(a, axis=1)\n    array([0, 0])\n\n    Indices of the minimum elements of a N-dimensional array:\n\n    >>> ind = np.unravel_index(np.argmin(a, axis=None), a.shape)\n    >>> ind\n    (0, 0)\n    >>> a[ind]\n    10\n\n    >>> b = np.arange(6) + 10\n    >>> b[4] = 10\n    >>> b\n    array([10, 11, 12, 13, 10, 15])\n    >>> np.argmin(b)  # Only the first occurrence is returned.\n    0\n\n    >>> x = np.array([[4,2,3], [1,0,3]])\n    >>> index_array = np.argmin(x, axis=-1)\n    >>> # Same as np.min(x, axis=-1, keepdims=True)\n    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\n    array([[2],\n           [0]])\n    >>> # Same as np.max(x, axis=-1)\n    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\n    array([2, 0])\n\n    \"\"\"\n    return _wrapfunc(a, 'argmin', axis=axis, out=out)\n\n\ndef _searchsorted_dispatcher(a, v, side=None, sorter=None):\n    return (a, v, sorter)\n\n\n@array_function_dispatch(_searchsorted_dispatcher)\ndef searchsorted(a, v, side='left', sorter=None):\n    \"\"\"\n    Find indices where elements should be inserted to maintain order.\n\n    Find the indices into a sorted array `a` such that, if the\n    corresponding elements in `v` were inserted before the indices, the\n    order of `a` would be preserved.\n\n    Assuming that `a` is sorted:\n\n    ======  ============================\n    `side`  returned index `i` satisfies\n    ======  ============================\n    left    ``a[i-1] < v <= a[i]``\n    right   ``a[i-1] <= v < a[i]``\n    ======  ============================\n\n    Parameters\n    ----------\n    a : 1-D array_like\n        Input array. If `sorter` is None, then it must be sorted in\n        ascending order, otherwise `sorter` must be an array of indices\n        that sort it.\n    v : array_like\n        Values to insert into `a`.\n    side : {'left', 'right'}, optional\n        If 'left', the index of the first suitable location found is given.\n        If 'right', return the last such index.  If there is no suitable\n        index, return either 0 or N (where N is the length of `a`).\n    sorter : 1-D array_like, optional\n        Optional array of integer indices that sort array a into ascending\n        order. They are typically the result of argsort.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    indices : array of ints\n        Array of insertion points with the same shape as `v`.\n\n    See Also\n    --------\n    sort : Return a sorted copy of an array.\n    histogram : Produce histogram from 1-D data.\n\n    Notes\n    -----\n    Binary search is used to find the required insertion points.\n\n    As of NumPy 1.4.0 `searchsorted` works with real/complex arrays containing\n    `nan` values. The enhanced sort order is documented in `sort`.\n\n    This function uses the same algorithm as the builtin python `bisect.bisect_left`\n    (``side='left'``) and `bisect.bisect_right` (``side='right'``) functions,\n    which is also vectorized in the `v` argument.\n\n    Examples\n    --------\n    >>> np.searchsorted([1,2,3,4,5], 3)\n    2\n    >>> np.searchsorted([1,2,3,4,5], 3, side='right')\n    3\n    >>> np.searchsorted([1,2,3,4,5], [-10, 10, 2, 3])\n    array([0, 5, 1, 2])\n\n    \"\"\"\n    return _wrapfunc(a, 'searchsorted', v, side=side, sorter=sorter)\n\n\ndef _resize_dispatcher(a, new_shape):\n    return (a,)\n\n\n@array_function_dispatch(_resize_dispatcher)\ndef resize(a, new_shape):\n    \"\"\"\n    Return a new array with the specified shape.\n\n    If the new array is larger than the original array, then the new\n    array is filled with repeated copies of `a`.  Note that this behavior\n    is different from a.resize(new_shape) which fills with zeros instead\n    of repeated copies of `a`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be resized.\n\n    new_shape : int or tuple of int\n        Shape of resized array.\n\n    Returns\n    -------\n    reshaped_array : ndarray\n        The new array is formed from the data in the old array, repeated\n        if necessary to fill out the required number of elements.  The\n        data are repeated in the order that they are stored in memory.\n\n    See Also\n    --------\n    np.reshape : Reshape an array without changing the total size.\n    np.pad : Enlarge and pad an array.\n    np.repeat: Repeat elements of an array.\n    ndarray.resize : resize an array in-place.\n\n    Notes\n    -----\n    When the total size of the array does not change `~numpy.reshape` should\n    be used.  In most other cases either indexing (to reduce the size)\n    or padding (to increase the size) may be a more appropriate solution.\n\n    Warning: This functionality does **not** consider axes separately,\n    i.e. it does not apply interpolation/extrapolation.\n    It fills the return array with the required number of elements, taken\n    from `a` as they are laid out in memory, disregarding strides and axes.\n    (This is in case the new shape is smaller. For larger, see above.)\n    This functionality is therefore not suitable to resize images,\n    or data where each axis represents a separate and distinct entity.\n\n    Examples\n    --------\n    >>> a=np.array([[0,1],[2,3]])\n    >>> np.resize(a,(2,3))\n    array([[0, 1, 2],\n           [3, 0, 1]])\n    >>> np.resize(a,(1,4))\n    array([[0, 1, 2, 3]])\n    >>> np.resize(a,(2,4))\n    array([[0, 1, 2, 3],\n           [0, 1, 2, 3]])\n\n    \"\"\"\n    if isinstance(new_shape, (int, nt.integer)):\n        new_shape = (new_shape,)\n\n    a = ravel(a)\n\n    new_size = 1\n    for dim_length in new_shape:\n        new_size *= dim_length\n        if dim_length < 0:\n            raise ValueError('all elements of `new_shape` must be non-negative')\n\n    if a.size == 0 or new_size == 0:\n        # First case must zero fill. The second would have repeats == 0.\n        return np.zeros_like(a, shape=new_shape)\n\n    repeats = -(-new_size // a.size)  # ceil division\n    a = concatenate((a,) * repeats)[:new_size]\n\n    return reshape(a, new_shape)\n\n\ndef _squeeze_dispatcher(a, axis=None):\n    return (a,)\n\n\n@array_function_dispatch(_squeeze_dispatcher)\ndef squeeze(a, axis=None):\n    \"\"\"\n    Remove axes of length one from `a`.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : None or int or tuple of ints, optional\n        .. versionadded:: 1.7.0\n\n        Selects a subset of the entries of length one in the\n        shape. If an axis is selected with shape entry greater than\n        one, an error is raised.\n\n    Returns\n    -------\n    squeezed : ndarray\n        The input array, but with all or a subset of the\n        dimensions of length 1 removed. This is always `a` itself\n        or a view into `a`. Note that if all axes are squeezed,\n        the result is a 0d array and not a scalar.\n\n    Raises\n    ------\n    ValueError\n        If `axis` is not None, and an axis being squeezed is not of length 1\n\n    See Also\n    --------\n    expand_dims : The inverse operation, adding entries of length one\n    reshape : Insert, remove, and combine dimensions, and resize existing ones\n\n    Examples\n    --------\n    >>> x = np.array([[[0], [1], [2]]])\n    >>> x.shape\n    (1, 3, 1)\n    >>> np.squeeze(x).shape\n    (3,)\n    >>> np.squeeze(x, axis=0).shape\n    (3, 1)\n    >>> np.squeeze(x, axis=1).shape\n    Traceback (most recent call last):\n    ...\n    ValueError: cannot select an axis to squeeze out which has size not equal to one\n    >>> np.squeeze(x, axis=2).shape\n    (1, 3)\n    >>> x = np.array([[1234]])\n    >>> x.shape\n    (1, 1)\n    >>> np.squeeze(x)\n    array(1234)  # 0d array\n    >>> np.squeeze(x).shape\n    ()\n    >>> np.squeeze(x)[()]\n    1234\n\n    \"\"\"\n    try:\n        squeeze = a.squeeze\n    except AttributeError:\n        return _wrapit(a, 'squeeze', axis=axis)\n    if axis is None:\n        return squeeze()\n    else:\n        return squeeze(axis=axis)\n\n\ndef _diagonal_dispatcher(a, offset=None, axis1=None, axis2=None):\n    return (a,)\n\n\n@array_function_dispatch(_diagonal_dispatcher)\ndef diagonal(a, offset=0, axis1=0, axis2=1):\n    \"\"\"\n    Return specified diagonals.\n\n    If `a` is 2-D, returns the diagonal of `a` with the given offset,\n    i.e., the collection of elements of the form ``a[i, i+offset]``.  If\n    `a` has more than two dimensions, then the axes specified by `axis1`\n    and `axis2` are used to determine the 2-D sub-array whose diagonal is\n    returned.  The shape of the resulting array can be determined by\n    removing `axis1` and `axis2` and appending an index to the right equal\n    to the size of the resulting diagonals.\n\n    In versions of NumPy prior to 1.7, this function always returned a new,\n    independent array containing a copy of the values in the diagonal.\n\n    In NumPy 1.7 and 1.8, it continues to return a copy of the diagonal,\n    but depending on this fact is deprecated. Writing to the resulting\n    array continues to work as it used to, but a FutureWarning is issued.\n\n    Starting in NumPy 1.9 it returns a read-only view on the original array.\n    Attempting to write to the resulting array will produce an error.\n\n    In some future release, it will return a read/write view and writing to\n    the returned array will alter your original array.  The returned array\n    will have the same type as the input array.\n\n    If you don't write to the array returned by this function, then you can\n    just ignore all of the above.\n\n    If you depend on the current behavior, then we suggest copying the\n    returned array explicitly, i.e., use ``np.diagonal(a).copy()`` instead\n    of just ``np.diagonal(a)``. This will work with both past and future\n    versions of NumPy.\n\n    Parameters\n    ----------\n    a : array_like\n        Array from which the diagonals are taken.\n    offset : int, optional\n        Offset of the diagonal from the main diagonal.  Can be positive or\n        negative.  Defaults to main diagonal (0).\n    axis1 : int, optional\n        Axis to be used as the first axis of the 2-D sub-arrays from which\n        the diagonals should be taken.  Defaults to first axis (0).\n    axis2 : int, optional\n        Axis to be used as the second axis of the 2-D sub-arrays from\n        which the diagonals should be taken. Defaults to second axis (1).\n\n    Returns\n    -------\n    array_of_diagonals : ndarray\n        If `a` is 2-D, then a 1-D array containing the diagonal and of the\n        same type as `a` is returned unless `a` is a `matrix`, in which case\n        a 1-D array rather than a (2-D) `matrix` is returned in order to\n        maintain backward compatibility.\n\n        If ``a.ndim > 2``, then the dimensions specified by `axis1` and `axis2`\n        are removed, and a new axis inserted at the end corresponding to the\n        diagonal.\n\n    Raises\n    ------\n    ValueError\n        If the dimension of `a` is less than 2.\n\n    See Also\n    --------\n    diag : MATLAB work-a-like for 1-D and 2-D arrays.\n    diagflat : Create diagonal arrays.\n    trace : Sum along diagonals.\n\n    Examples\n    --------\n    >>> a = np.arange(4).reshape(2,2)\n    >>> a\n    array([[0, 1],\n           [2, 3]])\n    >>> a.diagonal()\n    array([0, 3])\n    >>> a.diagonal(1)\n    array([1])\n\n    A 3-D example:\n\n    >>> a = np.arange(8).reshape(2,2,2); a\n    array([[[0, 1],\n            [2, 3]],\n           [[4, 5],\n            [6, 7]]])\n    >>> a.diagonal(0,  # Main diagonals of two arrays created by skipping\n    ...            0,  # across the outer(left)-most axis last and\n    ...            1)  # the \"middle\" (row) axis first.\n    array([[0, 6],\n           [1, 7]])\n\n    The sub-arrays whose main diagonals we just obtained; note that each\n    corresponds to fixing the right-most (column) axis, and that the\n    diagonals are \"packed\" in rows.\n\n    >>> a[:,:,0]  # main diagonal is [0 6]\n    array([[0, 2],\n           [4, 6]])\n    >>> a[:,:,1]  # main diagonal is [1 7]\n    array([[1, 3],\n           [5, 7]])\n\n    The anti-diagonal can be obtained by reversing the order of elements\n    using either `numpy.flipud` or `numpy.fliplr`.\n\n    >>> a = np.arange(9).reshape(3, 3)\n    >>> a\n    array([[0, 1, 2],\n           [3, 4, 5],\n           [6, 7, 8]])\n    >>> np.fliplr(a).diagonal()  # Horizontal flip\n    array([2, 4, 6])\n    >>> np.flipud(a).diagonal()  # Vertical flip\n    array([6, 4, 2])\n\n    Note that the order in which the diagonal is retrieved varies depending\n    on the flip function.\n    \"\"\"\n    if isinstance(a, np.matrix):\n        # Make diagonal of matrix 1-D to preserve backward compatibility.\n        return asarray(a).diagonal(offset=offset, axis1=axis1, axis2=axis2)\n    else:\n        return asanyarray(a).diagonal(offset=offset, axis1=axis1, axis2=axis2)\n\n\ndef _trace_dispatcher(\n        a, offset=None, axis1=None, axis2=None, dtype=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_trace_dispatcher)\ndef trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):\n    \"\"\"\n    Return the sum along diagonals of the array.\n\n    If `a` is 2-D, the sum along its diagonal with the given offset\n    is returned, i.e., the sum of elements ``a[i,i+offset]`` for all i.\n\n    If `a` has more than two dimensions, then the axes specified by axis1 and\n    axis2 are used to determine the 2-D sub-arrays whose traces are returned.\n    The shape of the resulting array is the same as that of `a` with `axis1`\n    and `axis2` removed.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array, from which the diagonals are taken.\n    offset : int, optional\n        Offset of the diagonal from the main diagonal. Can be both positive\n        and negative. Defaults to 0.\n    axis1, axis2 : int, optional\n        Axes to be used as the first and second axis of the 2-D sub-arrays\n        from which the diagonals should be taken. Defaults are the first two\n        axes of `a`.\n    dtype : dtype, optional\n        Determines the data-type of the returned array and of the accumulator\n        where the elements are summed. If dtype has the value None and `a` is\n        of integer type of precision less than the default integer\n        precision, then the default integer precision is used. Otherwise,\n        the precision is the same as that of `a`.\n    out : ndarray, optional\n        Array into which the output is placed. Its type is preserved and\n        it must be of the right shape to hold the output.\n\n    Returns\n    -------\n    sum_along_diagonals : ndarray\n        If `a` is 2-D, the sum along the diagonal is returned.  If `a` has\n        larger dimensions, then an array of sums along diagonals is returned.\n\n    See Also\n    --------\n    diag, diagonal, diagflat\n\n    Examples\n    --------\n    >>> np.trace(np.eye(3))\n    3.0\n    >>> a = np.arange(8).reshape((2,2,2))\n    >>> np.trace(a)\n    array([6, 8])\n\n    >>> a = np.arange(24).reshape((2,2,2,3))\n    >>> np.trace(a).shape\n    (2, 3)\n\n    \"\"\"\n    if isinstance(a, np.matrix):\n        # Get trace of matrix via an array to preserve backward compatibility.\n        return asarray(a).trace(offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out)\n    else:\n        return asanyarray(a).trace(offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out)\n\n\ndef _ravel_dispatcher(a, order=None):\n    return (a,)\n\n\n@array_function_dispatch(_ravel_dispatcher)\ndef ravel(a, order='C'):\n    \"\"\"Return a contiguous flattened array.\n\n    A 1-D array, containing the elements of the input, is returned.  A copy is\n    made only if needed.\n\n    As of NumPy 1.10, the returned array will have the same type as the input\n    array. (for example, a masked array will be returned for a masked array\n    input)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.  The elements in `a` are read in the order specified by\n        `order`, and packed as a 1-D array.\n    order : {'C','F', 'A', 'K'}, optional\n\n        The elements of `a` are read using this index order. 'C' means\n        to index the elements in row-major, C-style order,\n        with the last axis index changing fastest, back to the first\n        axis index changing slowest.  'F' means to index the elements\n        in column-major, Fortran-style order, with the\n        first index changing fastest, and the last index changing\n        slowest. Note that the 'C' and 'F' options take no account of\n        the memory layout of the underlying array, and only refer to\n        the order of axis indexing.  'A' means to read the elements in\n        Fortran-like index order if `a` is Fortran *contiguous* in\n        memory, C-like order otherwise.  'K' means to read the\n        elements in the order they occur in memory, except for\n        reversing the data when strides are negative.  By default, 'C'\n        index order is used.\n\n    Returns\n    -------\n    y : array_like\n        y is an array of the same subtype as `a`, with shape ``(a.size,)``.\n        Note that matrices are special cased for backward compatibility, if `a`\n        is a matrix, then y is a 1-D ndarray.\n\n    See Also\n    --------\n    ndarray.flat : 1-D iterator over an array.\n    ndarray.flatten : 1-D array copy of the elements of an array\n                      in row-major order.\n    ndarray.reshape : Change the shape of an array without changing its data.\n\n    Notes\n    -----\n    In row-major, C-style order, in two dimensions, the row index\n    varies the slowest, and the column index the quickest.  This can\n    be generalized to multiple dimensions, where row-major order\n    implies that the index along the first axis varies slowest, and\n    the index along the last quickest.  The opposite holds for\n    column-major, Fortran-style index ordering.\n\n    When a view is desired in as many cases as possible, ``arr.reshape(-1)``\n    may be preferable.\n\n    Examples\n    --------\n    It is equivalent to ``reshape(-1, order=order)``.\n\n    >>> x = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> np.ravel(x)\n    array([1, 2, 3, 4, 5, 6])\n\n    >>> x.reshape(-1)\n    array([1, 2, 3, 4, 5, 6])\n\n    >>> np.ravel(x, order='F')\n    array([1, 4, 2, 5, 3, 6])\n\n    When ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:\n\n    >>> np.ravel(x.T)\n    array([1, 4, 2, 5, 3, 6])\n    >>> np.ravel(x.T, order='A')\n    array([1, 2, 3, 4, 5, 6])\n\n    When ``order`` is 'K', it will preserve orderings that are neither 'C'\n    nor 'F', but won't reverse axes:\n\n    >>> a = np.arange(3)[::-1]; a\n    array([2, 1, 0])\n    >>> a.ravel(order='C')\n    array([2, 1, 0])\n    >>> a.ravel(order='K')\n    array([2, 1, 0])\n\n    >>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a\n    array([[[ 0,  2,  4],\n            [ 1,  3,  5]],\n           [[ 6,  8, 10],\n            [ 7,  9, 11]]])\n    >>> a.ravel(order='C')\n    array([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])\n    >>> a.ravel(order='K')\n    array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n    \"\"\"\n    if isinstance(a, np.matrix):\n        return asarray(a).ravel(order=order)\n    else:\n        return asanyarray(a).ravel(order=order)\n\n\ndef _nonzero_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_nonzero_dispatcher)\ndef nonzero(a):\n    \"\"\"\n    Return the indices of the elements that are non-zero.\n\n    Returns a tuple of arrays, one for each dimension of `a`,\n    containing the indices of the non-zero elements in that\n    dimension. The values in `a` are always tested and returned in\n    row-major, C-style order.\n\n    To group the indices by element, rather than dimension, use `argwhere`,\n    which returns a row for each non-zero element.\n\n    .. note::\n\n       When called on a zero-d array or scalar, ``nonzero(a)`` is treated\n       as ``nonzero(atleast_1d(a))``.\n\n       .. deprecated:: 1.17.0\n\n          Use `atleast_1d` explicitly if this behavior is deliberate.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    Returns\n    -------\n    tuple_of_arrays : tuple\n        Indices of elements that are non-zero.\n\n    See Also\n    --------\n    flatnonzero :\n        Return indices that are non-zero in the flattened version of the input\n        array.\n    ndarray.nonzero :\n        Equivalent ndarray method.\n    count_nonzero :\n        Counts the number of non-zero elements in the input array.\n\n    Notes\n    -----\n    While the nonzero values can be obtained with ``a[nonzero(a)]``, it is\n    recommended to use ``x[x.astype(bool)]`` or ``x[x != 0]`` instead, which\n    will correctly handle 0-d arrays.\n\n    Examples\n    --------\n    >>> x = np.array([[3, 0, 0], [0, 4, 0], [5, 6, 0]])\n    >>> x\n    array([[3, 0, 0],\n           [0, 4, 0],\n           [5, 6, 0]])\n    >>> np.nonzero(x)\n    (array([0, 1, 2, 2]), array([0, 1, 0, 1]))\n\n    >>> x[np.nonzero(x)]\n    array([3, 4, 5, 6])\n    >>> np.transpose(np.nonzero(x))\n    array([[0, 0],\n           [1, 1],\n           [2, 0],\n           [2, 1]])\n\n    A common use for ``nonzero`` is to find the indices of an array, where\n    a condition is True.  Given an array `a`, the condition `a` > 3 is a\n    boolean array and since False is interpreted as 0, np.nonzero(a > 3)\n    yields the indices of the `a` where the condition is true.\n\n    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> a > 3\n    array([[False, False, False],\n           [ True,  True,  True],\n           [ True,  True,  True]])\n    >>> np.nonzero(a > 3)\n    (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n\n    Using this result to index `a` is equivalent to using the mask directly:\n\n    >>> a[np.nonzero(a > 3)]\n    array([4, 5, 6, 7, 8, 9])\n    >>> a[a > 3]  # prefer this spelling\n    array([4, 5, 6, 7, 8, 9])\n\n    ``nonzero`` can also be called as a method of the array.\n\n    >>> (a > 3).nonzero()\n    (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))\n\n    \"\"\"\n    return _wrapfunc(a, 'nonzero')\n\n\ndef _shape_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_shape_dispatcher)\ndef shape(a):\n    \"\"\"\n    Return the shape of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    Returns\n    -------\n    shape : tuple of ints\n        The elements of the shape tuple give the lengths of the\n        corresponding array dimensions.\n\n    See Also\n    --------\n    len\n    ndarray.shape : Equivalent array method.\n\n    Examples\n    --------\n    >>> np.shape(np.eye(3))\n    (3, 3)\n    >>> np.shape([[1, 2]])\n    (1, 2)\n    >>> np.shape([0])\n    (1,)\n    >>> np.shape(0)\n    ()\n\n    >>> a = np.array([(1, 2), (3, 4)], dtype=[('x', 'i4'), ('y', 'i4')])\n    >>> np.shape(a)\n    (2,)\n    >>> a.shape\n    (2,)\n\n    \"\"\"\n    try:\n        result = a.shape\n    except AttributeError:\n        result = asarray(a).shape\n    return result\n\n\ndef _compress_dispatcher(condition, a, axis=None, out=None):\n    return (condition, a, out)\n\n\n@array_function_dispatch(_compress_dispatcher)\ndef compress(condition, a, axis=None, out=None):\n    \"\"\"\n    Return selected slices of an array along given axis.\n\n    When working along a given axis, a slice along that axis is returned in\n    `output` for each index where `condition` evaluates to True. When\n    working on a 1-D array, `compress` is equivalent to `extract`.\n\n    Parameters\n    ----------\n    condition : 1-D array of bools\n        Array that selects which entries to return. If len(condition)\n        is less than the size of `a` along the given axis, then output is\n        truncated to the length of the condition array.\n    a : array_like\n        Array from which to extract a part.\n    axis : int, optional\n        Axis along which to take slices. If None (default), work on the\n        flattened array.\n    out : ndarray, optional\n        Output array.  Its type is preserved and it must be of the right\n        shape to hold the output.\n\n    Returns\n    -------\n    compressed_array : ndarray\n        A copy of `a` without the slices along axis for which `condition`\n        is false.\n\n    See Also\n    --------\n    take, choose, diag, diagonal, select\n    ndarray.compress : Equivalent method in ndarray\n    extract: Equivalent method when working on 1-D arrays\n    :ref:`ufuncs-output-type`\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> a\n    array([[1, 2],\n           [3, 4],\n           [5, 6]])\n    >>> np.compress([0, 1], a, axis=0)\n    array([[3, 4]])\n    >>> np.compress([False, True, True], a, axis=0)\n    array([[3, 4],\n           [5, 6]])\n    >>> np.compress([False, True], a, axis=1)\n    array([[2],\n           [4],\n           [6]])\n\n    Working on the flattened array does not return slices along an axis but\n    selects elements.\n\n    >>> np.compress([False, True], a)\n    array([2])\n\n    \"\"\"\n    return _wrapfunc(a, 'compress', condition, axis=axis, out=out)\n\n\ndef _clip_dispatcher(a, a_min, a_max, out=None, **kwargs):\n    return (a, a_min, a_max)\n\n\n@array_function_dispatch(_clip_dispatcher)\ndef clip(a, a_min, a_max, out=None, **kwargs):\n    \"\"\"\n    Clip (limit) the values in an array.\n\n    Given an interval, values outside the interval are clipped to\n    the interval edges.  For example, if an interval of ``[0, 1]``\n    is specified, values smaller than 0 become 0, and values larger\n    than 1 become 1.\n\n    Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``.\n\n    No check is performed to ensure ``a_min < a_max``.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing elements to clip.\n    a_min, a_max : array_like or None\n        Minimum and maximum value. If ``None``, clipping is not performed on\n        the corresponding edge. Only one of `a_min` and `a_max` may be\n        ``None``. Both are broadcast against `a`.\n    out : ndarray, optional\n        The results will be placed in this array. It may be the input\n        array for in-place clipping.  `out` must be of the right shape\n        to hold the output.  Its type is preserved.\n    **kwargs\n        For other keyword-only arguments, see the\n        :ref:`ufunc docs <ufuncs.kwargs>`.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    clipped_array : ndarray\n        An array with the elements of `a`, but where values\n        < `a_min` are replaced with `a_min`, and those > `a_max`\n        with `a_max`.\n\n    See Also\n    --------\n    :ref:`ufuncs-output-type`\n\n    Examples\n    --------\n    >>> a = np.arange(10)\n    >>> np.clip(a, 1, 8)\n    array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])\n    >>> a\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> np.clip(a, 3, 6, out=a)\n    array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])\n    >>> a = np.arange(10)\n    >>> a\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8)\n    array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])\n\n    \"\"\"\n    return _wrapfunc(a, 'clip', a_min, a_max, out=out, **kwargs)\n\n\ndef _sum_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n                    initial=None, where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_sum_dispatcher)\ndef sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n        initial=np._NoValue, where=np._NoValue):\n    \"\"\"\n    Sum of array elements over a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Elements to sum.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a sum is performed.  The default,\n        axis=None, will sum all of the elements of the input array.  If\n        axis is negative it counts from the last to the first axis.\n\n        .. versionadded:: 1.7.0\n\n        If axis is a tuple of ints, a sum is performed on all of the axes\n        specified in the tuple instead of a single axis or all the axes as\n        before.\n    dtype : dtype, optional\n        The type of the returned array and of the accumulator in which the\n        elements are summed.  The dtype of `a` is used by default unless `a`\n        has an integer dtype of less precision than the default platform\n        integer.  In that case, if `a` is signed then the platform integer\n        is used while if `a` is unsigned then an unsigned integer of the\n        same precision as the platform integer is used.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must have\n        the same shape as the expected output, but the type of the output\n        values will be cast if necessary.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `sum` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n    initial : scalar, optional\n        Starting value for the sum. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.15.0\n\n    where : array_like of bool, optional\n        Elements to include in the sum. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    sum_along_axis : ndarray\n        An array with the same shape as `a`, with the specified\n        axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar\n        is returned.  If an output array is specified, a reference to\n        `out` is returned.\n\n    See Also\n    --------\n    ndarray.sum : Equivalent method.\n\n    add.reduce : Equivalent functionality of `add`.\n\n    cumsum : Cumulative sum of array elements.\n\n    trapz : Integration of array values using the composite trapezoidal rule.\n\n    mean, average\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.\n\n    The sum of an empty array is the neutral element 0:\n\n    >>> np.sum([])\n    0.0\n\n    For floating point numbers the numerical precision of sum (and\n    ``np.add.reduce``) is in general limited by directly adding each number\n    individually to the result causing rounding errors in every step.\n    However, often numpy will use a  numerically better approach (partial\n    pairwise summation) leading to improved precision in many use-cases.\n    This improved precision is always provided when no ``axis`` is given.\n    When ``axis`` is given, it will depend on which axis is summed.\n    Technically, to provide the best speed possible, the improved precision\n    is only used when the summation is along the fast axis in memory.\n    Note that the exact precision may vary depending on other parameters.\n    In contrast to NumPy, Python's ``math.fsum`` function uses a slower but\n    more precise approach to summation.\n    Especially when summing a large number of lower precision floating point\n    numbers, such as ``float32``, numerical errors can become significant.\n    In such cases it can be advisable to use `dtype=\"float64\"` to use a higher\n    precision for the output.\n\n    Examples\n    --------\n    >>> np.sum([0.5, 1.5])\n    2.0\n    >>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n    1\n    >>> np.sum([[0, 1], [0, 5]])\n    6\n    >>> np.sum([[0, 1], [0, 5]], axis=0)\n    array([0, 6])\n    >>> np.sum([[0, 1], [0, 5]], axis=1)\n    array([1, 5])\n    >>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\n    array([1., 5.])\n\n    If the accumulator is too small, overflow occurs:\n\n    >>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n    -128\n\n    You can also start the sum with a value other than zero:\n\n    >>> np.sum([10], initial=5)\n    15\n    \"\"\"\n    if isinstance(a, _gentype):\n        # 2018-02-25, 1.15.0\n        warnings.warn(\n            \"Calling np.sum(generator) is deprecated, and in the future will give a different result. \"\n            \"Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\",\n            DeprecationWarning, stacklevel=3)\n\n        res = _sum_(a)\n        if out is not None:\n            out[...] = res\n            return out\n        return res\n\n    return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n                          initial=initial, where=where)\n\n\ndef _any_dispatcher(a, axis=None, out=None, keepdims=None, *,\n                    where=np._NoValue):\n    return (a, where, out)\n\n\n@array_function_dispatch(_any_dispatcher)\ndef any(a, axis=None, out=None, keepdims=np._NoValue, *, where=np._NoValue):\n    \"\"\"\n    Test whether any array element along a given axis evaluates to True.\n\n    Returns single boolean unless `axis` is not ``None``\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a logical OR reduction is performed.\n        The default (``axis=None``) is to perform a logical OR over all\n        the dimensions of the input array. `axis` may be negative, in\n        which case it counts from the last to the first axis.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a reduction is performed on multiple\n        axes, instead of a single axis or all the axes as before.\n    out : ndarray, optional\n        Alternate output array in which to place the result.  It must have\n        the same shape as the expected output and its type is preserved\n        (e.g., if it is of type float, then it will remain so, returning\n        1.0 for True and 0.0 for False, regardless of the type of `a`).\n        See :ref:`ufuncs-output-type` for more details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `any` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in checking for any `True` values.\n        See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    any : bool or ndarray\n        A new boolean or `ndarray` is returned unless `out` is specified,\n        in which case a reference to `out` is returned.\n\n    See Also\n    --------\n    ndarray.any : equivalent method\n\n    all : Test whether all elements along a given axis evaluate to True.\n\n    Notes\n    -----\n    Not a Number (NaN), positive infinity and negative infinity evaluate\n    to `True` because these are not equal to zero.\n\n    Examples\n    --------\n    >>> np.any([[True, False], [True, True]])\n    True\n\n    >>> np.any([[True, False], [False, False]], axis=0)\n    array([ True, False])\n\n    >>> np.any([-1, 0, 5])\n    True\n\n    >>> np.any(np.nan)\n    True\n\n    >>> np.any([[True, False], [False, False]], where=[[False], [True]])\n    False\n\n    >>> o=np.array(False)\n    >>> z=np.any([-1, 4, 5], out=o)\n    >>> z, o\n    (array(True), array(True))\n    >>> # Check now that z is a reference to o\n    >>> z is o\n    True\n    >>> id(z), id(o) # identity of z and o              # doctest: +SKIP\n    (191614240, 191614240)\n\n    \"\"\"\n    return _wrapreduction(a, np.logical_or, 'any', axis, None, out,\n                          keepdims=keepdims, where=where)\n\n\ndef _all_dispatcher(a, axis=None, out=None, keepdims=None, *,\n                    where=None):\n    return (a, where, out)\n\n\n@array_function_dispatch(_all_dispatcher)\ndef all(a, axis=None, out=None, keepdims=np._NoValue, *, where=np._NoValue):\n    \"\"\"\n    Test whether all array elements along a given axis evaluate to True.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a logical AND reduction is performed.\n        The default (``axis=None``) is to perform a logical AND over all\n        the dimensions of the input array. `axis` may be negative, in\n        which case it counts from the last to the first axis.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a reduction is performed on multiple\n        axes, instead of a single axis or all the axes as before.\n    out : ndarray, optional\n        Alternate output array in which to place the result.\n        It must have the same shape as the expected output and its\n        type is preserved (e.g., if ``dtype(out)`` is float, the result\n        will consist of 0.0's and 1.0's). See :ref:`ufuncs-output-type` for more\n        details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `all` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in checking for all `True` values.\n        See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    all : ndarray, bool\n        A new boolean or array is returned unless `out` is specified,\n        in which case a reference to `out` is returned.\n\n    See Also\n    --------\n    ndarray.all : equivalent method\n\n    any : Test whether any element along a given axis evaluates to True.\n\n    Notes\n    -----\n    Not a Number (NaN), positive infinity and negative infinity\n    evaluate to `True` because these are not equal to zero.\n\n    Examples\n    --------\n    >>> np.all([[True,False],[True,True]])\n    False\n\n    >>> np.all([[True,False],[True,True]], axis=0)\n    array([ True, False])\n\n    >>> np.all([-1, 4, 5])\n    True\n\n    >>> np.all([1.0, np.nan])\n    True\n\n    >>> np.all([[True, True], [False, True]], where=[[True], [False]])\n    True\n\n    >>> o=np.array(False)\n    >>> z=np.all([-1, 4, 5], out=o)\n    >>> id(z), id(o), z\n    (28293632, 28293632, array(True)) # may vary\n\n    \"\"\"\n    return _wrapreduction(a, np.logical_and, 'all', axis, None, out,\n                          keepdims=keepdims, where=where)\n\n\ndef _cumsum_dispatcher(a, axis=None, dtype=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_cumsum_dispatcher)\ndef cumsum(a, axis=None, dtype=None, out=None):\n    \"\"\"\n    Return the cumulative sum of the elements along a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int, optional\n        Axis along which the cumulative sum is computed. The default\n        (None) is to compute the cumsum over the flattened array.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed.  If `dtype` is not specified, it defaults\n        to the dtype of `a`, unless `a` has an integer dtype with a\n        precision less than that of the default platform integer.  In\n        that case, the default platform integer is used.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output\n        but the type will be cast if necessary. See :ref:`ufuncs-output-type` for\n        more details.\n\n    Returns\n    -------\n    cumsum_along_axis : ndarray.\n        A new array holding the result is returned unless `out` is\n        specified, in which case a reference to `out` is returned. The\n        result has the same size as `a`, and the same shape as `a` if\n        `axis` is not None or `a` is a 1-d array.\n\n\n    See Also\n    --------\n    sum : Sum array elements.\n\n    trapz : Integration of array values using the composite trapezoidal rule.\n\n    diff :  Calculate the n-th discrete difference along given axis.\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.\n\n    Examples\n    --------\n    >>> a = np.array([[1,2,3], [4,5,6]])\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.cumsum(a)\n    array([ 1,  3,  6, 10, 15, 21])\n    >>> np.cumsum(a, dtype=float)     # specifies type of output value(s)\n    array([  1.,   3.,   6.,  10.,  15.,  21.])\n\n    >>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\n    array([[1, 2, 3],\n           [5, 7, 9]])\n    >>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\n    array([[ 1,  3,  6],\n           [ 4,  9, 15]])\n\n    \"\"\"\n    return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out)\n\n\ndef _ptp_dispatcher(a, axis=None, out=None, keepdims=None):\n    return (a, out)\n\n\n@array_function_dispatch(_ptp_dispatcher)\ndef ptp(a, axis=None, out=None, keepdims=np._NoValue):\n    \"\"\"\n    Range of values (maximum - minimum) along an axis.\n\n    The name of the function comes from the acronym for 'peak to peak'.\n\n    .. warning::\n        `ptp` preserves the data type of the array. This means the\n        return value for an input of signed integers with n bits\n        (e.g. `np.int8`, `np.int16`, etc) is also a signed integer\n        with n bits.  In that case, peak-to-peak values greater than\n        ``2**(n-1)-1`` will be returned as negative values. An example\n        with a work-around is shown below.\n\n    Parameters\n    ----------\n    a : array_like\n        Input values.\n    axis : None or int or tuple of ints, optional\n        Axis along which to find the peaks.  By default, flatten the\n        array.  `axis` may be negative, in\n        which case it counts from the last to the first axis.\n\n        .. versionadded:: 1.15.0\n\n        If this is a tuple of ints, a reduction is performed on multiple\n        axes, instead of a single axis or all the axes as before.\n    out : array_like\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output,\n        but the type of the output values will be cast if necessary.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `ptp` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    Returns\n    -------\n    ptp : ndarray\n        A new array holding the result, unless `out` was\n        specified, in which case a reference to `out` is returned.\n\n    Examples\n    --------\n    >>> x = np.array([[4, 9, 2, 10],\n    ...               [6, 9, 7, 12]])\n\n    >>> np.ptp(x, axis=1)\n    array([8, 6])\n\n    >>> np.ptp(x, axis=0)\n    array([2, 0, 5, 2])\n\n    >>> np.ptp(x)\n    10\n\n    This example shows that a negative value can be returned when\n    the input is an array of signed integers.\n\n    >>> y = np.array([[1, 127],\n    ...               [0, 127],\n    ...               [-1, 127],\n    ...               [-2, 127]], dtype=np.int8)\n    >>> np.ptp(y, axis=1)\n    array([ 126,  127, -128, -127], dtype=int8)\n\n    A work-around is to use the `view()` method to view the result as\n    unsigned integers with the same bit width:\n\n    >>> np.ptp(y, axis=1).view(np.uint8)\n    array([126, 127, 128, 129], dtype=uint8)\n\n    \"\"\"\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if type(a) is not mu.ndarray:\n        try:\n            ptp = a.ptp\n        except AttributeError:\n            pass\n        else:\n            return ptp(axis=axis, out=out, **kwargs)\n    return _methods._ptp(a, axis=axis, out=out, **kwargs)\n\n\ndef _amax_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n                     where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_amax_dispatcher)\ndef amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n         where=np._NoValue):\n    \"\"\"\n    Return the maximum of an array or maximum along an axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which to operate.  By default, flattened input is\n        used.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, the maximum is selected over multiple axes,\n        instead of a single axis or all the axes as before.\n    out : ndarray, optional\n        Alternative output array in which to place the result.  Must\n        be of the same shape and buffer length as the expected output.\n        See :ref:`ufuncs-output-type` for more details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `amax` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    initial : scalar, optional\n        The minimum value of an output element. Must be present to allow\n        computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.15.0\n\n    where : array_like of bool, optional\n        Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n        for details.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    amax : ndarray or scalar\n        Maximum of `a`. If `axis` is None, the result is a scalar value.\n        If `axis` is given, the result is an array of dimension\n        ``a.ndim - 1``.\n\n    See Also\n    --------\n    amin :\n        The minimum value of an array along a given axis, propagating any NaNs.\n    nanmax :\n        The maximum value of an array along a given axis, ignoring any NaNs.\n    maximum :\n        Element-wise maximum of two arrays, propagating any NaNs.\n    fmax :\n        Element-wise maximum of two arrays, ignoring any NaNs.\n    argmax :\n        Return the indices of the maximum values.\n\n    nanmin, minimum, fmin\n\n    Notes\n    -----\n    NaN values are propagated, that is if at least one item is NaN, the\n    corresponding max value will be NaN as well. To ignore NaN values\n    (MATLAB behavior), please use nanmax.\n\n    Don't use `amax` for element-wise comparison of 2 arrays; when\n    ``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n    ``amax(a, axis=0)``.\n\n    Examples\n    --------\n    >>> a = np.arange(4).reshape((2,2))\n    >>> a\n    array([[0, 1],\n           [2, 3]])\n    >>> np.amax(a)           # Maximum of the flattened array\n    3\n    >>> np.amax(a, axis=0)   # Maxima along the first axis\n    array([2, 3])\n    >>> np.amax(a, axis=1)   # Maxima along the second axis\n    array([1, 3])\n    >>> np.amax(a, where=[False, True], initial=-1, axis=0)\n    array([-1,  3])\n    >>> b = np.arange(5, dtype=float)\n    >>> b[2] = np.NaN\n    >>> np.amax(b)\n    nan\n    >>> np.amax(b, where=~np.isnan(b), initial=-1)\n    4.0\n    >>> np.nanmax(b)\n    4.0\n\n    You can use an initial value to compute the maximum of an empty slice, or\n    to initialize it to a different value:\n\n    >>> np.max([[-50], [10]], axis=-1, initial=0)\n    array([ 0, 10])\n\n    Notice that the initial value is used as one of the elements for which the\n    maximum is determined, unlike for the default argument Python's max\n    function, which is only used for empty iterables.\n\n    >>> np.max([5], initial=6)\n    6\n    >>> max([5], default=6)\n    5\n    \"\"\"\n    return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n                          keepdims=keepdims, initial=initial, where=where)\n\n\ndef _amin_dispatcher(a, axis=None, out=None, keepdims=None, initial=None,\n                     where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_amin_dispatcher)\ndef amin(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,\n         where=np._NoValue):\n    \"\"\"\n    Return the minimum of an array or minimum along an axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which to operate.  By default, flattened input is\n        used.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, the minimum is selected over multiple axes,\n        instead of a single axis or all the axes as before.\n    out : ndarray, optional\n        Alternative output array in which to place the result.  Must\n        be of the same shape and buffer length as the expected output.\n        See :ref:`ufuncs-output-type` for more details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `amin` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    initial : scalar, optional\n        The maximum value of an output element. Must be present to allow\n        computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.15.0\n\n    where : array_like of bool, optional\n        Elements to compare for the minimum. See `~numpy.ufunc.reduce`\n        for details.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    amin : ndarray or scalar\n        Minimum of `a`. If `axis` is None, the result is a scalar value.\n        If `axis` is given, the result is an array of dimension\n        ``a.ndim - 1``.\n\n    See Also\n    --------\n    amax :\n        The maximum value of an array along a given axis, propagating any NaNs.\n    nanmin :\n        The minimum value of an array along a given axis, ignoring any NaNs.\n    minimum :\n        Element-wise minimum of two arrays, propagating any NaNs.\n    fmin :\n        Element-wise minimum of two arrays, ignoring any NaNs.\n    argmin :\n        Return the indices of the minimum values.\n\n    nanmax, maximum, fmax\n\n    Notes\n    -----\n    NaN values are propagated, that is if at least one item is NaN, the\n    corresponding min value will be NaN as well. To ignore NaN values\n    (MATLAB behavior), please use nanmin.\n\n    Don't use `amin` for element-wise comparison of 2 arrays; when\n    ``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n    ``amin(a, axis=0)``.\n\n    Examples\n    --------\n    >>> a = np.arange(4).reshape((2,2))\n    >>> a\n    array([[0, 1],\n           [2, 3]])\n    >>> np.amin(a)           # Minimum of the flattened array\n    0\n    >>> np.amin(a, axis=0)   # Minima along the first axis\n    array([0, 1])\n    >>> np.amin(a, axis=1)   # Minima along the second axis\n    array([0, 2])\n    >>> np.amin(a, where=[False, True], initial=10, axis=0)\n    array([10,  1])\n\n    >>> b = np.arange(5, dtype=float)\n    >>> b[2] = np.NaN\n    >>> np.amin(b)\n    nan\n    >>> np.amin(b, where=~np.isnan(b), initial=10)\n    0.0\n    >>> np.nanmin(b)\n    0.0\n\n    >>> np.min([[-50], [10]], axis=-1, initial=0)\n    array([-50,   0])\n\n    Notice that the initial value is used as one of the elements for which the\n    minimum is determined, unlike for the default argument Python's max\n    function, which is only used for empty iterables.\n\n    Notice that this isn't the same as Python's ``default`` argument.\n\n    >>> np.min([6], initial=5)\n    5\n    >>> min([6], default=5)\n    6\n    \"\"\"\n    return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n                          keepdims=keepdims, initial=initial, where=where)\n\n\ndef _alen_dispathcer(a):\n    return (a,)\n\n\n@array_function_dispatch(_alen_dispathcer)\ndef alen(a):\n    \"\"\"\n    Return the length of the first dimension of the input array.\n\n    .. deprecated:: 1.18\n       `numpy.alen` is deprecated, use `len` instead.\n\n    Parameters\n    ----------\n    a : array_like\n       Input array.\n\n    Returns\n    -------\n    alen : int\n       Length of the first dimension of `a`.\n\n    See Also\n    --------\n    shape, size\n\n    Examples\n    --------\n    >>> a = np.zeros((7,4,5))\n    >>> a.shape[0]\n    7\n    >>> np.alen(a)\n    7\n\n    \"\"\"\n    # NumPy 1.18.0, 2019-08-02\n    warnings.warn(\n        \"`np.alen` is deprecated, use `len` instead\",\n        DeprecationWarning, stacklevel=2)\n    try:\n        return len(a)\n    except TypeError:\n        return len(array(a, ndmin=1))\n\n\ndef _prod_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n                     initial=None, where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_prod_dispatcher)\ndef prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n         initial=np._NoValue, where=np._NoValue):\n    \"\"\"\n    Return the product of array elements over a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which a product is performed.  The default,\n        axis=None, will calculate the product of all the elements in the\n        input array. If axis is negative it counts from the last to the\n        first axis.\n\n        .. versionadded:: 1.7.0\n\n        If axis is a tuple of ints, a product is performed on all of the\n        axes specified in the tuple instead of a single axis or all the\n        axes as before.\n    dtype : dtype, optional\n        The type of the returned array, as well as of the accumulator in\n        which the elements are multiplied.  The dtype of `a` is used by\n        default unless `a` has an integer dtype of less precision than the\n        default platform integer.  In that case, if `a` is signed then the\n        platform integer is used while if `a` is unsigned then an unsigned\n        integer of the same precision as the platform integer is used.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must have\n        the same shape as the expected output, but the type of the output\n        values will be cast if necessary.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left in the\n        result as dimensions with size one. With this option, the result\n        will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `prod` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n    initial : scalar, optional\n        The starting value for this product. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.15.0\n\n    where : array_like of bool, optional\n        Elements to include in the product. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    product_along_axis : ndarray, see `dtype` parameter above.\n        An array shaped as `a` but with the specified axis removed.\n        Returns a reference to `out` if specified.\n\n    See Also\n    --------\n    ndarray.prod : equivalent method\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.  That means that, on a 32-bit platform:\n\n    >>> x = np.array([536870910, 536870910, 536870910, 536870910])\n    >>> np.prod(x)\n    16 # may vary\n\n    The product of an empty array is the neutral element 1:\n\n    >>> np.prod([])\n    1.0\n\n    Examples\n    --------\n    By default, calculate the product of all elements:\n\n    >>> np.prod([1.,2.])\n    2.0\n\n    Even when the input array is two-dimensional:\n\n    >>> np.prod([[1.,2.],[3.,4.]])\n    24.0\n\n    But we can also specify the axis over which to multiply:\n\n    >>> np.prod([[1.,2.],[3.,4.]], axis=1)\n    array([  2.,  12.])\n\n    Or select specific elements to include:\n\n    >>> np.prod([1., np.nan, 3.], where=[True, False, True])\n    3.0\n\n    If the type of `x` is unsigned, then the output type is\n    the unsigned platform integer:\n\n    >>> x = np.array([1, 2, 3], dtype=np.uint8)\n    >>> np.prod(x).dtype == np.uint\n    True\n\n    If `x` is of a signed integer type, then the output type\n    is the default platform integer:\n\n    >>> x = np.array([1, 2, 3], dtype=np.int8)\n    >>> np.prod(x).dtype == int\n    True\n\n    You can also start the product with a value other than one:\n\n    >>> np.prod([1, 2], initial=5)\n    10\n    \"\"\"\n    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n                          keepdims=keepdims, initial=initial, where=where)\n\n\ndef _cumprod_dispatcher(a, axis=None, dtype=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_cumprod_dispatcher)\ndef cumprod(a, axis=None, dtype=None, out=None):\n    \"\"\"\n    Return the cumulative product of elements along a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int, optional\n        Axis along which the cumulative product is computed.  By default\n        the input is flattened.\n    dtype : dtype, optional\n        Type of the returned array, as well as of the accumulator in which\n        the elements are multiplied.  If *dtype* is not specified, it\n        defaults to the dtype of `a`, unless `a` has an integer dtype with\n        a precision less than that of the default platform integer.  In\n        that case, the default platform integer is used instead.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output\n        but the type of the resulting values will be cast if necessary.\n\n    Returns\n    -------\n    cumprod : ndarray\n        A new array holding the result is returned unless `out` is\n        specified, in which case a reference to out is returned.\n\n    See Also\n    --------\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.\n\n    Examples\n    --------\n    >>> a = np.array([1,2,3])\n    >>> np.cumprod(a) # intermediate results 1, 1*2\n    ...               # total product 1*2*3 = 6\n    array([1, 2, 6])\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> np.cumprod(a, dtype=float) # specify type of output\n    array([   1.,    2.,    6.,   24.,  120.,  720.])\n\n    The cumulative product for each column (i.e., over the rows) of `a`:\n\n    >>> np.cumprod(a, axis=0)\n    array([[ 1,  2,  3],\n           [ 4, 10, 18]])\n\n    The cumulative product for each row (i.e. over the columns) of `a`:\n\n    >>> np.cumprod(a,axis=1)\n    array([[  1,   2,   6],\n           [  4,  20, 120]])\n\n    \"\"\"\n    return _wrapfunc(a, 'cumprod', axis=axis, dtype=dtype, out=out)\n\n\ndef _ndim_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_ndim_dispatcher)\ndef ndim(a):\n    \"\"\"\n    Return the number of dimensions of an array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.  If it is not already an ndarray, a conversion is\n        attempted.\n\n    Returns\n    -------\n    number_of_dimensions : int\n        The number of dimensions in `a`.  Scalars are zero-dimensional.\n\n    See Also\n    --------\n    ndarray.ndim : equivalent method\n    shape : dimensions of array\n    ndarray.shape : dimensions of array\n\n    Examples\n    --------\n    >>> np.ndim([[1,2,3],[4,5,6]])\n    2\n    >>> np.ndim(np.array([[1,2,3],[4,5,6]]))\n    2\n    >>> np.ndim(1)\n    0\n\n    \"\"\"\n    try:\n        return a.ndim\n    except AttributeError:\n        return asarray(a).ndim\n\n\ndef _size_dispatcher(a, axis=None):\n    return (a,)\n\n\n@array_function_dispatch(_size_dispatcher)\ndef size(a, axis=None):\n    \"\"\"\n    Return the number of elements along a given axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : int, optional\n        Axis along which the elements are counted.  By default, give\n        the total number of elements.\n\n    Returns\n    -------\n    element_count : int\n        Number of elements along the specified axis.\n\n    See Also\n    --------\n    shape : dimensions of array\n    ndarray.shape : dimensions of array\n    ndarray.size : number of elements in array\n\n    Examples\n    --------\n    >>> a = np.array([[1,2,3],[4,5,6]])\n    >>> np.size(a)\n    6\n    >>> np.size(a,1)\n    3\n    >>> np.size(a,0)\n    2\n\n    \"\"\"\n    if axis is None:\n        try:\n            return a.size\n        except AttributeError:\n            return asarray(a).size\n    else:\n        try:\n            return a.shape[axis]\n        except AttributeError:\n            return asarray(a).shape[axis]\n\n\ndef _around_dispatcher(a, decimals=None, out=None):\n    return (a, out)\n\n\n@array_function_dispatch(_around_dispatcher)\ndef around(a, decimals=0, out=None):\n    \"\"\"\n    Evenly round to the given number of decimals.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    decimals : int, optional\n        Number of decimal places to round to (default: 0).  If\n        decimals is negative, it specifies the number of positions to\n        the left of the decimal point.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must have\n        the same shape as the expected output, but the type of the output\n        values will be cast if necessary. See :ref:`ufuncs-output-type` for more\n        details.\n\n    Returns\n    -------\n    rounded_array : ndarray\n        An array of the same type as `a`, containing the rounded values.\n        Unless `out` was specified, a new array is created.  A reference to\n        the result is returned.\n\n        The real and imaginary parts of complex numbers are rounded\n        separately.  The result of rounding a float is a float.\n\n    See Also\n    --------\n    ndarray.round : equivalent method\n\n    ceil, fix, floor, rint, trunc\n\n\n    Notes\n    -----\n    For values exactly halfway between rounded decimal values, NumPy\n    rounds to the nearest even value. Thus 1.5 and 2.5 round to 2.0,\n    -0.5 and 0.5 round to 0.0, etc.\n\n    ``np.around`` uses a fast but sometimes inexact algorithm to round\n    floating-point datatypes. For positive `decimals` it is equivalent to\n    ``np.true_divide(np.rint(a * 10**decimals), 10**decimals)``, which has\n    error due to the inexact representation of decimal fractions in the IEEE\n    floating point standard [1]_ and errors introduced when scaling by powers\n    of ten. For instance, note the extra \"1\" in the following:\n\n        >>> np.round(56294995342131.5, 3)\n        56294995342131.51\n\n    If your goal is to print such values with a fixed number of decimals, it is\n    preferable to use numpy's float printing routines to limit the number of\n    printed decimals:\n\n        >>> np.format_float_positional(56294995342131.5, precision=3)\n        '56294995342131.5'\n\n    The float printing routines use an accurate but much more computationally\n    demanding algorithm to compute the number of digits after the decimal\n    point.\n\n    Alternatively, Python's builtin `round` function uses a more accurate\n    but slower algorithm for 64-bit floating point values:\n\n        >>> round(56294995342131.5, 3)\n        56294995342131.5\n        >>> np.round(16.055, 2), round(16.055, 2)  # equals 16.0549999999999997\n        (16.06, 16.05)\n\n\n    References\n    ----------\n    .. [1] \"Lecture Notes on the Status of IEEE 754\", William Kahan,\n           https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF\n    .. [2] \"How Futile are Mindless Assessments of\n           Roundoff in Floating-Point Computation?\", William Kahan,\n           https://people.eecs.berkeley.edu/~wkahan/Mindless.pdf\n\n    Examples\n    --------\n    >>> np.around([0.37, 1.64])\n    array([0.,  2.])\n    >>> np.around([0.37, 1.64], decimals=1)\n    array([0.4,  1.6])\n    >>> np.around([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value\n    array([0.,  2.,  2.,  4.,  4.])\n    >>> np.around([1,2,3,11], decimals=1) # ndarray of ints is returned\n    array([ 1,  2,  3, 11])\n    >>> np.around([1,2,3,11], decimals=-1)\n    array([ 0,  0,  0, 10])\n\n    \"\"\"\n    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n\n\ndef _mean_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None, *,\n                     where=None):\n    return (a, where, out)\n\n\n@array_function_dispatch(_mean_dispatcher)\ndef mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,\n         where=np._NoValue):\n    \"\"\"\n    Compute the arithmetic mean along the specified axis.\n\n    Returns the average of the array elements.  The average is taken over\n    the flattened array by default, otherwise over the specified axis.\n    `float64` intermediate and return values are used for integer inputs.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose mean is desired. If `a` is not an\n        array, a conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the means are computed. The default is to\n        compute the mean of the flattened array.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a mean is performed over multiple axes,\n        instead of a single axis or all the axes as before.\n    dtype : data-type, optional\n        Type to use in computing the mean.  For integer inputs, the default\n        is `float64`; for floating point inputs, it is the same as the\n        input dtype.\n    out : ndarray, optional\n        Alternate output array in which to place the result.  The default\n        is ``None``; if provided, it must have the same shape as the\n        expected output, but the type will be cast if necessary.\n        See :ref:`ufuncs-output-type` for more details.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `mean` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    m : ndarray, see dtype parameter above\n        If `out=None`, returns a new array containing the mean values,\n        otherwise a reference to the output array is returned.\n\n    See Also\n    --------\n    average : Weighted average\n    std, var, nanmean, nanstd, nanvar\n\n    Notes\n    -----\n    The arithmetic mean is the sum of the elements along the axis divided\n    by the number of elements.\n\n    Note that for floating-point input, the mean is computed using the\n    same precision the input has.  Depending on the input data, this can\n    cause the results to be inaccurate, especially for `float32` (see\n    example below).  Specifying a higher-precision accumulator using the\n    `dtype` keyword can alleviate this issue.\n\n    By default, `float16` results are computed using `float32` intermediates\n    for extra precision.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> np.mean(a)\n    2.5\n    >>> np.mean(a, axis=0)\n    array([2., 3.])\n    >>> np.mean(a, axis=1)\n    array([1.5, 3.5])\n\n    In single precision, `mean` can be inaccurate:\n\n    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> np.mean(a)\n    0.54999924\n\n    Computing the mean in float64 is more accurate:\n\n    >>> np.mean(a, dtype=np.float64)\n    0.55000000074505806 # may vary\n\n    Specifying a where argument:\n    >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n    >>> np.mean(a)\n    12.0\n    >>> np.mean(a, where=[[True], [False], [False]])\n    9.0\n\n    \"\"\"\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n    if type(a) is not mu.ndarray:\n        try:\n            mean = a.mean\n        except AttributeError:\n            pass\n        else:\n            return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\n    return _methods._mean(a, axis=axis, dtype=dtype,\n                          out=out, **kwargs)\n\n\ndef _std_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n                    keepdims=None, *, where=None):\n    return (a, where, out)\n\n\n@array_function_dispatch(_std_dispatcher)\ndef std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n        where=np._NoValue):\n    \"\"\"\n    Compute the standard deviation along the specified axis.\n\n    Returns the standard deviation, a measure of the spread of a distribution,\n    of the array elements. The standard deviation is computed for the\n    flattened array by default, otherwise over the specified axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Calculate the standard deviation of these values.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the standard deviation is computed. The\n        default is to compute the standard deviation of the flattened array.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a standard deviation is performed over\n        multiple axes, instead of a single axis or all the axes as before.\n    dtype : dtype, optional\n        Type to use in computing the standard deviation. For arrays of\n        integer type the default is float64, for arrays of float types it is\n        the same as the array type.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must have\n        the same shape as the expected output but the type (of the calculated\n        values) will be cast if necessary.\n    ddof : int, optional\n        Means Delta Degrees of Freedom.  The divisor used in calculations\n        is ``N - ddof``, where ``N`` represents the number of elements.\n        By default `ddof` is zero.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `std` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in the standard deviation.\n        See `~numpy.ufunc.reduce` for details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    standard_deviation : ndarray, see dtype parameter above.\n        If `out` is None, return a new array containing the standard deviation,\n        otherwise return a reference to the output array.\n\n    See Also\n    --------\n    var, mean, nanmean, nanstd, nanvar\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    The standard deviation is the square root of the average of the squared\n    deviations from the mean, i.e., ``std = sqrt(mean(x))``, where\n    ``x = abs(a - a.mean())**2``.\n\n    The average squared deviation is typically calculated as ``x.sum() / N``,\n    where ``N = len(x)``. If, however, `ddof` is specified, the divisor\n    ``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``\n    provides an unbiased estimator of the variance of the infinite population.\n    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n    normally distributed variables. The standard deviation computed in this\n    function is the square root of the estimated variance, so even with\n    ``ddof=1``, it will not be an unbiased estimate of the standard deviation\n    per se.\n\n    Note that, for complex numbers, `std` takes the absolute\n    value before squaring, so that the result is always real and nonnegative.\n\n    For floating-point input, the *std* is computed using the same\n    precision the input has. Depending on the input data, this can cause\n    the results to be inaccurate, especially for float32 (see example below).\n    Specifying a higher-accuracy accumulator using the `dtype` keyword can\n    alleviate this issue.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> np.std(a)\n    1.1180339887498949 # may vary\n    >>> np.std(a, axis=0)\n    array([1.,  1.])\n    >>> np.std(a, axis=1)\n    array([0.5,  0.5])\n\n    In single precision, std() can be inaccurate:\n\n    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> np.std(a)\n    0.45000005\n\n    Computing the standard deviation in float64 is more accurate:\n\n    >>> np.std(a, dtype=np.float64)\n    0.44999999925494177 # may vary\n\n    Specifying a where argument:\n\n    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n    >>> np.std(a)\n    2.614064523559687 # may vary\n    >>> np.std(a, where=[[True], [True], [False]])\n    2.0\n\n    \"\"\"\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n    if type(a) is not mu.ndarray:\n        try:\n            std = a.std\n        except AttributeError:\n            pass\n        else:\n            return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)\n\n\ndef _var_dispatcher(a, axis=None, dtype=None, out=None, ddof=None,\n                    keepdims=None, *, where=None):\n    return (a, where, out)\n\n\n@array_function_dispatch(_var_dispatcher)\ndef var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=np._NoValue, *,\n        where=np._NoValue):\n    \"\"\"\n    Compute the variance along the specified axis.\n\n    Returns the variance of the array elements, a measure of the spread of a\n    distribution.  The variance is computed for the flattened array by\n    default, otherwise over the specified axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose variance is desired.  If `a` is not an\n        array, a conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the variance is computed.  The default is to\n        compute the variance of the flattened array.\n\n        .. versionadded:: 1.7.0\n\n        If this is a tuple of ints, a variance is performed over multiple axes,\n        instead of a single axis or all the axes as before.\n    dtype : data-type, optional\n        Type to use in computing the variance.  For arrays of integer type\n        the default is `float64`; for arrays of float types it is the same as\n        the array type.\n    out : ndarray, optional\n        Alternate output array in which to place the result.  It must have\n        the same shape as the expected output, but the type is cast if\n        necessary.\n    ddof : int, optional\n        \"Delta Degrees of Freedom\": the divisor used in the calculation is\n        ``N - ddof``, where ``N`` represents the number of elements. By\n        default `ddof` is zero.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        If the default value is passed, then `keepdims` will not be\n        passed through to the `var` method of sub-classes of\n        `ndarray`, however any non-default value will be.  If the\n        sub-class' method does not implement `keepdims` any\n        exceptions will be raised.\n\n    where : array_like of bool, optional\n        Elements to include in the variance. See `~numpy.ufunc.reduce` for\n        details.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    variance : ndarray, see dtype parameter above\n        If ``out=None``, returns a new array containing the variance;\n        otherwise, a reference to the output array is returned.\n\n    See Also\n    --------\n    std, mean, nanmean, nanstd, nanvar\n    :ref:`ufuncs-output-type`\n\n    Notes\n    -----\n    The variance is the average of the squared deviations from the mean,\n    i.e.,  ``var = mean(x)``, where ``x = abs(a - a.mean())**2``.\n\n    The mean is typically calculated as ``x.sum() / N``, where ``N = len(x)``.\n    If, however, `ddof` is specified, the divisor ``N - ddof`` is used\n    instead.  In standard statistical practice, ``ddof=1`` provides an\n    unbiased estimator of the variance of a hypothetical infinite population.\n    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n    normally distributed variables.\n\n    Note that for complex numbers, the absolute value is taken before\n    squaring, so that the result is always real and nonnegative.\n\n    For floating-point input, the variance is computed using the same\n    precision the input has.  Depending on the input data, this can cause\n    the results to be inaccurate, especially for `float32` (see example\n    below).  Specifying a higher-accuracy accumulator using the ``dtype``\n    keyword can alleviate this issue.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> np.var(a)\n    1.25\n    >>> np.var(a, axis=0)\n    array([1.,  1.])\n    >>> np.var(a, axis=1)\n    array([0.25,  0.25])\n\n    In single precision, var() can be inaccurate:\n\n    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> np.var(a)\n    0.20250003\n\n    Computing the variance in float64 is more accurate:\n\n    >>> np.var(a, dtype=np.float64)\n    0.20249999932944759 # may vary\n    >>> ((1-0.55)**2 + (0.1-0.55)**2)/2\n    0.2025\n\n    Specifying a where argument:\n\n    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n    >>> np.var(a)\n    6.833333333333333 # may vary\n    >>> np.var(a, where=[[True], [True], [False]])\n    4.0\n\n    \"\"\"\n    kwargs = {}\n    if keepdims is not np._NoValue:\n        kwargs['keepdims'] = keepdims\n    if where is not np._NoValue:\n        kwargs['where'] = where\n\n    if type(a) is not mu.ndarray:\n        try:\n            var = a.var\n\n        except AttributeError:\n            pass\n        else:\n            return var(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\n    return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n                         **kwargs)\n\n\n# Aliases of other functions. These have their own definitions only so that\n# they can have unique docstrings.\n\n@array_function_dispatch(_around_dispatcher)\ndef round_(a, decimals=0, out=None):\n    \"\"\"\n    Round an array to the given number of decimals.\n\n    See Also\n    --------\n    around : equivalent function; see for details.\n    \"\"\"\n    return around(a, decimals=decimals, out=out)\n\n\n@array_function_dispatch(_prod_dispatcher, verify=False)\ndef product(*args, **kwargs):\n    \"\"\"\n    Return the product of array elements over a given axis.\n\n    See Also\n    --------\n    prod : equivalent function; see for details.\n    \"\"\"\n    return prod(*args, **kwargs)\n\n\n@array_function_dispatch(_cumprod_dispatcher, verify=False)\ndef cumproduct(*args, **kwargs):\n    \"\"\"\n    Return the cumulative product over the given axis.\n\n    See Also\n    --------\n    cumprod : equivalent function; see for details.\n    \"\"\"\n    return cumprod(*args, **kwargs)\n\n\n@array_function_dispatch(_any_dispatcher, verify=False)\ndef sometrue(*args, **kwargs):\n    \"\"\"\n    Check whether some values are true.\n\n    Refer to `any` for full documentation.\n\n    See Also\n    --------\n    any : equivalent function; see for details.\n    \"\"\"\n    return any(*args, **kwargs)\n\n\n@array_function_dispatch(_all_dispatcher, verify=False)\ndef alltrue(*args, **kwargs):\n    \"\"\"\n    Check if all elements of input array are true.\n\n    See Also\n    --------\n    numpy.all : Equivalent function; see for details.\n    \"\"\"\n    return all(*args, **kwargs)\n",3768],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py":["\"\"\"\nExtended math utilities.\n\"\"\"\n# Authors: Gael Varoquaux\n#          Alexandre Gramfort\n#          Alexandre T. Passos\n#          Olivier Grisel\n#          Lars Buitinck\n#          Stefan van der Walt\n#          Kyle Kastner\n#          Giorgio Patrini\n# License: BSD 3 clause\n\nimport warnings\n\nimport numpy as np\nfrom scipy import linalg, sparse\n\nfrom . import check_random_state\nfrom ._logistic_sigmoid import _log_logistic_sigmoid\nfrom .fixes import np_version, parse_version\nfrom .sparsefuncs_fast import csr_row_norms\nfrom .validation import check_array\n\n\ndef squared_norm(x):\n    \"\"\"Squared Euclidean or Frobenius norm of x.\n\n    Faster than norm(x) ** 2.\n\n    Parameters\n    ----------\n    x : array-like\n\n    Returns\n    -------\n    float\n        The Euclidean norm when x is a vector, the Frobenius norm when x\n        is a matrix (2-d array).\n    \"\"\"\n    x = np.ravel(x, order='K')\n    if np.issubdtype(x.dtype, np.integer):\n        warnings.warn('Array type is integer, np.dot may overflow. '\n                      'Data should be float type to avoid this issue',\n                      UserWarning)\n    return np.dot(x, x)\n\n\ndef row_norms(X, squared=False):\n    \"\"\"Row-wise (squared) Euclidean norm of X.\n\n    Equivalent to np.sqrt((X * X).sum(axis=1)), but also supports sparse\n    matrices and does not create an X.shape-sized temporary.\n\n    Performs no input validation.\n\n    Parameters\n    ----------\n    X : array-like\n        The input array.\n    squared : bool, default=False\n        If True, return squared norms.\n\n    Returns\n    -------\n    array-like\n        The row-wise (squared) Euclidean norm of X.\n    \"\"\"\n    if sparse.issparse(X):\n        if not isinstance(X, sparse.csr_matrix):\n            X = sparse.csr_matrix(X)\n        norms = csr_row_norms(X)\n    else:\n        norms = np.einsum('ij,ij->i', X, X)\n\n    if not squared:\n        np.sqrt(norms, norms)\n    return norms\n\n\ndef fast_logdet(A):\n    \"\"\"Compute log(det(A)) for A symmetric.\n\n    Equivalent to : np.log(nl.det(A)) but more robust.\n    It returns -Inf if det(A) is non positive or is not defined.\n\n    Parameters\n    ----------\n    A : array-like\n        The matrix.\n    \"\"\"\n    sign, ld = np.linalg.slogdet(A)\n    if not sign > 0:\n        return -np.inf\n    return ld\n\n\ndef density(w, **kwargs):\n    \"\"\"Compute density of a sparse vector.\n\n    Parameters\n    ----------\n    w : array-like\n        The sparse vector.\n\n    Returns\n    -------\n    float\n        The density of w, between 0 and 1.\n    \"\"\"\n    if hasattr(w, \"toarray\"):\n        d = float(w.nnz) / (w.shape[0] * w.shape[1])\n    else:\n        d = 0 if w is None else float((w != 0).sum()) / w.size\n    return d\n\n\ndef safe_sparse_dot(a, b, *, dense_output=False):\n    \"\"\"Dot product that handle the sparse matrix case correctly.\n\n    Parameters\n    ----------\n    a : {ndarray, sparse matrix}\n    b : {ndarray, sparse matrix}\n    dense_output : bool, default=False\n        When False, ``a`` and ``b`` both being sparse will yield sparse output.\n        When True, output will always be a dense array.\n\n    Returns\n    -------\n    dot_product : {ndarray, sparse matrix}\n        Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.\n    \"\"\"\n    if a.ndim > 2 or b.ndim > 2:\n        if sparse.issparse(a):\n            # sparse is always 2D. Implies b is 3D+\n            # [i, j] @ [k, ..., l, m, n] -> [i, k, ..., l, n]\n            b_ = np.rollaxis(b, -2)\n            b_2d = b_.reshape((b.shape[-2], -1))\n            ret = a @ b_2d\n            ret = ret.reshape(a.shape[0], *b_.shape[1:])\n        elif sparse.issparse(b):\n            # sparse is always 2D. Implies a is 3D+\n            # [k, ..., l, m] @ [i, j] -> [k, ..., l, j]\n            a_2d = a.reshape(-1, a.shape[-1])\n            ret = a_2d @ b\n            ret = ret.reshape(*a.shape[:-1], b.shape[1])\n        else:\n            ret = np.dot(a, b)\n    else:\n        ret = a @ b\n\n    if (sparse.issparse(a) and sparse.issparse(b)\n            and dense_output and hasattr(ret, \"toarray\")):\n        return ret.toarray()\n    return ret\n\n\ndef randomized_range_finder(A, *, size, n_iter,\n                            power_iteration_normalizer='auto',\n                            random_state=None):\n    \"\"\"Computes an orthonormal matrix whose range approximates the range of A.\n\n    Parameters\n    ----------\n    A : 2D array\n        The input data matrix.\n\n    size : int\n        Size of the return array.\n\n    n_iter : int\n        Number of power iterations used to stabilize the result.\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n        .. versionadded:: 0.18\n\n    random_state : int, RandomState instance or None, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data, i.e. getting the random vectors to initialize the algorithm.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    Q : ndarray\n        A (size x size) projection matrix, the range of which\n        approximates well the range of the input matrix A.\n\n    Notes\n    -----\n\n    Follows Algorithm 4.3 of\n    Finding structure with randomness: Stochastic algorithms for constructing\n    approximate matrix decompositions\n    Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\n    An implementation of a randomized algorithm for principal component\n    analysis\n    A. Szlam et al. 2014\n    \"\"\"\n    random_state = check_random_state(random_state)\n\n    # Generating normal random vectors with shape: (A.shape[1], size)\n    Q = random_state.normal(size=(A.shape[1], size))\n    if A.dtype.kind == 'f':\n        # Ensure f32 is preserved as f32\n        Q = Q.astype(A.dtype, copy=False)\n\n    # Deal with \"auto\" mode\n    if power_iteration_normalizer == 'auto':\n        if n_iter <= 2:\n            power_iteration_normalizer = 'none'\n        else:\n            power_iteration_normalizer = 'LU'\n\n    # Perform power iterations with Q to further 'imprint' the top\n    # singular vectors of A in Q\n    for i in range(n_iter):\n        if power_iteration_normalizer == 'none':\n            Q = safe_sparse_dot(A, Q)\n            Q = safe_sparse_dot(A.T, Q)\n        elif power_iteration_normalizer == 'LU':\n            Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)\n            Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)\n        elif power_iteration_normalizer == 'QR':\n            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n            Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')\n\n    # Sample the range of A using by linear projection of Q\n    # Extract an orthonormal basis\n    Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')\n    return Q\n\n\ndef randomized_svd(M, n_components, *, n_oversamples=10, n_iter='auto',\n                   power_iteration_normalizer='auto', transpose='auto',\n                   flip_sign=True, random_state='warn'):\n    \"\"\"Computes a truncated randomized SVD.\n\n    This method solves the fixed-rank approximation problem described in the\n    Halko et al paper (problem (1.5), p5).\n\n    Parameters\n    ----------\n    M : {ndarray, sparse matrix}\n        Matrix to decompose.\n\n    n_components : int\n        Number of singular values and vectors to extract.\n\n    n_oversamples : int, default=10\n        Additional number of random vectors to sample the range of M so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of M is n_components + n_oversamples. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of singular vectors and singular values. Users might wish\n        to increase this parameter up to `2*k - n_components` where k is the\n        effective rank, for large matrices, noisy problems, matrices with\n        slowly decaying spectrums, or to increase precision accuracy. See Halko\n        et al (pages 5, 23 and 26).\n\n    n_iter : int or 'auto', default='auto'\n        Number of power iterations. It can be used to deal with very noisy\n        problems. When 'auto', it is set to 4, unless `n_components` is small\n        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.\n        This improves precision with few components. Note that in general\n        users should rather increase `n_oversamples` before increasing `n_iter`\n        as the principle of the randomized method is to avoid usage of these\n        more costly power iterations steps. When `n_components` is equal\n        or greater to the effective matrix rank and the spectrum does not\n        present a slow decay, `n_iter=0` or `1` should even work fine in theory\n        (see Halko et al paper, page 9).\n\n        .. versionchanged:: 0.18\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n        .. versionadded:: 0.18\n\n    transpose : bool or 'auto', default='auto'\n        Whether the algorithm should be applied to M.T instead of M. The\n        result should approximately be the same. The 'auto' mode will\n        trigger the transposition if M.shape[1] > M.shape[0] since this\n        implementation of randomized SVD tend to be a little faster in that\n        case.\n\n        .. versionchanged:: 0.18\n\n    flip_sign : bool, default=True\n        The output of a singular value decomposition is only unique up to a\n        permutation of the signs of the singular vectors. If `flip_sign` is\n        set to `True`, the sign ambiguity is resolved by making the largest\n        loadings for each component in the left singular vectors positive.\n\n    random_state : int, RandomState instance or None, default='warn'\n        The seed of the pseudo random number generator to use when\n        shuffling the data, i.e. getting the random vectors to initialize\n        the algorithm. Pass an int for reproducible results across multiple\n        function calls. See :term:`Glossary <random_state>`.\n\n        .. versionchanged:: 1.2\n            The previous behavior (`random_state=0`) is deprecated, and\n            from v1.2 the default value will be `random_state=None`. Set\n            the value of `random_state` explicitly to suppress the deprecation\n            warning.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    singular value decomposition using randomization to speed up the\n    computations. It is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    References\n    ----------\n    * Finding structure with randomness: Stochastic algorithms for constructing\n      approximate matrix decompositions (Algorithm 4.3)\n      Halko, et al., 2009 https://arxiv.org/abs/0909.4061\n\n    * A randomized algorithm for the decomposition of matrices\n      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n    * An implementation of a randomized algorithm for principal component\n      analysis\n      A. Szlam et al. 2014\n    \"\"\"\n    if isinstance(M, (sparse.lil_matrix, sparse.dok_matrix)):\n        warnings.warn(\"Calculating SVD of a {} is expensive. \"\n                      \"csr_matrix is more efficient.\".format(type(M).__name__),\n                      sparse.SparseEfficiencyWarning)\n\n    if random_state == 'warn':\n        warnings.warn(\n            \"If 'random_state' is not supplied, the current default \"\n            \"is to use 0 as a fixed seed. This will change to  \"\n            \"None in version 1.2 leading to non-deterministic results \"\n            \"that better reflect nature of the randomized_svd solver. \"\n            \"If you want to silence this warning, set 'random_state' \"\n            \"to an integer seed or to None explicitly depending \"\n            \"if you want your code to be deterministic or not.\",\n            FutureWarning\n        )\n        random_state = 0\n\n    random_state = check_random_state(random_state)\n    n_random = n_components + n_oversamples\n    n_samples, n_features = M.shape\n\n    if n_iter == 'auto':\n        # Checks if the number of iterations is explicitly specified\n        # Adjust n_iter. 7 was found a good compromise for PCA. See #5299\n        n_iter = 7 if n_components < .1 * min(M.shape) else 4\n\n    if transpose == 'auto':\n        transpose = n_samples < n_features\n    if transpose:\n        # this implementation is a bit faster with smaller shape[1]\n        M = M.T\n\n    Q = randomized_range_finder(\n        M, size=n_random, n_iter=n_iter,\n        power_iteration_normalizer=power_iteration_normalizer,\n        random_state=random_state)\n\n    # project M to the (k + p) dimensional space using the basis vectors\n    B = safe_sparse_dot(Q.T, M)\n\n    # compute the SVD on the thin matrix: (k + p) wide\n    Uhat, s, Vt = linalg.svd(B, full_matrices=False)\n\n    del B\n    U = np.dot(Q, Uhat)\n\n    if flip_sign:\n        if not transpose:\n            U, Vt = svd_flip(U, Vt)\n        else:\n            # In case of transpose u_based_decision=false\n            # to actually flip based on u and not v.\n            U, Vt = svd_flip(U, Vt, u_based_decision=False)\n\n    if transpose:\n        # transpose back the results according to the input convention\n        return Vt[:n_components, :].T, s[:n_components], U[:, :n_components].T\n    else:\n        return U[:, :n_components], s[:n_components], Vt[:n_components, :]\n\n\ndef _randomized_eigsh(M, n_components, *, n_oversamples=10, n_iter='auto',\n                      power_iteration_normalizer='auto',\n                      selection='module', random_state=None):\n    \"\"\"Computes a truncated eigendecomposition using randomized methods\n\n    This method solves the fixed-rank approximation problem described in the\n    Halko et al paper.\n\n    The choice of which components to select can be tuned with the `selection`\n    parameter.\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    M : ndarray or sparse matrix\n        Matrix to decompose, it should be real symmetric square or complex\n        hermitian\n\n    n_components : int\n        Number of eigenvalues and vectors to extract.\n\n    n_oversamples : int, default=10\n        Additional number of random vectors to sample the range of M so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of M is n_components + n_oversamples. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of eigenvectors and eigenvalues. Users might wish\n        to increase this parameter up to `2*k - n_components` where k is the\n        effective rank, for large matrices, noisy problems, matrices with\n        slowly decaying spectrums, or to increase precision accuracy. See Halko\n        et al (pages 5, 23 and 26).\n\n    n_iter : int or 'auto', default='auto'\n        Number of power iterations. It can be used to deal with very noisy\n        problems. When 'auto', it is set to 4, unless `n_components` is small\n        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.\n        This improves precision with few components. Note that in general\n        users should rather increase `n_oversamples` before increasing `n_iter`\n        as the principle of the randomized method is to avoid usage of these\n        more costly power iterations steps. When `n_components` is equal\n        or greater to the effective matrix rank and the spectrum does not\n        present a slow decay, `n_iter=0` or `1` should even work fine in theory\n        (see Halko et al paper, page 9).\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n    selection : {'value', 'module'}, default='module'\n        Strategy used to select the n components. When `selection` is `'value'`\n        (not yet implemented, will become the default when implemented), the\n        components corresponding to the n largest eigenvalues are returned.\n        When `selection` is `'module'`, the components corresponding to the n\n        eigenvalues with largest modules are returned.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data, i.e. getting the random vectors to initialize the algorithm.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    eigendecomposition using randomized methods to speed up the computations.\n\n    This method is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    Strategy 'value': not implemented yet.\n    Algorithms 5.3, 5.4 and 5.5 in the Halko et al paper should provide good\n    condidates for a future implementation.\n\n    Strategy 'module':\n    The principle is that for diagonalizable matrices, the singular values and\n    eigenvalues are related: if t is an eigenvalue of A, then :math:`|t|` is a\n    singular value of A. This method relies on a randomized SVD to find the n\n    singular components corresponding to the n singular values with largest\n    modules, and then uses the signs of the singular vectors to find the true\n    sign of t: if the sign of left and right singular vectors are different\n    then the corresponding eigenvalue is negative.\n\n    Returns\n    -------\n    eigvals : 1D array of shape (n_components,) containing the `n_components`\n        eigenvalues selected (see ``selection`` parameter).\n    eigvecs : 2D array of shape (M.shape[0], n_components) containing the\n        `n_components` eigenvectors corresponding to the `eigvals`, in the\n        corresponding order. Note that this follows the `scipy.linalg.eigh`\n        convention.\n\n    See Also\n    --------\n    :func:`randomized_svd`\n\n    References\n    ----------\n    * Finding structure with randomness: Stochastic algorithms for constructing\n      approximate matrix decompositions (Algorithm 4.3 for strategy 'module')\n      Halko, et al., 2009 https://arxiv.org/abs/0909.4061\n\n    \"\"\"\n    if selection == 'value':  # pragma: no cover\n        # to do : an algorithm can be found in the Halko et al reference\n        raise NotImplementedError()\n\n    elif selection == 'module':\n        # Note: no need for deterministic U and Vt (flip_sign=True),\n        # as we only use the dot product UVt afterwards\n        U, S, Vt = randomized_svd(\n            M, n_components=n_components, n_oversamples=n_oversamples,\n            n_iter=n_iter,\n            power_iteration_normalizer=power_iteration_normalizer,\n            flip_sign=False, random_state=random_state)\n\n        eigvecs = U[:, :n_components]\n        eigvals = S[:n_components]\n\n        # Conversion of Singular values into Eigenvalues:\n        # For any eigenvalue t, the corresponding singular value is |t|.\n        # So if there is a negative eigenvalue t, the corresponding singular\n        # value will be -t, and the left (U) and right (V) singular vectors\n        # will have opposite signs.\n        # Fastest way: see <https://stackoverflow.com/a/61974002/7262247>\n        diag_VtU = np.einsum('ji,ij->j',\n                             Vt[:n_components, :], U[:, :n_components])\n        signs = np.sign(diag_VtU)\n        eigvals = eigvals * signs\n\n    else:  # pragma: no cover\n        raise ValueError(\"Invalid `selection`: %r\" % selection)\n\n    return eigvals, eigvecs\n\n\ndef weighted_mode(a, w, *, axis=0):\n    \"\"\"Returns an array of the weighted modal (most common) value in a.\n\n    If there is more than one such value, only the first is returned.\n    The bin-count for the modal bins is also returned.\n\n    This is an extension of the algorithm in scipy.stats.mode.\n\n    Parameters\n    ----------\n    a : array-like\n        n-dimensional array of which to find mode(s).\n    w : array-like\n        n-dimensional array of weights for each value.\n    axis : int, default=0\n        Axis along which to operate. Default is 0, i.e. the first axis.\n\n    Returns\n    -------\n    vals : ndarray\n        Array of modal values.\n    score : ndarray\n        Array of weighted counts for each mode.\n\n    Examples\n    --------\n    >>> from sklearn.utils.extmath import weighted_mode\n    >>> x = [4, 1, 4, 2, 4, 2]\n    >>> weights = [1, 1, 1, 1, 1, 1]\n    >>> weighted_mode(x, weights)\n    (array([4.]), array([3.]))\n\n    The value 4 appears three times: with uniform weights, the result is\n    simply the mode of the distribution.\n\n    >>> weights = [1, 3, 0.5, 1.5, 1, 2]  # deweight the 4's\n    >>> weighted_mode(x, weights)\n    (array([2.]), array([3.5]))\n\n    The value 2 has the highest score: it appears twice with weights of\n    1.5 and 2: the sum of these is 3.5.\n\n    See Also\n    --------\n    scipy.stats.mode\n    \"\"\"\n    if axis is None:\n        a = np.ravel(a)\n        w = np.ravel(w)\n        axis = 0\n    else:\n        a = np.asarray(a)\n        w = np.asarray(w)\n\n    if a.shape != w.shape:\n        w = np.full(a.shape, w, dtype=w.dtype)\n\n    scores = np.unique(np.ravel(a))       # get ALL unique values\n    testshape = list(a.shape)\n    testshape[axis] = 1\n    oldmostfreq = np.zeros(testshape)\n    oldcounts = np.zeros(testshape)\n    for score in scores:\n        template = np.zeros(a.shape)\n        ind = (a == score)\n        template[ind] = w[ind]\n        counts = np.expand_dims(np.sum(template, axis), axis)\n        mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)\n        oldcounts = np.maximum(counts, oldcounts)\n        oldmostfreq = mostfrequent\n    return mostfrequent, oldcounts\n\n\ndef cartesian(arrays, out=None):\n    \"\"\"Generate a cartesian product of input arrays.\n\n    Parameters\n    ----------\n    arrays : list of array-like\n        1-D arrays to form the cartesian product of.\n    out : ndarray, default=None\n        Array to place the cartesian product in.\n\n    Returns\n    -------\n    out : ndarray\n        2-D array of shape (M, len(arrays)) containing cartesian products\n        formed of input arrays.\n\n    Examples\n    --------\n    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n    array([[1, 4, 6],\n           [1, 4, 7],\n           [1, 5, 6],\n           [1, 5, 7],\n           [2, 4, 6],\n           [2, 4, 7],\n           [2, 5, 6],\n           [2, 5, 7],\n           [3, 4, 6],\n           [3, 4, 7],\n           [3, 5, 6],\n           [3, 5, 7]])\n\n    Notes\n    -----\n    This function may not be used on more than 32 arrays\n    because the underlying numpy functions do not support it.\n    \"\"\"\n    arrays = [np.asarray(x) for x in arrays]\n    shape = (len(x) for x in arrays)\n    dtype = arrays[0].dtype\n\n    ix = np.indices(shape)\n    ix = ix.reshape(len(arrays), -1).T\n\n    if out is None:\n        out = np.empty_like(ix, dtype=dtype)\n\n    for n, arr in enumerate(arrays):\n        out[:, n] = arrays[n][ix[:, n]]\n\n    return out\n\n\ndef svd_flip(u, v, u_based_decision=True):\n    \"\"\"Sign correction to ensure deterministic output from SVD.\n\n    Adjusts the columns of u and the rows of v such that the loadings in the\n    columns in u that are largest in absolute value are always positive.\n\n    Parameters\n    ----------\n    u : ndarray\n        u and v are the output of `linalg.svd` or\n        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`.\n\n    v : ndarray\n        u and v are the output of `linalg.svd` or\n        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`.\n        The input v should really be called vt to be consistent with scipy's\n        ouput.\n\n    u_based_decision : bool, default=True\n        If True, use the columns of u as the basis for sign flipping.\n        Otherwise, use the rows of v. The choice of which variable to base the\n        decision on is generally algorithm dependent.\n\n\n    Returns\n    -------\n    u_adjusted, v_adjusted : arrays with the same dimensions as the input.\n\n    \"\"\"\n    if u_based_decision:\n        # columns of u, rows of v\n        max_abs_cols = np.argmax(np.abs(u), axis=0)\n        signs = np.sign(u[max_abs_cols, range(u.shape[1])])\n        u *= signs\n        v *= signs[:, np.newaxis]\n    else:\n        # rows of v, columns of u\n        max_abs_rows = np.argmax(np.abs(v), axis=1)\n        signs = np.sign(v[range(v.shape[0]), max_abs_rows])\n        u *= signs\n        v *= signs[:, np.newaxis]\n    return u, v\n\n\ndef log_logistic(X, out=None):\n    \"\"\"Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.\n\n    This implementation is numerically stable because it splits positive and\n    negative values::\n\n        -log(1 + exp(-x_i))     if x_i > 0\n        x_i - log(1 + exp(x_i)) if x_i <= 0\n\n    For the ordinary logistic function, use ``scipy.special.expit``.\n\n    Parameters\n    ----------\n    X : array-like of shape (M, N) or (M,)\n        Argument to the logistic function.\n\n    out : array-like of shape (M, N) or (M,), default=None\n        Preallocated output array.\n\n    Returns\n    -------\n    out : ndarray of shape (M, N) or (M,)\n        Log of the logistic function evaluated at every point in x.\n\n    Notes\n    -----\n    See the blog post describing this implementation:\n    http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/\n    \"\"\"\n    is_1d = X.ndim == 1\n    X = np.atleast_2d(X)\n    X = check_array(X, dtype=np.float64)\n\n    n_samples, n_features = X.shape\n\n    if out is None:\n        out = np.empty_like(X)\n\n    _log_logistic_sigmoid(n_samples, n_features, X, out)\n\n    if is_1d:\n        return np.squeeze(out)\n    return out\n\n\ndef softmax(X, copy=True):\n    \"\"\"\n    Calculate the softmax function.\n\n    The softmax function is calculated by\n    np.exp(X) / np.sum(np.exp(X), axis=1)\n\n    This will cause overflow when large values are exponentiated.\n    Hence the largest value in each row is subtracted from each data\n    point to prevent this.\n\n    Parameters\n    ----------\n    X : array-like of float of shape (M, N)\n        Argument to the logistic function.\n\n    copy : bool, default=True\n        Copy X or not.\n\n    Returns\n    -------\n    out : ndarray of shape (M, N)\n        Softmax function evaluated at every point in x.\n    \"\"\"\n    if copy:\n        X = np.copy(X)\n    max_prob = np.max(X, axis=1).reshape((-1, 1))\n    X -= max_prob\n    np.exp(X, X)\n    sum_prob = np.sum(X, axis=1).reshape((-1, 1))\n    X /= sum_prob\n    return X\n\n\ndef make_nonnegative(X, min_value=0):\n    \"\"\"Ensure `X.min()` >= `min_value`.\n\n    Parameters\n    ----------\n    X : array-like\n        The matrix to make non-negative.\n    min_value : float, default=0\n        The threshold value.\n\n    Returns\n    -------\n    array-like\n        The thresholded array.\n\n    Raises\n    ------\n    ValueError\n        When X is sparse.\n    \"\"\"\n    min_ = X.min()\n    if min_ < min_value:\n        if sparse.issparse(X):\n            raise ValueError(\"Cannot make the data matrix\"\n                             \" nonnegative because it is sparse.\"\n                             \" Adding a value to every entry would\"\n                             \" make it no longer sparse.\")\n        X = X + (min_value - min_)\n    return X\n\n\n# Use at least float64 for the accumulating functions to avoid precision issue\n# see https://github.com/numpy/numpy/issues/9393. The float64 is also retained\n# as it is in case the float overflows\ndef _safe_accumulator_op(op, x, *args, **kwargs):\n    \"\"\"\n    This function provides numpy accumulator functions with a float64 dtype\n    when used on a floating point input. This prevents accumulator overflow on\n    smaller floating point dtypes.\n\n    Parameters\n    ----------\n    op : function\n        A numpy accumulator function such as np.mean or np.sum.\n    x : ndarray\n        A numpy array to apply the accumulator function.\n    *args : positional arguments\n        Positional arguments passed to the accumulator function after the\n        input x.\n    **kwargs : keyword arguments\n        Keyword arguments passed to the accumulator function.\n\n    Returns\n    -------\n    result\n        The output of the accumulator function passed to this function.\n    \"\"\"\n    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n        result = op(x, *args, **kwargs, dtype=np.float64)\n    else:\n        result = op(x, *args, **kwargs)\n    return result\n\n\ndef _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count,\n                              sample_weight=None):\n    \"\"\"Calculate mean update and a Youngs and Cramer variance update.\n\n    If sample_weight is given, the weighted mean and variance is computed.\n\n    Update a given mean and (possibly) variance according to new data given\n    in X. last_mean is always required to compute the new mean.\n    If last_variance is None, no variance is computed and None return for\n    updated_variance.\n\n    From the paper \"Algorithms for computing the sample variance: analysis and\n    recommendations\", by Chan, Golub, and LeVeque.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data to use for variance update.\n\n    last_mean : array-like of shape (n_features,)\n\n    last_variance : array-like of shape (n_features,)\n\n    last_sample_count : array-like of shape (n_features,)\n        The number of samples encountered until now if sample_weight is None.\n        If sample_weight is not None, this is the sum of sample_weight\n        encountered.\n\n    sample_weight : array-like of shape (n_samples,) or None\n        Sample weights. If None, compute the unweighted mean/variance.\n\n    Returns\n    -------\n    updated_mean : ndarray of shape (n_features,)\n\n    updated_variance : ndarray of shape (n_features,)\n        None if last_variance was None.\n\n    updated_sample_count : ndarray of shape (n_features,)\n\n    Notes\n    -----\n    NaNs are ignored during the algorithm.\n\n    References\n    ----------\n    T. Chan, G. Golub, R. LeVeque. Algorithms for computing the sample\n        variance: recommendations, The American Statistician, Vol. 37, No. 3,\n        pp. 242-247\n\n    Also, see the sparse implementation of this in\n    `utils.sparsefuncs.incr_mean_variance_axis` and\n    `utils.sparsefuncs_fast.incr_mean_variance_axis0`\n    \"\"\"\n    # old = stats until now\n    # new = the current increment\n    # updated = the aggregated stats\n    last_sum = last_mean * last_sample_count\n    if sample_weight is not None:\n        if np_version >= parse_version(\"1.16.6\"):\n            # equivalent to np.nansum(X * sample_weight, axis=0)\n            # safer because np.float64(X*W) != np.float64(X)*np.float64(W)\n            # dtype arg of np.matmul only exists since version 1.16\n            new_sum = _safe_accumulator_op(\n                np.matmul, sample_weight, np.where(np.isnan(X), 0, X))\n        else:\n            new_sum = _safe_accumulator_op(\n                np.nansum, X * sample_weight[:, None], axis=0)\n        new_sample_count = _safe_accumulator_op(\n            np.sum, sample_weight[:, None] * (~np.isnan(X)), axis=0)\n    else:\n        new_sum = _safe_accumulator_op(np.nansum, X, axis=0)\n        new_sample_count = np.sum(~np.isnan(X), axis=0)\n\n    updated_sample_count = last_sample_count + new_sample_count\n\n    updated_mean = (last_sum + new_sum) / updated_sample_count\n\n    if last_variance is None:\n        updated_variance = None\n    else:\n        T = new_sum / new_sample_count\n        if sample_weight is not None:\n            if np_version >= parse_version(\"1.16.6\"):\n                # equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\n                # safer because np.float64(X*W) != np.float64(X)*np.float64(W)\n                # dtype arg of np.matmul only exists since version 1.16\n                new_unnormalized_variance = _safe_accumulator_op(\n                    np.matmul, sample_weight,\n                    np.where(np.isnan(X), 0, (X - T)**2))\n                correction = _safe_accumulator_op(\n                    np.matmul, sample_weight, np.where(np.isnan(X), 0, X - T))\n            else:\n                new_unnormalized_variance = _safe_accumulator_op(\n                    np.nansum, (X - T)**2 * sample_weight[:, None], axis=0)\n                correction = _safe_accumulator_op(\n                    np.nansum, (X - T) * sample_weight[:, None], axis=0)\n        else:\n            new_unnormalized_variance = _safe_accumulator_op(\n                np.nansum, (X - T)**2, axis=0)\n            correction = _safe_accumulator_op(np.nansum, X - T, axis=0)\n\n        # correction term of the corrected 2 pass algorithm.\n        # See \"Algorithms for computing the sample variance: analysis\n        # and recommendations\", by Chan, Golub, and LeVeque.\n        new_unnormalized_variance -= correction**2 / new_sample_count\n\n        last_unnormalized_variance = last_variance * last_sample_count\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            last_over_new_count = last_sample_count / new_sample_count\n            updated_unnormalized_variance = (\n                last_unnormalized_variance + new_unnormalized_variance +\n                last_over_new_count / updated_sample_count *\n                (last_sum / last_over_new_count - new_sum) ** 2)\n\n        zeros = last_sample_count == 0\n        updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]\n        updated_variance = updated_unnormalized_variance / updated_sample_count\n\n    return updated_mean, updated_variance, updated_sample_count\n\n\ndef _deterministic_vector_sign_flip(u):\n    \"\"\"Modify the sign of vectors for reproducibility.\n\n    Flips the sign of elements of all the vectors (rows of u) such that\n    the absolute maximum element of each vector is positive.\n\n    Parameters\n    ----------\n    u : ndarray\n        Array with vectors as its rows.\n\n    Returns\n    -------\n    u_flipped : ndarray with same shape as u\n        Array with the sign flipped vectors as its rows.\n    \"\"\"\n    max_abs_rows = np.argmax(np.abs(u), axis=1)\n    signs = np.sign(u[range(u.shape[0]), max_abs_rows])\n    u *= signs[:, np.newaxis]\n    return u\n\n\ndef stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):\n    \"\"\"Use high precision for cumsum and check that final value matches sum.\n\n    Parameters\n    ----------\n    arr : array-like\n        To be cumulatively summed as flat.\n    axis : int, default=None\n        Axis along which the cumulative sum is computed.\n        The default (None) is to compute the cumsum over the flattened array.\n    rtol : float, default=1e-05\n        Relative tolerance, see ``np.allclose``.\n    atol : float, default=1e-08\n        Absolute tolerance, see ``np.allclose``.\n    \"\"\"\n    out = np.cumsum(arr, axis=axis, dtype=np.float64)\n    expected = np.sum(arr, axis=axis, dtype=np.float64)\n    if not np.all(np.isclose(out.take(-1, axis=axis), expected, rtol=rtol,\n                             atol=atol, equal_nan=True)):\n        warnings.warn('cumsum was found to be unstable: '\n                      'its last element does not correspond to sum',\n                      RuntimeWarning)\n    return out\n",1035],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py":["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.  abstractmethod() may be used to declare\n    abstract methods for properties and descriptors.\n\n    Usage:\n\n        class C(metaclass=ABCMeta):\n            @abstractmethod\n            def my_abstract_method(self, ...):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractclassmethod(classmethod):\n    \"\"\"A decorator indicating abstract classmethods.\n\n    Deprecated, use 'classmethod' with 'abstractmethod' instead.\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractstaticmethod(staticmethod):\n    \"\"\"A decorator indicating abstract staticmethods.\n\n    Deprecated, use 'staticmethod' with 'abstractmethod' instead.\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Deprecated, use 'property' with 'abstractmethod' instead.\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n\ntry:\n    from _abc import (get_cache_token, _abc_init, _abc_register,\n                      _abc_instancecheck, _abc_subclasscheck, _get_dump,\n                      _reset_registry, _reset_caches)\nexcept ImportError:\n    from _py_abc import ABCMeta, get_cache_token\n    ABCMeta.__module__ = 'abc'\nelse:\n    class ABCMeta(type):\n        \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n        Use this metaclass to create an ABC.  An ABC can be subclassed\n        directly, and then acts as a mix-in class.  You can also register\n        unrelated concrete classes (even built-in classes) and unrelated\n        ABCs as 'virtual subclasses' -- these and their descendants will\n        be considered subclasses of the registering ABC by the built-in\n        issubclass() function, but the registering ABC won't show up in\n        their MRO (Method Resolution Order) nor will method\n        implementations defined by the registering ABC be callable (not\n        even via super()).\n        \"\"\"\n        def __new__(mcls, name, bases, namespace, **kwargs):\n            cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n            _abc_init(cls)\n            return cls\n\n        def register(cls, subclass):\n            \"\"\"Register a virtual subclass of an ABC.\n\n            Returns the subclass, to allow usage as a class decorator.\n            \"\"\"\n            return _abc_register(cls, subclass)\n\n        def __instancecheck__(cls, instance):\n            \"\"\"Override for isinstance(instance, cls).\"\"\"\n            return _abc_instancecheck(cls, instance)\n\n        def __subclasscheck__(cls, subclass):\n            \"\"\"Override for issubclass(subclass, cls).\"\"\"\n            return _abc_subclasscheck(cls, subclass)\n\n        def _dump_registry(cls, file=None):\n            \"\"\"Debug helper to print the ABC registry.\"\"\"\n            print(f\"Class: {cls.__module__}.{cls.__qualname__}\", file=file)\n            print(f\"Inv. counter: {get_cache_token()}\", file=file)\n            (_abc_registry, _abc_cache, _abc_negative_cache,\n             _abc_negative_cache_version) = _get_dump(cls)\n            print(f\"_abc_registry: {_abc_registry!r}\", file=file)\n            print(f\"_abc_cache: {_abc_cache!r}\", file=file)\n            print(f\"_abc_negative_cache: {_abc_negative_cache!r}\", file=file)\n            print(f\"_abc_negative_cache_version: {_abc_negative_cache_version!r}\",\n                  file=file)\n\n        def _abc_registry_clear(cls):\n            \"\"\"Clear the registry (for debugging or testing).\"\"\"\n            _reset_registry(cls)\n\n        def _abc_caches_clear(cls):\n            \"\"\"Clear the caches (for debugging or testing).\"\"\"\n            _reset_caches(cls)\n\n\nclass ABC(metaclass=ABCMeta):\n    \"\"\"Helper class that provides a standard way to create an ABC using\n    inheritance.\n    \"\"\"\n    __slots__ = ()\n",129],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py":["\"\"\"Base classes for all estimators.\"\"\"\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n# License: BSD 3 clause\n\nimport copy\nimport warnings\nfrom collections import defaultdict\nimport platform\nimport inspect\nimport re\n\nimport numpy as np\n\nfrom . import __version__\nfrom ._config import get_config\nfrom .utils import _IS_32BIT\nfrom .utils._tags import (\n    _DEFAULT_TAGS,\n    _safe_tags,\n)\nfrom .utils.validation import check_X_y\nfrom .utils.validation import check_array\nfrom .utils.validation import _num_features\nfrom .utils._estimator_html_repr import estimator_html_repr\n\n\ndef clone(estimator, *, safe=True):\n    \"\"\"Constructs a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It yields a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    yield different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators.\n\n    \"\"\"\n    estimator_type = type(estimator)\n    # XXX: not handling dictionaries\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            if isinstance(estimator, type):\n                raise TypeError(\"Cannot clone object. \" +\n                                \"You should provide an instance of \" +\n                                \"scikit-learn estimator instead of a class.\")\n            else:\n                raise TypeError(\"Cannot clone object '%s' (type %s): \"\n                                \"it does not seem to be a scikit-learn \"\n                                \"estimator as it does not implement a \"\n                                \"'get_params' method.\"\n                                % (repr(estimator), type(estimator)))\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError('Cannot clone object %s, as the constructor '\n                               'either does not set or modifies parameter %s' %\n                               (estimator, name))\n    return new_object\n\n\ndef _pprint(params, offset=0, printer=repr):\n    \"\"\"Pretty print the dictionary 'params'\n\n    Parameters\n    ----------\n    params : dict\n        The dictionary to pretty print\n\n    offset : int, default=0\n        The offset in characters to add at the begin of each line.\n\n    printer : callable, default=repr\n        The function to convert entries to strings, typically\n        the builtin str or repr\n\n    \"\"\"\n    # Do a multi-line justified repr:\n    options = np.get_printoptions()\n    np.set_printoptions(precision=5, threshold=64, edgeitems=2)\n    params_list = list()\n    this_line_length = offset\n    line_sep = ',\\n' + (1 + offset // 2) * ' '\n    for i, (k, v) in enumerate(sorted(params.items())):\n        if type(v) is float:\n            # use str for representing floating point numbers\n            # this way we get consistent representation across\n            # architectures and versions.\n            this_repr = '%s=%s' % (k, str(v))\n        else:\n            # use repr of the rest\n            this_repr = '%s=%s' % (k, printer(v))\n        if len(this_repr) > 500:\n            this_repr = this_repr[:300] + '...' + this_repr[-100:]\n        if i > 0:\n            if (this_line_length + len(this_repr) >= 75 or '\\n' in this_repr):\n                params_list.append(line_sep)\n                this_line_length = len(line_sep)\n            else:\n                params_list.append(', ')\n                this_line_length += 2\n        params_list.append(this_repr)\n        this_line_length += len(this_repr)\n\n    np.set_printoptions(**options)\n    lines = ''.join(params_list)\n    # Strip trailing space to avoid nightmare in doctests\n    lines = '\\n'.join(l.rstrip(' ') for l in lines.split('\\n'))\n    return lines\n\n\nclass BaseEstimator:\n    \"\"\"Base class for all estimators in scikit-learn.\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n    \"\"\"\n\n    @classmethod\n    def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        init_signature = inspect.signature(init)\n        # Consider the constructor parameters excluding 'self'\n        parameters = [p for p in init_signature.parameters.values()\n                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n        for p in parameters:\n            if p.kind == p.VAR_POSITIONAL:\n                raise RuntimeError(\"scikit-learn estimators should always \"\n                                   \"specify their parameters in the signature\"\n                                   \" of their __init__ (no varargs).\"\n                                   \" %s with constructor %s doesn't \"\n                                   \" follow this convention.\"\n                                   % (cls, init_signature))\n        # Extract and sort argument names excluding 'self'\n        return sorted([p.name for p in parameters])\n\n    def get_params(self, deep=True):\n        \"\"\"\n        Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        out = dict()\n        for key in self._get_param_names():\n            value = getattr(self, key)\n            if deep and hasattr(value, 'get_params'):\n                deep_items = value.get_params().items()\n                out.update((key + '__' + k, val) for k, val in deep_items)\n            out[key] = value\n        return out\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n        parameters of the form ``<component>__<parameter>`` so that it's\n        possible to update each component of a nested object.\n\n        Parameters\n        ----------\n        **params : dict\n            Estimator parameters.\n\n        Returns\n        -------\n        self : estimator instance\n            Estimator instance.\n        \"\"\"\n        if not params:\n            # Simple optimization to gain speed (inspect is slow)\n            return self\n        valid_params = self.get_params(deep=True)\n\n        nested_params = defaultdict(dict)  # grouped by prefix\n        for key, value in params.items():\n            key, delim, sub_key = key.partition('__')\n            if key not in valid_params:\n                raise ValueError('Invalid parameter %s for estimator %s. '\n                                 'Check the list of available parameters '\n                                 'with `estimator.get_params().keys()`.' %\n                                 (key, self))\n\n            if delim:\n                nested_params[key][sub_key] = value\n            else:\n                setattr(self, key, value)\n                valid_params[key] = value\n\n        for key, sub_params in nested_params.items():\n            valid_params[key].set_params(**sub_params)\n\n        return self\n\n    def __repr__(self, N_CHAR_MAX=700):\n        # N_CHAR_MAX is the (approximate) maximum number of non-blank\n        # characters to render. We pass it as an optional parameter to ease\n        # the tests.\n\n        from .utils._pprint import _EstimatorPrettyPrinter\n\n        N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n\n        # use ellipsis for sequences with a lot of elements\n        pp = _EstimatorPrettyPrinter(\n            compact=True, indent=1, indent_at_name=True,\n            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n\n        repr_ = pp.pformat(self)\n\n        # Use bruteforce ellipsis when there are a lot of non-blank characters\n        n_nonblank = len(''.join(repr_.split()))\n        if n_nonblank > N_CHAR_MAX:\n            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n            regex = r'^(\\s*\\S){%d}' % lim\n            # The regex '^(\\s*\\S){%d}' % n\n            # matches from the start of the string until the nth non-blank\n            # character:\n            # - ^ matches the start of string\n            # - (pattern){n} matches n repetitions of pattern\n            # - \\s*\\S matches a non-blank char following zero or more blanks\n            left_lim = re.match(regex, repr_).end()\n            right_lim = re.match(regex, repr_[::-1]).end()\n\n            if '\\n' in repr_[left_lim:-right_lim]:\n                # The left side and right side aren't on the same line.\n                # To avoid weird cuts, e.g.:\n                # categoric...ore',\n                # we need to start the right side with an appropriate newline\n                # character so that it renders properly as:\n                # categoric...\n                # handle_unknown='ignore',\n                # so we add [^\\n]*\\n which matches until the next \\n\n                regex += r'[^\\n]*\\n'\n                right_lim = re.match(regex, repr_[::-1]).end()\n\n            ellipsis = '...'\n            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n                # Only add ellipsis if it results in a shorter repr\n                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n\n        return repr_\n\n    def __getstate__(self):\n        try:\n            state = super().__getstate__()\n        except AttributeError:\n            state = self.__dict__.copy()\n\n        if type(self).__module__.startswith('sklearn.'):\n            return dict(state.items(), _sklearn_version=__version__)\n        else:\n            return state\n\n    def __setstate__(self, state):\n        if type(self).__module__.startswith('sklearn.'):\n            pickle_version = state.pop(\"_sklearn_version\", \"pre-0.18\")\n            if pickle_version != __version__:\n                warnings.warn(\n                    \"Trying to unpickle estimator {0} from version {1} when \"\n                    \"using version {2}. This might lead to breaking code or \"\n                    \"invalid results. Use at your own risk.\".format(\n                        self.__class__.__name__, pickle_version, __version__),\n                    UserWarning)\n        try:\n            super().__setstate__(state)\n        except AttributeError:\n            self.__dict__.update(state)\n\n    def _more_tags(self):\n        return _DEFAULT_TAGS\n\n    def _get_tags(self):\n        collected_tags = {}\n        for base_class in reversed(inspect.getmro(self.__class__)):\n            if hasattr(base_class, '_more_tags'):\n                # need the if because mixins might not have _more_tags\n                # but might do redundant work in estimators\n                # (i.e. calling more tags on BaseEstimator multiple times)\n                more_tags = base_class._more_tags(self)\n                collected_tags.update(more_tags)\n        return collected_tags\n\n    def _check_n_features(self, X, reset):\n        \"\"\"Set the `n_features_in_` attribute, or check against it.\n\n        Parameters\n        ----------\n        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n        reset : bool\n            If True, the `n_features_in_` attribute is set to `X.shape[1]`.\n            If False and the attribute exists, then check that it is equal to\n            `X.shape[1]`. If False and the attribute does *not* exist, then\n            the check is skipped.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n        \"\"\"\n        try:\n            n_features = _num_features(X)\n        except TypeError as e:\n            if not reset and hasattr(self, \"n_features_in_\"):\n                raise ValueError(\n                    \"X does not contain any features, but \"\n                    f\"{self.__class__.__name__} is expecting \"\n                    f\"{self.n_features_in_} features\"\n                ) from e\n            # If the number of features is not defined and reset=True,\n            # then we skip this check\n            return\n\n        if reset:\n            self.n_features_in_ = n_features\n            return\n\n        if not hasattr(self, \"n_features_in_\"):\n            # Skip this check if the expected number of expected input features\n            # was not recorded by calling fit first. This is typically the case\n            # for stateless transformers.\n            return\n\n        if n_features != self.n_features_in_:\n            raise ValueError(\n                f\"X has {n_features} features, but {self.__class__.__name__} \"\n                f\"is expecting {self.n_features_in_} features as input.\")\n\n    def _validate_data(self, X, y='no_validation', reset=True,\n                       validate_separately=False, **check_params):\n        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape \\\n                (n_samples, n_features)\n            The input samples.\n        y : array-like of shape (n_samples,), default='no_validation'\n            The targets.\n\n            - If `None`, `check_array` is called on `X`. If the estimator's\n              requires_y tag is True, then an error will be raised.\n            - If `'no_validation'`, `check_array` is called on `X` and the\n              estimator's requires_y tag is ignored. This is a default\n              placeholder and is never meant to be explicitly set.\n            - Otherwise, both `X` and `y` are checked with either `check_array`\n              or `check_X_y` depending on `validate_separately`.\n\n        reset : bool, default=True\n            Whether to reset the `n_features_in_` attribute.\n            If False, the input will be checked for consistency with data\n            provided when reset was last True.\n            .. note::\n               It is recommended to call reset=True in `fit` and in the first\n               call to `partial_fit`. All other methods that validate `X`\n               should set `reset=False`.\n        validate_separately : False or tuple of dicts, default=False\n            Only used if y is not None.\n            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n            to be used for calling check_array() on X and y respectively.\n        **check_params : kwargs\n            Parameters passed to :func:`sklearn.utils.check_array` or\n            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n            is not False.\n\n        Returns\n        -------\n        out : {ndarray, sparse matrix} or tuple of these\n            The validated input. A tuple is returned if `y` is not None.\n        \"\"\"\n\n        if y is None:\n            if self._get_tags()['requires_y']:\n                raise ValueError(\n                    f\"This {self.__class__.__name__} estimator \"\n                    f\"requires y to be passed, but the target y is None.\"\n                )\n            X = check_array(X, **check_params)\n            out = X\n        elif isinstance(y, str) and y == 'no_validation':\n            X = check_array(X, **check_params)\n            out = X\n        else:\n            if validate_separately:\n                # We need this because some estimators validate X and y\n                # separately, and in general, separately calling check_array()\n                # on X and y isn't equivalent to just calling check_X_y()\n                # :(\n                check_X_params, check_y_params = validate_separately\n                X = check_array(X, **check_X_params)\n                y = check_array(y, **check_y_params)\n            else:\n                X, y = check_X_y(X, y, **check_params)\n            out = X, y\n\n        if check_params.get('ensure_2d', True):\n            self._check_n_features(X, reset=reset)\n\n        return out\n\n    @property\n    def _repr_html_(self):\n        \"\"\"HTML representation of estimator.\n\n        This is redundant with the logic of `_repr_mimebundle_`. The latter\n        should be favorted in the long term, `_repr_html_` is only\n        implemented for consumers who do not interpret `_repr_mimbundle_`.\n        \"\"\"\n        if get_config()[\"display\"] != 'diagram':\n            raise AttributeError(\"_repr_html_ is only defined when the \"\n                                 \"'display' configuration option is set to \"\n                                 \"'diagram'\")\n        return self._repr_html_inner\n\n    def _repr_html_inner(self):\n        \"\"\"This function is returned by the @property `_repr_html_` to make\n        `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending\n        on `get_config()[\"display\"]`.\n        \"\"\"\n        return estimator_html_repr(self)\n\n    def _repr_mimebundle_(self, **kwargs):\n        \"\"\"Mime bundle used by jupyter kernels to display estimator\"\"\"\n        output = {\"text/plain\": repr(self)}\n        if get_config()[\"display\"] == 'diagram':\n            output[\"text/html\"] = estimator_html_repr(self)\n        return output\n\n\nclass ClassifierMixin:\n    \"\"\"Mixin class for all classifiers in scikit-learn.\"\"\"\n\n    _estimator_type = \"classifier\"\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of ``self.predict(X)`` wrt. `y`.\n        \"\"\"\n        from .metrics import accuracy_score\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {'requires_y': True}\n\n\nclass RegressorMixin:\n    \"\"\"Mixin class for all regression estimators in scikit-learn.\"\"\"\n    _estimator_type = \"regressor\"\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the coefficient of determination :math:`R^2` of the\n        prediction.\n\n        The coefficient :math:`R^2` is defined as :math:`(1 - \\\\frac{u}{v})`,\n        where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n        ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n        y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n        can be negative (because the model can be arbitrarily worse). A\n        constant model that always predicts the expected value of `y`,\n        disregarding the input features, would get a :math:`R^2` score of\n        0.0.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).\n        \"\"\"\n\n        from .metrics import r2_score\n        y_pred = self.predict(X)\n        return r2_score(y, y_pred, sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {'requires_y': True}\n\n\nclass ClusterMixin:\n    \"\"\"Mixin class for all cluster estimators in scikit-learn.\"\"\"\n    _estimator_type = \"clusterer\"\n\n    def fit_predict(self, X, y=None):\n        \"\"\"\n        Perform clustering on `X` and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,), dtype=np.int64\n            Cluster labels.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n        self.fit(X)\n        return self.labels_\n\n    def _more_tags(self):\n        return {\"preserves_dtype\": []}\n\n\nclass BiclusterMixin:\n    \"\"\"Mixin class for all bicluster estimators in scikit-learn.\"\"\"\n\n    @property\n    def biclusters_(self):\n        \"\"\"Convenient way to get row and column indicators together.\n\n        Returns the ``rows_`` and ``columns_`` members.\n        \"\"\"\n        return self.rows_, self.columns_\n\n    def get_indices(self, i):\n        \"\"\"Row and column indices of the `i`'th bicluster.\n\n        Only works if ``rows_`` and ``columns_`` attributes exist.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n\n        Returns\n        -------\n        row_ind : ndarray, dtype=np.intp\n            Indices of rows in the dataset that belong to the bicluster.\n        col_ind : ndarray, dtype=np.intp\n            Indices of columns in the dataset that belong to the bicluster.\n\n        \"\"\"\n        rows = self.rows_[i]\n        columns = self.columns_[i]\n        return np.nonzero(rows)[0], np.nonzero(columns)[0]\n\n    def get_shape(self, i):\n        \"\"\"Shape of the `i`'th bicluster.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n\n        Returns\n        -------\n        n_rows : int\n            Number of rows in the bicluster.\n\n        n_cols : int\n            Number of columns in the bicluster.\n        \"\"\"\n        indices = self.get_indices(i)\n        return tuple(len(i) for i in indices)\n\n    def get_submatrix(self, i, data):\n        \"\"\"Return the submatrix corresponding to bicluster `i`.\n\n        Parameters\n        ----------\n        i : int\n            The index of the cluster.\n        data : array-like of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        submatrix : ndarray of shape (n_rows, n_cols)\n            The submatrix corresponding to bicluster `i`.\n\n        Notes\n        -----\n        Works with sparse matrices. Only works if ``rows_`` and\n        ``columns_`` attributes exist.\n        \"\"\"\n        from .utils.validation import check_array\n        data = check_array(data, accept_sparse='csr')\n        row_ind, col_ind = self.get_indices(i)\n        return data[row_ind[:, np.newaxis], col_ind]\n\n\nclass TransformerMixin:\n    \"\"\"Mixin class for all transformers in scikit-learn.\"\"\"\n\n    def fit_transform(self, X, y=None, **fit_params):\n        \"\"\"\n        Fit to data, then transform it.\n\n        Fits transformer to `X` and `y` with optional parameters `fit_params`\n        and returns a transformed version of `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input samples.\n\n        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n                default=None\n            Target values (None for unsupervised transformations).\n\n        **fit_params : dict\n            Additional fit parameters.\n\n        Returns\n        -------\n        X_new : ndarray array of shape (n_samples, n_features_new)\n            Transformed array.\n        \"\"\"\n        # non-optimized default implementation; override when a better\n        # method is possible for a given clustering algorithm\n        if y is None:\n            # fit method of arity 1 (unsupervised transformation)\n            return self.fit(X, **fit_params).transform(X)\n        else:\n            # fit method of arity 2 (supervised transformation)\n            return self.fit(X, y, **fit_params).transform(X)\n\n\nclass DensityMixin:\n    \"\"\"Mixin class for all density estimators in scikit-learn.\"\"\"\n    _estimator_type = \"DensityEstimator\"\n\n    def score(self, X, y=None):\n        \"\"\"Return the score of the model on the data `X`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        score : float\n        \"\"\"\n        pass\n\n\nclass OutlierMixin:\n    \"\"\"Mixin class for all outlier detection estimators in scikit-learn.\"\"\"\n    _estimator_type = \"outlier_detector\"\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Perform fit on X and returns labels for X.\n\n        Returns -1 for outliers and 1 for inliers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features)\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            1 for inliers, -1 for outliers.\n        \"\"\"\n        # override for transductive outlier detectors like LocalOulierFactor\n        return self.fit(X).predict(X)\n\n\nclass MetaEstimatorMixin:\n    _required_parameters = [\"estimator\"]\n    \"\"\"Mixin class for all meta estimators in scikit-learn.\"\"\"\n\n\nclass MultiOutputMixin:\n    \"\"\"Mixin to mark estimators that support multioutput.\"\"\"\n    def _more_tags(self):\n        return {'multioutput': True}\n\n\nclass _UnstableArchMixin:\n    \"\"\"Mark estimators that are non-determinstic on 32bit or PowerPC\"\"\"\n    def _more_tags(self):\n        return {'non_deterministic': (\n            _IS_32BIT or platform.machine().startswith(('ppc', 'powerpc')))}\n\n\ndef is_classifier(estimator):\n    \"\"\"Return True if the given estimator is (probably) a classifier.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a classifier and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"classifier\"\n\n\ndef is_regressor(estimator):\n    \"\"\"Return True if the given estimator is (probably) a regressor.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a regressor and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"regressor\"\n\n\ndef is_outlier_detector(estimator):\n    \"\"\"Return True if the given estimator is (probably) an outlier detector.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is an outlier detector and False otherwise.\n    \"\"\"\n    return getattr(estimator, \"_estimator_type\", None) == \"outlier_detector\"\n\n\ndef _is_pairwise(estimator):\n    \"\"\"Returns True if estimator is pairwise.\n\n    - If the `_pairwise` attribute and the tag are present and consistent,\n      then use the value and not issue a warning.\n    - If the `_pairwise` attribute and the tag are present and not\n      consistent, use the `_pairwise` value and issue a deprecation\n      warning.\n    - If only the `_pairwise` attribute is present and it is not False,\n      issue a deprecation warning and use the `_pairwise` value.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if the estimator is pairwise and False otherwise.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=FutureWarning)\n        has_pairwise_attribute = hasattr(estimator, '_pairwise')\n        pairwise_attribute = getattr(estimator, '_pairwise', False)\n    pairwise_tag = _safe_tags(estimator, key=\"pairwise\")\n\n    if has_pairwise_attribute:\n        if pairwise_attribute != pairwise_tag:\n            warnings.warn(\n                \"_pairwise was deprecated in 0.24 and will be removed in 1.1 \"\n                \"(renaming of 0.26). Set the estimator tags of your estimator \"\n                \"instead\",\n                FutureWarning\n            )\n        return pairwise_attribute\n\n    # use pairwise tag when the attribute is not present\n    return pairwise_tag\n",867],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/inspect.py":["\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargvalues(), getcallargs() - get info about function arguments\n    getfullargspec() - same, with support for Python 3 features\n    formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\n    signature() - get a Signature object for the callable\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = ('Ka-Ping Yee <ping@lfw.org>',\n              'Yury Selivanov <yselivanov@sprymix.com>')\n\nimport abc\nimport ast\nimport dis\nimport collections.abc\nimport enum\nimport importlib.machinery\nimport itertools\nimport linecache\nimport os\nimport re\nimport sys\nimport tokenize\nimport token\nimport types\nimport warnings\nimport functools\nimport builtins\nfrom operator import attrgetter\nfrom collections import namedtuple, OrderedDict\n\n# Create constants for the compiler flags in Include/code.h\n# We try to get them from dis to avoid duplication\nmod_dict = globals()\nfor k, v in dis.COMPILER_FLAG_NAMES.items():\n    mod_dict[\"CO_\" + v] = k\n\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __cached__      pathname to byte compiled file\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, type)\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        __func__        function object containing implementation of method\n        __self__        instance to which this method is bound\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    __func__ attribute (etc) when an object passes ismethod().\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__get__\") and not hasattr(tp, \"__set__\")\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have a __set__ or a __delete__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__set__\") or hasattr(tp, \"__delete__\")\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        __code__        code object containing compiled function bytecode\n        __defaults__    tuple of any default values for arguments\n        __globals__     global namespace in which this function was defined\n        __annotations__ dict of parameter annotations\n        __kwdefaults__  dict of keyword only parameters with defaults\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef _has_code_flag(f, flag):\n    \"\"\"Return true if ``f`` is a function (or a method or functools.partial\n    wrapper wrapping a function) whose code object has the given ``flag``\n    set in its flags.\"\"\"\n    while ismethod(f):\n        f = f.__func__\n    f = functools._unwrap_partial(f)\n    if not isfunction(f):\n        return False\n    return bool(f.__code__.co_flags & flag)\n\ndef isgeneratorfunction(obj):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provide the same attributes as functions.\n    See help(isfunction) for a list of attributes.\"\"\"\n    return _has_code_flag(obj, CO_GENERATOR)\n\ndef iscoroutinefunction(obj):\n    \"\"\"Return true if the object is a coroutine function.\n\n    Coroutine functions are defined with \"async def\" syntax.\n    \"\"\"\n    return _has_code_flag(obj, CO_COROUTINE)\n\ndef isasyncgenfunction(obj):\n    \"\"\"Return true if the object is an asynchronous generator function.\n\n    Asynchronous generator functions are defined with \"async def\"\n    syntax and have \"yield\" expressions in their body.\n    \"\"\"\n    return _has_code_flag(obj, CO_ASYNC_GENERATOR)\n\ndef isasyncgen(object):\n    \"\"\"Return true if the object is an asynchronous generator.\"\"\"\n    return isinstance(object, types.AsyncGeneratorType)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef iscoroutine(object):\n    \"\"\"Return true if the object is a coroutine.\"\"\"\n    return isinstance(object, types.CoroutineType)\n\ndef isawaitable(object):\n    \"\"\"Return true if object can be passed to an ``await`` expression.\"\"\"\n    return (isinstance(object, types.CoroutineType) or\n            isinstance(object, types.GeneratorType) and\n                bool(object.gi_code.co_flags & CO_ITERABLE_COROUTINE) or\n            isinstance(object, collections.abc.Awaitable))\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount         number of arguments (not including *, ** args\n                            or keyword only arguments)\n        co_code             string of raw compiled bytecode\n        co_cellvars         tuple of names of cell variables\n        co_consts           tuple of constants used in the bytecode\n        co_filename         name of file in which this code object was created\n        co_firstlineno      number of first line in Python source code\n        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n                            | 16=nested | 32=generator | 64=nofree | 128=coroutine\n                            | 256=iterable_coroutine | 512=async_generator\n        co_freevars         tuple of names of free variables\n        co_posonlyargcount  number of positional only arguments\n        co_kwonlyargcount   number of keyword only arguments (not including ** arg)\n        co_lnotab           encoded mapping of line numbers to bytecode indices\n        co_name             name with which this code object was defined\n        co_names            tuple of names of local variables\n        co_nlocals          number of local variables\n        co_stacksize        virtual machine stack space required\n        co_varnames         tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    if not isinstance(object, type):\n        return False\n    if object.__flags__ & TPFLAGS_IS_ABSTRACT:\n        return True\n    if not issubclass(type(object), abc.ABCMeta):\n        return False\n    if hasattr(object, '__abstractmethods__'):\n        # It looks like ABCMeta.__new__ has finished running;\n        # TPFLAGS_IS_ABSTRACT should have been accurate.\n        return False\n    # It looks like ABCMeta.__new__ has not finished running yet; we're\n    # probably in __init_subclass__. We'll look for abstractmethods manually.\n    for name, value in object.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            return True\n    for base in object.__bases__:\n        for name in getattr(base, \"__abstractmethods__\", ()):\n            value = getattr(object, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                return True\n    return False\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    if isclass(object):\n        mro = (object,) + getmro(object)\n    else:\n        mro = ()\n    results = []\n    processed = set()\n    names = dir(object)\n    # :dd any DynamicClassAttributes to the list of names if object is a class;\n    # this may result in duplicate entries if, for example, a virtual\n    # attribute with the same name as a DynamicClassAttribute exists\n    try:\n        for base in object.__bases__:\n            for k, v in base.__dict__.items():\n                if isinstance(v, types.DynamicClassAttribute):\n                    names.append(k)\n    except AttributeError:\n        pass\n    for key in names:\n        # First try to get the value via getattr.  Some descriptors don't\n        # like calling their __get__ (see bug #1785), so fall back to\n        # looking in the __dict__.\n        try:\n            value = getattr(object, key)\n            # handle the duplicate key\n            if key in processed:\n                raise AttributeError\n        except AttributeError:\n            for base in mro:\n                if key in base.__dict__:\n                    value = base.__dict__[key]\n                    break\n            else:\n                # could be a (currently) missing slot member, or a buggy\n                # __dir__; discard and move on\n                continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n        processed.add(key)\n    results.sort(key=lambda pair: pair[0])\n    return results\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method or descriptor\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained by calling getattr; if this fails, or if the\n           resulting object does not live anywhere in the class' mro (including\n           metaclasses) then the object is looked up in the defining class's\n           dict (found by walking the mro).\n\n    If one of the items in dir(cls) is stored in the metaclass it will now\n    be discovered and not have None be listed as the class in which it was\n    defined.  Any items whose home class cannot be discovered are skipped.\n    \"\"\"\n\n    mro = getmro(cls)\n    metamro = getmro(type(cls)) # for attributes stored in the metaclass\n    metamro = tuple(cls for cls in metamro if cls not in (type, object))\n    class_bases = (cls,) + mro\n    all_bases = class_bases + metamro\n    names = dir(cls)\n    # :dd any DynamicClassAttributes to the list of names;\n    # this may result in duplicate entries if, for example, a virtual\n    # attribute with the same name as a DynamicClassAttribute exists.\n    for base in mro:\n        for k, v in base.__dict__.items():\n            if isinstance(v, types.DynamicClassAttribute):\n                names.append(k)\n    result = []\n    processed = set()\n\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Normal objects will be looked up with both getattr and directly in\n        # its class' dict (in case getattr fails [bug #1785], and also to look\n        # for a docstring).\n        # For DynamicClassAttributes on the second pass we only look in the\n        # class's dict.\n        #\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        homecls = None\n        get_obj = None\n        dict_obj = None\n        if name not in processed:\n            try:\n                if name == '__dict__':\n                    raise Exception(\"__dict__ is special, don't want the proxy\")\n                get_obj = getattr(cls, name)\n            except Exception as exc:\n                pass\n            else:\n                homecls = getattr(get_obj, \"__objclass__\", homecls)\n                if homecls not in class_bases:\n                    # if the resulting object does not live somewhere in the\n                    # mro, drop it and search the mro manually\n                    homecls = None\n                    last_cls = None\n                    # first look in the classes\n                    for srch_cls in class_bases:\n                        srch_obj = getattr(srch_cls, name, None)\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    # then check the metaclasses\n                    for srch_cls in metamro:\n                        try:\n                            srch_obj = srch_cls.__getattr__(cls, name)\n                        except AttributeError:\n                            continue\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    if last_cls is not None:\n                        homecls = last_cls\n        for base in all_bases:\n            if name in base.__dict__:\n                dict_obj = base.__dict__[name]\n                if homecls not in metamro:\n                    homecls = base\n                break\n        if homecls is None:\n            # unable to locate the attribute anywhere, most likely due to\n            # buggy custom __dir__; discard and move on\n            continue\n        obj = get_obj if get_obj is not None else dict_obj\n        # Classify the object or its descriptor.\n        if isinstance(dict_obj, (staticmethod, types.BuiltinMethodType)):\n            kind = \"static method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, (classmethod, types.ClassMethodDescriptorType)):\n            kind = \"class method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, property):\n            kind = \"property\"\n            obj = dict_obj\n        elif isroutine(obj):\n            kind = \"method\"\n        else:\n            kind = \"data\"\n        result.append(Attribute(name, kind, homecls, obj))\n        processed.add(name)\n    return result\n\n# ----------------------------------------------------------- class helpers\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    return cls.__mro__\n\n# -------------------------------------------------------- function helpers\n\ndef unwrap(func, *, stop=None):\n    \"\"\"Get the object wrapped by *func*.\n\n   Follows the chain of :attr:`__wrapped__` attributes returning the last\n   object in the chain.\n\n   *stop* is an optional callback accepting an object in the wrapper chain\n   as its sole argument that allows the unwrapping to be terminated early if\n   the callback returns a true value. If the callback never returns a true\n   value, the last object in the chain is returned as usual. For example,\n   :func:`signature` uses this to stop unwrapping if any object in the\n   chain has a ``__signature__`` attribute defined.\n\n   :exc:`ValueError` is raised if a cycle is encountered.\n\n    \"\"\"\n    if stop is None:\n        def _is_wrapper(f):\n            return hasattr(f, '__wrapped__')\n    else:\n        def _is_wrapper(f):\n            return hasattr(f, '__wrapped__') and not stop(f)\n    f = func  # remember the original func for error reporting\n    # Memoise by id to tolerate non-hashable objects, but store objects to\n    # ensure they aren't destroyed, which would allow their IDs to be reused.\n    memo = {id(f): f}\n    recursion_limit = sys.getrecursionlimit()\n    while _is_wrapper(func):\n        func = func.__wrapped__\n        id_func = id(func)\n        if (id_func in memo) or (len(memo) >= recursion_limit):\n            raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n        memo[id_func] = func\n    return func\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = line.expandtabs()\n    return len(expline) - len(expline.lstrip())\n\ndef _findclass(func):\n    cls = sys.modules.get(func.__module__)\n    if cls is None:\n        return None\n    for name in func.__qualname__.split('.')[:-1]:\n        cls = getattr(cls, name)\n    if not isclass(cls):\n        return None\n    return cls\n\ndef _finddoc(obj):\n    if isclass(obj):\n        for base in obj.__mro__:\n            if base is not object:\n                try:\n                    doc = base.__doc__\n                except AttributeError:\n                    continue\n                if doc is not None:\n                    return doc\n        return None\n\n    if ismethod(obj):\n        name = obj.__func__.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            getattr(getattr(self, name, None), '__func__') is obj.__func__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    elif isfunction(obj):\n        name = obj.__name__\n        cls = _findclass(obj)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif isbuiltin(obj):\n        name = obj.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            self.__qualname__ + '.' + name == obj.__qualname__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    # Should be tested before isdatadescriptor().\n    elif isinstance(obj, property):\n        func = obj.fget\n        name = func.__name__\n        cls = _findclass(func)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif ismethoddescriptor(obj) or isdatadescriptor(obj):\n        name = obj.__name__\n        cls = obj.__objclass__\n        if getattr(cls, name) is not obj:\n            return None\n        if ismemberdescriptor(obj):\n            slots = getattr(cls, '__slots__', None)\n            if isinstance(slots, dict) and name in slots:\n                return slots[name]\n    else:\n        return None\n    for base in cls.__mro__:\n        try:\n            doc = getattr(base, name).__doc__\n        except AttributeError:\n            continue\n        if doc is not None:\n            return doc\n    return None\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if doc is None:\n        try:\n            doc = _finddoc(object)\n        except (AttributeError, TypeError):\n            return None\n    if not isinstance(doc, str):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = doc.expandtabs().split('\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxsize\n        for line in lines[1:]:\n            content = len(line.lstrip())\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxsize:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return '\\n'.join(lines)\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if getattr(object, '__file__', None):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        if hasattr(object, '__module__'):\n            module = sys.modules.get(object.__module__)\n            if getattr(module, '__file__', None):\n                return module.__file__\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('module, class, method, function, traceback, frame, or '\n                    'code object was expected, got {}'.format(\n                    type(object).__name__))\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    fname = os.path.basename(path)\n    # Check for paths that look like an actual module file\n    suffixes = [(-len(suffix), suffix)\n                    for suffix in importlib.machinery.all_suffixes()]\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix in suffixes:\n        if fname.endswith(suffix):\n            return fname[:neglen]\n    return None\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]\n    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]\n    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n        filename = (os.path.splitext(filename)[0] +\n                    importlib.machinery.SOURCE_SUFFIXES[0])\n    elif any(filename.endswith(s) for s in\n                 importlib.machinery.EXTENSION_SUFFIXES):\n        return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n        return filename\n    # or it is in the linecache\n    if filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.copy().items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['builtins']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\n\nclass ClassFoundException(Exception):\n    pass\n\n\nclass _ClassFinder(ast.NodeVisitor):\n\n    def __init__(self, qualname):\n        self.stack = []\n        self.qualname = qualname\n\n    def visit_FunctionDef(self, node):\n        self.stack.append(node.name)\n        self.stack.append('<locals>')\n        self.generic_visit(node)\n        self.stack.pop()\n        self.stack.pop()\n\n    visit_AsyncFunctionDef = visit_FunctionDef\n\n    def visit_ClassDef(self, node):\n        self.stack.append(node.name)\n        if self.qualname == '.'.join(self.stack):\n            # Return the decorator for the class if present\n            if node.decorator_list:\n                line_number = node.decorator_list[0].lineno\n            else:\n                line_number = node.lineno\n\n            # decrement by one since lines starts with indexing by zero\n            line_number -= 1\n            raise ClassFoundException(line_number)\n        self.generic_visit(node)\n        self.stack.pop()\n\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An OSError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getsourcefile(object)\n    if file:\n        # Invalidate cache if needed.\n        linecache.checkcache(file)\n    else:\n        file = getfile(object)\n        # Allow filenames in form of \"<something>\" to pass through.\n        # `doctest` monkeypatches `linecache` module to enable\n        # inspection, so let `linecache.getlines` to be called.\n        if not (file.startswith('<') and file.endswith('>')):\n            raise OSError('source code not available')\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise OSError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        qualname = object.__qualname__\n        source = ''.join(lines)\n        tree = ast.parse(source)\n        class_finder = _ClassFinder(qualname)\n        try:\n            class_finder.visit(tree)\n        except ClassFoundException as e:\n            line_number = e.args[0]\n            return lines, line_number\n        else:\n            raise OSError('could not find class definition')\n\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise OSError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            try:\n                line = lines[lnum]\n            except IndexError:\n                raise OSError('lineno is out of bounds')\n            if pat.match(line):\n                break\n            lnum = lnum - 1\n        return lines, lnum\n    raise OSError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (OSError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and lines[start].strip() in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(lines[end].expandtabs())\n                end = end + 1\n            return ''.join(comments)\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and lines[end].lstrip()[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [lines[end].expandtabs().lstrip()]\n            if end > 0:\n                end = end - 1\n                comment = lines[end].expandtabs().lstrip()\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = lines[end].expandtabs().lstrip()\n            while comments and comments[0].strip() == '#':\n                comments[:1] = []\n            while comments and comments[-1].strip() == '#':\n                comments[-1:] = []\n            return ''.join(comments)\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.indecorator = False\n        self.decoratorhasargs = False\n        self.last = 1\n        self.body_col0 = None\n\n    def tokeneater(self, type, token, srowcol, erowcol, line):\n        if not self.started and not self.indecorator:\n            # skip any decorators\n            if token == \"@\":\n                self.indecorator = True\n            # look for the first \"def\", \"class\" or \"lambda\"\n            elif token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif token == \"(\":\n            if self.indecorator:\n                self.decoratorhasargs = True\n        elif token == \")\":\n            if self.indecorator:\n                self.indecorator = False\n                self.decoratorhasargs = False\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srowcol[0]\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n            # hitting a NEWLINE when in a decorator without args\n            # ends the decorator\n            if self.indecorator and not self.decoratorhasargs:\n                self.indecorator = False\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            if self.body_col0 is None and self.started:\n                self.body_col0 = erowcol[1]\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif type == tokenize.COMMENT:\n            if self.body_col0 is not None and srowcol[1] >= self.body_col0:\n                # Include comments if indented at least as much as the block\n                self.last = srowcol[0]\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokens = tokenize.generate_tokens(iter(lines).__next__)\n        for _token in tokens:\n            blockfinder.tokeneater(*_token)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An OSError is\n    raised if the source code cannot be retrieved.\"\"\"\n    object = unwrap(object)\n    lines, lnum = findsource(object)\n\n    if istraceback(object):\n        object = object.tb_frame\n\n    # for module or frame that corresponds to module, return all source lines\n    if (ismodule(object) or\n        (isframe(object) and object.f_code.co_name == \"<module>\")):\n        return lines, 0\n    else:\n        return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    OSError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return ''.join(lines)\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=False):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if parent not in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args, varargs, varkw')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where\n    'args' is the list of argument names. Keyword-only arguments are\n    appended. 'varargs' and 'varkw' are the names of the * and **\n    arguments or None.\"\"\"\n    if not iscode(co):\n        raise TypeError('{!r} is not a code object'.format(co))\n\n    names = co.co_varnames\n    nargs = co.co_argcount\n    nkwargs = co.co_kwonlyargcount\n    args = list(names[:nargs])\n    kwonlyargs = list(names[nargs:nargs+nkwargs])\n    step = 0\n\n    nargs += nkwargs\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args + kwonlyargs, varargs, varkw)\n\nArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n\ndef getargspec(func):\n    \"\"\"Get the names and default values of a function's parameters.\n\n    A tuple of four things is returned: (args, varargs, keywords, defaults).\n    'args' is a list of the argument names, including keyword-only argument names.\n    'varargs' and 'keywords' are the names of the * and ** parameters or None.\n    'defaults' is an n-tuple of the default values of the last n parameters.\n\n    This function is deprecated, as it does not support annotations or\n    keyword-only parameters and will raise ValueError if either is present\n    on the supplied callable.\n\n    For a more structured introspection API, use inspect.signature() instead.\n\n    Alternatively, use getfullargspec() for an API with a similar namedtuple\n    based interface, but full support for annotations and keyword-only\n    parameters.\n\n    Deprecated since Python 3.5, use `inspect.getfullargspec()`.\n    \"\"\"\n    warnings.warn(\"inspect.getargspec() is deprecated since Python 3.0, \"\n                  \"use inspect.signature() or inspect.getfullargspec()\",\n                  DeprecationWarning, stacklevel=2)\n    args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = \\\n        getfullargspec(func)\n    if kwonlyargs or ann:\n        raise ValueError(\"Function has keyword-only parameters or annotations\"\n                         \", use inspect.signature() API which can support them\")\n    return ArgSpec(args, varargs, varkw, defaults)\n\nFullArgSpec = namedtuple('FullArgSpec',\n    'args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations')\n\ndef getfullargspec(func):\n    \"\"\"Get the names and default values of a callable object's parameters.\n\n    A tuple of seven things is returned:\n    (args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations).\n    'args' is a list of the parameter names.\n    'varargs' and 'varkw' are the names of the * and ** parameters or None.\n    'defaults' is an n-tuple of the default values of the last n parameters.\n    'kwonlyargs' is a list of keyword-only parameter names.\n    'kwonlydefaults' is a dictionary mapping names from kwonlyargs to defaults.\n    'annotations' is a dictionary mapping parameter names to annotations.\n\n    Notable differences from inspect.signature():\n      - the \"self\" parameter is always reported, even for bound methods\n      - wrapper chains defined by __wrapped__ *not* unwrapped automatically\n    \"\"\"\n    try:\n        # Re: `skip_bound_arg=False`\n        #\n        # There is a notable difference in behaviour between getfullargspec\n        # and Signature: the former always returns 'self' parameter for bound\n        # methods, whereas the Signature always shows the actual calling\n        # signature of the passed object.\n        #\n        # To simulate this behaviour, we \"unbind\" bound methods, to trick\n        # inspect.signature to always return their first parameter (\"self\",\n        # usually)\n\n        # Re: `follow_wrapper_chains=False`\n        #\n        # getfullargspec() historically ignored __wrapped__ attributes,\n        # so we ensure that remains the case in 3.3+\n\n        sig = _signature_from_callable(func,\n                                       follow_wrapper_chains=False,\n                                       skip_bound_arg=False,\n                                       sigcls=Signature)\n    except Exception as ex:\n        # Most of the times 'signature' will raise ValueError.\n        # But, it can also raise AttributeError, and, maybe something\n        # else. So to be fully backwards compatible, we catch all\n        # possible exceptions here, and reraise a TypeError.\n        raise TypeError('unsupported callable') from ex\n\n    args = []\n    varargs = None\n    varkw = None\n    posonlyargs = []\n    kwonlyargs = []\n    annotations = {}\n    defaults = ()\n    kwdefaults = {}\n\n    if sig.return_annotation is not sig.empty:\n        annotations['return'] = sig.return_annotation\n\n    for param in sig.parameters.values():\n        kind = param.kind\n        name = param.name\n\n        if kind is _POSITIONAL_ONLY:\n            posonlyargs.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _POSITIONAL_OR_KEYWORD:\n            args.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _VAR_POSITIONAL:\n            varargs = name\n        elif kind is _KEYWORD_ONLY:\n            kwonlyargs.append(name)\n            if param.default is not param.empty:\n                kwdefaults[name] = param.default\n        elif kind is _VAR_KEYWORD:\n            varkw = name\n\n        if param.annotation is not param.empty:\n            annotations[name] = param.annotation\n\n    if not kwdefaults:\n        # compatibility with 'func.__kwdefaults__'\n        kwdefaults = None\n\n    if not defaults:\n        # compatibility with 'func.__defaults__'\n        defaults = None\n\n    return FullArgSpec(posonlyargs + args, varargs, varkw, defaults,\n                       kwonlyargs, kwdefaults, annotations)\n\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names.\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef formatannotation(annotation, base_module=None):\n    if getattr(annotation, '__module__', None) == 'typing':\n        return repr(annotation).replace('typing.', '')\n    if isinstance(annotation, type):\n        if annotation.__module__ in ('builtins', base_module):\n            return annotation.__qualname__\n        return annotation.__module__+'.'+annotation.__qualname__\n    return repr(annotation)\n\ndef formatannotationrelativeto(object):\n    module = getattr(object, '__module__', None)\n    def _formatannotation(annotation):\n        return formatannotation(annotation, module)\n    return _formatannotation\n\ndef formatargspec(args, varargs=None, varkw=None, defaults=None,\n                  kwonlyargs=(), kwonlydefaults={}, annotations={},\n                  formatarg=str,\n                  formatvarargs=lambda name: '*' + name,\n                  formatvarkw=lambda name: '**' + name,\n                  formatvalue=lambda value: '=' + repr(value),\n                  formatreturns=lambda text: ' -> ' + text,\n                  formatannotation=formatannotation):\n    \"\"\"Format an argument spec from the values returned by getfullargspec.\n\n    The first seven arguments are (args, varargs, varkw, defaults,\n    kwonlyargs, kwonlydefaults, annotations).  The other five arguments\n    are the corresponding optional formatting functions that are called to\n    turn names and values into strings.  The last argument is an optional\n    function to format the sequence of arguments.\n\n    Deprecated since Python 3.5: use the `signature` function and `Signature`\n    objects.\n    \"\"\"\n\n    from warnings import warn\n\n    warn(\"`formatargspec` is deprecated since Python 3.5. Use `signature` and \"\n         \"the `Signature` object directly\",\n         DeprecationWarning,\n         stacklevel=2)\n\n    def formatargandannotation(arg):\n        result = formatarg(arg)\n        if arg in annotations:\n            result += ': ' + formatannotation(annotations[arg])\n        return result\n    specs = []\n    if defaults:\n        firstdefault = len(args) - len(defaults)\n    for i, arg in enumerate(args):\n        spec = formatargandannotation(arg)\n        if defaults and i >= firstdefault:\n            spec = spec + formatvalue(defaults[i - firstdefault])\n        specs.append(spec)\n    if varargs is not None:\n        specs.append(formatvarargs(formatargandannotation(varargs)))\n    else:\n        if kwonlyargs:\n            specs.append('*')\n    if kwonlyargs:\n        for kwonlyarg in kwonlyargs:\n            spec = formatargandannotation(kwonlyarg)\n            if kwonlydefaults and kwonlyarg in kwonlydefaults:\n                spec += formatvalue(kwonlydefaults[kwonlyarg])\n            specs.append(spec)\n    if varkw is not None:\n        specs.append(formatvarkw(formatargandannotation(varkw)))\n    result = '(' + ', '.join(specs) + ')'\n    if 'return' in annotations:\n        result += formatreturns(formatannotation(annotations['return']))\n    return result\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value)):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(convert(args[i]))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + ', '.join(specs) + ')'\n\ndef _missing_arguments(f_name, argnames, pos, values):\n    names = [repr(name) for name in argnames if name not in values]\n    missing = len(names)\n    if missing == 1:\n        s = names[0]\n    elif missing == 2:\n        s = \"{} and {}\".format(*names)\n    else:\n        tail = \", {} and {}\".format(*names[-2:])\n        del names[-2:]\n        s = \", \".join(names) + tail\n    raise TypeError(\"%s() missing %i required %s argument%s: %s\" %\n                    (f_name, missing,\n                      \"positional\" if pos else \"keyword-only\",\n                      \"\" if missing == 1 else \"s\", s))\n\ndef _too_many(f_name, args, kwonly, varargs, defcount, given, values):\n    atleast = len(args) - defcount\n    kwonly_given = len([arg for arg in kwonly if arg in values])\n    if varargs:\n        plural = atleast != 1\n        sig = \"at least %d\" % (atleast,)\n    elif defcount:\n        plural = True\n        sig = \"from %d to %d\" % (atleast, len(args))\n    else:\n        plural = len(args) != 1\n        sig = str(len(args))\n    kwonly_sig = \"\"\n    if kwonly_given:\n        msg = \" positional argument%s (and %d keyword-only argument%s)\"\n        kwonly_sig = (msg % (\"s\" if given != 1 else \"\", kwonly_given,\n                             \"s\" if kwonly_given != 1 else \"\"))\n    raise TypeError(\"%s() takes %s positional argument%s but %d%s %s given\" %\n            (f_name, sig, \"s\" if plural else \"\", given, kwonly_sig,\n             \"was\" if given == 1 and not kwonly_given else \"were\"))\n\ndef getcallargs(func, /, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    spec = getfullargspec(func)\n    args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = spec\n    f_name = func.__name__\n    arg2value = {}\n\n\n    if ismethod(func) and func.__self__ is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.__self__,) + positional\n    num_pos = len(positional)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n\n    n = min(num_pos, num_args)\n    for i in range(n):\n        arg2value[args[i]] = positional[i]\n    if varargs:\n        arg2value[varargs] = tuple(positional[n:])\n    possible_kwargs = set(args + kwonlyargs)\n    if varkw:\n        arg2value[varkw] = {}\n    for kw, value in named.items():\n        if kw not in possible_kwargs:\n            if not varkw:\n                raise TypeError(\"%s() got an unexpected keyword argument %r\" %\n                                (f_name, kw))\n            arg2value[varkw][kw] = value\n            continue\n        if kw in arg2value:\n            raise TypeError(\"%s() got multiple values for argument %r\" %\n                            (f_name, kw))\n        arg2value[kw] = value\n    if num_pos > num_args and not varargs:\n        _too_many(f_name, args, kwonlyargs, varargs, num_defaults,\n                   num_pos, arg2value)\n    if num_pos < num_args:\n        req = args[:num_args - num_defaults]\n        for arg in req:\n            if arg not in arg2value:\n                _missing_arguments(f_name, req, True, arg2value)\n        for i, arg in enumerate(args[num_args - num_defaults:]):\n            if arg not in arg2value:\n                arg2value[arg] = defaults[i]\n    missing = 0\n    for kwarg in kwonlyargs:\n        if kwarg not in arg2value:\n            if kwonlydefaults and kwarg in kwonlydefaults:\n                arg2value[kwarg] = kwonlydefaults[kwarg]\n            else:\n                missing += 1\n    if missing:\n        _missing_arguments(f_name, kwonlyargs, False, arg2value)\n    return arg2value\n\nClosureVars = namedtuple('ClosureVars', 'nonlocals globals builtins unbound')\n\ndef getclosurevars(func):\n    \"\"\"\n    Get the mapping of free variables to their current values.\n\n    Returns a named tuple of dicts mapping the current nonlocal, global\n    and builtin references as seen by the body of the function. A final\n    set of unbound names that could not be resolved is also provided.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.__func__\n\n    if not isfunction(func):\n        raise TypeError(\"{!r} is not a Python function\".format(func))\n\n    code = func.__code__\n    # Nonlocal references are named in co_freevars and resolved\n    # by looking them up in __closure__ by positional index\n    if func.__closure__ is None:\n        nonlocal_vars = {}\n    else:\n        nonlocal_vars = {\n            var : cell.cell_contents\n            for var, cell in zip(code.co_freevars, func.__closure__)\n       }\n\n    # Global and builtin references are named in co_names and resolved\n    # by looking them up in __globals__ or __builtins__\n    global_ns = func.__globals__\n    builtin_ns = global_ns.get(\"__builtins__\", builtins.__dict__)\n    if ismodule(builtin_ns):\n        builtin_ns = builtin_ns.__dict__\n    global_vars = {}\n    builtin_vars = {}\n    unbound_names = set()\n    for name in code.co_names:\n        if name in (\"None\", \"True\", \"False\"):\n            # Because these used to be builtins instead of keywords, they\n            # may still show up as name references. We ignore them.\n            continue\n        try:\n            global_vars[name] = global_ns[name]\n        except KeyError:\n            try:\n                builtin_vars[name] = builtin_ns[name]\n            except KeyError:\n                unbound_names.add(name)\n\n    return ClosureVars(nonlocal_vars, global_vars,\n                       builtin_vars, unbound_names)\n\n# -------------------------------------------------- stack frame extraction\n\nTraceback = namedtuple('Traceback', 'filename lineno function code_context index')\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except OSError:\n            lines = index = None\n        else:\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\nFrameInfo = namedtuple('FrameInfo', ('frame',) + Traceback._fields)\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        frameinfo = (frame,) + getframeinfo(frame, context)\n        framelist.append(FrameInfo(*frameinfo))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n        framelist.append(FrameInfo(*frameinfo))\n        tb = tb.tb_next\n    return framelist\n\ndef currentframe():\n    \"\"\"Return the frame of the caller or None if this is not possible.\"\"\"\n    return sys._getframe(1) if hasattr(sys, \"_getframe\") else None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n\n\n# ------------------------------------------------ static version of getattr\n\n_sentinel = object()\n\ndef _static_getmro(klass):\n    return type.__dict__['__mro__'].__get__(klass)\n\ndef _check_instance(obj, attr):\n    instance_dict = {}\n    try:\n        instance_dict = object.__getattribute__(obj, \"__dict__\")\n    except AttributeError:\n        pass\n    return dict.get(instance_dict, attr, _sentinel)\n\n\ndef _check_class(klass, attr):\n    for entry in _static_getmro(klass):\n        if _shadowed_dict(type(entry)) is _sentinel:\n            try:\n                return entry.__dict__[attr]\n            except KeyError:\n                pass\n    return _sentinel\n\ndef _is_type(obj):\n    try:\n        _static_getmro(obj)\n    except TypeError:\n        return False\n    return True\n\ndef _shadowed_dict(klass):\n    dict_attr = type.__dict__[\"__dict__\"]\n    for entry in _static_getmro(klass):\n        try:\n            class_dict = dict_attr.__get__(entry)[\"__dict__\"]\n        except KeyError:\n            pass\n        else:\n            if not (type(class_dict) is types.GetSetDescriptorType and\n                    class_dict.__name__ == \"__dict__\" and\n                    class_dict.__objclass__ is entry):\n                return class_dict\n    return _sentinel\n\ndef getattr_static(obj, attr, default=_sentinel):\n    \"\"\"Retrieve attributes without triggering dynamic lookup via the\n       descriptor protocol,  __getattr__ or __getattribute__.\n\n       Note: this function may not be able to retrieve all attributes\n       that getattr can fetch (like dynamically created attributes)\n       and may find attributes that getattr can't (like descriptors\n       that raise AttributeError). It can also return descriptor objects\n       instead of instance members in some cases. See the\n       documentation for details.\n    \"\"\"\n    instance_result = _sentinel\n    if not _is_type(obj):\n        klass = type(obj)\n        dict_attr = _shadowed_dict(klass)\n        if (dict_attr is _sentinel or\n            type(dict_attr) is types.MemberDescriptorType):\n            instance_result = _check_instance(obj, attr)\n    else:\n        klass = obj\n\n    klass_result = _check_class(klass, attr)\n\n    if instance_result is not _sentinel and klass_result is not _sentinel:\n        if (_check_class(type(klass_result), '__get__') is not _sentinel and\n            _check_class(type(klass_result), '__set__') is not _sentinel):\n            return klass_result\n\n    if instance_result is not _sentinel:\n        return instance_result\n    if klass_result is not _sentinel:\n        return klass_result\n\n    if obj is klass:\n        # for types we check the metaclass too\n        for entry in _static_getmro(type(klass)):\n            if _shadowed_dict(type(entry)) is _sentinel:\n                try:\n                    return entry.__dict__[attr]\n                except KeyError:\n                    pass\n    if default is not _sentinel:\n        return default\n    raise AttributeError(attr)\n\n\n# ------------------------------------------------ generator introspection\n\nGEN_CREATED = 'GEN_CREATED'\nGEN_RUNNING = 'GEN_RUNNING'\nGEN_SUSPENDED = 'GEN_SUSPENDED'\nGEN_CLOSED = 'GEN_CLOSED'\n\ndef getgeneratorstate(generator):\n    \"\"\"Get current state of a generator-iterator.\n\n    Possible states are:\n      GEN_CREATED: Waiting to start execution.\n      GEN_RUNNING: Currently being executed by the interpreter.\n      GEN_SUSPENDED: Currently suspended at a yield expression.\n      GEN_CLOSED: Execution has completed.\n    \"\"\"\n    if generator.gi_running:\n        return GEN_RUNNING\n    if generator.gi_frame is None:\n        return GEN_CLOSED\n    if generator.gi_frame.f_lasti == -1:\n        return GEN_CREATED\n    return GEN_SUSPENDED\n\n\ndef getgeneratorlocals(generator):\n    \"\"\"\n    Get the mapping of generator local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n\n    if not isgenerator(generator):\n        raise TypeError(\"{!r} is not a Python generator\".format(generator))\n\n    frame = getattr(generator, \"gi_frame\", None)\n    if frame is not None:\n        return generator.gi_frame.f_locals\n    else:\n        return {}\n\n\n# ------------------------------------------------ coroutine introspection\n\nCORO_CREATED = 'CORO_CREATED'\nCORO_RUNNING = 'CORO_RUNNING'\nCORO_SUSPENDED = 'CORO_SUSPENDED'\nCORO_CLOSED = 'CORO_CLOSED'\n\ndef getcoroutinestate(coroutine):\n    \"\"\"Get current state of a coroutine object.\n\n    Possible states are:\n      CORO_CREATED: Waiting to start execution.\n      CORO_RUNNING: Currently being executed by the interpreter.\n      CORO_SUSPENDED: Currently suspended at an await expression.\n      CORO_CLOSED: Execution has completed.\n    \"\"\"\n    if coroutine.cr_running:\n        return CORO_RUNNING\n    if coroutine.cr_frame is None:\n        return CORO_CLOSED\n    if coroutine.cr_frame.f_lasti == -1:\n        return CORO_CREATED\n    return CORO_SUSPENDED\n\n\ndef getcoroutinelocals(coroutine):\n    \"\"\"\n    Get the mapping of coroutine local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n    frame = getattr(coroutine, \"cr_frame\", None)\n    if frame is not None:\n        return frame.f_locals\n    else:\n        return {}\n\n\n###############################################################################\n### Function Signature Object (PEP 362)\n###############################################################################\n\n\n_WrapperDescriptor = type(type.__call__)\n_MethodWrapper = type(all.__call__)\n_ClassMethodWrapper = type(int.__dict__['from_bytes'])\n\n_NonUserDefinedCallables = (_WrapperDescriptor,\n                            _MethodWrapper,\n                            _ClassMethodWrapper,\n                            types.BuiltinFunctionType)\n\n\ndef _signature_get_user_defined_method(cls, method_name):\n    \"\"\"Private helper. Checks if ``cls`` has an attribute\n    named ``method_name`` and returns it only if it is a\n    pure python function.\n    \"\"\"\n    try:\n        meth = getattr(cls, method_name)\n    except AttributeError:\n        return\n    else:\n        if not isinstance(meth, _NonUserDefinedCallables):\n            # Once '__signature__' will be added to 'C'-level\n            # callables, this check won't be necessary\n            return meth\n\n\ndef _signature_get_partial(wrapped_sig, partial, extra_args=()):\n    \"\"\"Private helper to calculate how 'wrapped_sig' signature will\n    look like after applying a 'functools.partial' object (or alike)\n    on it.\n    \"\"\"\n\n    old_params = wrapped_sig.parameters\n    new_params = OrderedDict(old_params.items())\n\n    partial_args = partial.args or ()\n    partial_keywords = partial.keywords or {}\n\n    if extra_args:\n        partial_args = extra_args + partial_args\n\n    try:\n        ba = wrapped_sig.bind_partial(*partial_args, **partial_keywords)\n    except TypeError as ex:\n        msg = 'partial object {!r} has incorrect arguments'.format(partial)\n        raise ValueError(msg) from ex\n\n\n    transform_to_kwonly = False\n    for param_name, param in old_params.items():\n        try:\n            arg_value = ba.arguments[param_name]\n        except KeyError:\n            pass\n        else:\n            if param.kind is _POSITIONAL_ONLY:\n                # If positional-only parameter is bound by partial,\n                # it effectively disappears from the signature\n                new_params.pop(param_name)\n                continue\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                if param_name in partial_keywords:\n                    # This means that this parameter, and all parameters\n                    # after it should be keyword-only (and var-positional\n                    # should be removed). Here's why. Consider the following\n                    # function:\n                    #     foo(a, b, *args, c):\n                    #         pass\n                    #\n                    # \"partial(foo, a='spam')\" will have the following\n                    # signature: \"(*, a='spam', b, c)\". Because attempting\n                    # to call that partial with \"(10, 20)\" arguments will\n                    # raise a TypeError, saying that \"a\" argument received\n                    # multiple values.\n                    transform_to_kwonly = True\n                    # Set the new default value\n                    new_params[param_name] = param.replace(default=arg_value)\n                else:\n                    # was passed as a positional argument\n                    new_params.pop(param.name)\n                    continue\n\n            if param.kind is _KEYWORD_ONLY:\n                # Set the new default value\n                new_params[param_name] = param.replace(default=arg_value)\n\n        if transform_to_kwonly:\n            assert param.kind is not _POSITIONAL_ONLY\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                new_param = new_params[param_name].replace(kind=_KEYWORD_ONLY)\n                new_params[param_name] = new_param\n                new_params.move_to_end(param_name)\n            elif param.kind in (_KEYWORD_ONLY, _VAR_KEYWORD):\n                new_params.move_to_end(param_name)\n            elif param.kind is _VAR_POSITIONAL:\n                new_params.pop(param.name)\n\n    return wrapped_sig.replace(parameters=new_params.values())\n\n\ndef _signature_bound_method(sig):\n    \"\"\"Private helper to transform signatures for unbound\n    functions to bound methods.\n    \"\"\"\n\n    params = tuple(sig.parameters.values())\n\n    if not params or params[0].kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n        raise ValueError('invalid method signature')\n\n    kind = params[0].kind\n    if kind in (_POSITIONAL_OR_KEYWORD, _POSITIONAL_ONLY):\n        # Drop first parameter:\n        # '(p1, p2[, ...])' -> '(p2[, ...])'\n        params = params[1:]\n    else:\n        if kind is not _VAR_POSITIONAL:\n            # Unless we add a new parameter type we never\n            # get here\n            raise ValueError('invalid argument type')\n        # It's a var-positional parameter.\n        # Do nothing. '(*args[, ...])' -> '(*args[, ...])'\n\n    return sig.replace(parameters=params)\n\n\ndef _signature_is_builtin(obj):\n    \"\"\"Private helper to test if `obj` is a callable that might\n    support Argument Clinic's __text_signature__ protocol.\n    \"\"\"\n    return (isbuiltin(obj) or\n            ismethoddescriptor(obj) or\n            isinstance(obj, _NonUserDefinedCallables) or\n            # Can't test 'isinstance(type)' here, as it would\n            # also be True for regular python classes\n            obj in (type, object))\n\n\ndef _signature_is_functionlike(obj):\n    \"\"\"Private helper to test if `obj` is a duck type of FunctionType.\n    A good example of such objects are functions compiled with\n    Cython, which have all attributes that a pure Python function\n    would have, but have their code statically compiled.\n    \"\"\"\n\n    if not callable(obj) or isclass(obj):\n        # All function-like objects are obviously callables,\n        # and not classes.\n        return False\n\n    name = getattr(obj, '__name__', None)\n    code = getattr(obj, '__code__', None)\n    defaults = getattr(obj, '__defaults__', _void) # Important to use _void ...\n    kwdefaults = getattr(obj, '__kwdefaults__', _void) # ... and not None here\n    annotations = getattr(obj, '__annotations__', None)\n\n    return (isinstance(code, types.CodeType) and\n            isinstance(name, str) and\n            (defaults is None or isinstance(defaults, tuple)) and\n            (kwdefaults is None or isinstance(kwdefaults, dict)) and\n            isinstance(annotations, dict))\n\n\ndef _signature_get_bound_param(spec):\n    \"\"\" Private helper to get first parameter name from a\n    __text_signature__ of a builtin method, which should\n    be in the following format: '($param1, ...)'.\n    Assumptions are that the first argument won't have\n    a default value or an annotation.\n    \"\"\"\n\n    assert spec.startswith('($')\n\n    pos = spec.find(',')\n    if pos == -1:\n        pos = spec.find(')')\n\n    cpos = spec.find(':')\n    assert cpos == -1 or cpos > pos\n\n    cpos = spec.find('=')\n    assert cpos == -1 or cpos > pos\n\n    return spec[2:pos]\n\n\ndef _signature_strip_non_python_syntax(signature):\n    \"\"\"\n    Private helper function. Takes a signature in Argument Clinic's\n    extended signature format.\n\n    Returns a tuple of three things:\n      * that signature re-rendered in standard Python syntax,\n      * the index of the \"self\" parameter (generally 0), or None if\n        the function does not have a \"self\" parameter, and\n      * the index of the last \"positional only\" parameter,\n        or None if the signature has no positional-only parameters.\n    \"\"\"\n\n    if not signature:\n        return signature, None, None\n\n    self_parameter = None\n    last_positional_only = None\n\n    lines = [l.encode('ascii') for l in signature.split('\\n')]\n    generator = iter(lines).__next__\n    token_stream = tokenize.tokenize(generator)\n\n    delayed_comma = False\n    skip_next_comma = False\n    text = []\n    add = text.append\n\n    current_parameter = 0\n    OP = token.OP\n    ERRORTOKEN = token.ERRORTOKEN\n\n    # token stream always starts with ENCODING token, skip it\n    t = next(token_stream)\n    assert t.type == tokenize.ENCODING\n\n    for t in token_stream:\n        type, string = t.type, t.string\n\n        if type == OP:\n            if string == ',':\n                if skip_next_comma:\n                    skip_next_comma = False\n                else:\n                    assert not delayed_comma\n                    delayed_comma = True\n                    current_parameter += 1\n                continue\n\n            if string == '/':\n                assert not skip_next_comma\n                assert last_positional_only is None\n                skip_next_comma = True\n                last_positional_only = current_parameter - 1\n                continue\n\n        if (type == ERRORTOKEN) and (string == '$'):\n            assert self_parameter is None\n            self_parameter = current_parameter\n            continue\n\n        if delayed_comma:\n            delayed_comma = False\n            if not ((type == OP) and (string == ')')):\n                add(', ')\n        add(string)\n        if (string == ','):\n            add(' ')\n    clean_signature = ''.join(text)\n    return clean_signature, self_parameter, last_positional_only\n\n\ndef _signature_fromstr(cls, obj, s, skip_bound_arg=True):\n    \"\"\"Private helper to parse content of '__text_signature__'\n    and return a Signature based on it.\n    \"\"\"\n    # Lazy import ast because it's relatively heavy and\n    # it's not used for other than this function.\n    import ast\n\n    Parameter = cls._parameter_cls\n\n    clean_signature, self_parameter, last_positional_only = \\\n        _signature_strip_non_python_syntax(s)\n\n    program = \"def foo\" + clean_signature + \": pass\"\n\n    try:\n        module = ast.parse(program)\n    except SyntaxError:\n        module = None\n\n    if not isinstance(module, ast.Module):\n        raise ValueError(\"{!r} builtin has invalid signature\".format(obj))\n\n    f = module.body[0]\n\n    parameters = []\n    empty = Parameter.empty\n    invalid = object()\n\n    module = None\n    module_dict = {}\n    module_name = getattr(obj, '__module__', None)\n    if module_name:\n        module = sys.modules.get(module_name, None)\n        if module:\n            module_dict = module.__dict__\n    sys_module_dict = sys.modules.copy()\n\n    def parse_name(node):\n        assert isinstance(node, ast.arg)\n        if node.annotation is not None:\n            raise ValueError(\"Annotations are not currently supported\")\n        return node.arg\n\n    def wrap_value(s):\n        try:\n            value = eval(s, module_dict)\n        except NameError:\n            try:\n                value = eval(s, sys_module_dict)\n            except NameError:\n                raise RuntimeError()\n\n        if isinstance(value, (str, int, float, bytes, bool, type(None))):\n            return ast.Constant(value)\n        raise RuntimeError()\n\n    class RewriteSymbolics(ast.NodeTransformer):\n        def visit_Attribute(self, node):\n            a = []\n            n = node\n            while isinstance(n, ast.Attribute):\n                a.append(n.attr)\n                n = n.value\n            if not isinstance(n, ast.Name):\n                raise RuntimeError()\n            a.append(n.id)\n            value = \".\".join(reversed(a))\n            return wrap_value(value)\n\n        def visit_Name(self, node):\n            if not isinstance(node.ctx, ast.Load):\n                raise ValueError()\n            return wrap_value(node.id)\n\n    def p(name_node, default_node, default=empty):\n        name = parse_name(name_node)\n        if name is invalid:\n            return None\n        if default_node and default_node is not _empty:\n            try:\n                default_node = RewriteSymbolics().visit(default_node)\n                o = ast.literal_eval(default_node)\n            except ValueError:\n                o = invalid\n            if o is invalid:\n                return None\n            default = o if o is not invalid else default\n        parameters.append(Parameter(name, kind, default=default, annotation=empty))\n\n    # non-keyword-only parameters\n    args = reversed(f.args.args)\n    defaults = reversed(f.args.defaults)\n    iter = itertools.zip_longest(args, defaults, fillvalue=None)\n    if last_positional_only is not None:\n        kind = Parameter.POSITIONAL_ONLY\n    else:\n        kind = Parameter.POSITIONAL_OR_KEYWORD\n    for i, (name, default) in enumerate(reversed(list(iter))):\n        p(name, default)\n        if i == last_positional_only:\n            kind = Parameter.POSITIONAL_OR_KEYWORD\n\n    # *args\n    if f.args.vararg:\n        kind = Parameter.VAR_POSITIONAL\n        p(f.args.vararg, empty)\n\n    # keyword-only arguments\n    kind = Parameter.KEYWORD_ONLY\n    for name, default in zip(f.args.kwonlyargs, f.args.kw_defaults):\n        p(name, default)\n\n    # **kwargs\n    if f.args.kwarg:\n        kind = Parameter.VAR_KEYWORD\n        p(f.args.kwarg, empty)\n\n    if self_parameter is not None:\n        # Possibly strip the bound argument:\n        #    - We *always* strip first bound argument if\n        #      it is a module.\n        #    - We don't strip first bound argument if\n        #      skip_bound_arg is False.\n        assert parameters\n        _self = getattr(obj, '__self__', None)\n        self_isbound = _self is not None\n        self_ismodule = ismodule(_self)\n        if self_isbound and (self_ismodule or skip_bound_arg):\n            parameters.pop(0)\n        else:\n            # for builtins, self parameter is always positional-only!\n            p = parameters[0].replace(kind=Parameter.POSITIONAL_ONLY)\n            parameters[0] = p\n\n    return cls(parameters, return_annotation=cls.empty)\n\n\ndef _signature_from_builtin(cls, func, skip_bound_arg=True):\n    \"\"\"Private helper function to get signature for\n    builtin callables.\n    \"\"\"\n\n    if not _signature_is_builtin(func):\n        raise TypeError(\"{!r} is not a Python builtin \"\n                        \"function\".format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if not s:\n        raise ValueError(\"no signature found for builtin {!r}\".format(func))\n\n    return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n\ndef _signature_from_function(cls, func, skip_bound_arg=True):\n    \"\"\"Private helper: constructs Signature for the given python function.\"\"\"\n\n    is_duck_function = False\n    if not isfunction(func):\n        if _signature_is_functionlike(func):\n            is_duck_function = True\n        else:\n            # If it's not a pure Python function, and not a duck type\n            # of pure function:\n            raise TypeError('{!r} is not a Python function'.format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if s:\n        return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n    Parameter = cls._parameter_cls\n\n    # Parameter information.\n    func_code = func.__code__\n    pos_count = func_code.co_argcount\n    arg_names = func_code.co_varnames\n    posonly_count = func_code.co_posonlyargcount\n    positional = arg_names[:pos_count]\n    keyword_only_count = func_code.co_kwonlyargcount\n    keyword_only = arg_names[pos_count:pos_count + keyword_only_count]\n    annotations = func.__annotations__\n    defaults = func.__defaults__\n    kwdefaults = func.__kwdefaults__\n\n    if defaults:\n        pos_default_count = len(defaults)\n    else:\n        pos_default_count = 0\n\n    parameters = []\n\n    non_default_count = pos_count - pos_default_count\n    posonly_left = posonly_count\n\n    # Non-keyword-only parameters w/o defaults.\n    for name in positional[:non_default_count]:\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind))\n        if posonly_left:\n            posonly_left -= 1\n\n    # ... w/ defaults.\n    for offset, name in enumerate(positional[non_default_count:]):\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind,\n                                    default=defaults[offset]))\n        if posonly_left:\n            posonly_left -= 1\n\n    # *args\n    if func_code.co_flags & CO_VARARGS:\n        name = arg_names[pos_count + keyword_only_count]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_POSITIONAL))\n\n    # Keyword-only parameters.\n    for name in keyword_only:\n        default = _empty\n        if kwdefaults is not None:\n            default = kwdefaults.get(name, _empty)\n\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_KEYWORD_ONLY,\n                                    default=default))\n    # **kwargs\n    if func_code.co_flags & CO_VARKEYWORDS:\n        index = pos_count + keyword_only_count\n        if func_code.co_flags & CO_VARARGS:\n            index += 1\n\n        name = arg_names[index]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_KEYWORD))\n\n    # Is 'func' is a pure Python function - don't validate the\n    # parameters list (for correct order and defaults), it should be OK.\n    return cls(parameters,\n               return_annotation=annotations.get('return', _empty),\n               __validate_parameters__=is_duck_function)\n\n\ndef _signature_from_callable(obj, *,\n                             follow_wrapper_chains=True,\n                             skip_bound_arg=True,\n                             sigcls):\n\n    \"\"\"Private helper function to get signature for arbitrary\n    callable objects.\n    \"\"\"\n\n    if not callable(obj):\n        raise TypeError('{!r} is not a callable object'.format(obj))\n\n    if isinstance(obj, types.MethodType):\n        # In this case we skip the first parameter of the underlying\n        # function (usually `self` or `cls`).\n        sig = _signature_from_callable(\n            obj.__func__,\n            follow_wrapper_chains=follow_wrapper_chains,\n            skip_bound_arg=skip_bound_arg,\n            sigcls=sigcls)\n\n        if skip_bound_arg:\n            return _signature_bound_method(sig)\n        else:\n            return sig\n\n    # Was this function wrapped by a decorator?\n    if follow_wrapper_chains:\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")))\n        if isinstance(obj, types.MethodType):\n            # If the unwrapped object is a *method*, we might want to\n            # skip its first parameter (self).\n            # See test_signature_wrapped_bound_method for details.\n            return _signature_from_callable(\n                obj,\n                follow_wrapper_chains=follow_wrapper_chains,\n                skip_bound_arg=skip_bound_arg,\n                sigcls=sigcls)\n\n    try:\n        sig = obj.__signature__\n    except AttributeError:\n        pass\n    else:\n        if sig is not None:\n            if not isinstance(sig, Signature):\n                raise TypeError(\n                    'unexpected object {!r} in __signature__ '\n                    'attribute'.format(sig))\n            return sig\n\n    try:\n        partialmethod = obj._partialmethod\n    except AttributeError:\n        pass\n    else:\n        if isinstance(partialmethod, functools.partialmethod):\n            # Unbound partialmethod (see functools.partialmethod)\n            # This means, that we need to calculate the signature\n            # as if it's a regular partial object, but taking into\n            # account that the first positional argument\n            # (usually `self`, or `cls`) will not be passed\n            # automatically (as for boundmethods)\n\n            wrapped_sig = _signature_from_callable(\n                partialmethod.func,\n                follow_wrapper_chains=follow_wrapper_chains,\n                skip_bound_arg=skip_bound_arg,\n                sigcls=sigcls)\n\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\n            if first_wrapped_param.kind is Parameter.VAR_POSITIONAL:\n                # First argument of the wrapped callable is `*args`, as in\n                # `partialmethod(lambda *args)`.\n                return sig\n            else:\n                sig_params = tuple(sig.parameters.values())\n                assert (not sig_params or\n                        first_wrapped_param is not sig_params[0])\n                new_params = (first_wrapped_param,) + sig_params\n                return sig.replace(parameters=new_params)\n\n    if isfunction(obj) or _signature_is_functionlike(obj):\n        # If it's a pure Python function, or an object that is duck type\n        # of a Python function (Cython functions, for instance), then:\n        return _signature_from_function(sigcls, obj,\n                                        skip_bound_arg=skip_bound_arg)\n\n    if _signature_is_builtin(obj):\n        return _signature_from_builtin(sigcls, obj,\n                                       skip_bound_arg=skip_bound_arg)\n\n    if isinstance(obj, functools.partial):\n        wrapped_sig = _signature_from_callable(\n            obj.func,\n            follow_wrapper_chains=follow_wrapper_chains,\n            skip_bound_arg=skip_bound_arg,\n            sigcls=sigcls)\n        return _signature_get_partial(wrapped_sig, obj)\n\n    sig = None\n    if isinstance(obj, type):\n        # obj is a class or a metaclass\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = _signature_get_user_defined_method(type(obj), '__call__')\n        if call is not None:\n            sig = _signature_from_callable(\n                call,\n                follow_wrapper_chains=follow_wrapper_chains,\n                skip_bound_arg=skip_bound_arg,\n                sigcls=sigcls)\n        else:\n            # Now we check if the 'obj' class has a '__new__' method\n            new = _signature_get_user_defined_method(obj, '__new__')\n            if new is not None:\n                sig = _signature_from_callable(\n                    new,\n                    follow_wrapper_chains=follow_wrapper_chains,\n                    skip_bound_arg=skip_bound_arg,\n                    sigcls=sigcls)\n            else:\n                # Finally, we should have at least __init__ implemented\n                init = _signature_get_user_defined_method(obj, '__init__')\n                if init is not None:\n                    sig = _signature_from_callable(\n                        init,\n                        follow_wrapper_chains=follow_wrapper_chains,\n                        skip_bound_arg=skip_bound_arg,\n                        sigcls=sigcls)\n\n        if sig is None:\n            # At this point we know, that `obj` is a class, with no user-\n            # defined '__init__', '__new__', or class-level '__call__'\n\n            for base in obj.__mro__[:-1]:\n                # Since '__text_signature__' is implemented as a\n                # descriptor that extracts text signature from the\n                # class docstring, if 'obj' is derived from a builtin\n                # class, its own '__text_signature__' may be 'None'.\n                # Therefore, we go through the MRO (except the last\n                # class in there, which is 'object') to find the first\n                # class with non-empty text signature.\n                try:\n                    text_sig = base.__text_signature__\n                except AttributeError:\n                    pass\n                else:\n                    if text_sig:\n                        # If 'obj' class has a __text_signature__ attribute:\n                        # return a signature based on it\n                        return _signature_fromstr(sigcls, obj, text_sig)\n\n            # No '__text_signature__' was found for the 'obj' class.\n            # Last option is to check if its '__init__' is\n            # object.__init__ or type.__init__.\n            if type not in obj.__mro__:\n                # We have a class (not metaclass), but no user-defined\n                # __init__ or __new__ for it\n                if (obj.__init__ is object.__init__ and\n                    obj.__new__ is object.__new__):\n                    # Return a signature of 'object' builtin.\n                    return sigcls.from_callable(object)\n                else:\n                    raise ValueError(\n                        'no signature found for builtin type {!r}'.format(obj))\n\n    elif not isinstance(obj, _NonUserDefinedCallables):\n        # An object with __call__\n        # We also check that the 'obj' is not an instance of\n        # _WrapperDescriptor or _MethodWrapper to avoid\n        # infinite recursion (and even potential segfault)\n        call = _signature_get_user_defined_method(type(obj), '__call__')\n        if call is not None:\n            try:\n                sig = _signature_from_callable(\n                    call,\n                    follow_wrapper_chains=follow_wrapper_chains,\n                    skip_bound_arg=skip_bound_arg,\n                    sigcls=sigcls)\n            except ValueError as ex:\n                msg = 'no signature found for {!r}'.format(obj)\n                raise ValueError(msg) from ex\n\n    if sig is not None:\n        # For classes and objects we skip the first parameter of their\n        # __call__, __new__, or __init__ methods\n        if skip_bound_arg:\n            return _signature_bound_method(sig)\n        else:\n            return sig\n\n    if isinstance(obj, types.BuiltinFunctionType):\n        # Raise a nicer error message for builtins\n        msg = 'no signature found for builtin function {!r}'.format(obj)\n        raise ValueError(msg)\n\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))\n\n\nclass _void:\n    \"\"\"A private marker - used in Parameter & Signature.\"\"\"\n\n\nclass _empty:\n    \"\"\"Marker object for Signature.empty and Parameter.empty.\"\"\"\n\n\nclass _ParameterKind(enum.IntEnum):\n    POSITIONAL_ONLY = 0\n    POSITIONAL_OR_KEYWORD = 1\n    VAR_POSITIONAL = 2\n    KEYWORD_ONLY = 3\n    VAR_KEYWORD = 4\n\n    def __str__(self):\n        return self._name_\n\n    @property\n    def description(self):\n        return _PARAM_NAME_MAPPING[self]\n\n_POSITIONAL_ONLY         = _ParameterKind.POSITIONAL_ONLY\n_POSITIONAL_OR_KEYWORD   = _ParameterKind.POSITIONAL_OR_KEYWORD\n_VAR_POSITIONAL          = _ParameterKind.VAR_POSITIONAL\n_KEYWORD_ONLY            = _ParameterKind.KEYWORD_ONLY\n_VAR_KEYWORD             = _ParameterKind.VAR_KEYWORD\n\n_PARAM_NAME_MAPPING = {\n    _POSITIONAL_ONLY: 'positional-only',\n    _POSITIONAL_OR_KEYWORD: 'positional or keyword',\n    _VAR_POSITIONAL: 'variadic positional',\n    _KEYWORD_ONLY: 'keyword-only',\n    _VAR_KEYWORD: 'variadic keyword'\n}\n\n\nclass Parameter:\n    \"\"\"Represents a parameter in a function signature.\n\n    Has the following public attributes:\n\n    * name : str\n        The name of the parameter as a string.\n    * default : object\n        The default value for the parameter if specified.  If the\n        parameter has no default value, this attribute is set to\n        `Parameter.empty`.\n    * annotation\n        The annotation for the parameter if specified.  If the\n        parameter has no annotation, this attribute is set to\n        `Parameter.empty`.\n    * kind : str\n        Describes how argument values are bound to the parameter.\n        Possible values: `Parameter.POSITIONAL_ONLY`,\n        `Parameter.POSITIONAL_OR_KEYWORD`, `Parameter.VAR_POSITIONAL`,\n        `Parameter.KEYWORD_ONLY`, `Parameter.VAR_KEYWORD`.\n    \"\"\"\n\n    __slots__ = ('_name', '_kind', '_default', '_annotation')\n\n    POSITIONAL_ONLY         = _POSITIONAL_ONLY\n    POSITIONAL_OR_KEYWORD   = _POSITIONAL_OR_KEYWORD\n    VAR_POSITIONAL          = _VAR_POSITIONAL\n    KEYWORD_ONLY            = _KEYWORD_ONLY\n    VAR_KEYWORD             = _VAR_KEYWORD\n\n    empty = _empty\n\n    def __init__(self, name, kind, *, default=_empty, annotation=_empty):\n        try:\n            self._kind = _ParameterKind(kind)\n        except ValueError:\n            raise ValueError(f'value {kind!r} is not a valid Parameter.kind')\n        if default is not _empty:\n            if self._kind in (_VAR_POSITIONAL, _VAR_KEYWORD):\n                msg = '{} parameters cannot have default values'\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n        self._default = default\n        self._annotation = annotation\n\n        if name is _empty:\n            raise ValueError('name is a required attribute for Parameter')\n\n        if not isinstance(name, str):\n            msg = 'name must be a str, not a {}'.format(type(name).__name__)\n            raise TypeError(msg)\n\n        if name[0] == '.' and name[1:].isdigit():\n            # These are implicit arguments generated by comprehensions. In\n            # order to provide a friendlier interface to users, we recast\n            # their name as \"implicitN\" and treat them as positional-only.\n            # See issue 19611.\n            if self._kind != _POSITIONAL_OR_KEYWORD:\n                msg = (\n                    'implicit arguments must be passed as '\n                    'positional or keyword arguments, not {}'\n                )\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n            self._kind = _POSITIONAL_ONLY\n            name = 'implicit{}'.format(name[1:])\n\n        if not name.isidentifier():\n            raise ValueError('{!r} is not a valid parameter name'.format(name))\n\n        self._name = name\n\n    def __reduce__(self):\n        return (type(self),\n                (self._name, self._kind),\n                {'_default': self._default,\n                 '_annotation': self._annotation})\n\n    def __setstate__(self, state):\n        self._default = state['_default']\n        self._annotation = state['_annotation']\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def default(self):\n        return self._default\n\n    @property\n    def annotation(self):\n        return self._annotation\n\n    @property\n    def kind(self):\n        return self._kind\n\n    def replace(self, *, name=_void, kind=_void,\n                annotation=_void, default=_void):\n        \"\"\"Creates a customized copy of the Parameter.\"\"\"\n\n        if name is _void:\n            name = self._name\n\n        if kind is _void:\n            kind = self._kind\n\n        if annotation is _void:\n            annotation = self._annotation\n\n        if default is _void:\n            default = self._default\n\n        return type(self)(name, kind, default=default, annotation=annotation)\n\n    def __str__(self):\n        kind = self.kind\n        formatted = self._name\n\n        # Add annotation and default value\n        if self._annotation is not _empty:\n            formatted = '{}: {}'.format(formatted,\n                                       formatannotation(self._annotation))\n\n        if self._default is not _empty:\n            if self._annotation is not _empty:\n                formatted = '{} = {}'.format(formatted, repr(self._default))\n            else:\n                formatted = '{}={}'.format(formatted, repr(self._default))\n\n        if kind == _VAR_POSITIONAL:\n            formatted = '*' + formatted\n        elif kind == _VAR_KEYWORD:\n            formatted = '**' + formatted\n\n        return formatted\n\n    def __repr__(self):\n        return '<{} \"{}\">'.format(self.__class__.__name__, self)\n\n    def __hash__(self):\n        return hash((self.name, self.kind, self.annotation, self.default))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Parameter):\n            return NotImplemented\n        return (self._name == other._name and\n                self._kind == other._kind and\n                self._default == other._default and\n                self._annotation == other._annotation)\n\n\nclass BoundArguments:\n    \"\"\"Result of `Signature.bind` call.  Holds the mapping of arguments\n    to the function's parameters.\n\n    Has the following public attributes:\n\n    * arguments : dict\n        An ordered mutable mapping of parameters' names to arguments' values.\n        Does not contain arguments' default values.\n    * signature : Signature\n        The Signature object that created this instance.\n    * args : tuple\n        Tuple of positional arguments values.\n    * kwargs : dict\n        Dict of keyword arguments values.\n    \"\"\"\n\n    __slots__ = ('arguments', '_signature', '__weakref__')\n\n    def __init__(self, signature, arguments):\n        self.arguments = arguments\n        self._signature = signature\n\n    @property\n    def signature(self):\n        return self._signature\n\n    @property\n    def args(self):\n        args = []\n        for param_name, param in self._signature.parameters.items():\n            if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                break\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                # We're done here. Other arguments\n                # will be mapped in 'BoundArguments.kwargs'\n                break\n            else:\n                if param.kind == _VAR_POSITIONAL:\n                    # *args\n                    args.extend(arg)\n                else:\n                    # plain argument\n                    args.append(arg)\n\n        return tuple(args)\n\n    @property\n    def kwargs(self):\n        kwargs = {}\n        kwargs_started = False\n        for param_name, param in self._signature.parameters.items():\n            if not kwargs_started:\n                if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                    kwargs_started = True\n                else:\n                    if param_name not in self.arguments:\n                        kwargs_started = True\n                        continue\n\n            if not kwargs_started:\n                continue\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                pass\n            else:\n                if param.kind == _VAR_KEYWORD:\n                    # **kwargs\n                    kwargs.update(arg)\n                else:\n                    # plain keyword argument\n                    kwargs[param_name] = arg\n\n        return kwargs\n\n    def apply_defaults(self):\n        \"\"\"Set default values for missing arguments.\n\n        For variable-positional arguments (*args) the default is an\n        empty tuple.\n\n        For variable-keyword arguments (**kwargs) the default is an\n        empty dict.\n        \"\"\"\n        arguments = self.arguments\n        new_arguments = []\n        for name, param in self._signature.parameters.items():\n            try:\n                new_arguments.append((name, arguments[name]))\n            except KeyError:\n                if param.default is not _empty:\n                    val = param.default\n                elif param.kind is _VAR_POSITIONAL:\n                    val = ()\n                elif param.kind is _VAR_KEYWORD:\n                    val = {}\n                else:\n                    # This BoundArguments was likely produced by\n                    # Signature.bind_partial().\n                    continue\n                new_arguments.append((name, val))\n        self.arguments = dict(new_arguments)\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, BoundArguments):\n            return NotImplemented\n        return (self.signature == other.signature and\n                self.arguments == other.arguments)\n\n    def __setstate__(self, state):\n        self._signature = state['_signature']\n        self.arguments = state['arguments']\n\n    def __getstate__(self):\n        return {'_signature': self._signature, 'arguments': self.arguments}\n\n    def __repr__(self):\n        args = []\n        for arg, value in self.arguments.items():\n            args.append('{}={!r}'.format(arg, value))\n        return '<{} ({})>'.format(self.__class__.__name__, ', '.join(args))\n\n\nclass Signature:\n    \"\"\"A Signature object represents the overall signature of a function.\n    It stores a Parameter object for each parameter accepted by the\n    function, as well as information specific to the function itself.\n\n    A Signature object has the following public attributes and methods:\n\n    * parameters : OrderedDict\n        An ordered mapping of parameters' names to the corresponding\n        Parameter objects (keyword-only arguments are in the same order\n        as listed in `code.co_varnames`).\n    * return_annotation : object\n        The annotation for the return type of the function if specified.\n        If the function has no annotation for its return type, this\n        attribute is set to `Signature.empty`.\n    * bind(*args, **kwargs) -> BoundArguments\n        Creates a mapping from positional and keyword arguments to\n        parameters.\n    * bind_partial(*args, **kwargs) -> BoundArguments\n        Creates a partial mapping from positional and keyword arguments\n        to parameters (simulating 'functools.partial' behavior.)\n    \"\"\"\n\n    __slots__ = ('_return_annotation', '_parameters')\n\n    _parameter_cls = Parameter\n    _bound_arguments_cls = BoundArguments\n\n    empty = _empty\n\n    def __init__(self, parameters=None, *, return_annotation=_empty,\n                 __validate_parameters__=True):\n        \"\"\"Constructs Signature from the given list of Parameter\n        objects and 'return_annotation'.  All arguments are optional.\n        \"\"\"\n\n        if parameters is None:\n            params = OrderedDict()\n        else:\n            if __validate_parameters__:\n                params = OrderedDict()\n                top_kind = _POSITIONAL_ONLY\n                kind_defaults = False\n\n                for param in parameters:\n                    kind = param.kind\n                    name = param.name\n\n                    if kind < top_kind:\n                        msg = (\n                            'wrong parameter order: {} parameter before {} '\n                            'parameter'\n                        )\n                        msg = msg.format(top_kind.description,\n                                         kind.description)\n                        raise ValueError(msg)\n                    elif kind > top_kind:\n                        kind_defaults = False\n                        top_kind = kind\n\n                    if kind in (_POSITIONAL_ONLY, _POSITIONAL_OR_KEYWORD):\n                        if param.default is _empty:\n                            if kind_defaults:\n                                # No default for this parameter, but the\n                                # previous parameter of the same kind had\n                                # a default\n                                msg = 'non-default argument follows default ' \\\n                                      'argument'\n                                raise ValueError(msg)\n                        else:\n                            # There is a default for this parameter.\n                            kind_defaults = True\n\n                    if name in params:\n                        msg = 'duplicate parameter name: {!r}'.format(name)\n                        raise ValueError(msg)\n\n                    params[name] = param\n            else:\n                params = OrderedDict((param.name, param) for param in parameters)\n\n        self._parameters = types.MappingProxyType(params)\n        self._return_annotation = return_annotation\n\n    @classmethod\n    def from_function(cls, func):\n        \"\"\"Constructs Signature for the given python function.\n\n        Deprecated since Python 3.5, use `Signature.from_callable()`.\n        \"\"\"\n\n        warnings.warn(\"inspect.Signature.from_function() is deprecated since \"\n                      \"Python 3.5, use Signature.from_callable()\",\n                      DeprecationWarning, stacklevel=2)\n        return _signature_from_function(cls, func)\n\n    @classmethod\n    def from_builtin(cls, func):\n        \"\"\"Constructs Signature for the given builtin function.\n\n        Deprecated since Python 3.5, use `Signature.from_callable()`.\n        \"\"\"\n\n        warnings.warn(\"inspect.Signature.from_builtin() is deprecated since \"\n                      \"Python 3.5, use Signature.from_callable()\",\n                      DeprecationWarning, stacklevel=2)\n        return _signature_from_builtin(cls, func)\n\n    @classmethod\n    def from_callable(cls, obj, *, follow_wrapped=True):\n        \"\"\"Constructs Signature for the given callable object.\"\"\"\n        return _signature_from_callable(obj, sigcls=cls,\n                                        follow_wrapper_chains=follow_wrapped)\n\n    @property\n    def parameters(self):\n        return self._parameters\n\n    @property\n    def return_annotation(self):\n        return self._return_annotation\n\n    def replace(self, *, parameters=_void, return_annotation=_void):\n        \"\"\"Creates a customized copy of the Signature.\n        Pass 'parameters' and/or 'return_annotation' arguments\n        to override them in the new copy.\n        \"\"\"\n\n        if parameters is _void:\n            parameters = self.parameters.values()\n\n        if return_annotation is _void:\n            return_annotation = self._return_annotation\n\n        return type(self)(parameters,\n                          return_annotation=return_annotation)\n\n    def _hash_basis(self):\n        params = tuple(param for param in self.parameters.values()\n                             if param.kind != _KEYWORD_ONLY)\n\n        kwo_params = {param.name: param for param in self.parameters.values()\n                                        if param.kind == _KEYWORD_ONLY}\n\n        return params, kwo_params, self.return_annotation\n\n    def __hash__(self):\n        params, kwo_params, return_annotation = self._hash_basis()\n        kwo_params = frozenset(kwo_params.values())\n        return hash((params, kwo_params, return_annotation))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Signature):\n            return NotImplemented\n        return self._hash_basis() == other._hash_basis()\n\n    def _bind(self, args, kwargs, *, partial=False):\n        \"\"\"Private method. Don't use directly.\"\"\"\n\n        arguments = {}\n\n        parameters = iter(self.parameters.values())\n        parameters_ex = ()\n        arg_vals = iter(args)\n\n        while True:\n            # Let's iterate through the positional arguments and corresponding\n            # parameters\n            try:\n                arg_val = next(arg_vals)\n            except StopIteration:\n                # No more positional arguments\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    # No more parameters. That's it. Just need to check that\n                    # we have no `kwargs` after this while loop\n                    break\n                else:\n                    if param.kind == _VAR_POSITIONAL:\n                        # That's OK, just empty *args.  Let's start parsing\n                        # kwargs\n                        break\n                    elif param.name in kwargs:\n                        if param.kind == _POSITIONAL_ONLY:\n                            msg = '{arg!r} parameter is positional only, ' \\\n                                  'but was passed as a keyword'\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg) from None\n                        parameters_ex = (param,)\n                        break\n                    elif (param.kind == _VAR_KEYWORD or\n                                                param.default is not _empty):\n                        # That's fine too - we have a default value for this\n                        # parameter.  So, lets start parsing `kwargs`, starting\n                        # with the current parameter\n                        parameters_ex = (param,)\n                        break\n                    else:\n                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,\n                        # not in `kwargs`\n                        if partial:\n                            parameters_ex = (param,)\n                            break\n                        else:\n                            msg = 'missing a required argument: {arg!r}'\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg) from None\n            else:\n                # We have a positional argument to process\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    raise TypeError('too many positional arguments') from None\n                else:\n                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                        # Looks like we have no parameter for this positional\n                        # argument\n                        raise TypeError(\n                            'too many positional arguments') from None\n\n                    if param.kind == _VAR_POSITIONAL:\n                        # We have an '*args'-like argument, let's fill it with\n                        # all positional arguments we have left and move on to\n                        # the next phase\n                        values = [arg_val]\n                        values.extend(arg_vals)\n                        arguments[param.name] = tuple(values)\n                        break\n\n                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:\n                        raise TypeError(\n                            'multiple values for argument {arg!r}'.format(\n                                arg=param.name)) from None\n\n                    arguments[param.name] = arg_val\n\n        # Now, we iterate through the remaining parameters to process\n        # keyword arguments\n        kwargs_param = None\n        for param in itertools.chain(parameters_ex, parameters):\n            if param.kind == _VAR_KEYWORD:\n                # Memorize that we have a '**kwargs'-like parameter\n                kwargs_param = param\n                continue\n\n            if param.kind == _VAR_POSITIONAL:\n                # Named arguments don't refer to '*args'-like parameters.\n                # We only arrive here if the positional arguments ended\n                # before reaching the last parameter before *args.\n                continue\n\n            param_name = param.name\n            try:\n                arg_val = kwargs.pop(param_name)\n            except KeyError:\n                # We have no value for this parameter.  It's fine though,\n                # if it has a default value, or it is an '*args'-like\n                # parameter, left alone by the processing of positional\n                # arguments.\n                if (not partial and param.kind != _VAR_POSITIONAL and\n                                                    param.default is _empty):\n                    raise TypeError('missing a required argument: {arg!r}'. \\\n                                    format(arg=param_name)) from None\n\n            else:\n                if param.kind == _POSITIONAL_ONLY:\n                    # This should never happen in case of a properly built\n                    # Signature object (but let's have this check here\n                    # to ensure correct behaviour just in case)\n                    raise TypeError('{arg!r} parameter is positional only, '\n                                    'but was passed as a keyword'. \\\n                                    format(arg=param.name))\n\n                arguments[param_name] = arg_val\n\n        if kwargs:\n            if kwargs_param is not None:\n                # Process our '**kwargs'-like parameter\n                arguments[kwargs_param.name] = kwargs\n            else:\n                raise TypeError(\n                    'got an unexpected keyword argument {arg!r}'.format(\n                        arg=next(iter(kwargs))))\n\n        return self._bound_arguments_cls(self, arguments)\n\n    def bind(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that maps the passed `args`\n        and `kwargs` to the function's signature.  Raises `TypeError`\n        if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs)\n\n    def bind_partial(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that partially maps the\n        passed `args` and `kwargs` to the function's signature.\n        Raises `TypeError` if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs, partial=True)\n\n    def __reduce__(self):\n        return (type(self),\n                (tuple(self._parameters.values()),),\n                {'_return_annotation': self._return_annotation})\n\n    def __setstate__(self, state):\n        self._return_annotation = state['_return_annotation']\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__, self)\n\n    def __str__(self):\n        result = []\n        render_pos_only_separator = False\n        render_kw_only_separator = True\n        for param in self.parameters.values():\n            formatted = str(param)\n\n            kind = param.kind\n\n            if kind == _POSITIONAL_ONLY:\n                render_pos_only_separator = True\n            elif render_pos_only_separator:\n                # It's not a positional-only parameter, and the flag\n                # is set to 'True' (there were pos-only params before.)\n                result.append('/')\n                render_pos_only_separator = False\n\n            if kind == _VAR_POSITIONAL:\n                # OK, we have an '*args'-like parameter, so we won't need\n                # a '*' to separate keyword-only arguments\n                render_kw_only_separator = False\n            elif kind == _KEYWORD_ONLY and render_kw_only_separator:\n                # We have a keyword-only parameter to render and we haven't\n                # rendered an '*args'-like parameter before, so add a '*'\n                # separator to the parameters list (\"foo(arg1, *, arg2)\" case)\n                result.append('*')\n                # This condition should be only triggered once, so\n                # reset the flag\n                render_kw_only_separator = False\n\n            result.append(formatted)\n\n        if render_pos_only_separator:\n            # There were only positional-only parameters, hence the\n            # flag was not reset to 'False'\n            result.append('/')\n\n        rendered = '({})'.format(', '.join(result))\n\n        if self.return_annotation is not _empty:\n            anno = formatannotation(self.return_annotation)\n            rendered += ' -> {}'.format(anno)\n\n        return rendered\n\n\ndef signature(obj, *, follow_wrapped=True):\n    \"\"\"Get a signature object for the passed callable.\"\"\"\n    return Signature.from_callable(obj, follow_wrapped=follow_wrapped)\n\n\ndef _main():\n    \"\"\" Logic for inspecting an object given at command line \"\"\"\n    import argparse\n    import importlib\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'object',\n         help=\"The object to be analysed. \"\n              \"It supports the 'module:qualname' syntax\")\n    parser.add_argument(\n        '-d', '--details', action='store_true',\n        help='Display info about the module rather than its source code')\n\n    args = parser.parse_args()\n\n    target = args.object\n    mod_name, has_attrs, attrs = target.partition(\":\")\n    try:\n        obj = module = importlib.import_module(mod_name)\n    except Exception as exc:\n        msg = \"Failed to import {} ({}: {})\".format(mod_name,\n                                                    type(exc).__name__,\n                                                    exc)\n        print(msg, file=sys.stderr)\n        sys.exit(2)\n\n    if has_attrs:\n        parts = attrs.split(\".\")\n        obj = module\n        for part in parts:\n            obj = getattr(obj, part)\n\n    if module.__name__ in sys.builtin_module_names:\n        print(\"Can't get info for builtin modules.\", file=sys.stderr)\n        sys.exit(1)\n\n    if args.details:\n        print('Target: {}'.format(target))\n        print('Origin: {}'.format(getsourcefile(module)))\n        print('Cached: {}'.format(module.__cached__))\n        if obj is module:\n            print('Loader: {}'.format(repr(module.__loader__)))\n            if hasattr(module, '__path__'):\n                print('Submodule search path: {}'.format(module.__path__))\n        else:\n            try:\n                __, lineno = findsource(obj)\n            except Exception:\n                pass\n            else:\n                print('Line: {}'.format(lineno))\n\n        print('\\n')\n    else:\n        print(getsource(obj))\n\n\nif __name__ == \"__main__\":\n    _main()\n",3192],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py":["\"\"\"\nBackends for embarrassingly parallel code.\n\"\"\"\n\nimport gc\nimport os\nimport warnings\nimport threading\nimport functools\nimport contextlib\nfrom abc import ABCMeta, abstractmethod\n\nfrom .my_exceptions import WorkerInterrupt\nfrom ._multiprocessing_helpers import mp\n\nif mp is not None:\n    from .pool import MemmappingPool\n    from multiprocessing.pool import ThreadPool\n    from .executor import get_memmapping_executor\n\n    # Compat between concurrent.futures and multiprocessing TimeoutError\n    from multiprocessing import TimeoutError\n    from concurrent.futures._base import TimeoutError as CfTimeoutError\n    from .externals.loky import process_executor, cpu_count\n\n\nclass ParallelBackendBase(metaclass=ABCMeta):\n    \"\"\"Helper abc which defines all methods a ParallelBackend must implement\"\"\"\n\n    supports_timeout = False\n    supports_inner_max_num_threads = False\n    nesting_level = None\n\n    def __init__(self, nesting_level=None, inner_max_num_threads=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.nesting_level = nesting_level\n        self.inner_max_num_threads = inner_max_num_threads\n\n    MAX_NUM_THREADS_VARS = [\n        'OMP_NUM_THREADS', 'OPENBLAS_NUM_THREADS', 'MKL_NUM_THREADS',\n        'BLIS_NUM_THREADS', 'VECLIB_MAXIMUM_THREADS', 'NUMBA_NUM_THREADS',\n        'NUMEXPR_NUM_THREADS',\n    ]\n\n    TBB_ENABLE_IPC_VAR = \"ENABLE_IPC\"\n\n    @abstractmethod\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs that can actually run in parallel\n\n        n_jobs is the number of workers requested by the callers. Passing\n        n_jobs=-1 means requesting all available workers for instance matching\n        the number of CPU cores on the worker host(s).\n\n        This method should return a guesstimate of the number of workers that\n        can actually perform work concurrently. The primary use case is to make\n        it possible for the caller to know in how many chunks to slice the\n        work.\n\n        In general working on larger data chunks is more efficient (less\n        scheduling overhead and better use of CPU cache prefetching heuristics)\n        as long as all the workers have enough work to do.\n        \"\"\"\n\n    @abstractmethod\n    def apply_async(self, func, callback=None):\n        \"\"\"Schedule a func to be run\"\"\"\n\n    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,\n                  **backend_args):\n        \"\"\"Reconfigure the backend and return the number of workers.\n\n        This makes it possible to reuse an existing backend instance for\n        successive independent calls to Parallel with different parameters.\n        \"\"\"\n        self.parallel = parallel\n        return self.effective_n_jobs(n_jobs)\n\n    def start_call(self):\n        \"\"\"Call-back method called at the beginning of a Parallel call\"\"\"\n\n    def stop_call(self):\n        \"\"\"Call-back method called at the end of a Parallel call\"\"\"\n\n    def terminate(self):\n        \"\"\"Shutdown the workers and free the shared memory.\"\"\"\n\n    def compute_batch_size(self):\n        \"\"\"Determine the optimal batch size\"\"\"\n        return 1\n\n    def batch_completed(self, batch_size, duration):\n        \"\"\"Callback indicate how long it took to run a batch\"\"\"\n\n    def get_exceptions(self):\n        \"\"\"List of exception types to be captured.\"\"\"\n        return []\n\n    def abort_everything(self, ensure_ready=True):\n        \"\"\"Abort any running tasks\n\n        This is called when an exception has been raised when executing a tasks\n        and all the remaining tasks will be ignored and can therefore be\n        aborted to spare computation resources.\n\n        If ensure_ready is True, the backend should be left in an operating\n        state as future tasks might be re-submitted via that same backend\n        instance.\n\n        If ensure_ready is False, the implementer of this method can decide\n        to leave the backend in a closed / terminated state as no new task\n        are expected to be submitted to this backend.\n\n        Setting ensure_ready to False is an optimization that can be leveraged\n        when aborting tasks via killing processes from a local process pool\n        managed by the backend it-self: if we expect no new tasks, there is no\n        point in re-creating new workers.\n        \"\"\"\n        # Does nothing by default: to be overridden in subclasses when\n        # canceling tasks is possible.\n        pass\n\n    def get_nested_backend(self):\n        \"\"\"Backend instance to be used by nested Parallel calls.\n\n        By default a thread-based backend is used for the first level of\n        nesting. Beyond, switch to sequential backend to avoid spawning too\n        many threads on the host.\n        \"\"\"\n        nesting_level = getattr(self, 'nesting_level', 0) + 1\n        if nesting_level > 1:\n            return SequentialBackend(nesting_level=nesting_level), None\n        else:\n            return ThreadingBackend(nesting_level=nesting_level), None\n\n    @contextlib.contextmanager\n    def retrieval_context(self):\n        \"\"\"Context manager to manage an execution context.\n\n        Calls to Parallel.retrieve will be made inside this context.\n\n        By default, this does nothing. It may be useful for subclasses to\n        handle nested parallelism. In particular, it may be required to avoid\n        deadlocks if a backend manages a fixed number of workers, when those\n        workers may be asked to do nested Parallel calls. Without\n        'retrieval_context' this could lead to deadlock, as all the workers\n        managed by the backend may be \"busy\" waiting for the nested parallel\n        calls to finish, but the backend has no free workers to execute those\n        tasks.\n        \"\"\"\n        yield\n\n    def _prepare_worker_env(self, n_jobs):\n        \"\"\"Return environment variables limiting threadpools in external libs.\n\n        This function return a dict containing environment variables to pass\n        when creating a pool of process. These environment variables limit the\n        number of threads to `n_threads` for OpenMP, MKL, Accelerated and\n        OpenBLAS libraries in the child processes.\n        \"\"\"\n        explicit_n_threads = self.inner_max_num_threads\n        default_n_threads = str(max(cpu_count() // n_jobs, 1))\n\n        # Set the inner environment variables to self.inner_max_num_threads if\n        # it is given. Else, default to cpu_count // n_jobs unless the variable\n        # is already present in the parent process environment.\n        env = {}\n        for var in self.MAX_NUM_THREADS_VARS:\n            if explicit_n_threads is None:\n                var_value = os.environ.get(var, None)\n                if var_value is None:\n                    var_value = default_n_threads\n            else:\n                var_value = str(explicit_n_threads)\n\n            env[var] = var_value\n\n        if self.TBB_ENABLE_IPC_VAR not in os.environ:\n            # To avoid over-subscription when using TBB, let the TBB schedulers\n            # use Inter Process Communication to coordinate:\n            env[self.TBB_ENABLE_IPC_VAR] = \"1\"\n        return env\n\n    @staticmethod\n    def in_main_thread():\n        return isinstance(threading.current_thread(), threading._MainThread)\n\n\nclass SequentialBackend(ParallelBackendBase):\n    \"\"\"A ParallelBackend which will execute all batches sequentially.\n\n    Does not use/create any threading objects, and hence has minimal\n    overhead. Used when n_jobs == 1.\n    \"\"\"\n\n    uses_threads = True\n    supports_sharedmem = True\n\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs which are going to run in parallel\"\"\"\n        if n_jobs == 0:\n            raise ValueError('n_jobs == 0 in Parallel has no meaning')\n        return 1\n\n    def apply_async(self, func, callback=None):\n        \"\"\"Schedule a func to be run\"\"\"\n        result = ImmediateResult(func)\n        if callback:\n            callback(result)\n        return result\n\n    def get_nested_backend(self):\n        # import is not top level to avoid cyclic import errors.\n        from .parallel import get_active_backend\n\n        # SequentialBackend should neither change the nesting level, the\n        # default backend or the number of jobs. Just return the current one.\n        return get_active_backend()\n\n\nclass PoolManagerMixin(object):\n    \"\"\"A helper class for managing pool of workers.\"\"\"\n\n    _pool = None\n\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs which are going to run in parallel\"\"\"\n        if n_jobs == 0:\n            raise ValueError('n_jobs == 0 in Parallel has no meaning')\n        elif mp is None or n_jobs is None:\n            # multiprocessing is not available or disabled, fallback\n            # to sequential mode\n            return 1\n        elif n_jobs < 0:\n            n_jobs = max(cpu_count() + 1 + n_jobs, 1)\n        return n_jobs\n\n    def terminate(self):\n        \"\"\"Shutdown the process or thread pool\"\"\"\n        if self._pool is not None:\n            self._pool.close()\n            self._pool.terminate()  # terminate does a join()\n            self._pool = None\n\n    def _get_pool(self):\n        \"\"\"Used by apply_async to make it possible to implement lazy init\"\"\"\n        return self._pool\n\n    def apply_async(self, func, callback=None):\n        \"\"\"Schedule a func to be run\"\"\"\n        return self._get_pool().apply_async(\n            SafeFunction(func), callback=callback)\n\n    def abort_everything(self, ensure_ready=True):\n        \"\"\"Shutdown the pool and restart a new one with the same parameters\"\"\"\n        self.terminate()\n        if ensure_ready:\n            self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel,\n                           **self.parallel._backend_args)\n\n\nclass AutoBatchingMixin(object):\n    \"\"\"A helper class for automagically batching jobs.\"\"\"\n\n    # In seconds, should be big enough to hide multiprocessing dispatching\n    # overhead.\n    # This settings was found by running benchmarks/bench_auto_batching.py\n    # with various parameters on various platforms.\n    MIN_IDEAL_BATCH_DURATION = .2\n\n    # Should not be too high to avoid stragglers: long jobs running alone\n    # on a single worker while other workers have no work to process any more.\n    MAX_IDEAL_BATCH_DURATION = 2\n\n    # Batching counters default values\n    _DEFAULT_EFFECTIVE_BATCH_SIZE = 1\n    _DEFAULT_SMOOTHED_BATCH_DURATION = 0.0\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE\n        self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION\n\n    def compute_batch_size(self):\n        \"\"\"Determine the optimal batch size\"\"\"\n        old_batch_size = self._effective_batch_size\n        batch_duration = self._smoothed_batch_duration\n        if (batch_duration > 0 and\n                batch_duration < self.MIN_IDEAL_BATCH_DURATION):\n            # The current batch size is too small: the duration of the\n            # processing of a batch of task is not large enough to hide\n            # the scheduling overhead.\n            ideal_batch_size = int(old_batch_size *\n                                   self.MIN_IDEAL_BATCH_DURATION /\n                                   batch_duration)\n            # Multiply by two to limit oscilations between min and max.\n            ideal_batch_size *= 2\n\n            # dont increase the batch size too fast to limit huge batch sizes\n            # potentially leading to starving worker\n            batch_size = min(2 * old_batch_size, ideal_batch_size)\n\n            batch_size = max(batch_size, 1)\n\n            self._effective_batch_size = batch_size\n            if self.parallel.verbose >= 10:\n                self.parallel._print(\n                    \"Batch computation too fast (%.4fs.) \"\n                    \"Setting batch_size=%d.\", (batch_duration, batch_size))\n        elif (batch_duration > self.MAX_IDEAL_BATCH_DURATION and\n              old_batch_size >= 2):\n            # The current batch size is too big. If we schedule overly long\n            # running batches some CPUs might wait with nothing left to do\n            # while a couple of CPUs a left processing a few long running\n            # batches. Better reduce the batch size a bit to limit the\n            # likelihood of scheduling such stragglers.\n\n            # decrease the batch size quickly to limit potential starving\n            ideal_batch_size = int(\n                old_batch_size * self.MIN_IDEAL_BATCH_DURATION / batch_duration\n            )\n            # Multiply by two to limit oscilations between min and max.\n            batch_size = max(2 * ideal_batch_size, 1)\n            self._effective_batch_size = batch_size\n            if self.parallel.verbose >= 10:\n                self.parallel._print(\n                    \"Batch computation too slow (%.4fs.) \"\n                    \"Setting batch_size=%d.\", (batch_duration, batch_size))\n        else:\n            # No batch size adjustment\n            batch_size = old_batch_size\n\n        if batch_size != old_batch_size:\n            # Reset estimation of the smoothed mean batch duration: this\n            # estimate is updated in the multiprocessing apply_async\n            # CallBack as long as the batch_size is constant. Therefore\n            # we need to reset the estimate whenever we re-tune the batch\n            # size.\n            self._smoothed_batch_duration = \\\n                self._DEFAULT_SMOOTHED_BATCH_DURATION\n\n        return batch_size\n\n    def batch_completed(self, batch_size, duration):\n        \"\"\"Callback indicate how long it took to run a batch\"\"\"\n        if batch_size == self._effective_batch_size:\n            # Update the smoothed streaming estimate of the duration of a batch\n            # from dispatch to completion\n            old_duration = self._smoothed_batch_duration\n            if old_duration == self._DEFAULT_SMOOTHED_BATCH_DURATION:\n                # First record of duration for this batch size after the last\n                # reset.\n                new_duration = duration\n            else:\n                # Update the exponentially weighted average of the duration of\n                # batch for the current effective size.\n                new_duration = 0.8 * old_duration + 0.2 * duration\n            self._smoothed_batch_duration = new_duration\n\n    def reset_batch_stats(self):\n        \"\"\"Reset batch statistics to default values.\n\n        This avoids interferences with future jobs.\n        \"\"\"\n        self._effective_batch_size = self._DEFAULT_EFFECTIVE_BATCH_SIZE\n        self._smoothed_batch_duration = self._DEFAULT_SMOOTHED_BATCH_DURATION\n\n\nclass ThreadingBackend(PoolManagerMixin, ParallelBackendBase):\n    \"\"\"A ParallelBackend which will use a thread pool to execute batches in.\n\n    This is a low-overhead backend but it suffers from the Python Global\n    Interpreter Lock if the called function relies a lot on Python objects.\n    Mostly useful when the execution bottleneck is a compiled extension that\n    explicitly releases the GIL (for instance a Cython loop wrapped in a \"with\n    nogil\" block or an expensive call to a library such as NumPy).\n\n    The actual thread pool is lazily initialized: the actual thread pool\n    construction is delayed to the first call to apply_async.\n\n    ThreadingBackend is used as the default backend for nested calls.\n    \"\"\"\n\n    supports_timeout = True\n    uses_threads = True\n    supports_sharedmem = True\n\n    def configure(self, n_jobs=1, parallel=None, **backend_args):\n        \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n        n_jobs = self.effective_n_jobs(n_jobs)\n        if n_jobs == 1:\n            # Avoid unnecessary overhead and use sequential backend instead.\n            raise FallbackToBackend(\n                SequentialBackend(nesting_level=self.nesting_level))\n        self.parallel = parallel\n        self._n_jobs = n_jobs\n        return n_jobs\n\n    def _get_pool(self):\n        \"\"\"Lazily initialize the thread pool\n\n        The actual pool of worker threads is only initialized at the first\n        call to apply_async.\n        \"\"\"\n        if self._pool is None:\n            self._pool = ThreadPool(self._n_jobs)\n        return self._pool\n\n\nclass MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,\n                             ParallelBackendBase):\n    \"\"\"A ParallelBackend which will use a multiprocessing.Pool.\n\n    Will introduce some communication and memory overhead when exchanging\n    input and output data with the with the worker Python processes.\n    However, does not suffer from the Python Global Interpreter Lock.\n    \"\"\"\n\n    supports_timeout = True\n\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs which are going to run in parallel.\n\n        This also checks if we are attempting to create a nested parallel\n        loop.\n        \"\"\"\n        if mp is None:\n            return 1\n\n        if mp.current_process().daemon:\n            # Daemonic processes cannot have children\n            if n_jobs != 1:\n                warnings.warn(\n                    'Multiprocessing-backed parallel loops cannot be nested,'\n                    ' setting n_jobs=1',\n                    stacklevel=3)\n            return 1\n\n        if process_executor._CURRENT_DEPTH > 0:\n            # Mixing loky and multiprocessing in nested loop is not supported\n            if n_jobs != 1:\n                warnings.warn(\n                    'Multiprocessing-backed parallel loops cannot be nested,'\n                    ' below loky, setting n_jobs=1',\n                    stacklevel=3)\n            return 1\n\n        elif not (self.in_main_thread() or self.nesting_level == 0):\n            # Prevent posix fork inside in non-main posix threads\n            if n_jobs != 1:\n                warnings.warn(\n                    'Multiprocessing-backed parallel loops cannot be nested'\n                    ' below threads, setting n_jobs=1',\n                    stacklevel=3)\n            return 1\n\n        return super(MultiprocessingBackend, self).effective_n_jobs(n_jobs)\n\n    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,\n                  **memmappingpool_args):\n        \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n        n_jobs = self.effective_n_jobs(n_jobs)\n        if n_jobs == 1:\n            raise FallbackToBackend(\n                SequentialBackend(nesting_level=self.nesting_level))\n\n        # Make sure to free as much memory as possible before forking\n        gc.collect()\n        self._pool = MemmappingPool(n_jobs, **memmappingpool_args)\n        self.parallel = parallel\n        return n_jobs\n\n    def terminate(self):\n        \"\"\"Shutdown the process or thread pool\"\"\"\n        super(MultiprocessingBackend, self).terminate()\n        self.reset_batch_stats()\n\n\nclass LokyBackend(AutoBatchingMixin, ParallelBackendBase):\n    \"\"\"Managing pool of workers with loky instead of multiprocessing.\"\"\"\n\n    supports_timeout = True\n    supports_inner_max_num_threads = True\n\n    def configure(self, n_jobs=1, parallel=None, prefer=None, require=None,\n                  idle_worker_timeout=300, **memmappingexecutor_args):\n        \"\"\"Build a process executor and return the number of workers\"\"\"\n        n_jobs = self.effective_n_jobs(n_jobs)\n        if n_jobs == 1:\n            raise FallbackToBackend(\n                SequentialBackend(nesting_level=self.nesting_level))\n\n        self._workers = get_memmapping_executor(\n            n_jobs, timeout=idle_worker_timeout,\n            env=self._prepare_worker_env(n_jobs=n_jobs),\n            context_id=parallel._id, **memmappingexecutor_args)\n        self.parallel = parallel\n        return n_jobs\n\n    def effective_n_jobs(self, n_jobs):\n        \"\"\"Determine the number of jobs which are going to run in parallel\"\"\"\n        if n_jobs == 0:\n            raise ValueError('n_jobs == 0 in Parallel has no meaning')\n        elif mp is None or n_jobs is None:\n            # multiprocessing is not available or disabled, fallback\n            # to sequential mode\n            return 1\n        elif mp.current_process().daemon:\n            # Daemonic processes cannot have children\n            if n_jobs != 1:\n                warnings.warn(\n                    'Loky-backed parallel loops cannot be called in a'\n                    ' multiprocessing, setting n_jobs=1',\n                    stacklevel=3)\n            return 1\n        elif not (self.in_main_thread() or self.nesting_level == 0):\n            # Prevent posix fork inside in non-main posix threads\n            if n_jobs != 1:\n                warnings.warn(\n                    'Loky-backed parallel loops cannot be nested below '\n                    'threads, setting n_jobs=1',\n                    stacklevel=3)\n            return 1\n        elif n_jobs < 0:\n            n_jobs = max(cpu_count() + 1 + n_jobs, 1)\n        return n_jobs\n\n    def apply_async(self, func, callback=None):\n        \"\"\"Schedule a func to be run\"\"\"\n        future = self._workers.submit(SafeFunction(func))\n        future.get = functools.partial(self.wrap_future_result, future)\n        if callback is not None:\n            future.add_done_callback(callback)\n        return future\n\n    @staticmethod\n    def wrap_future_result(future, timeout=None):\n        \"\"\"Wrapper for Future.result to implement the same behaviour as\n        AsyncResults.get from multiprocessing.\"\"\"\n        try:\n            return future.result(timeout=timeout)\n        except CfTimeoutError as e:\n            raise TimeoutError from e\n\n    def terminate(self):\n        if self._workers is not None:\n            # Don't terminate the workers as we want to reuse them in later\n            # calls, but cleanup the temporary resources that the Parallel call\n            # created. This 'hack' requires a private, low-level operation.\n            self._workers._temp_folder_manager._unlink_temporary_resources(\n                context_id=self.parallel._id\n            )\n            self._workers = None\n\n        self.reset_batch_stats()\n\n    def abort_everything(self, ensure_ready=True):\n        \"\"\"Shutdown the workers and restart a new one with the same parameters\n        \"\"\"\n        self._workers.terminate(kill_workers=True)\n        self._workers = None\n\n        if ensure_ready:\n            self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel)\n\n\nclass ImmediateResult(object):\n    def __init__(self, batch):\n        # Don't delay the application, to avoid keeping the input\n        # arguments in memory\n        self.results = batch()\n\n    def get(self):\n        return self.results\n\n\nclass SafeFunction(object):\n    \"\"\"Wrapper that handles the serialization of exception tracebacks.\n\n    TODO python2_drop: check whether SafeFunction is still needed since we\n    dropped support for Python 2. If not needed anymore it should be\n    deprecated.\n\n    If an exception is triggered when calling the inner function, a copy of\n    the full traceback is captured to make it possible to serialize\n    it so that it can be rendered in a different Python process.\n\n    \"\"\"\n    def __init__(self, func):\n        self.func = func\n\n    def __call__(self, *args, **kwargs):\n        try:\n            return self.func(*args, **kwargs)\n        except KeyboardInterrupt as e:\n            # We capture the KeyboardInterrupt and reraise it as\n            # something different, as multiprocessing does not\n            # interrupt processing for a KeyboardInterrupt\n            raise WorkerInterrupt() from e\n        except BaseException:\n            # Rely on Python 3 built-in Remote Traceback reporting\n            raise\n\n\nclass FallbackToBackend(Exception):\n    \"\"\"Raised when configuration should fallback to another backend\"\"\"\n\n    def __init__(self, backend):\n        self.backend = backend\n",610],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py":["\"\"\"\nHelpers for embarrassingly parallel code.\n\"\"\"\n# Author: Gael Varoquaux < gael dot varoquaux at normalesup dot org >\n# Copyright: 2010, Gael Varoquaux\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport os\nimport sys\nfrom math import sqrt\nimport functools\nimport time\nimport threading\nimport itertools\nfrom uuid import uuid4\nfrom numbers import Integral\nimport warnings\nimport queue\n\nfrom ._multiprocessing_helpers import mp\n\nfrom .logger import Logger, short_format_time\nfrom .disk import memstr_to_bytes\nfrom ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n                                 ThreadingBackend, SequentialBackend,\n                                 LokyBackend)\nfrom .externals.cloudpickle import dumps, loads\nfrom .externals import loky\n\n# Make sure that those two classes are part of the public joblib.parallel API\n# so that 3rd party backend implementers can import them from here.\nfrom ._parallel_backends import AutoBatchingMixin  # noqa\nfrom ._parallel_backends import ParallelBackendBase  # noqa\n\n\nBACKENDS = {\n    'multiprocessing': MultiprocessingBackend,\n    'threading': ThreadingBackend,\n    'sequential': SequentialBackend,\n    'loky': LokyBackend,\n}\n# name of the backend used by default by Parallel outside of any context\n# managed by ``parallel_backend``.\nDEFAULT_BACKEND = 'loky'\nDEFAULT_N_JOBS = 1\nDEFAULT_THREAD_BACKEND = 'threading'\n\n# Thread local value that can be overridden by the ``parallel_backend`` context\n# manager\n_backend = threading.local()\n\nVALID_BACKEND_HINTS = ('processes', 'threads', None)\nVALID_BACKEND_CONSTRAINTS = ('sharedmem', None)\n\n\ndef _register_dask():\n    \"\"\" Register Dask Backend if called with parallel_backend(\"dask\") \"\"\"\n    try:\n        from ._dask import DaskDistributedBackend\n        register_parallel_backend('dask', DaskDistributedBackend)\n    except ImportError as e:\n        msg = (\"To use the dask.distributed backend you must install both \"\n               \"the `dask` and distributed modules.\\n\\n\"\n               \"See https://dask.pydata.org/en/latest/install.html for more \"\n               \"information.\")\n        raise ImportError(msg) from e\n\n\nEXTERNAL_BACKENDS = {\n    'dask': _register_dask,\n}\n\n\ndef get_active_backend(prefer=None, require=None, verbose=0):\n    \"\"\"Return the active default backend\"\"\"\n    if prefer not in VALID_BACKEND_HINTS:\n        raise ValueError(\"prefer=%r is not a valid backend hint, \"\n                         \"expected one of %r\" % (prefer, VALID_BACKEND_HINTS))\n    if require not in VALID_BACKEND_CONSTRAINTS:\n        raise ValueError(\"require=%r is not a valid backend constraint, \"\n                         \"expected one of %r\"\n                         % (require, VALID_BACKEND_CONSTRAINTS))\n\n    if prefer == 'processes' and require == 'sharedmem':\n        raise ValueError(\"prefer == 'processes' and require == 'sharedmem'\"\n                         \" are inconsistent settings\")\n    backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n    if backend_and_jobs is not None:\n        # Try to use the backend set by the user with the context manager.\n        backend, n_jobs = backend_and_jobs\n        nesting_level = backend.nesting_level\n        supports_sharedmem = getattr(backend, 'supports_sharedmem', False)\n        if require == 'sharedmem' and not supports_sharedmem:\n            # This backend does not match the shared memory constraint:\n            # fallback to the default thead-based backend.\n            sharedmem_backend = BACKENDS[DEFAULT_THREAD_BACKEND](\n                nesting_level=nesting_level)\n            if verbose >= 10:\n                print(\"Using %s as joblib.Parallel backend instead of %s \"\n                      \"as the latter does not provide shared memory semantics.\"\n                      % (sharedmem_backend.__class__.__name__,\n                         backend.__class__.__name__))\n            return sharedmem_backend, DEFAULT_N_JOBS\n        else:\n            return backend_and_jobs\n\n    # We are outside of the scope of any parallel_backend context manager,\n    # create the default backend instance now.\n    backend = BACKENDS[DEFAULT_BACKEND](nesting_level=0)\n    supports_sharedmem = getattr(backend, 'supports_sharedmem', False)\n    uses_threads = getattr(backend, 'uses_threads', False)\n    if ((require == 'sharedmem' and not supports_sharedmem) or\n            (prefer == 'threads' and not uses_threads)):\n        # Make sure the selected default backend match the soft hints and\n        # hard constraints:\n        backend = BACKENDS[DEFAULT_THREAD_BACKEND](nesting_level=0)\n    return backend, DEFAULT_N_JOBS\n\n\nclass parallel_backend(object):\n    \"\"\"Change the default backend used by Parallel inside a with block.\n\n    If ``backend`` is a string it must match a previously registered\n    implementation using the ``register_parallel_backend`` function.\n\n    By default the following backends are available:\n\n    - 'loky': single-host, process-based parallelism (used by default),\n    - 'threading': single-host, thread-based parallelism,\n    - 'multiprocessing': legacy single-host, process-based parallelism.\n\n    'loky' is recommended to run functions that manipulate Python objects.\n    'threading' is a low-overhead alternative that is most efficient for\n    functions that release the Global Interpreter Lock: e.g. I/O-bound code or\n    CPU-bound code in a few calls to native code that explicitly releases the\n    GIL.\n\n    In addition, if the `dask` and `distributed` Python packages are installed,\n    it is possible to use the 'dask' backend for better scheduling of nested\n    parallel calls without over-subscription and potentially distribute\n    parallel calls over a networked cluster of several hosts.\n\n    It is also possible to use the distributed 'ray' backend for distributing\n    the workload to a cluster of nodes. To use the 'ray' joblib backend add\n    the following lines::\n\n     >>> from ray.util.joblib import register_ray  # doctest: +SKIP\n     >>> register_ray()  # doctest: +SKIP\n     >>> with parallel_backend(\"ray\"):  # doctest: +SKIP\n     ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n     [-1, -2, -3, -4, -5]\n\n    Alternatively the backend can be passed directly as an instance.\n\n    By default all available workers will be used (``n_jobs=-1``) unless the\n    caller passes an explicit value for the ``n_jobs`` parameter.\n\n    This is an alternative to passing a ``backend='backend_name'`` argument to\n    the ``Parallel`` class constructor. It is particularly useful when calling\n    into library code that uses joblib internally but does not expose the\n    backend argument in its own API.\n\n    >>> from operator import neg\n    >>> with parallel_backend('threading'):\n    ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n    ...\n    [-1, -2, -3, -4, -5]\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    Joblib also tries to limit the oversubscription by limiting the number of\n    threads usable in some third-party library threadpools like OpenBLAS, MKL\n    or OpenMP. The default limit in each worker is set to\n    ``max(cpu_count() // effective_n_jobs, 1)`` but this limit can be\n    overwritten with the ``inner_max_num_threads`` argument which will be used\n    to set this limit in the child processes.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    def __init__(self, backend, n_jobs=-1, inner_max_num_threads=None,\n                 **backend_params):\n        if isinstance(backend, str):\n            if backend not in BACKENDS and backend in EXTERNAL_BACKENDS:\n                register = EXTERNAL_BACKENDS[backend]\n                register()\n\n            backend = BACKENDS[backend](**backend_params)\n\n        if inner_max_num_threads is not None:\n            msg = (\"{} does not accept setting the inner_max_num_threads \"\n                   \"argument.\".format(backend.__class__.__name__))\n            assert backend.supports_inner_max_num_threads, msg\n            backend.inner_max_num_threads = inner_max_num_threads\n\n        # If the nesting_level of the backend is not set previously, use the\n        # nesting level from the previous active_backend to set it\n        current_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n        if backend.nesting_level is None:\n            if current_backend_and_jobs is None:\n                nesting_level = 0\n            else:\n                nesting_level = current_backend_and_jobs[0].nesting_level\n\n            backend.nesting_level = nesting_level\n\n        # Save the backends info and set the active backend\n        self.old_backend_and_jobs = current_backend_and_jobs\n        self.new_backend_and_jobs = (backend, n_jobs)\n\n        _backend.backend_and_jobs = (backend, n_jobs)\n\n    def __enter__(self):\n        return self.new_backend_and_jobs\n\n    def __exit__(self, type, value, traceback):\n        self.unregister()\n\n    def unregister(self):\n        if self.old_backend_and_jobs is None:\n            if getattr(_backend, 'backend_and_jobs', None) is not None:\n                del _backend.backend_and_jobs\n        else:\n            _backend.backend_and_jobs = self.old_backend_and_jobs\n\n\n# Under Linux or OS X the default start method of multiprocessing\n# can cause third party libraries to crash. Under Python 3.4+ it is possible\n# to set an environment variable to switch the default start method from\n# 'fork' to 'forkserver' or 'spawn' to avoid this issue albeit at the cost\n# of causing semantic changes and some additional pool instantiation overhead.\nDEFAULT_MP_CONTEXT = None\nif hasattr(mp, 'get_context'):\n    method = os.environ.get('JOBLIB_START_METHOD', '').strip() or None\n    if method is not None:\n        DEFAULT_MP_CONTEXT = mp.get_context(method=method)\n\n\nclass BatchedCalls(object):\n    \"\"\"Wrap a sequence of (func, args, kwargs) tuples as a single callable\"\"\"\n\n    def __init__(self, iterator_slice, backend_and_jobs, reducer_callback=None,\n                 pickle_cache=None):\n        self.items = list(iterator_slice)\n        self._size = len(self.items)\n        self._reducer_callback = reducer_callback\n        if isinstance(backend_and_jobs, tuple):\n            self._backend, self._n_jobs = backend_and_jobs\n        else:\n            # this is for backward compatibility purposes. Before 0.12.6,\n            # nested backends were returned without n_jobs indications.\n            self._backend, self._n_jobs = backend_and_jobs, None\n        self._pickle_cache = pickle_cache if pickle_cache is not None else {}\n\n    def __call__(self):\n        # Set the default nested backend to self._backend but do not set the\n        # change the default number of processes to -1\n        with parallel_backend(self._backend, n_jobs=self._n_jobs):\n            return [func(*args, **kwargs)\n                    for func, args, kwargs in self.items]\n\n    def __reduce__(self):\n        if self._reducer_callback is not None:\n            self._reducer_callback()\n        # no need pickle the callback.\n        return (\n            BatchedCalls,\n            (self.items, (self._backend, self._n_jobs), None,\n             self._pickle_cache)\n        )\n\n    def __len__(self):\n        return self._size\n\n\n###############################################################################\n# CPU count that works also when multiprocessing has been disabled via\n# the JOBLIB_MULTIPROCESSING environment variable\ndef cpu_count(only_physical_cores=False):\n    \"\"\"Return the number of CPUs.\n\n    This delegates to loky.cpu_count that takes into account additional\n    constraints such as Linux CFS scheduler quotas (typically set by container\n    runtimes such as docker) and CPU affinity (for instance using the taskset\n    command on Linux).\n\n    If only_physical_cores is True, do not take hyperthreading / SMT logical\n    cores into account.\n    \"\"\"\n    if mp is None:\n        return 1\n\n    return loky.cpu_count(only_physical_cores=only_physical_cores)\n\n\n###############################################################################\n# For verbosity\n\ndef _verbosity_filter(index, verbose):\n    \"\"\" Returns False for indices increasingly apart, the distance\n        depending on the value of verbose.\n\n        We use a lag increasing as the square of index\n    \"\"\"\n    if not verbose:\n        return True\n    elif verbose > 10:\n        return False\n    if index == 0:\n        return False\n    verbose = .5 * (11 - verbose) ** 2\n    scale = sqrt(index / verbose)\n    next_scale = sqrt((index + 1) / verbose)\n    return (int(next_scale) == int(scale))\n\n\n###############################################################################\ndef delayed(function):\n    \"\"\"Decorator used to capture the arguments of a function.\"\"\"\n\n    def delayed_function(*args, **kwargs):\n        return function, args, kwargs\n    try:\n        delayed_function = functools.wraps(function)(delayed_function)\n    except AttributeError:\n        \" functools.wraps fails on some callable objects \"\n    return delayed_function\n\n\n###############################################################################\nclass BatchCompletionCallBack(object):\n    \"\"\"Callback used by joblib.Parallel's multiprocessing backend.\n\n    This callable is executed by the parent process whenever a worker process\n    has returned the results of a batch of tasks.\n\n    It is used for progress reporting, to update estimate of the batch\n    processing duration and to schedule the next batch of tasks to be\n    processed.\n\n    \"\"\"\n    def __init__(self, dispatch_timestamp, batch_size, parallel):\n        self.dispatch_timestamp = dispatch_timestamp\n        self.batch_size = batch_size\n        self.parallel = parallel\n\n    def __call__(self, out):\n        self.parallel.n_completed_tasks += self.batch_size\n        this_batch_duration = time.time() - self.dispatch_timestamp\n\n        self.parallel._backend.batch_completed(self.batch_size,\n                                               this_batch_duration)\n        self.parallel.print_progress()\n        with self.parallel._lock:\n            if self.parallel._original_iterator is not None:\n                self.parallel.dispatch_next()\n\n\n###############################################################################\ndef register_parallel_backend(name, factory, make_default=False):\n    \"\"\"Register a new Parallel backend factory.\n\n    The new backend can then be selected by passing its name as the backend\n    argument to the Parallel class. Moreover, the default backend can be\n    overwritten globally by setting make_default=True.\n\n    The factory can be any callable that takes no argument and return an\n    instance of ``ParallelBackendBase``.\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    BACKENDS[name] = factory\n    if make_default:\n        global DEFAULT_BACKEND\n        DEFAULT_BACKEND = name\n\n\ndef effective_n_jobs(n_jobs=-1):\n    \"\"\"Determine the number of jobs that can actually run in parallel\n\n    n_jobs is the number of workers requested by the callers. Passing n_jobs=-1\n    means requesting all available workers for instance matching the number of\n    CPU cores on the worker host(s).\n\n    This method should return a guesstimate of the number of workers that can\n    actually perform work concurrently with the currently enabled default\n    backend. The primary use case is to make it possible for the caller to know\n    in how many chunks to slice the work.\n\n    In general working on larger data chunks is more efficient (less scheduling\n    overhead and better use of CPU cache prefetching heuristics) as long as all\n    the workers have enough work to do.\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    backend, backend_n_jobs = get_active_backend()\n    if n_jobs is None:\n        n_jobs = backend_n_jobs\n    return backend.effective_n_jobs(n_jobs=n_jobs)\n\n\n###############################################################################\nclass Parallel(Logger):\n    ''' Helper class for readable parallel mapping.\n\n        Read more in the :ref:`User Guide <parallel>`.\n\n        Parameters\n        -----------\n        n_jobs: int, default: None\n            The maximum number of concurrently running jobs, such as the number\n            of Python worker processes when backend=\"multiprocessing\"\n            or the size of the thread-pool when backend=\"threading\".\n            If -1 all CPUs are used. If 1 is given, no parallel computing code\n            is used at all, which is useful for debugging. For n_jobs below -1,\n            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all\n            CPUs but one are used.\n            None is a marker for 'unset' that will be interpreted as n_jobs=1\n            (sequential execution) unless the call is performed under a\n            parallel_backend context manager that sets another value for\n            n_jobs.\n        backend: str, ParallelBackendBase instance or None, default: 'loky'\n            Specify the parallelization backend implementation.\n            Supported backends are:\n\n            - \"loky\" used by default, can induce some\n              communication and memory overhead when exchanging input and\n              output data with the worker Python processes.\n            - \"multiprocessing\" previous process-based backend based on\n              `multiprocessing.Pool`. Less robust than `loky`.\n            - \"threading\" is a very low-overhead backend but it suffers\n              from the Python Global Interpreter Lock if the called function\n              relies a lot on Python objects. \"threading\" is mostly useful\n              when the execution bottleneck is a compiled extension that\n              explicitly releases the GIL (for instance a Cython loop wrapped\n              in a \"with nogil\" block or an expensive call to a library such\n              as NumPy).\n            - finally, you can register backends by calling\n              register_parallel_backend. This will allow you to implement\n              a backend of your liking.\n\n            It is not recommended to hard-code the backend name in a call to\n            Parallel in a library. Instead it is recommended to set soft hints\n            (prefer) or hard constraints (require) so as to make it possible\n            for library users to change the backend from the outside using the\n            parallel_backend context manager.\n        prefer: str in {'processes', 'threads'} or None, default: None\n            Soft hint to choose the default backend if no specific backend\n            was selected with the parallel_backend context manager. The\n            default process-based backend is 'loky' and the default\n            thread-based backend is 'threading'. Ignored if the ``backend``\n            parameter is specified.\n        require: 'sharedmem' or None, default None\n            Hard constraint to select the backend. If set to 'sharedmem',\n            the selected backend will be single-host and thread-based even\n            if the user asked for a non-thread based backend with\n            parallel_backend.\n        verbose: int, optional\n            The verbosity level: if non zero, progress messages are\n            printed. Above 50, the output is sent to stdout.\n            The frequency of the messages increases with the verbosity level.\n            If it more than 10, all iterations are reported.\n        timeout: float, optional\n            Timeout limit for each task to complete.  If any task takes longer\n            a TimeOutError will be raised. Only applied when n_jobs != 1\n        pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}\n            The number of batches (of tasks) to be pre-dispatched.\n            Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\n            default and the workers should never starve.\n        batch_size: int or 'auto', default: 'auto'\n            The number of atomic tasks to dispatch at once to each\n            worker. When individual evaluations are very fast, dispatching\n            calls to workers can be slower than sequential computation because\n            of the overhead. Batching fast computations together can mitigate\n            this.\n            The ``'auto'`` strategy keeps track of the time it takes for a batch\n            to complete, and dynamically adjusts the batch size to keep the time\n            on the order of half a second, using a heuristic. The initial batch\n            size is 1.\n            ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\n            batches of a single task at a time as the threading backend has\n            very little overhead and using larger batch size has not proved to\n            bring any gain in that case.\n        temp_folder: str, optional\n            Folder to be used by the pool for memmapping large arrays\n            for sharing memory with worker processes. If None, this will try in\n            order:\n\n            - a folder pointed by the JOBLIB_TEMP_FOLDER environment\n              variable,\n            - /dev/shm if the folder exists and is writable: this is a\n              RAM disk filesystem available by default on modern Linux\n              distributions,\n            - the default system temporary folder that can be\n              overridden with TMP, TMPDIR or TEMP environment\n              variables, typically /tmp under Unix operating systems.\n\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        max_nbytes int, str, or None, optional, 1M by default\n            Threshold on the size of arrays passed to the workers that\n            triggers automated memory mapping in temp_folder. Can be an int\n            in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\n            Use None to disable memmapping of large arrays.\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n            Memmapping mode for numpy arrays passed to workers.\n            See 'max_nbytes' parameter documentation for more details.\n\n        Notes\n        -----\n\n        This object uses workers to compute in parallel the application of a\n        function to many different arguments. The main functionality it brings\n        in addition to using the raw multiprocessing or concurrent.futures API\n        are (see examples for details):\n\n        * More readable code, in particular since it avoids\n          constructing list of arguments.\n\n        * Easier debugging:\n            - informative tracebacks even when the error happens on\n              the client side\n            - using 'n_jobs=1' enables to turn off parallel computing\n              for debugging without changing the codepath\n            - early capture of pickling errors\n\n        * An optional progress meter.\n\n        * Interruption of multiprocesses jobs with 'Ctrl-C'\n\n        * Flexible pickling control for the communication to and from\n          the worker processes.\n\n        * Ability to use shared memory efficiently with worker\n          processes for large numpy-based datastructures.\n\n        Examples\n        --------\n\n        A simple example:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n\n        Reshaping the output when the function has several return\n        values:\n\n        >>> from math import modf\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\n        >>> res, i = zip(*r)\n        >>> res\n        (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\n        >>> i\n        (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\n\n        The progress meter: the higher the value of `verbose`, the more\n        messages:\n\n        >>> from time import sleep\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP\n        [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n        [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s\n        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished\n\n        Traceback example, note how the line of the error is indicated\n        as well as the values of the parameter passed to the function that\n        triggered the exception, even though the traceback happens in the\n        child process:\n\n        >>> from heapq import nlargest\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP\n        #...\n        ---------------------------------------------------------------------------\n        Sub-process traceback:\n        ---------------------------------------------------------------------------\n        TypeError                                          Mon Nov 12 11:37:46 2012\n        PID: 12934                                    Python 2.7.3: /usr/bin/python\n        ...........................................................................\n        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\n            419         if n >= size:\n            420             return sorted(iterable, key=key, reverse=True)[:n]\n            421\n            422     # When key is none, use simpler decoration\n            423     if key is None:\n        --> 424         it = izip(iterable, count(0,-1))                    # decorate\n            425         result = _nlargest(n, it)\n            426         return map(itemgetter(0), result)                   # undecorate\n            427\n            428     # General case, slowest method\n         TypeError: izip argument #1 must support iteration\n        ___________________________________________________________________________\n\n\n        Using pre_dispatch in a producer/consumer situation, where the\n        data is generated on the fly. Note how the producer is first\n        called 3 times before the parallel loop is initiated, and then\n        called to generate new data on the fly:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> def producer():\n        ...     for i in range(6):\n        ...         print('Produced %s' % i)\n        ...         yield i\n        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\n        ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\n        Produced 0\n        Produced 1\n        Produced 2\n        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\n        Produced 3\n        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\n        Produced 4\n        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\n        Produced 5\n        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\n\n    '''\n    def __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None,\n                 pre_dispatch='2 * n_jobs', batch_size='auto',\n                 temp_folder=None, max_nbytes='1M', mmap_mode='r',\n                 prefer=None, require=None):\n        active_backend, context_n_jobs = get_active_backend(\n            prefer=prefer, require=require, verbose=verbose)\n        nesting_level = active_backend.nesting_level\n        if backend is None and n_jobs is None:\n            # If we are under a parallel_backend context manager, look up\n            # the default number of jobs and use that instead:\n            n_jobs = context_n_jobs\n        if n_jobs is None:\n            # No specific context override and no specific value request:\n            # default to 1.\n            n_jobs = 1\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.timeout = timeout\n        self.pre_dispatch = pre_dispatch\n        self._ready_batches = queue.Queue()\n        self._id = uuid4().hex\n        self._reducer_callback = None\n\n        if isinstance(max_nbytes, str):\n            max_nbytes = memstr_to_bytes(max_nbytes)\n\n        self._backend_args = dict(\n            max_nbytes=max_nbytes,\n            mmap_mode=mmap_mode,\n            temp_folder=temp_folder,\n            prefer=prefer,\n            require=require,\n            verbose=max(0, self.verbose - 50),\n        )\n        if DEFAULT_MP_CONTEXT is not None:\n            self._backend_args['context'] = DEFAULT_MP_CONTEXT\n        elif hasattr(mp, \"get_context\"):\n            self._backend_args['context'] = mp.get_context()\n\n        if backend is None:\n            backend = active_backend\n\n        elif isinstance(backend, ParallelBackendBase):\n            # Use provided backend as is, with the current nesting_level if it\n            # is not set yet.\n            if backend.nesting_level is None:\n                backend.nesting_level = nesting_level\n\n        elif hasattr(backend, 'Pool') and hasattr(backend, 'Lock'):\n            # Make it possible to pass a custom multiprocessing context as\n            # backend to change the start method to forkserver or spawn or\n            # preload modules on the forkserver helper process.\n            self._backend_args['context'] = backend\n            backend = MultiprocessingBackend(nesting_level=nesting_level)\n        else:\n            try:\n                backend_factory = BACKENDS[backend]\n            except KeyError as e:\n                raise ValueError(\"Invalid backend: %s, expected one of %r\"\n                                 % (backend, sorted(BACKENDS.keys()))) from e\n            backend = backend_factory(nesting_level=nesting_level)\n\n        if (require == 'sharedmem' and\n                not getattr(backend, 'supports_sharedmem', False)):\n            raise ValueError(\"Backend %s does not support shared memory\"\n                             % backend)\n\n        if (batch_size == 'auto' or isinstance(batch_size, Integral) and\n                batch_size > 0):\n            self.batch_size = batch_size\n        else:\n            raise ValueError(\n                \"batch_size must be 'auto' or a positive integer, got: %r\"\n                % batch_size)\n\n        self._backend = backend\n        self._output = None\n        self._jobs = list()\n        self._managed_backend = False\n\n        # This lock is used coordinate the main thread of this process with\n        # the async callback thread of our the pool.\n        self._lock = threading.RLock()\n\n    def __enter__(self):\n        self._managed_backend = True\n        self._initialize_backend()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._terminate_backend()\n        self._managed_backend = False\n\n    def _initialize_backend(self):\n        \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n        try:\n            n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n                                             **self._backend_args)\n            if self.timeout is not None and not self._backend.supports_timeout:\n                warnings.warn(\n                    'The backend class {!r} does not support timeout. '\n                    \"You have set 'timeout={}' in Parallel but \"\n                    \"the 'timeout' parameter will not be used.\".format(\n                        self._backend.__class__.__name__,\n                        self.timeout))\n\n        except FallbackToBackend as e:\n            # Recursively initialize the backend in case of requested fallback.\n            self._backend = e.backend\n            n_jobs = self._initialize_backend()\n\n        return n_jobs\n\n    def _effective_n_jobs(self):\n        if self._backend:\n            return self._backend.effective_n_jobs(self.n_jobs)\n        return 1\n\n    def _terminate_backend(self):\n        if self._backend is not None:\n            self._backend.terminate()\n\n    def _dispatch(self, batch):\n        \"\"\"Queue the batch for computing, with or without multiprocessing\n\n        WARNING: this method is not thread-safe: it should be only called\n        indirectly via dispatch_one_batch.\n\n        \"\"\"\n        # If job.get() catches an exception, it closes the queue:\n        if self._aborting:\n            return\n\n        self.n_dispatched_tasks += len(batch)\n        self.n_dispatched_batches += 1\n\n        dispatch_timestamp = time.time()\n        cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n        with self._lock:\n            job_idx = len(self._jobs)\n            job = self._backend.apply_async(batch, callback=cb)\n            # A job can complete so quickly than its callback is\n            # called before we get here, causing self._jobs to\n            # grow. To ensure correct results ordering, .insert is\n            # used (rather than .append) in the following line\n            self._jobs.insert(job_idx, job)\n\n    def dispatch_next(self):\n        \"\"\"Dispatch more data for parallel processing\n\n        This method is meant to be called concurrently by the multiprocessing\n        callback. We rely on the thread-safety of dispatch_one_batch to protect\n        against concurrent consumption of the unprotected iterator.\n\n        \"\"\"\n        if not self.dispatch_one_batch(self._original_iterator):\n            self._iterating = False\n            self._original_iterator = None\n\n    def dispatch_one_batch(self, iterator):\n        \"\"\"Prefetch the tasks for the next batch and dispatch them.\n\n        The effective size of the batch is computed here.\n        If there are no more jobs to dispatch, return False, else return True.\n\n        The iterator consumption and dispatching is protected by the same\n        lock so calling this function should be thread safe.\n\n        \"\"\"\n        if self.batch_size == 'auto':\n            batch_size = self._backend.compute_batch_size()\n        else:\n            # Fixed batch size strategy\n            batch_size = self.batch_size\n\n        with self._lock:\n            # to ensure an even distribution of the workolad between workers,\n            # we look ahead in the original iterators more than batch_size\n            # tasks - However, we keep consuming only one batch at each\n            # dispatch_one_batch call. The extra tasks are stored in a local\n            # queue, _ready_batches, that is looked-up prior to re-consuming\n            # tasks from the origal iterator.\n            try:\n                tasks = self._ready_batches.get(block=False)\n            except queue.Empty:\n                # slice the iterator n_jobs * batchsize items at a time. If the\n                # slice returns less than that, then the current batchsize puts\n                # too much weight on a subset of workers, while other may end\n                # up starving. So in this case, re-scale the batch size\n                # accordingly to distribute evenly the last items between all\n                # workers.\n                n_jobs = self._cached_effective_n_jobs\n                big_batch_size = batch_size * n_jobs\n\n                islice = list(itertools.islice(iterator, big_batch_size))\n                if len(islice) == 0:\n                    return False\n                elif (iterator is self._original_iterator\n                      and len(islice) < big_batch_size):\n                    # We reached the end of the original iterator (unless\n                    # iterator is the ``pre_dispatch``-long initial slice of\n                    # the original iterator) -- decrease the batch size to\n                    # account for potential variance in the batches running\n                    # time.\n                    final_batch_size = max(1, len(islice) // (10 * n_jobs))\n                else:\n                    final_batch_size = max(1, len(islice) // n_jobs)\n\n                # enqueue n_jobs batches in a local queue\n                for i in range(0, len(islice), final_batch_size):\n                    tasks = BatchedCalls(islice[i:i + final_batch_size],\n                                         self._backend.get_nested_backend(),\n                                         self._reducer_callback,\n                                         self._pickle_cache)\n                    self._ready_batches.put(tasks)\n\n                # finally, get one task.\n                tasks = self._ready_batches.get(block=False)\n            if len(tasks) == 0:\n                # No more tasks available in the iterator: tell caller to stop.\n                return False\n            else:\n                self._dispatch(tasks)\n                return True\n\n    def _print(self, msg, msg_args):\n        \"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\n        # XXX: Not using the logger framework: need to\n        # learn to use logger better.\n        if not self.verbose:\n            return\n        if self.verbose < 50:\n            writer = sys.stderr.write\n        else:\n            writer = sys.stdout.write\n        msg = msg % msg_args\n        writer('[%s]: %s\\n' % (self, msg))\n\n    def print_progress(self):\n        \"\"\"Display the process of the parallel execution only a fraction\n           of time, controlled by self.verbose.\n        \"\"\"\n        if not self.verbose:\n            return\n        elapsed_time = time.time() - self._start_time\n\n        # Original job iterator becomes None once it has been fully\n        # consumed : at this point we know the total number of jobs and we are\n        # able to display an estimation of the remaining time based on already\n        # completed jobs. Otherwise, we simply display the number of completed\n        # tasks.\n        if self._original_iterator is not None:\n            if _verbosity_filter(self.n_dispatched_batches, self.verbose):\n                return\n            self._print('Done %3i tasks      | elapsed: %s',\n                        (self.n_completed_tasks,\n                         short_format_time(elapsed_time), ))\n        else:\n            index = self.n_completed_tasks\n            # We are finished dispatching\n            total_tasks = self.n_dispatched_tasks\n            # We always display the first loop\n            if not index == 0:\n                # Display depending on the number of remaining items\n                # A message as soon as we finish dispatching, cursor is 0\n                cursor = (total_tasks - index + 1 -\n                          self._pre_dispatch_amount)\n                frequency = (total_tasks // self.verbose) + 1\n                is_last_item = (index + 1 == total_tasks)\n                if (is_last_item or cursor % frequency):\n                    return\n            remaining_time = (elapsed_time / index) * \\\n                             (self.n_dispatched_tasks - index * 1.0)\n            # only display status if remaining time is greater or equal to 0\n            self._print('Done %3i out of %3i | elapsed: %s remaining: %s',\n                        (index,\n                         total_tasks,\n                         short_format_time(elapsed_time),\n                         short_format_time(remaining_time),\n                         ))\n\n    def retrieve(self):\n        self._output = list()\n        while self._iterating or len(self._jobs) > 0:\n            if len(self._jobs) == 0:\n                # Wait for an async callback to dispatch new jobs\n                time.sleep(0.01)\n                continue\n            # We need to be careful: the job list can be filling up as\n            # we empty it and Python list are not thread-safe by default hence\n            # the use of the lock\n            with self._lock:\n                job = self._jobs.pop(0)\n\n            try:\n                if getattr(self._backend, 'supports_timeout', False):\n                    self._output.extend(job.get(timeout=self.timeout))\n                else:\n                    self._output.extend(job.get())\n\n            except BaseException as exception:\n                # Note: we catch any BaseException instead of just Exception\n                # instances to also include KeyboardInterrupt.\n\n                # Stop dispatching any new job in the async callback thread\n                self._aborting = True\n\n                # If the backend allows it, cancel or kill remaining running\n                # tasks without waiting for the results as we will raise\n                # the exception we got back to the caller instead of returning\n                # any result.\n                backend = self._backend\n                if (backend is not None and\n                        hasattr(backend, 'abort_everything')):\n                    # If the backend is managed externally we need to make sure\n                    # to leave it in a working state to allow for future jobs\n                    # scheduling.\n                    ensure_ready = self._managed_backend\n                    backend.abort_everything(ensure_ready=ensure_ready)\n                raise\n\n    def __call__(self, iterable):\n        if self._jobs:\n            raise ValueError('This Parallel instance is already running')\n        # A flag used to abort the dispatching of jobs in case an\n        # exception is found\n        self._aborting = False\n\n        if not self._managed_backend:\n            n_jobs = self._initialize_backend()\n        else:\n            n_jobs = self._effective_n_jobs()\n\n        if isinstance(self._backend, LokyBackend):\n            # For the loky backend, we add a callback executed when reducing\n            # BatchCalls, that makes the loky executor use a temporary folder\n            # specific to this Parallel object when pickling temporary memmaps.\n            # This callback is necessary to ensure that several Parallel\n            # objects using the same resuable executor don't use the same\n            # temporary resources.\n\n            def _batched_calls_reducer_callback():\n                # Relevant implementation detail: the following lines, called\n                # when reducing BatchedCalls, are called in a thread-safe\n                # situation, meaning that the context of the temporary folder\n                # manager will not be changed in between the callback execution\n                # and the end of the BatchedCalls pickling. The reason is that\n                # pickling (the only place where set_current_context is used)\n                # is done from a single thread (the queue_feeder_thread).\n                self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n                    self._id\n                )\n            self._reducer_callback = _batched_calls_reducer_callback\n\n        # self._effective_n_jobs should be called in the Parallel.__call__\n        # thread only -- store its value in an attribute for further queries.\n        self._cached_effective_n_jobs = n_jobs\n\n        backend_name = self._backend.__class__.__name__\n        if n_jobs == 0:\n            raise RuntimeError(\"%s has no active worker.\" % backend_name)\n\n        self._print(\"Using backend %s with %d concurrent workers.\",\n                    (backend_name, n_jobs))\n        if hasattr(self._backend, 'start_call'):\n            self._backend.start_call()\n        iterator = iter(iterable)\n        pre_dispatch = self.pre_dispatch\n\n        if pre_dispatch == 'all' or n_jobs == 1:\n            # prevent further dispatch via multiprocessing callback thread\n            self._original_iterator = None\n            self._pre_dispatch_amount = 0\n        else:\n            self._original_iterator = iterator\n            if hasattr(pre_dispatch, 'endswith'):\n                pre_dispatch = eval(pre_dispatch)\n            self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n\n            # The main thread will consume the first pre_dispatch items and\n            # the remaining items will later be lazily dispatched by async\n            # callbacks upon task completions.\n\n            # TODO: this iterator should be batch_size * n_jobs\n            iterator = itertools.islice(iterator, self._pre_dispatch_amount)\n\n        self._start_time = time.time()\n        self.n_dispatched_batches = 0\n        self.n_dispatched_tasks = 0\n        self.n_completed_tasks = 0\n        # Use a caching dict for callables that are pickled with cloudpickle to\n        # improve performances. This cache is used only in the case of\n        # functions that are defined in the __main__ module, functions that are\n        # defined locally (inside another function) and lambda expressions.\n        self._pickle_cache = dict()\n        try:\n            # Only set self._iterating to True if at least a batch\n            # was dispatched. In particular this covers the edge\n            # case of Parallel used with an exhausted iterator. If\n            # self._original_iterator is None, then this means either\n            # that pre_dispatch == \"all\", n_jobs == 1 or that the first batch\n            # was very quick and its callback already dispatched all the\n            # remaining jobs.\n            self._iterating = False\n            if self.dispatch_one_batch(iterator):\n                self._iterating = self._original_iterator is not None\n\n            while self.dispatch_one_batch(iterator):\n                pass\n\n            if pre_dispatch == \"all\" or n_jobs == 1:\n                # The iterable was consumed all at once by the above for loop.\n                # No need to wait for async callbacks to trigger to\n                # consumption.\n                self._iterating = False\n\n            with self._backend.retrieval_context():\n                self.retrieve()\n            # Make sure that we get a last message telling us we are done\n            elapsed_time = time.time() - self._start_time\n            self._print('Done %3i out of %3i | elapsed: %s finished',\n                        (len(self._output), len(self._output),\n                         short_format_time(elapsed_time)))\n        finally:\n            if hasattr(self._backend, 'stop_call'):\n                self._backend.stop_call()\n            if not self._managed_backend:\n                self._terminate_backend()\n            self._jobs = list()\n            self._pickle_cache = None\n        output = self._output\n        self._output = None\n        return output\n\n    def __repr__(self):\n        return '%s(n_jobs=%s)' % (self.__class__.__name__, self.n_jobs)\n",1072],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/process.py":["#\n# Module providing the `Process` class which emulates `threading.Thread`\n#\n# multiprocessing/process.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\n__all__ = ['BaseProcess', 'current_process', 'active_children',\n           'parent_process']\n\n#\n# Imports\n#\n\nimport os\nimport sys\nimport signal\nimport itertools\nimport threading\nfrom _weakrefset import WeakSet\n\n#\n#\n#\n\ntry:\n    ORIGINAL_DIR = os.path.abspath(os.getcwd())\nexcept OSError:\n    ORIGINAL_DIR = None\n\n#\n# Public functions\n#\n\ndef current_process():\n    '''\n    Return process object representing the current process\n    '''\n    return _current_process\n\ndef active_children():\n    '''\n    Return list of process objects corresponding to live child processes\n    '''\n    _cleanup()\n    return list(_children)\n\n\ndef parent_process():\n    '''\n    Return process object representing the parent process\n    '''\n    return _parent_process\n\n#\n#\n#\n\ndef _cleanup():\n    # check for processes which have finished\n    for p in list(_children):\n        if p._popen.poll() is not None:\n            _children.discard(p)\n\n#\n# The `Process` class\n#\n\nclass BaseProcess(object):\n    '''\n    Process objects represent activity that is run in a separate process\n\n    The class is analogous to `threading.Thread`\n    '''\n    def _Popen(self):\n        raise NotImplementedError\n\n    def __init__(self, group=None, target=None, name=None, args=(), kwargs={},\n                 *, daemon=None):\n        assert group is None, 'group argument must be None for now'\n        count = next(_process_counter)\n        self._identity = _current_process._identity + (count,)\n        self._config = _current_process._config.copy()\n        self._parent_pid = os.getpid()\n        self._parent_name = _current_process.name\n        self._popen = None\n        self._closed = False\n        self._target = target\n        self._args = tuple(args)\n        self._kwargs = dict(kwargs)\n        self._name = name or type(self).__name__ + '-' + \\\n                     ':'.join(str(i) for i in self._identity)\n        if daemon is not None:\n            self.daemon = daemon\n        _dangling.add(self)\n\n    def _check_closed(self):\n        if self._closed:\n            raise ValueError(\"process object is closed\")\n\n    def run(self):\n        '''\n        Method to be run in sub-process; can be overridden in sub-class\n        '''\n        if self._target:\n            self._target(*self._args, **self._kwargs)\n\n    def start(self):\n        '''\n        Start child process\n        '''\n        self._check_closed()\n        assert self._popen is None, 'cannot start a process twice'\n        assert self._parent_pid == os.getpid(), \\\n               'can only start a process object created by current process'\n        assert not _current_process._config.get('daemon'), \\\n               'daemonic processes are not allowed to have children'\n        _cleanup()\n        self._popen = self._Popen(self)\n        self._sentinel = self._popen.sentinel\n        # Avoid a refcycle if the target function holds an indirect\n        # reference to the process object (see bpo-30775)\n        del self._target, self._args, self._kwargs\n        _children.add(self)\n\n    def terminate(self):\n        '''\n        Terminate process; sends SIGTERM signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.terminate()\n\n    def kill(self):\n        '''\n        Terminate process; sends SIGKILL signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.kill()\n\n    def join(self, timeout=None):\n        '''\n        Wait until child process terminates\n        '''\n        self._check_closed()\n        assert self._parent_pid == os.getpid(), 'can only join a child process'\n        assert self._popen is not None, 'can only join a started process'\n        res = self._popen.wait(timeout)\n        if res is not None:\n            _children.discard(self)\n\n    def is_alive(self):\n        '''\n        Return whether process is alive\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return True\n        assert self._parent_pid == os.getpid(), 'can only test a child process'\n\n        if self._popen is None:\n            return False\n\n        returncode = self._popen.poll()\n        if returncode is None:\n            return True\n        else:\n            _children.discard(self)\n            return False\n\n    def close(self):\n        '''\n        Close the Process object.\n\n        This method releases resources held by the Process object.  It is\n        an error to call this method if the child process is still running.\n        '''\n        if self._popen is not None:\n            if self._popen.poll() is None:\n                raise ValueError(\"Cannot close a process while it is still running. \"\n                                 \"You should first call join() or terminate().\")\n            self._popen.close()\n            self._popen = None\n            del self._sentinel\n            _children.discard(self)\n        self._closed = True\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert isinstance(name, str), 'name must be a string'\n        self._name = name\n\n    @property\n    def daemon(self):\n        '''\n        Return whether process is a daemon\n        '''\n        return self._config.get('daemon', False)\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        '''\n        Set whether process is a daemon\n        '''\n        assert self._popen is None, 'process has already started'\n        self._config['daemon'] = daemonic\n\n    @property\n    def authkey(self):\n        return self._config['authkey']\n\n    @authkey.setter\n    def authkey(self, authkey):\n        '''\n        Set authorization key of process\n        '''\n        self._config['authkey'] = AuthenticationString(authkey)\n\n    @property\n    def exitcode(self):\n        '''\n        Return exit code of process or `None` if it has yet to stop\n        '''\n        self._check_closed()\n        if self._popen is None:\n            return self._popen\n        return self._popen.poll()\n\n    @property\n    def ident(self):\n        '''\n        Return identifier (PID) of process or `None` if it has yet to start\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return os.getpid()\n        else:\n            return self._popen and self._popen.pid\n\n    pid = ident\n\n    @property\n    def sentinel(self):\n        '''\n        Return a file descriptor (Unix) or handle (Windows) suitable for\n        waiting for process termination.\n        '''\n        self._check_closed()\n        try:\n            return self._sentinel\n        except AttributeError:\n            raise ValueError(\"process not started\") from None\n\n    def __repr__(self):\n        exitcode = None\n        if self is _current_process:\n            status = 'started'\n        elif self._closed:\n            status = 'closed'\n        elif self._parent_pid != os.getpid():\n            status = 'unknown'\n        elif self._popen is None:\n            status = 'initial'\n        else:\n            exitcode = self._popen.poll()\n            if exitcode is not None:\n                status = 'stopped'\n            else:\n                status = 'started'\n\n        info = [type(self).__name__, 'name=%r' % self._name]\n        if self._popen is not None:\n            info.append('pid=%s' % self._popen.pid)\n        info.append('parent=%s' % self._parent_pid)\n        info.append(status)\n        if exitcode is not None:\n            exitcode = _exitcode_to_name.get(exitcode, exitcode)\n            info.append('exitcode=%s' % exitcode)\n        if self.daemon:\n            info.append('daemon')\n        return '<%s>' % ' '.join(info)\n\n    ##\n\n    def _bootstrap(self, parent_sentinel=None):\n        from . import util, context\n        global _current_process, _parent_process, _process_counter, _children\n\n        try:\n            if self._start_method is not None:\n                context._force_start_method(self._start_method)\n            _process_counter = itertools.count(1)\n            _children = set()\n            util._close_stdin()\n            old_process = _current_process\n            _current_process = self\n            _parent_process = _ParentProcess(\n                self._parent_name, self._parent_pid, parent_sentinel)\n            if threading._HAVE_THREAD_NATIVE_ID:\n                threading.main_thread()._set_native_id()\n            try:\n                util._finalizer_registry.clear()\n                util._run_after_forkers()\n            finally:\n                # delay finalization of the old process object until after\n                # _run_after_forkers() is executed\n                del old_process\n            util.info('child process calling self.run()')\n            try:\n                self.run()\n                exitcode = 0\n            finally:\n                util._exit_function()\n        except SystemExit as e:\n            if e.code is None:\n                exitcode = 0\n            elif isinstance(e.code, int):\n                exitcode = e.code\n            else:\n                sys.stderr.write(str(e.code) + '\\n')\n                exitcode = 1\n        except:\n            exitcode = 1\n            import traceback\n            sys.stderr.write('Process %s:\\n' % self.name)\n            traceback.print_exc()\n        finally:\n            threading._shutdown()\n            util.info('process exiting with exitcode %d' % exitcode)\n            util._flush_std_streams()\n\n        return exitcode\n\n#\n# We subclass bytes to avoid accidental transmission of auth keys over network\n#\n\nclass AuthenticationString(bytes):\n    def __reduce__(self):\n        from .context import get_spawning_popen\n        if get_spawning_popen() is None:\n            raise TypeError(\n                'Pickling an AuthenticationString object is '\n                'disallowed for security reasons'\n                )\n        return AuthenticationString, (bytes(self),)\n\n\n#\n# Create object representing the parent process\n#\n\nclass _ParentProcess(BaseProcess):\n\n    def __init__(self, name, pid, sentinel):\n        self._identity = ()\n        self._name = name\n        self._pid = pid\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._sentinel = sentinel\n        self._config = {}\n\n    def is_alive(self):\n        from multiprocessing.connection import wait\n        return not wait([self._sentinel], timeout=0)\n\n    @property\n    def ident(self):\n        return self._pid\n\n    def join(self, timeout=None):\n        '''\n        Wait until parent process terminates\n        '''\n        from multiprocessing.connection import wait\n        wait([self._sentinel], timeout=timeout)\n\n    pid = ident\n\n#\n# Create object representing the main process\n#\n\nclass _MainProcess(BaseProcess):\n\n    def __init__(self):\n        self._identity = ()\n        self._name = 'MainProcess'\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._config = {'authkey': AuthenticationString(os.urandom(32)),\n                        'semprefix': '/mp'}\n        # Note that some versions of FreeBSD only allow named\n        # semaphores to have names of up to 14 characters.  Therefore\n        # we choose a short prefix.\n        #\n        # On MacOSX in a sandbox it may be necessary to use a\n        # different prefix -- see #19478.\n        #\n        # Everything in self._config will be inherited by descendant\n        # processes.\n\n    def close(self):\n        pass\n\n\n_parent_process = None\n_current_process = _MainProcess()\n_process_counter = itertools.count(1)\n_children = set()\ndel _MainProcess\n\n#\n# Give names to some return codes\n#\n\n_exitcode_to_name = {}\n\nfor name, signum in list(signal.__dict__.items()):\n    if name[:3]=='SIG' and '_' not in name:\n        _exitcode_to_name[-signum] = f'-{name}'\n\n# For debug and leak testing\n_dangling = WeakSet()\n",432],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py":["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport functools\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import islice as _islice, count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_new_thread = _thread.start_new_thread\n_allocate_lock = _thread.allocate_lock\n_set_sentinel = _thread._set_sentinel\nget_ident = _thread.get_ident\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\n# Synchronization classes\n\nLock = _allocate_lock\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        all_waiters = self._waiters\n        waiters_to_notify = _deque(_islice(all_waiters, n))\n        if not waiters_to_notify:\n            return\n        for waiter in waiters_to_notify:\n            waiter.release()\n            try:\n                all_waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    notifyAll = notify_all\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        Semaphore.__init__(self, value)\n        self._initial_value = value\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._reset_internal_locks()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    isSet = is_set\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0 #0 filling, 1, draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count().__next__\n_counter() # Consume 0 so first non-main thread has id 1.\ndef _newname(template=\"Thread-%d\"):\n    return template % _counter()\n\n# Active thread administration\n_active_limbo_lock = _allocate_lock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\n# to wait until all Python thread states get deleted:\n# see Thread._set_tstate_lock().\n_shutdown_locks_lock = _allocate_lock()\n_shutdown_locks = set()\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is the argument tuple for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        self._target = target\n        self._name = str(name or _newname())\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._tstate_lock = None\n        self._started = Event()\n        self._is_stopped = False\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _reset_internal_locks(self, is_alive):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        self._started._at_fork_reinit()\n        if is_alive:\n            # bpo-42350: If the fork happens when the thread is already stopped\n            # (ex: after threading._shutdown() has been called), _tstate_lock\n            # is None. Do nothing in this case.\n            if self._tstate_lock is not None:\n                self._tstate_lock._at_fork_reinit()\n                self._tstate_lock.acquire()\n        else:\n            # The thread isn't alive after fork: it doesn't have a tstate\n            # anymore.\n            self._is_stopped = True\n            self._tstate_lock = None\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\n        if self._is_stopped:\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self._bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _set_tstate_lock(self):\n        \"\"\"\n        Set a lock object which will be released by the interpreter when\n        the underlying thread state (see pystate.h) gets deleted.\n        \"\"\"\n        self._tstate_lock = _set_sentinel()\n        self._tstate_lock.acquire()\n\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _shutdown_locks.add(self._tstate_lock)\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self._set_tstate_lock()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            with _active_limbo_lock:\n                try:\n                    # We don't call self._delete() because it also\n                    # grabs _active_limbo_lock.\n                    del _active[get_ident()]\n                except:\n                    pass\n\n    def _stop(self):\n        # After calling ._stop(), .is_alive() returns False and .join() returns\n        # immediately.  ._tstate_lock must be released before calling ._stop().\n        #\n        # Normal case:  C code at the end of the thread's life\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\n        # and .is_alive().  Any number of threads _may_ call ._stop()\n        # simultaneously (for example, if multiple threads are blocked in\n        # .join() calls), and they're not serialized.  That's harmless -\n        # they'll just make redundant rebindings of ._is_stopped and\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\n        # \"assert self._is_stopped\" in ._wait_for_tstate_lock() always works\n        # (the assert is executed only if ._tstate_lock is None).\n        #\n        # Special case:  _main_thread releases ._tstate_lock via this\n        # module's _shutdown() function.\n        lock = self._tstate_lock\n        if lock is not None:\n            assert not lock.locked()\n        self._is_stopped = True\n        self._tstate_lock = None\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _shutdown_locks.discard(lock)\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if timeout is None:\n            self._wait_for_tstate_lock()\n        else:\n            # the behavior of a negative timeout isn't documented, but\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\n\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\n        # Issue #18808: wait for the thread state to be gone.\n        # At the end of the thread's life, after all knowledge of the thread\n        # is removed from C data structures, C code releases our _tstate_lock.\n        # This method passes its arguments to _tstate_lock.acquire().\n        # If the lock is acquired, the C code is done, and self._stop() is\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\n        lock = self._tstate_lock\n        if lock is None:  # already determined that the C code is done\n            assert self._is_stopped\n        elif lock.acquire(block, timeout):\n            lock.release()\n            self._stop()\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. The module function enumerate()\n        returns a list of all alive threads.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        if self._is_stopped or not self._started.is_set():\n            return False\n        self._wait_for_tstate_lock(False)\n        return not self._is_stopped\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        self.daemon = daemonic\n\n    def getName(self):\n        return self.name\n\n    def setName(self, name):\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._set_tstate_lock()\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"), daemon=True)\n\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n    def _stop(self):\n        pass\n\n    def is_alive(self):\n        assert not self._is_stopped and self._started.is_set()\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ncurrentThread = current_thread\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\nactiveCount = active_count\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    call = functools.partial(func, *arg, **kwargs)\n    _threading_atexits.append(call)\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it.  We can't wait for C code to release\n    # the main thread's tstate_lock - that won't happen until the interpreter\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\n    if _main_thread._is_stopped:\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n    # Main thread\n    tlock = _main_thread._tstate_lock\n    # The main thread isn't finished yet, so its thread state lock can't have\n    # been released.\n    assert tlock is not None\n    assert tlock.locked()\n    tlock.release()\n    _main_thread._stop()\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    # Join all non-deamon threads\n    while True:\n        with _shutdown_locks_lock:\n            locks = list(_shutdown_locks)\n            _shutdown_locks.clear()\n\n        if not locks:\n            break\n\n        for lock in locks:\n            # mimick Thread.join()\n            lock.acquire()\n            lock.release()\n\n        # new threads can be spawned while we were waiting for the other\n        # threads to complete\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    return _main_thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    global _shutdown_locks_lock, _shutdown_locks\n    _active_limbo_lock = _allocate_lock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\n    _shutdown_locks_lock = _allocate_lock()\n    _shutdown_locks = set()\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                thread._reset_internal_locks(True)\n                ident = get_ident()\n                thread._ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._reset_internal_locks(False)\n                thread._stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n",1506],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py":["\"\"\"Vendoered from\nhttps://github.com/pypa/packaging/blob/main/packaging/version.py\n\"\"\"\n# Copyright (c) Donald Stufft and individual contributors.\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n#     1. Redistributions of source code must retain the above copyright notice,\n#        this list of conditions and the following disclaimer.\n\n#     2. Redistributions in binary form must reproduce the above copyright\n#        notice, this list of conditions and the following disclaimer in the\n#        documentation and/or other materials provided with the distribution.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport collections\nimport itertools\nimport re\nimport warnings\nfrom typing import Callable, Iterator, List, Optional, SupportsInt, Tuple, Union\n\nfrom ._structures import Infinity, InfinityType, NegativeInfinity, NegativeInfinityType\n\n__all__ = [\"parse\", \"Version\", \"LegacyVersion\", \"InvalidVersion\", \"VERSION_PATTERN\"]\n\nInfiniteTypes = Union[InfinityType, NegativeInfinityType]\nPrePostDevType = Union[InfiniteTypes, Tuple[str, int]]\nSubLocalType = Union[InfiniteTypes, int, str]\nLocalType = Union[\n    NegativeInfinityType,\n    Tuple[\n        Union[\n            SubLocalType,\n            Tuple[SubLocalType, str],\n            Tuple[NegativeInfinityType, SubLocalType],\n        ],\n        ...,\n    ],\n]\nCmpKey = Tuple[\n    int, Tuple[int, ...], PrePostDevType, PrePostDevType, PrePostDevType, LocalType\n]\nLegacyCmpKey = Tuple[int, Tuple[str, ...]]\nVersionComparisonMethod = Callable[\n    [Union[CmpKey, LegacyCmpKey], Union[CmpKey, LegacyCmpKey]], bool\n]\n\n_Version = collections.namedtuple(\n    \"_Version\", [\"epoch\", \"release\", \"dev\", \"pre\", \"post\", \"local\"]\n)\n\n\ndef parse(version: str) -> Union[\"LegacyVersion\", \"Version\"]:\n    \"\"\"\n    Parse the given version string and return either a :class:`Version` object\n    or a :class:`LegacyVersion` object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n    \"\"\"\n    try:\n        return Version(version)\n    except InvalidVersion:\n        return LegacyVersion(version)\n\n\nclass InvalidVersion(ValueError):\n    \"\"\"\n    An invalid version was found, users should refer to PEP 440.\n    \"\"\"\n\n\nclass _BaseVersion:\n    _key: Union[CmpKey, LegacyCmpKey]\n\n    def __hash__(self) -> int:\n        return hash(self._key)\n\n    # Please keep the duplicated `isinstance` check\n    # in the six comparisons hereunder\n    # unless you find a way to avoid adding overhead function calls.\n    def __lt__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key < other._key\n\n    def __le__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key <= other._key\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key == other._key\n\n    def __ge__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key >= other._key\n\n    def __gt__(self, other: \"_BaseVersion\") -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key > other._key\n\n    def __ne__(self, other: object) -> bool:\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return self._key != other._key\n\n\nclass LegacyVersion(_BaseVersion):\n    def __init__(self, version: str) -> None:\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n        warnings.warn(\n            \"Creating a LegacyVersion has been deprecated and will be \"\n            \"removed in the next major release\",\n            DeprecationWarning,\n        )\n\n    def __str__(self) -> str:\n        return self._version\n\n    def __repr__(self) -> str:\n        return f\"<LegacyVersion('{self}')>\"\n\n    @property\n    def public(self) -> str:\n        return self._version\n\n    @property\n    def base_version(self) -> str:\n        return self._version\n\n    @property\n    def epoch(self) -> int:\n        return -1\n\n    @property\n    def release(self) -> None:\n        return None\n\n    @property\n    def pre(self) -> None:\n        return None\n\n    @property\n    def post(self) -> None:\n        return None\n\n    @property\n    def dev(self) -> None:\n        return None\n\n    @property\n    def local(self) -> None:\n        return None\n\n    @property\n    def is_prerelease(self) -> bool:\n        return False\n\n    @property\n    def is_postrelease(self) -> bool:\n        return False\n\n    @property\n    def is_devrelease(self) -> bool:\n        return False\n\n\n_legacy_version_component_re = re.compile(r\"(\\d+ | [a-z]+ | \\.| -)\", re.VERBOSE)\n\n_legacy_version_replacement_map = {\n    \"pre\": \"c\",\n    \"preview\": \"c\",\n    \"-\": \"final-\",\n    \"rc\": \"c\",\n    \"dev\": \"@\",\n}\n\n\ndef _parse_version_parts(s: str) -> Iterator[str]:\n    for part in _legacy_version_component_re.split(s):\n        part = _legacy_version_replacement_map.get(part, part)\n\n        if not part or part == \".\":\n            continue\n\n        if part[:1] in \"0123456789\":\n            # pad for numeric comparison\n            yield part.zfill(8)\n        else:\n            yield \"*\" + part\n\n    # ensure that alpha/beta/candidate are before final\n    yield \"*final\"\n\n\ndef _legacy_cmpkey(version: str) -> LegacyCmpKey:\n\n    # We hardcode an epoch of -1 here. A PEP 440 version can only have a epoch\n    # greater than or equal to 0. This will effectively put the LegacyVersion,\n    # which uses the defacto standard originally implemented by setuptools,\n    # as before all PEP 440 versions.\n    epoch = -1\n\n    # This scheme is taken from pkg_resources.parse_version setuptools prior to\n    # it's adoption of the packaging library.\n    parts: List[str] = []\n    for part in _parse_version_parts(version.lower()):\n        if part.startswith(\"*\"):\n            # remove \"-\" before a prerelease tag\n            if part < \"*final\":\n                while parts and parts[-1] == \"*final-\":\n                    parts.pop()\n\n            # remove trailing zeros from each series of numeric parts\n            while parts and parts[-1] == \"00000000\":\n                parts.pop()\n\n        parts.append(part)\n\n    return epoch, tuple(parts)\n\n\n# Deliberately not anchored to the start and end of the string, to make it\n# easier for 3rd party code to reuse\nVERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>(a|b|c|rc|alpha|beta|pre|preview))\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\"\n\n\nclass Version(_BaseVersion):\n\n    _regex = re.compile(r\"^\\s*\" + VERSION_PATTERN + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    def __init__(self, version: str) -> None:\n\n        # Validate the version and parse it into pieces\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(f\"Invalid version: '{version}'\")\n\n        # Store the parsed out pieces of the version\n        self._version = _Version(\n            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n            pre=_parse_letter_version(match.group(\"pre_l\"), match.group(\"pre_n\")),\n            post=_parse_letter_version(\n                match.group(\"post_l\"), match.group(\"post_n1\") or match.group(\"post_n2\")\n            ),\n            dev=_parse_letter_version(match.group(\"dev_l\"), match.group(\"dev_n\")),\n            local=_parse_local_version(match.group(\"local\")),\n        )\n\n        # Generate a key which will be used for sorting\n        self._key = _cmpkey(\n            self._version.epoch,\n            self._version.release,\n            self._version.pre,\n            self._version.post,\n            self._version.dev,\n            self._version.local,\n        )\n\n    def __repr__(self) -> str:\n        return f\"<Version('{self}')>\"\n\n    def __str__(self) -> str:\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        # Pre-release\n        if self.pre is not None:\n            parts.append(\"\".join(str(x) for x in self.pre))\n\n        # Post-release\n        if self.post is not None:\n            parts.append(f\".post{self.post}\")\n\n        # Development release\n        if self.dev is not None:\n            parts.append(f\".dev{self.dev}\")\n\n        # Local version segment\n        if self.local is not None:\n            parts.append(f\"+{self.local}\")\n\n        return \"\".join(parts)\n\n    @property\n    def epoch(self) -> int:\n        _epoch: int = self._version.epoch\n        return _epoch\n\n    @property\n    def release(self) -> Tuple[int, ...]:\n        _release: Tuple[int, ...] = self._version.release\n        return _release\n\n    @property\n    def pre(self) -> Optional[Tuple[str, int]]:\n        _pre: Optional[Tuple[str, int]] = self._version.pre\n        return _pre\n\n    @property\n    def post(self) -> Optional[int]:\n        return self._version.post[1] if self._version.post else None\n\n    @property\n    def dev(self) -> Optional[int]:\n        return self._version.dev[1] if self._version.dev else None\n\n    @property\n    def local(self) -> Optional[str]:\n        if self._version.local:\n            return \".\".join(str(x) for x in self._version.local)\n        else:\n            return None\n\n    @property\n    def public(self) -> str:\n        return str(self).split(\"+\", 1)[0]\n\n    @property\n    def base_version(self) -> str:\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(f\"{self.epoch}!\")\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        return \"\".join(parts)\n\n    @property\n    def is_prerelease(self) -> bool:\n        return self.dev is not None or self.pre is not None\n\n    @property\n    def is_postrelease(self) -> bool:\n        return self.post is not None\n\n    @property\n    def is_devrelease(self) -> bool:\n        return self.dev is not None\n\n    @property\n    def major(self) -> int:\n        return self.release[0] if len(self.release) >= 1 else 0\n\n    @property\n    def minor(self) -> int:\n        return self.release[1] if len(self.release) >= 2 else 0\n\n    @property\n    def micro(self) -> int:\n        return self.release[2] if len(self.release) >= 3 else 0\n\n\ndef _parse_letter_version(\n    letter: str, number: Union[str, bytes, SupportsInt]\n) -> Optional[Tuple[str, int]]:\n\n    if letter:\n        # We consider there to be an implicit 0 in a pre-release if there is\n        # not a numeral associated with it.\n        if number is None:\n            number = 0\n\n        # We normalize any letters to their lower case form\n        letter = letter.lower()\n\n        # We consider some words to be alternate spellings of other words and\n        # in those cases we want to normalize the spellings to our preferred\n        # spelling.\n        if letter == \"alpha\":\n            letter = \"a\"\n        elif letter == \"beta\":\n            letter = \"b\"\n        elif letter in [\"c\", \"pre\", \"preview\"]:\n            letter = \"rc\"\n        elif letter in [\"rev\", \"r\"]:\n            letter = \"post\"\n\n        return letter, int(number)\n    if not letter and number:\n        # We assume if we are given a number, but we are not given a letter\n        # then this is using the implicit post release syntax (e.g. 1.0-1)\n        letter = \"post\"\n\n        return letter, int(number)\n\n    return None\n\n\n_local_version_separators = re.compile(r\"[\\._-]\")\n\n\ndef _parse_local_version(local: str) -> Optional[LocalType]:\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )\n    return None\n\n\ndef _cmpkey(\n    epoch: int,\n    release: Tuple[int, ...],\n    pre: Optional[Tuple[str, int]],\n    post: Optional[Tuple[str, int]],\n    dev: Optional[Tuple[str, int]],\n    local: Optional[Tuple[SubLocalType]],\n) -> CmpKey:\n\n    # When we compare a release version, we want to compare it with all of the\n    # trailing zeros removed. So we'll use a reverse the list, drop all the now\n    # leading zeros until we come to something non zero, then take the rest\n    # re-reverse it back into the correct order and make it a tuple and use\n    # that for our sorting key.\n    _release = tuple(\n        reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))\n    )\n\n    # We need to \"trick\" the sorting algorithm to put 1.0.dev0 before 1.0a0.\n    # We'll do this by abusing the pre segment, but we _only_ want to do this\n    # if there is not a pre or a post segment. If we have one of those then\n    # the normal sorting rules will handle this case correctly.\n    if pre is None and post is None and dev is not None:\n        _pre: PrePostDevType = NegativeInfinity\n    # Versions without a pre-release (except as noted above) should sort after\n    # those with one.\n    elif pre is None:\n        _pre = Infinity\n    else:\n        _pre = pre\n\n    # Versions without a post segment should sort before those with one.\n    if post is None:\n        _post: PrePostDevType = NegativeInfinity\n\n    else:\n        _post = post\n\n    # Versions without a development segment should sort after those with one.\n    if dev is None:\n        _dev: PrePostDevType = Infinity\n\n    else:\n        _dev = dev\n\n    if local is None:\n        # Versions without a local segment should sort before those with one.\n        _local: LocalType = NegativeInfinity\n    else:\n        # Versions with a local segment need that segment parsed to implement\n        # the sorting rules in PEP440.\n        # - Alpha numeric segments sort before numeric segments\n        # - Alpha numeric segments sort lexicographically\n        # - Numeric segments sort numerically\n        # - Shorter versions sort before longer versions when the prefixes\n        #   match exactly\n        _local = tuple(\n            (i, \"\") if isinstance(i, int) else (NegativeInfinity, i) for i in local\n        )\n\n    return epoch, _release, _pre, _post, _dev, _local\n",527],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py":["'''A multi-producer, multi-consumer queue.'''\n\nimport threading\nimport types\nfrom collections import deque\nfrom heapq import heappush, heappop\nfrom time import monotonic as time\ntry:\n    from _queue import SimpleQueue\nexcept ImportError:\n    SimpleQueue = None\n\n__all__ = ['Empty', 'Full', 'Queue', 'PriorityQueue', 'LifoQueue', 'SimpleQueue']\n\n\ntry:\n    from _queue import Empty\nexcept ImportError:\n    class Empty(Exception):\n        'Exception raised by Queue.get(block=0)/get_nowait().'\n        pass\n\nclass Full(Exception):\n    'Exception raised by Queue.put(block=0)/put_nowait().'\n    pass\n\n\nclass Queue:\n    '''Create a queue object with a given maximum size.\n\n    If maxsize is <= 0, the queue size is infinite.\n    '''\n\n    def __init__(self, maxsize=0):\n        self.maxsize = maxsize\n        self._init(maxsize)\n\n        # mutex must be held whenever the queue is mutating.  All methods\n        # that acquire mutex must release it before returning.  mutex\n        # is shared between the three conditions, so acquiring and\n        # releasing the conditions also acquires and releases mutex.\n        self.mutex = threading.Lock()\n\n        # Notify not_empty whenever an item is added to the queue; a\n        # thread waiting to get is notified then.\n        self.not_empty = threading.Condition(self.mutex)\n\n        # Notify not_full whenever an item is removed from the queue;\n        # a thread waiting to put is notified then.\n        self.not_full = threading.Condition(self.mutex)\n\n        # Notify all_tasks_done whenever the number of unfinished tasks\n        # drops to zero; thread waiting to join() is notified to resume\n        self.all_tasks_done = threading.Condition(self.mutex)\n        self.unfinished_tasks = 0\n\n    def task_done(self):\n        '''Indicate that a formerly enqueued task is complete.\n\n        Used by Queue consumer threads.  For each get() used to fetch a task,\n        a subsequent call to task_done() tells the queue that the processing\n        on the task is complete.\n\n        If a join() is currently blocking, it will resume when all items\n        have been processed (meaning that a task_done() call was received\n        for every item that had been put() into the queue).\n\n        Raises a ValueError if called more times than there were items\n        placed in the queue.\n        '''\n        with self.all_tasks_done:\n            unfinished = self.unfinished_tasks - 1\n            if unfinished <= 0:\n                if unfinished < 0:\n                    raise ValueError('task_done() called too many times')\n                self.all_tasks_done.notify_all()\n            self.unfinished_tasks = unfinished\n\n    def join(self):\n        '''Blocks until all items in the Queue have been gotten and processed.\n\n        The count of unfinished tasks goes up whenever an item is added to the\n        queue. The count goes down whenever a consumer thread calls task_done()\n        to indicate the item was retrieved and all work on it is complete.\n\n        When the count of unfinished tasks drops to zero, join() unblocks.\n        '''\n        with self.all_tasks_done:\n            while self.unfinished_tasks:\n                self.all_tasks_done.wait()\n\n    def qsize(self):\n        '''Return the approximate size of the queue (not reliable!).'''\n        with self.mutex:\n            return self._qsize()\n\n    def empty(self):\n        '''Return True if the queue is empty, False otherwise (not reliable!).\n\n        This method is likely to be removed at some point.  Use qsize() == 0\n        as a direct substitute, but be aware that either approach risks a race\n        condition where a queue can grow before the result of empty() or\n        qsize() can be used.\n\n        To create code that needs to wait for all queued tasks to be\n        completed, the preferred technique is to use the join() method.\n        '''\n        with self.mutex:\n            return not self._qsize()\n\n    def full(self):\n        '''Return True if the queue is full, False otherwise (not reliable!).\n\n        This method is likely to be removed at some point.  Use qsize() >= n\n        as a direct substitute, but be aware that either approach risks a race\n        condition where a queue can shrink before the result of full() or\n        qsize() can be used.\n        '''\n        with self.mutex:\n            return 0 < self.maxsize <= self._qsize()\n\n    def put(self, item, block=True, timeout=None):\n        '''Put an item into the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until a free slot is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Full exception if no free slot was available within that time.\n        Otherwise ('block' is false), put an item on the queue if a free slot\n        is immediately available, else raise the Full exception ('timeout'\n        is ignored in that case).\n        '''\n        with self.not_full:\n            if self.maxsize > 0:\n                if not block:\n                    if self._qsize() >= self.maxsize:\n                        raise Full\n                elif timeout is None:\n                    while self._qsize() >= self.maxsize:\n                        self.not_full.wait()\n                elif timeout < 0:\n                    raise ValueError(\"'timeout' must be a non-negative number\")\n                else:\n                    endtime = time() + timeout\n                    while self._qsize() >= self.maxsize:\n                        remaining = endtime - time()\n                        if remaining <= 0.0:\n                            raise Full\n                        self.not_full.wait(remaining)\n            self._put(item)\n            self.unfinished_tasks += 1\n            self.not_empty.notify()\n\n    def get(self, block=True, timeout=None):\n        '''Remove and return an item from the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until an item is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Empty exception if no item was available within that time.\n        Otherwise ('block' is false), return an item if one is immediately\n        available, else raise the Empty exception ('timeout' is ignored\n        in that case).\n        '''\n        with self.not_empty:\n            if not block:\n                if not self._qsize():\n                    raise Empty\n            elif timeout is None:\n                while not self._qsize():\n                    self.not_empty.wait()\n            elif timeout < 0:\n                raise ValueError(\"'timeout' must be a non-negative number\")\n            else:\n                endtime = time() + timeout\n                while not self._qsize():\n                    remaining = endtime - time()\n                    if remaining <= 0.0:\n                        raise Empty\n                    self.not_empty.wait(remaining)\n            item = self._get()\n            self.not_full.notify()\n            return item\n\n    def put_nowait(self, item):\n        '''Put an item into the queue without blocking.\n\n        Only enqueue the item if a free slot is immediately available.\n        Otherwise raise the Full exception.\n        '''\n        return self.put(item, block=False)\n\n    def get_nowait(self):\n        '''Remove and return an item from the queue without blocking.\n\n        Only get an item if one is immediately available. Otherwise\n        raise the Empty exception.\n        '''\n        return self.get(block=False)\n\n    # Override these methods to implement other queue organizations\n    # (e.g. stack or priority queue).\n    # These will only be called with appropriate locks held\n\n    # Initialize the queue representation\n    def _init(self, maxsize):\n        self.queue = deque()\n\n    def _qsize(self):\n        return len(self.queue)\n\n    # Put a new item in the queue\n    def _put(self, item):\n        self.queue.append(item)\n\n    # Get an item from the queue\n    def _get(self):\n        return self.queue.popleft()\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\nclass PriorityQueue(Queue):\n    '''Variant of Queue that retrieves open entries in priority order (lowest first).\n\n    Entries are typically tuples of the form:  (priority number, data).\n    '''\n\n    def _init(self, maxsize):\n        self.queue = []\n\n    def _qsize(self):\n        return len(self.queue)\n\n    def _put(self, item):\n        heappush(self.queue, item)\n\n    def _get(self):\n        return heappop(self.queue)\n\n\nclass LifoQueue(Queue):\n    '''Variant of Queue that retrieves most recently added entries first.'''\n\n    def _init(self, maxsize):\n        self.queue = []\n\n    def _qsize(self):\n        return len(self.queue)\n\n    def _put(self, item):\n        self.queue.append(item)\n\n    def _get(self):\n        return self.queue.pop()\n\n\nclass _PySimpleQueue:\n    '''Simple, unbounded FIFO queue.\n\n    This pure Python implementation is not reentrant.\n    '''\n    # Note: while this pure Python version provides fairness\n    # (by using a threading.Semaphore which is itself fair, being based\n    #  on threading.Condition), fairness is not part of the API contract.\n    # This allows the C version to use a different implementation.\n\n    def __init__(self):\n        self._queue = deque()\n        self._count = threading.Semaphore(0)\n\n    def put(self, item, block=True, timeout=None):\n        '''Put the item on the queue.\n\n        The optional 'block' and 'timeout' arguments are ignored, as this method\n        never blocks.  They are provided for compatibility with the Queue class.\n        '''\n        self._queue.append(item)\n        self._count.release()\n\n    def get(self, block=True, timeout=None):\n        '''Remove and return an item from the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until an item is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Empty exception if no item was available within that time.\n        Otherwise ('block' is false), return an item if one is immediately\n        available, else raise the Empty exception ('timeout' is ignored\n        in that case).\n        '''\n        if timeout is not None and timeout < 0:\n            raise ValueError(\"'timeout' must be a non-negative number\")\n        if not self._count.acquire(block, timeout):\n            raise Empty\n        return self._queue.popleft()\n\n    def put_nowait(self, item):\n        '''Put an item into the queue without blocking.\n\n        This is exactly equivalent to `put(item)` and is only provided\n        for compatibility with the Queue class.\n        '''\n        return self.put(item, block=False)\n\n    def get_nowait(self):\n        '''Remove and return an item from the queue without blocking.\n\n        Only get an item if one is immediately available. Otherwise\n        raise the Empty exception.\n        '''\n        return self.get(block=False)\n\n    def empty(self):\n        '''Return True if the queue is empty, False otherwise (not reliable!).'''\n        return len(self._queue) == 0\n\n    def qsize(self):\n        '''Return the approximate size of the queue (not reliable!).'''\n        return len(self._queue)\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\nif SimpleQueue is None:\n    SimpleQueue = _PySimpleQueue\n",326],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py":["r\"\"\"UUID objects (universally unique identifiers) according to RFC 4122.\n\nThis module provides immutable UUID objects (class UUID) and the functions\nuuid1(), uuid3(), uuid4(), uuid5() for generating version 1, 3, 4, and 5\nUUIDs as specified in RFC 4122.\n\nIf all you want is a unique ID, you should probably call uuid1() or uuid4().\nNote that uuid1() may compromise privacy since it creates a UUID containing\nthe computer's network address.  uuid4() creates a random UUID.\n\nTypical usage:\n\n    >>> import uuid\n\n    # make a UUID based on the host ID and current time\n    >>> uuid.uuid1()    # doctest: +SKIP\n    UUID('a8098c1a-f86e-11da-bd1a-00112444be1e')\n\n    # make a UUID using an MD5 hash of a namespace UUID and a name\n    >>> uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org')\n    UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e')\n\n    # make a random UUID\n    >>> uuid.uuid4()    # doctest: +SKIP\n    UUID('16fd2706-8baf-433b-82eb-8c7fada847da')\n\n    # make a UUID using a SHA-1 hash of a namespace UUID and a name\n    >>> uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org')\n    UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')\n\n    # make a UUID from a string of hex digits (braces and hyphens ignored)\n    >>> x = uuid.UUID('{00010203-0405-0607-0809-0a0b0c0d0e0f}')\n\n    # convert a UUID to a string of hex digits in standard form\n    >>> str(x)\n    '00010203-0405-0607-0809-0a0b0c0d0e0f'\n\n    # get the raw 16 bytes of the UUID\n    >>> x.bytes\n    b'\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\x0c\\r\\x0e\\x0f'\n\n    # make a UUID from a 16-byte string\n    >>> uuid.UUID(bytes=x.bytes)\n    UUID('00010203-0405-0607-0809-0a0b0c0d0e0f')\n\"\"\"\n\nimport os\nimport sys\n\nfrom enum import Enum\n\n\n__author__ = 'Ka-Ping Yee <ping@zesty.ca>'\n\n# The recognized platforms - known behaviors\nif sys.platform in ('win32', 'darwin'):\n    _AIX = _LINUX = False\nelse:\n    import platform\n    _platform_system = platform.system()\n    _AIX     = _platform_system == 'AIX'\n    _LINUX   = _platform_system == 'Linux'\n\n_MAC_DELIM = b':'\n_MAC_OMITS_LEADING_ZEROES = False\nif _AIX:\n    _MAC_DELIM = b'.'\n    _MAC_OMITS_LEADING_ZEROES = True\n\nRESERVED_NCS, RFC_4122, RESERVED_MICROSOFT, RESERVED_FUTURE = [\n    'reserved for NCS compatibility', 'specified in RFC 4122',\n    'reserved for Microsoft compatibility', 'reserved for future definition']\n\nint_ = int      # The built-in int type\nbytes_ = bytes  # The built-in bytes type\n\n\nclass SafeUUID(Enum):\n    safe = 0\n    unsafe = -1\n    unknown = None\n\n\nclass UUID:\n    \"\"\"Instances of the UUID class represent UUIDs as specified in RFC 4122.\n    UUID objects are immutable, hashable, and usable as dictionary keys.\n    Converting a UUID to a string with str() yields something in the form\n    '12345678-1234-1234-1234-123456789abc'.  The UUID constructor accepts\n    five possible forms: a similar string of hexadecimal digits, or a tuple\n    of six integer fields (with 32-bit, 16-bit, 16-bit, 8-bit, 8-bit, and\n    48-bit values respectively) as an argument named 'fields', or a string\n    of 16 bytes (with all the integer fields in big-endian order) as an\n    argument named 'bytes', or a string of 16 bytes (with the first three\n    fields in little-endian order) as an argument named 'bytes_le', or a\n    single 128-bit integer as an argument named 'int'.\n\n    UUIDs have these read-only attributes:\n\n        bytes       the UUID as a 16-byte string (containing the six\n                    integer fields in big-endian byte order)\n\n        bytes_le    the UUID as a 16-byte string (with time_low, time_mid,\n                    and time_hi_version in little-endian byte order)\n\n        fields      a tuple of the six integer fields of the UUID,\n                    which are also available as six individual attributes\n                    and two derived attributes:\n\n            time_low                the first 32 bits of the UUID\n            time_mid                the next 16 bits of the UUID\n            time_hi_version         the next 16 bits of the UUID\n            clock_seq_hi_variant    the next 8 bits of the UUID\n            clock_seq_low           the next 8 bits of the UUID\n            node                    the last 48 bits of the UUID\n\n            time                    the 60-bit timestamp\n            clock_seq               the 14-bit sequence number\n\n        hex         the UUID as a 32-character hexadecimal string\n\n        int         the UUID as a 128-bit integer\n\n        urn         the UUID as a URN as specified in RFC 4122\n\n        variant     the UUID variant (one of the constants RESERVED_NCS,\n                    RFC_4122, RESERVED_MICROSOFT, or RESERVED_FUTURE)\n\n        version     the UUID version number (1 through 5, meaningful only\n                    when the variant is RFC_4122)\n\n        is_safe     An enum indicating whether the UUID has been generated in\n                    a way that is safe for multiprocessing applications, via\n                    uuid_generate_time_safe(3).\n    \"\"\"\n\n    __slots__ = ('int', 'is_safe', '__weakref__')\n\n    def __init__(self, hex=None, bytes=None, bytes_le=None, fields=None,\n                       int=None, version=None,\n                       *, is_safe=SafeUUID.unknown):\n        r\"\"\"Create a UUID from either a string of 32 hexadecimal digits,\n        a string of 16 bytes as the 'bytes' argument, a string of 16 bytes\n        in little-endian order as the 'bytes_le' argument, a tuple of six\n        integers (32-bit time_low, 16-bit time_mid, 16-bit time_hi_version,\n        8-bit clock_seq_hi_variant, 8-bit clock_seq_low, 48-bit node) as\n        the 'fields' argument, or a single 128-bit integer as the 'int'\n        argument.  When a string of hex digits is given, curly braces,\n        hyphens, and a URN prefix are all optional.  For example, these\n        expressions all yield the same UUID:\n\n        UUID('{12345678-1234-5678-1234-567812345678}')\n        UUID('12345678123456781234567812345678')\n        UUID('urn:uuid:12345678-1234-5678-1234-567812345678')\n        UUID(bytes='\\x12\\x34\\x56\\x78'*4)\n        UUID(bytes_le='\\x78\\x56\\x34\\x12\\x34\\x12\\x78\\x56' +\n                      '\\x12\\x34\\x56\\x78\\x12\\x34\\x56\\x78')\n        UUID(fields=(0x12345678, 0x1234, 0x5678, 0x12, 0x34, 0x567812345678))\n        UUID(int=0x12345678123456781234567812345678)\n\n        Exactly one of 'hex', 'bytes', 'bytes_le', 'fields', or 'int' must\n        be given.  The 'version' argument is optional; if given, the resulting\n        UUID will have its variant and version set according to RFC 4122,\n        overriding the given 'hex', 'bytes', 'bytes_le', 'fields', or 'int'.\n\n        is_safe is an enum exposed as an attribute on the instance.  It\n        indicates whether the UUID has been generated in a way that is safe\n        for multiprocessing applications, via uuid_generate_time_safe(3).\n        \"\"\"\n\n        if [hex, bytes, bytes_le, fields, int].count(None) != 4:\n            raise TypeError('one of the hex, bytes, bytes_le, fields, '\n                            'or int arguments must be given')\n        if hex is not None:\n            hex = hex.replace('urn:', '').replace('uuid:', '')\n            hex = hex.strip('{}').replace('-', '')\n            if len(hex) != 32:\n                raise ValueError('badly formed hexadecimal UUID string')\n            int = int_(hex, 16)\n        if bytes_le is not None:\n            if len(bytes_le) != 16:\n                raise ValueError('bytes_le is not a 16-char string')\n            bytes = (bytes_le[4-1::-1] + bytes_le[6-1:4-1:-1] +\n                     bytes_le[8-1:6-1:-1] + bytes_le[8:])\n        if bytes is not None:\n            if len(bytes) != 16:\n                raise ValueError('bytes is not a 16-char string')\n            assert isinstance(bytes, bytes_), repr(bytes)\n            int = int_.from_bytes(bytes, byteorder='big')\n        if fields is not None:\n            if len(fields) != 6:\n                raise ValueError('fields is not a 6-tuple')\n            (time_low, time_mid, time_hi_version,\n             clock_seq_hi_variant, clock_seq_low, node) = fields\n            if not 0 <= time_low < 1<<32:\n                raise ValueError('field 1 out of range (need a 32-bit value)')\n            if not 0 <= time_mid < 1<<16:\n                raise ValueError('field 2 out of range (need a 16-bit value)')\n            if not 0 <= time_hi_version < 1<<16:\n                raise ValueError('field 3 out of range (need a 16-bit value)')\n            if not 0 <= clock_seq_hi_variant < 1<<8:\n                raise ValueError('field 4 out of range (need an 8-bit value)')\n            if not 0 <= clock_seq_low < 1<<8:\n                raise ValueError('field 5 out of range (need an 8-bit value)')\n            if not 0 <= node < 1<<48:\n                raise ValueError('field 6 out of range (need a 48-bit value)')\n            clock_seq = (clock_seq_hi_variant << 8) | clock_seq_low\n            int = ((time_low << 96) | (time_mid << 80) |\n                   (time_hi_version << 64) | (clock_seq << 48) | node)\n        if int is not None:\n            if not 0 <= int < 1<<128:\n                raise ValueError('int is out of range (need a 128-bit value)')\n        if version is not None:\n            if not 1 <= version <= 5:\n                raise ValueError('illegal version number')\n            # Set the variant to RFC 4122.\n            int &= ~(0xc000 << 48)\n            int |= 0x8000 << 48\n            # Set the version number.\n            int &= ~(0xf000 << 64)\n            int |= version << 76\n        object.__setattr__(self, 'int', int)\n        object.__setattr__(self, 'is_safe', is_safe)\n\n    def __getstate__(self):\n        d = {'int': self.int}\n        if self.is_safe != SafeUUID.unknown:\n            # is_safe is a SafeUUID instance.  Return just its value, so that\n            # it can be un-pickled in older Python versions without SafeUUID.\n            d['is_safe'] = self.is_safe.value\n        return d\n\n    def __setstate__(self, state):\n        object.__setattr__(self, 'int', state['int'])\n        # is_safe was added in 3.7; it is also omitted when it is \"unknown\"\n        object.__setattr__(self, 'is_safe',\n                           SafeUUID(state['is_safe'])\n                           if 'is_safe' in state else SafeUUID.unknown)\n\n    def __eq__(self, other):\n        if isinstance(other, UUID):\n            return self.int == other.int\n        return NotImplemented\n\n    # Q. What's the value of being able to sort UUIDs?\n    # A. Use them as keys in a B-Tree or similar mapping.\n\n    def __lt__(self, other):\n        if isinstance(other, UUID):\n            return self.int < other.int\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, UUID):\n            return self.int > other.int\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, UUID):\n            return self.int <= other.int\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, UUID):\n            return self.int >= other.int\n        return NotImplemented\n\n    def __hash__(self):\n        return hash(self.int)\n\n    def __int__(self):\n        return self.int\n\n    def __repr__(self):\n        return '%s(%r)' % (self.__class__.__name__, str(self))\n\n    def __setattr__(self, name, value):\n        raise TypeError('UUID objects are immutable')\n\n    def __str__(self):\n        hex = '%032x' % self.int\n        return '%s-%s-%s-%s-%s' % (\n            hex[:8], hex[8:12], hex[12:16], hex[16:20], hex[20:])\n\n    @property\n    def bytes(self):\n        return self.int.to_bytes(16, 'big')\n\n    @property\n    def bytes_le(self):\n        bytes = self.bytes\n        return (bytes[4-1::-1] + bytes[6-1:4-1:-1] + bytes[8-1:6-1:-1] +\n                bytes[8:])\n\n    @property\n    def fields(self):\n        return (self.time_low, self.time_mid, self.time_hi_version,\n                self.clock_seq_hi_variant, self.clock_seq_low, self.node)\n\n    @property\n    def time_low(self):\n        return self.int >> 96\n\n    @property\n    def time_mid(self):\n        return (self.int >> 80) & 0xffff\n\n    @property\n    def time_hi_version(self):\n        return (self.int >> 64) & 0xffff\n\n    @property\n    def clock_seq_hi_variant(self):\n        return (self.int >> 56) & 0xff\n\n    @property\n    def clock_seq_low(self):\n        return (self.int >> 48) & 0xff\n\n    @property\n    def time(self):\n        return (((self.time_hi_version & 0x0fff) << 48) |\n                (self.time_mid << 32) | self.time_low)\n\n    @property\n    def clock_seq(self):\n        return (((self.clock_seq_hi_variant & 0x3f) << 8) |\n                self.clock_seq_low)\n\n    @property\n    def node(self):\n        return self.int & 0xffffffffffff\n\n    @property\n    def hex(self):\n        return '%032x' % self.int\n\n    @property\n    def urn(self):\n        return 'urn:uuid:' + str(self)\n\n    @property\n    def variant(self):\n        if not self.int & (0x8000 << 48):\n            return RESERVED_NCS\n        elif not self.int & (0x4000 << 48):\n            return RFC_4122\n        elif not self.int & (0x2000 << 48):\n            return RESERVED_MICROSOFT\n        else:\n            return RESERVED_FUTURE\n\n    @property\n    def version(self):\n        # The version bits are only meaningful for RFC 4122 UUIDs.\n        if self.variant == RFC_4122:\n            return int((self.int >> 76) & 0xf)\n\n\ndef _get_command_stdout(command, *args):\n    import io, os, shutil, subprocess\n\n    try:\n        path_dirs = os.environ.get('PATH', os.defpath).split(os.pathsep)\n        path_dirs.extend(['/sbin', '/usr/sbin'])\n        executable = shutil.which(command, path=os.pathsep.join(path_dirs))\n        if executable is None:\n            return None\n        # LC_ALL=C to ensure English output, stderr=DEVNULL to prevent output\n        # on stderr (Note: we don't have an example where the words we search\n        # for are actually localized, but in theory some system could do so.)\n        env = dict(os.environ)\n        env['LC_ALL'] = 'C'\n        proc = subprocess.Popen((executable,) + args,\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.DEVNULL,\n                                env=env)\n        if not proc:\n            return None\n        stdout, stderr = proc.communicate()\n        return io.BytesIO(stdout)\n    except (OSError, subprocess.SubprocessError):\n        return None\n\n\n# For MAC (a.k.a. IEEE 802, or EUI-48) addresses, the second least significant\n# bit of the first octet signifies whether the MAC address is universally (0)\n# or locally (1) administered.  Network cards from hardware manufacturers will\n# always be universally administered to guarantee global uniqueness of the MAC\n# address, but any particular machine may have other interfaces which are\n# locally administered.  An example of the latter is the bridge interface to\n# the Touch Bar on MacBook Pros.\n#\n# This bit works out to be the 42nd bit counting from 1 being the least\n# significant, or 1<<41.  We'll prefer universally administered MAC addresses\n# over locally administered ones since the former are globally unique, but\n# we'll return the first of the latter found if that's all the machine has.\n#\n# See https://en.wikipedia.org/wiki/MAC_address#Universal_vs._local\n\ndef _is_universal(mac):\n    return not (mac & (1 << 41))\n\n\ndef _find_mac_near_keyword(command, args, keywords, get_word_index):\n    \"\"\"Searches a command's output for a MAC address near a keyword.\n\n    Each line of words in the output is case-insensitively searched for\n    any of the given keywords.  Upon a match, get_word_index is invoked\n    to pick a word from the line, given the index of the match.  For\n    example, lambda i: 0 would get the first word on the line, while\n    lambda i: i - 1 would get the word preceding the keyword.\n    \"\"\"\n    stdout = _get_command_stdout(command, args)\n    if stdout is None:\n        return None\n\n    first_local_mac = None\n    for line in stdout:\n        words = line.lower().rstrip().split()\n        for i in range(len(words)):\n            if words[i] in keywords:\n                try:\n                    word = words[get_word_index(i)]\n                    mac = int(word.replace(_MAC_DELIM, b''), 16)\n                except (ValueError, IndexError):\n                    # Virtual interfaces, such as those provided by\n                    # VPNs, do not have a colon-delimited MAC address\n                    # as expected, but a 16-byte HWAddr separated by\n                    # dashes. These should be ignored in favor of a\n                    # real MAC address\n                    pass\n                else:\n                    if _is_universal(mac):\n                        return mac\n                    first_local_mac = first_local_mac or mac\n    return first_local_mac or None\n\n\ndef _parse_mac(word):\n    # Accept 'HH:HH:HH:HH:HH:HH' MAC address (ex: '52:54:00:9d:0e:67'),\n    # but reject IPv6 address (ex: 'fe80::5054:ff:fe9' or '123:2:3:4:5:6:7:8').\n    #\n    # Virtual interfaces, such as those provided by VPNs, do not have a\n    # colon-delimited MAC address as expected, but a 16-byte HWAddr separated\n    # by dashes. These should be ignored in favor of a real MAC address\n    parts = word.split(_MAC_DELIM)\n    if len(parts) != 6:\n        return\n    if _MAC_OMITS_LEADING_ZEROES:\n        # (Only) on AIX the macaddr value given is not prefixed by 0, e.g.\n        # en0   1500  link#2      fa.bc.de.f7.62.4 110854824     0 160133733     0     0\n        # not\n        # en0   1500  link#2      fa.bc.de.f7.62.04 110854824     0 160133733     0     0\n        if not all(1 <= len(part) <= 2 for part in parts):\n            return\n        hexstr = b''.join(part.rjust(2, b'0') for part in parts)\n    else:\n        if not all(len(part) == 2 for part in parts):\n            return\n        hexstr = b''.join(parts)\n    try:\n        return int(hexstr, 16)\n    except ValueError:\n        return\n\n\ndef _find_mac_under_heading(command, args, heading):\n    \"\"\"Looks for a MAC address under a heading in a command's output.\n\n    The first line of words in the output is searched for the given\n    heading. Words at the same word index as the heading in subsequent\n    lines are then examined to see if they look like MAC addresses.\n    \"\"\"\n    stdout = _get_command_stdout(command, args)\n    if stdout is None:\n        return None\n\n    keywords = stdout.readline().rstrip().split()\n    try:\n        column_index = keywords.index(heading)\n    except ValueError:\n        return None\n\n    first_local_mac = None\n    for line in stdout:\n        words = line.rstrip().split()\n        try:\n            word = words[column_index]\n        except IndexError:\n            continue\n\n        mac = _parse_mac(word)\n        if mac is None:\n            continue\n        if _is_universal(mac):\n            return mac\n        if first_local_mac is None:\n            first_local_mac = mac\n\n    return first_local_mac\n\n\n# The following functions call external programs to 'get' a macaddr value to\n# be used as basis for an uuid\ndef _ifconfig_getnode():\n    \"\"\"Get the hardware address on Unix by running ifconfig.\"\"\"\n    # This works on Linux ('' or '-a'), Tru64 ('-av'), but not all Unixes.\n    keywords = (b'hwaddr', b'ether', b'address:', b'lladdr')\n    for args in ('', '-a', '-av'):\n        mac = _find_mac_near_keyword('ifconfig', args, keywords, lambda i: i+1)\n        if mac:\n            return mac\n        return None\n\ndef _ip_getnode():\n    \"\"\"Get the hardware address on Unix by running ip.\"\"\"\n    # This works on Linux with iproute2.\n    mac = _find_mac_near_keyword('ip', 'link', [b'link/ether'], lambda i: i+1)\n    if mac:\n        return mac\n    return None\n\ndef _arp_getnode():\n    \"\"\"Get the hardware address on Unix by running arp.\"\"\"\n    import os, socket\n    try:\n        ip_addr = socket.gethostbyname(socket.gethostname())\n    except OSError:\n        return None\n\n    # Try getting the MAC addr from arp based on our IP address (Solaris).\n    mac = _find_mac_near_keyword('arp', '-an', [os.fsencode(ip_addr)], lambda i: -1)\n    if mac:\n        return mac\n\n    # This works on OpenBSD\n    mac = _find_mac_near_keyword('arp', '-an', [os.fsencode(ip_addr)], lambda i: i+1)\n    if mac:\n        return mac\n\n    # This works on Linux, FreeBSD and NetBSD\n    mac = _find_mac_near_keyword('arp', '-an', [os.fsencode('(%s)' % ip_addr)],\n                    lambda i: i+2)\n    # Return None instead of 0.\n    if mac:\n        return mac\n    return None\n\ndef _lanscan_getnode():\n    \"\"\"Get the hardware address on Unix by running lanscan.\"\"\"\n    # This might work on HP-UX.\n    return _find_mac_near_keyword('lanscan', '-ai', [b'lan0'], lambda i: 0)\n\ndef _netstat_getnode():\n    \"\"\"Get the hardware address on Unix by running netstat.\"\"\"\n    # This works on AIX and might work on Tru64 UNIX.\n    return _find_mac_under_heading('netstat', '-ian', b'Address')\n\ndef _ipconfig_getnode():\n    \"\"\"[DEPRECATED] Get the hardware address on Windows.\"\"\"\n    # bpo-40501: UuidCreateSequential() is now the only supported approach\n    return _windll_getnode()\n\ndef _netbios_getnode():\n    \"\"\"[DEPRECATED] Get the hardware address on Windows.\"\"\"\n    # bpo-40501: UuidCreateSequential() is now the only supported approach\n    return _windll_getnode()\n\n\n# Import optional C extension at toplevel, to help disabling it when testing\ntry:\n    import _uuid\n    _generate_time_safe = getattr(_uuid, \"generate_time_safe\", None)\n    _UuidCreate = getattr(_uuid, \"UuidCreate\", None)\n    _has_uuid_generate_time_safe = _uuid.has_uuid_generate_time_safe\nexcept ImportError:\n    _uuid = None\n    _generate_time_safe = None\n    _UuidCreate = None\n    _has_uuid_generate_time_safe = None\n\n\ndef _load_system_functions():\n    \"\"\"[DEPRECATED] Platform-specific functions loaded at import time\"\"\"\n\n\ndef _unix_getnode():\n    \"\"\"Get the hardware address on Unix using the _uuid extension module.\"\"\"\n    if _generate_time_safe:\n        uuid_time, _ = _generate_time_safe()\n        return UUID(bytes=uuid_time).node\n\ndef _windll_getnode():\n    \"\"\"Get the hardware address on Windows using the _uuid extension module.\"\"\"\n    if _UuidCreate:\n        uuid_bytes = _UuidCreate()\n        return UUID(bytes_le=uuid_bytes).node\n\ndef _random_getnode():\n    \"\"\"Get a random node ID.\"\"\"\n    # RFC 4122, $4.1.6 says \"For systems with no IEEE address, a randomly or\n    # pseudo-randomly generated value may be used; see Section 4.5.  The\n    # multicast bit must be set in such addresses, in order that they will\n    # never conflict with addresses obtained from network cards.\"\n    #\n    # The \"multicast bit\" of a MAC address is defined to be \"the least\n    # significant bit of the first octet\".  This works out to be the 41st bit\n    # counting from 1 being the least significant bit, or 1<<40.\n    #\n    # See https://en.wikipedia.org/wiki/MAC_address#Unicast_vs._multicast\n    import random\n    return random.getrandbits(48) | (1 << 40)\n\n\n# _OS_GETTERS, when known, are targeted for a specific OS or platform.\n# The order is by 'common practice' on the specified platform.\n# Note: 'posix' and 'windows' _OS_GETTERS are prefixed by a dll/dlload() method\n# which, when successful, means none of these \"external\" methods are called.\n# _GETTERS is (also) used by test_uuid.py to SkipUnless(), e.g.,\n#     @unittest.skipUnless(_uuid._ifconfig_getnode in _uuid._GETTERS, ...)\nif _LINUX:\n    _OS_GETTERS = [_ip_getnode, _ifconfig_getnode]\nelif sys.platform == 'darwin':\n    _OS_GETTERS = [_ifconfig_getnode, _arp_getnode, _netstat_getnode]\nelif sys.platform == 'win32':\n    # bpo-40201: _windll_getnode will always succeed, so these are not needed\n    _OS_GETTERS = []\nelif _AIX:\n    _OS_GETTERS = [_netstat_getnode]\nelse:\n    _OS_GETTERS = [_ifconfig_getnode, _ip_getnode, _arp_getnode,\n                   _netstat_getnode, _lanscan_getnode]\nif os.name == 'posix':\n    _GETTERS = [_unix_getnode] + _OS_GETTERS\nelif os.name == 'nt':\n    _GETTERS = [_windll_getnode] + _OS_GETTERS\nelse:\n    _GETTERS = _OS_GETTERS\n\n_node = None\n\ndef getnode():\n    \"\"\"Get the hardware address as a 48-bit positive integer.\n\n    The first time this runs, it may launch a separate program, which could\n    be quite slow.  If all attempts to obtain the hardware address fail, we\n    choose a random 48-bit number with its eighth bit set to 1 as recommended\n    in RFC 4122.\n    \"\"\"\n    global _node\n    if _node is not None:\n        return _node\n\n    for getter in _GETTERS + [_random_getnode]:\n        try:\n            _node = getter()\n        except:\n            continue\n        if (_node is not None) and (0 <= _node < (1 << 48)):\n            return _node\n    assert False, '_random_getnode() returned invalid value: {}'.format(_node)\n\n\n_last_timestamp = None\n\ndef uuid1(node=None, clock_seq=None):\n    \"\"\"Generate a UUID from a host ID, sequence number, and the current time.\n    If 'node' is not given, getnode() is used to obtain the hardware\n    address.  If 'clock_seq' is given, it is used as the sequence number;\n    otherwise a random 14-bit sequence number is chosen.\"\"\"\n\n    # When the system provides a version-1 UUID generator, use it (but don't\n    # use UuidCreate here because its UUIDs don't conform to RFC 4122).\n    if _generate_time_safe is not None and node is clock_seq is None:\n        uuid_time, safely_generated = _generate_time_safe()\n        try:\n            is_safe = SafeUUID(safely_generated)\n        except ValueError:\n            is_safe = SafeUUID.unknown\n        return UUID(bytes=uuid_time, is_safe=is_safe)\n\n    global _last_timestamp\n    import time\n    nanoseconds = time.time_ns()\n    # 0x01b21dd213814000 is the number of 100-ns intervals between the\n    # UUID epoch 1582-10-15 00:00:00 and the Unix epoch 1970-01-01 00:00:00.\n    timestamp = nanoseconds // 100 + 0x01b21dd213814000\n    if _last_timestamp is not None and timestamp <= _last_timestamp:\n        timestamp = _last_timestamp + 1\n    _last_timestamp = timestamp\n    if clock_seq is None:\n        import random\n        clock_seq = random.getrandbits(14) # instead of stable storage\n    time_low = timestamp & 0xffffffff\n    time_mid = (timestamp >> 32) & 0xffff\n    time_hi_version = (timestamp >> 48) & 0x0fff\n    clock_seq_low = clock_seq & 0xff\n    clock_seq_hi_variant = (clock_seq >> 8) & 0x3f\n    if node is None:\n        node = getnode()\n    return UUID(fields=(time_low, time_mid, time_hi_version,\n                        clock_seq_hi_variant, clock_seq_low, node), version=1)\n\ndef uuid3(namespace, name):\n    \"\"\"Generate a UUID from the MD5 hash of a namespace UUID and a name.\"\"\"\n    from hashlib import md5\n    digest = md5(\n        namespace.bytes + bytes(name, \"utf-8\"),\n        usedforsecurity=False\n    ).digest()\n    return UUID(bytes=digest[:16], version=3)\n\ndef uuid4():\n    \"\"\"Generate a random UUID.\"\"\"\n    return UUID(bytes=os.urandom(16), version=4)\n\ndef uuid5(namespace, name):\n    \"\"\"Generate a UUID from the SHA-1 hash of a namespace UUID and a name.\"\"\"\n    from hashlib import sha1\n    hash = sha1(namespace.bytes + bytes(name, \"utf-8\")).digest()\n    return UUID(bytes=hash[:16], version=5)\n\n# The following standard UUIDs are for use with uuid3() or uuid5().\n\nNAMESPACE_DNS = UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')\nNAMESPACE_URL = UUID('6ba7b811-9dad-11d1-80b4-00c04fd430c8')\nNAMESPACE_OID = UUID('6ba7b812-9dad-11d1-80b4-00c04fd430c8')\nNAMESPACE_X500 = UUID('6ba7b814-9dad-11d1-80b4-00c04fd430c8')\n",728],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/disk.py":["\"\"\"\nDisk management utilities.\n\"\"\"\n\n# Authors: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n#          Lars Buitinck\n# Copyright (c) 2010 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\n\nimport os\nimport sys\nimport time\nimport errno\nimport shutil\n\nfrom multiprocessing import util\n\n\ntry:\n    WindowsError\nexcept NameError:\n    WindowsError = OSError\n\n\ndef disk_used(path):\n    \"\"\" Return the disk usage in a directory.\"\"\"\n    size = 0\n    for file in os.listdir(path) + ['.']:\n        stat = os.stat(os.path.join(path, file))\n        if hasattr(stat, 'st_blocks'):\n            size += stat.st_blocks * 512\n        else:\n            # on some platform st_blocks is not available (e.g., Windows)\n            # approximate by rounding to next multiple of 512\n            size += (stat.st_size // 512 + 1) * 512\n    # We need to convert to int to avoid having longs on some systems (we\n    # don't want longs to avoid problems we SQLite)\n    return int(size / 1024.)\n\n\ndef memstr_to_bytes(text):\n    \"\"\" Convert a memory text to its value in bytes.\n    \"\"\"\n    kilo = 1024\n    units = dict(K=kilo, M=kilo ** 2, G=kilo ** 3)\n    try:\n        size = int(units[text[-1]] * float(text[:-1]))\n    except (KeyError, ValueError) as e:\n        raise ValueError(\n            \"Invalid literal for size give: %s (type %s) should be \"\n            \"alike '10G', '500M', '50K'.\" % (text, type(text))) from e\n    return size\n\n\ndef mkdirp(d):\n    \"\"\"Ensure directory d exists (like mkdir -p on Unix)\n    No guarantee that the directory is writable.\n    \"\"\"\n    try:\n        os.makedirs(d)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n\n# if a rmtree operation fails in rm_subdirs, wait for this much time (in secs),\n# then retry up to RM_SUBDIRS_N_RETRY times. If it still fails, raise the\n# exception. this mecanism ensures that the sub-process gc have the time to\n# collect and close the memmaps before we fail.\nRM_SUBDIRS_RETRY_TIME = 0.1\nRM_SUBDIRS_N_RETRY = 5\n\n\ndef rm_subdirs(path, onerror=None):\n    \"\"\"Remove all subdirectories in this path.\n\n    The directory indicated by `path` is left in place, and its subdirectories\n    are erased.\n\n    If onerror is set, it is called to handle the error with arguments (func,\n    path, exc_info) where func is os.listdir, os.remove, or os.rmdir;\n    path is the argument to that function that caused it to fail; and\n    exc_info is a tuple returned by sys.exc_info().  If onerror is None,\n    an exception is raised.\n    \"\"\"\n\n    # NOTE this code is adapted from the one in shutil.rmtree, and is\n    # just as fast\n\n    names = []\n    try:\n        names = os.listdir(path)\n    except os.error:\n        if onerror is not None:\n            onerror(os.listdir, path, sys.exc_info())\n        else:\n            raise\n\n    for name in names:\n        fullname = os.path.join(path, name)\n        delete_folder(fullname, onerror=onerror)\n\n\ndef delete_folder(folder_path, onerror=None, allow_non_empty=True):\n    \"\"\"Utility function to cleanup a temporary folder if it still exists.\"\"\"\n    if os.path.isdir(folder_path):\n        if onerror is not None:\n            shutil.rmtree(folder_path, False, onerror)\n        else:\n            # allow the rmtree to fail once, wait and re-try.\n            # if the error is raised again, fail\n            err_count = 0\n            while True:\n                files = os.listdir(folder_path)\n                try:\n                    if len(files) == 0 or allow_non_empty:\n                        shutil.rmtree(\n                            folder_path, ignore_errors=False, onerror=None\n                        )\n                        util.debug(\n                            \"Sucessfully deleted {}\".format(folder_path))\n                        break\n                    else:\n                        raise OSError(\n                            \"Expected empty folder {} but got {} \"\n                            \"files.\".format(folder_path, len(files))\n                        )\n                except (OSError, WindowsError):\n                    err_count += 1\n                    if err_count > RM_SUBDIRS_N_RETRY:\n                        # the folder cannot be deleted right now. It maybe\n                        # because some temporary files have not been deleted\n                        # yet.\n                        raise\n                time.sleep(RM_SUBDIRS_RETRY_TIME)\n",136],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/context.py":["import os\nimport sys\nimport threading\n\nfrom . import process\nfrom . import reduction\n\n__all__ = ()\n\n#\n# Exceptions\n#\n\nclass ProcessError(Exception):\n    pass\n\nclass BufferTooShort(ProcessError):\n    pass\n\nclass TimeoutError(ProcessError):\n    pass\n\nclass AuthenticationError(ProcessError):\n    pass\n\n#\n# Base type for contexts. Bound methods of an instance of this type are included in __all__ of __init__.py\n#\n\nclass BaseContext(object):\n\n    ProcessError = ProcessError\n    BufferTooShort = BufferTooShort\n    TimeoutError = TimeoutError\n    AuthenticationError = AuthenticationError\n\n    current_process = staticmethod(process.current_process)\n    parent_process = staticmethod(process.parent_process)\n    active_children = staticmethod(process.active_children)\n\n    def cpu_count(self):\n        '''Returns the number of CPUs in the system'''\n        num = os.cpu_count()\n        if num is None:\n            raise NotImplementedError('cannot determine number of cpus')\n        else:\n            return num\n\n    def Manager(self):\n        '''Returns a manager associated with a running server process\n\n        The managers methods such as `Lock()`, `Condition()` and `Queue()`\n        can be used to create shared objects.\n        '''\n        from .managers import SyncManager\n        m = SyncManager(ctx=self.get_context())\n        m.start()\n        return m\n\n    def Pipe(self, duplex=True):\n        '''Returns two connection object connected by a pipe'''\n        from .connection import Pipe\n        return Pipe(duplex)\n\n    def Lock(self):\n        '''Returns a non-recursive lock object'''\n        from .synchronize import Lock\n        return Lock(ctx=self.get_context())\n\n    def RLock(self):\n        '''Returns a recursive lock object'''\n        from .synchronize import RLock\n        return RLock(ctx=self.get_context())\n\n    def Condition(self, lock=None):\n        '''Returns a condition object'''\n        from .synchronize import Condition\n        return Condition(lock, ctx=self.get_context())\n\n    def Semaphore(self, value=1):\n        '''Returns a semaphore object'''\n        from .synchronize import Semaphore\n        return Semaphore(value, ctx=self.get_context())\n\n    def BoundedSemaphore(self, value=1):\n        '''Returns a bounded semaphore object'''\n        from .synchronize import BoundedSemaphore\n        return BoundedSemaphore(value, ctx=self.get_context())\n\n    def Event(self):\n        '''Returns an event object'''\n        from .synchronize import Event\n        return Event(ctx=self.get_context())\n\n    def Barrier(self, parties, action=None, timeout=None):\n        '''Returns a barrier object'''\n        from .synchronize import Barrier\n        return Barrier(parties, action, timeout, ctx=self.get_context())\n\n    def Queue(self, maxsize=0):\n        '''Returns a queue object'''\n        from .queues import Queue\n        return Queue(maxsize, ctx=self.get_context())\n\n    def JoinableQueue(self, maxsize=0):\n        '''Returns a queue object'''\n        from .queues import JoinableQueue\n        return JoinableQueue(maxsize, ctx=self.get_context())\n\n    def SimpleQueue(self):\n        '''Returns a queue object'''\n        from .queues import SimpleQueue\n        return SimpleQueue(ctx=self.get_context())\n\n    def Pool(self, processes=None, initializer=None, initargs=(),\n             maxtasksperchild=None):\n        '''Returns a process pool object'''\n        from .pool import Pool\n        return Pool(processes, initializer, initargs, maxtasksperchild,\n                    context=self.get_context())\n\n    def RawValue(self, typecode_or_type, *args):\n        '''Returns a shared object'''\n        from .sharedctypes import RawValue\n        return RawValue(typecode_or_type, *args)\n\n    def RawArray(self, typecode_or_type, size_or_initializer):\n        '''Returns a shared array'''\n        from .sharedctypes import RawArray\n        return RawArray(typecode_or_type, size_or_initializer)\n\n    def Value(self, typecode_or_type, *args, lock=True):\n        '''Returns a synchronized shared object'''\n        from .sharedctypes import Value\n        return Value(typecode_or_type, *args, lock=lock,\n                     ctx=self.get_context())\n\n    def Array(self, typecode_or_type, size_or_initializer, *, lock=True):\n        '''Returns a synchronized shared array'''\n        from .sharedctypes import Array\n        return Array(typecode_or_type, size_or_initializer, lock=lock,\n                     ctx=self.get_context())\n\n    def freeze_support(self):\n        '''Check whether this is a fake forked process in a frozen executable.\n        If so then run code specified by commandline and exit.\n        '''\n        if sys.platform == 'win32' and getattr(sys, 'frozen', False):\n            from .spawn import freeze_support\n            freeze_support()\n\n    def get_logger(self):\n        '''Return package logger -- if it does not already exist then\n        it is created.\n        '''\n        from .util import get_logger\n        return get_logger()\n\n    def log_to_stderr(self, level=None):\n        '''Turn on logging and add a handler which prints to stderr'''\n        from .util import log_to_stderr\n        return log_to_stderr(level)\n\n    def allow_connection_pickling(self):\n        '''Install support for sending connections and sockets\n        between processes\n        '''\n        # This is undocumented.  In previous versions of multiprocessing\n        # its only effect was to make socket objects inheritable on Windows.\n        from . import connection\n\n    def set_executable(self, executable):\n        '''Sets the path to a python.exe or pythonw.exe binary used to run\n        child processes instead of sys.executable when using the 'spawn'\n        start method.  Useful for people embedding Python.\n        '''\n        from .spawn import set_executable\n        set_executable(executable)\n\n    def set_forkserver_preload(self, module_names):\n        '''Set list of module names to try to load in forkserver process.\n        This is really just a hint.\n        '''\n        from .forkserver import set_forkserver_preload\n        set_forkserver_preload(module_names)\n\n    def get_context(self, method=None):\n        if method is None:\n            return self\n        try:\n            ctx = _concrete_contexts[method]\n        except KeyError:\n            raise ValueError('cannot find context for %r' % method) from None\n        ctx._check_available()\n        return ctx\n\n    def get_start_method(self, allow_none=False):\n        return self._name\n\n    def set_start_method(self, method, force=False):\n        raise ValueError('cannot set start method of concrete context')\n\n    @property\n    def reducer(self):\n        '''Controls how objects will be reduced to a form that can be\n        shared with other processes.'''\n        return globals().get('reduction')\n\n    @reducer.setter\n    def reducer(self, reduction):\n        globals()['reduction'] = reduction\n\n    def _check_available(self):\n        pass\n\n#\n# Type of default context -- underlying context can be set at most once\n#\n\nclass Process(process.BaseProcess):\n    _start_method = None\n    @staticmethod\n    def _Popen(process_obj):\n        return _default_context.get_context().Process._Popen(process_obj)\n\nclass DefaultContext(BaseContext):\n    Process = Process\n\n    def __init__(self, context):\n        self._default_context = context\n        self._actual_context = None\n\n    def get_context(self, method=None):\n        if method is None:\n            if self._actual_context is None:\n                self._actual_context = self._default_context\n            return self._actual_context\n        else:\n            return super().get_context(method)\n\n    def set_start_method(self, method, force=False):\n        if self._actual_context is not None and not force:\n            raise RuntimeError('context has already been set')\n        if method is None and force:\n            self._actual_context = None\n            return\n        self._actual_context = self.get_context(method)\n\n    def get_start_method(self, allow_none=False):\n        if self._actual_context is None:\n            if allow_none:\n                return None\n            self._actual_context = self._default_context\n        return self._actual_context._name\n\n    def get_all_start_methods(self):\n        if sys.platform == 'win32':\n            return ['spawn']\n        else:\n            methods = ['spawn', 'fork'] if sys.platform == 'darwin' else ['fork', 'spawn']\n            if reduction.HAVE_SEND_HANDLE:\n                methods.append('forkserver')\n            return methods\n\n\n#\n# Context types for fixed start method\n#\n\nif sys.platform != 'win32':\n\n    class ForkProcess(process.BaseProcess):\n        _start_method = 'fork'\n        @staticmethod\n        def _Popen(process_obj):\n            from .popen_fork import Popen\n            return Popen(process_obj)\n\n    class SpawnProcess(process.BaseProcess):\n        _start_method = 'spawn'\n        @staticmethod\n        def _Popen(process_obj):\n            from .popen_spawn_posix import Popen\n            return Popen(process_obj)\n\n    class ForkServerProcess(process.BaseProcess):\n        _start_method = 'forkserver'\n        @staticmethod\n        def _Popen(process_obj):\n            from .popen_forkserver import Popen\n            return Popen(process_obj)\n\n    class ForkContext(BaseContext):\n        _name = 'fork'\n        Process = ForkProcess\n\n    class SpawnContext(BaseContext):\n        _name = 'spawn'\n        Process = SpawnProcess\n\n    class ForkServerContext(BaseContext):\n        _name = 'forkserver'\n        Process = ForkServerProcess\n        def _check_available(self):\n            if not reduction.HAVE_SEND_HANDLE:\n                raise ValueError('forkserver start method not available')\n\n    _concrete_contexts = {\n        'fork': ForkContext(),\n        'spawn': SpawnContext(),\n        'forkserver': ForkServerContext(),\n    }\n    if sys.platform == 'darwin':\n        # bpo-33725: running arbitrary code after fork() is no longer reliable\n        # on macOS since macOS 10.14 (Mojave). Use spawn by default instead.\n        _default_context = DefaultContext(_concrete_contexts['spawn'])\n    else:\n        _default_context = DefaultContext(_concrete_contexts['fork'])\n\nelse:\n\n    class SpawnProcess(process.BaseProcess):\n        _start_method = 'spawn'\n        @staticmethod\n        def _Popen(process_obj):\n            from .popen_spawn_win32 import Popen\n            return Popen(process_obj)\n\n    class SpawnContext(BaseContext):\n        _name = 'spawn'\n        Process = SpawnProcess\n\n    _concrete_contexts = {\n        'spawn': SpawnContext(),\n    }\n    _default_context = DefaultContext(_concrete_contexts['spawn'])\n\n#\n# Force the start method\n#\n\ndef _force_start_method(method):\n    _default_context._actual_context = _concrete_contexts[method]\n\n#\n# Check that the current thread is spawning a child process\n#\n\n_tls = threading.local()\n\ndef get_spawning_popen():\n    return getattr(_tls, 'spawning_popen', None)\n\ndef set_spawning_popen(popen):\n    _tls.spawning_popen = popen\n\ndef assert_spawning(obj):\n    if get_spawning_popen() is None:\n        raise RuntimeError(\n            '%s objects should only be shared between processes'\n            ' through inheritance' % type(obj).__name__\n            )\n",362],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/__init__.py":["\"\"\"\nThe :mod:`sklearn.utils` module includes various utilities.\n\"\"\"\nimport pkgutil\nimport inspect\nfrom importlib import import_module\nfrom operator import itemgetter\nfrom collections.abc import Sequence\nfrom contextlib import contextmanager\nfrom itertools import compress\nfrom itertools import islice\nimport numbers\nimport platform\nimport struct\nimport timeit\nfrom pathlib import Path\n\nimport warnings\nimport numpy as np\nfrom scipy.sparse import issparse\n\nfrom .murmurhash import murmurhash3_32\nfrom .class_weight import compute_class_weight, compute_sample_weight\nfrom . import _joblib\nfrom ..exceptions import DataConversionWarning\nfrom .deprecation import deprecated\nfrom .fixes import np_version, parse_version\nfrom ._estimator_html_repr import estimator_html_repr\nfrom .validation import (as_float_array,\n                         assert_all_finite,\n                         check_random_state, column_or_1d, check_array,\n                         check_consistent_length, check_X_y, indexable,\n                         check_symmetric, check_scalar)\nfrom .. import get_config\n\n\n# Do not deprecate parallel_backend and register_parallel_backend as they are\n# needed to tune `scikit-learn` behavior and have different effect if called\n# from the vendored version or or the site-package version. The other are\n# utilities that are independent of scikit-learn so they are not part of\n# scikit-learn public API.\nparallel_backend = _joblib.parallel_backend\nregister_parallel_backend = _joblib.register_parallel_backend\n\n__all__ = [\"murmurhash3_32\", \"as_float_array\",\n           \"assert_all_finite\", \"check_array\",\n           \"check_random_state\",\n           \"compute_class_weight\", \"compute_sample_weight\",\n           \"column_or_1d\",\n           \"check_consistent_length\", \"check_X_y\", \"check_scalar\", 'indexable',\n           \"check_symmetric\", \"indices_to_mask\", \"deprecated\",\n           \"parallel_backend\", \"register_parallel_backend\",\n           \"resample\", \"shuffle\", \"check_matplotlib_support\", \"all_estimators\",\n           \"DataConversionWarning\", \"estimator_html_repr\"]\n\nIS_PYPY = platform.python_implementation() == 'PyPy'\n_IS_32BIT = 8 * struct.calcsize(\"P\") == 32\n\n\nclass Bunch(dict):\n    \"\"\"Container object exposing keys as attributes.\n\n    Bunch objects are sometimes used as an output for functions and methods.\n    They extend dictionaries by enabling values to be accessed by key,\n    `bunch[\"value_key\"]`, or by an attribute, `bunch.value_key`.\n\n    Examples\n    --------\n    >>> b = Bunch(a=1, b=2)\n    >>> b['b']\n    2\n    >>> b.b\n    2\n    >>> b.a = 3\n    >>> b['a']\n    3\n    >>> b.c = 6\n    >>> b['c']\n    6\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(kwargs)\n\n    def __setattr__(self, key, value):\n        self[key] = value\n\n    def __dir__(self):\n        return self.keys()\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key)\n\n    def __setstate__(self, state):\n        # Bunch pickles generated with scikit-learn 0.16.* have an non\n        # empty __dict__. This causes a surprising behaviour when\n        # loading these pickles scikit-learn 0.17: reading bunch.key\n        # uses __dict__ but assigning to bunch.key use __setattr__ and\n        # only changes bunch['key']. More details can be found at:\n        # https://github.com/scikit-learn/scikit-learn/issues/6196.\n        # Overriding __setstate__ to be a noop has the effect of\n        # ignoring the pickled __dict__\n        pass\n\n\ndef safe_mask(X, mask):\n    \"\"\"Return a mask which is safe to use on X.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Data on which to apply mask.\n\n    mask : ndarray\n        Mask to be used on X.\n\n    Returns\n    -------\n        mask\n    \"\"\"\n    mask = np.asarray(mask)\n    if np.issubdtype(mask.dtype, np.signedinteger):\n        return mask\n\n    if hasattr(X, \"toarray\"):\n        ind = np.arange(mask.shape[0])\n        mask = ind[mask]\n    return mask\n\n\ndef axis0_safe_slice(X, mask, len_mask):\n    \"\"\"\n    This mask is safer than safe_mask since it returns an\n    empty array, when a sparse matrix is sliced with a boolean mask\n    with all False, instead of raising an unhelpful error in older\n    versions of SciPy.\n\n    See: https://github.com/scipy/scipy/issues/5361\n\n    Also note that we can avoid doing the dot product by checking if\n    the len_mask is not zero in _huber_loss_and_gradient but this\n    is not going to be the bottleneck, since the number of outliers\n    and non_outliers are typically non-zero and it makes the code\n    tougher to follow.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Data on which to apply mask.\n\n    mask : ndarray\n        Mask to be used on X.\n\n    len_mask : int\n        The length of the mask.\n\n    Returns\n    -------\n        mask\n    \"\"\"\n    if len_mask != 0:\n        return X[safe_mask(X, mask), :]\n    return np.zeros(shape=(0, X.shape[1]))\n\n\ndef _array_indexing(array, key, key_dtype, axis):\n    \"\"\"Index an array or scipy.sparse consistently across NumPy version.\"\"\"\n    if np_version < parse_version('1.12') or issparse(array):\n        # FIXME: Remove the check for NumPy when using >= 1.12\n        # check if we have an boolean array-likes to make the proper indexing\n        if key_dtype == 'bool':\n            key = np.asarray(key)\n    if isinstance(key, tuple):\n        key = list(key)\n    return array[key] if axis == 0 else array[:, key]\n\n\ndef _pandas_indexing(X, key, key_dtype, axis):\n    \"\"\"Index a pandas dataframe or a series.\"\"\"\n    if hasattr(key, 'shape'):\n        # Work-around for indexing with read-only key in pandas\n        # FIXME: solved in pandas 0.25\n        key = np.asarray(key)\n        key = key if key.flags.writeable else key.copy()\n    elif isinstance(key, tuple):\n        key = list(key)\n    # check whether we should index with loc or iloc\n    indexer = X.iloc if key_dtype == 'int' else X.loc\n    return indexer[:, key] if axis else indexer[key]\n\n\ndef _list_indexing(X, key, key_dtype):\n    \"\"\"Index a Python list.\"\"\"\n    if np.isscalar(key) or isinstance(key, slice):\n        # key is a slice or a scalar\n        return X[key]\n    if key_dtype == 'bool':\n        # key is a boolean array-like\n        return list(compress(X, key))\n    # key is a integer array-like of key\n    return [X[idx] for idx in key]\n\n\ndef _determine_key_type(key, accept_slice=True):\n    \"\"\"Determine the data type of key.\n\n    Parameters\n    ----------\n    key : scalar, slice or array-like\n        The key from which we want to infer the data type.\n\n    accept_slice : bool, default=True\n        Whether or not to raise an error if the key is a slice.\n\n    Returns\n    -------\n    dtype : {'int', 'str', 'bool', None}\n        Returns the data type of key.\n    \"\"\"\n    err_msg = (\"No valid specification of the columns. Only a scalar, list or \"\n               \"slice of all integers or all strings, or boolean mask is \"\n               \"allowed\")\n\n    dtype_to_str = {int: 'int', str: 'str', bool: 'bool', np.bool_: 'bool'}\n    array_dtype_to_str = {'i': 'int', 'u': 'int', 'b': 'bool', 'O': 'str',\n                          'U': 'str', 'S': 'str'}\n\n    if key is None:\n        return None\n    if isinstance(key, tuple(dtype_to_str.keys())):\n        try:\n            return dtype_to_str[type(key)]\n        except KeyError:\n            raise ValueError(err_msg)\n    if isinstance(key, slice):\n        if not accept_slice:\n            raise TypeError(\n                'Only array-like or scalar are supported. '\n                'A Python slice was given.'\n            )\n        if key.start is None and key.stop is None:\n            return None\n        key_start_type = _determine_key_type(key.start)\n        key_stop_type = _determine_key_type(key.stop)\n        if key_start_type is not None and key_stop_type is not None:\n            if key_start_type != key_stop_type:\n                raise ValueError(err_msg)\n        if key_start_type is not None:\n            return key_start_type\n        return key_stop_type\n    if isinstance(key, (list, tuple)):\n        unique_key = set(key)\n        key_type = {_determine_key_type(elt) for elt in unique_key}\n        if not key_type:\n            return None\n        if len(key_type) != 1:\n            raise ValueError(err_msg)\n        return key_type.pop()\n    if hasattr(key, 'dtype'):\n        try:\n            return array_dtype_to_str[key.dtype.kind]\n        except KeyError:\n            raise ValueError(err_msg)\n    raise ValueError(err_msg)\n\n\ndef _safe_indexing(X, indices, *, axis=0):\n    \"\"\"Return rows, items or columns of X using indices.\n\n    .. warning::\n\n        This utility is documented, but **private**. This means that\n        backward compatibility might be broken without any deprecation\n        cycle.\n\n    Parameters\n    ----------\n    X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series\n        Data from which to sample rows, items or columns. `list` are only\n        supported when `axis=0`.\n    indices : bool, int, str, slice, array-like\n        - If `axis=0`, boolean and integer array-like, integer slice,\n          and scalar integer are supported.\n        - If `axis=1`:\n            - to select a single column, `indices` can be of `int` type for\n              all `X` types and `str` only for dataframe. The selected subset\n              will be 1D, unless `X` is a sparse matrix in which case it will\n              be 2D.\n            - to select multiples columns, `indices` can be one of the\n              following: `list`, `array`, `slice`. The type used in\n              these containers can be one of the following: `int`, 'bool' and\n              `str`. However, `str` is only supported when `X` is a dataframe.\n              The selected subset will be 2D.\n    axis : int, default=0\n        The axis along which `X` will be subsampled. `axis=0` will select\n        rows while `axis=1` will select columns.\n\n    Returns\n    -------\n    subset\n        Subset of X on axis 0 or 1.\n\n    Notes\n    -----\n    CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are\n    not supported.\n    \"\"\"\n    if indices is None:\n        return X\n\n    if axis not in (0, 1):\n        raise ValueError(\n            \"'axis' should be either 0 (to index rows) or 1 (to index \"\n            \" column). Got {} instead.\".format(axis)\n        )\n\n    indices_dtype = _determine_key_type(indices)\n\n    if axis == 0 and indices_dtype == 'str':\n        raise ValueError(\n            \"String indexing is not supported with 'axis=0'\"\n        )\n\n    if axis == 1 and X.ndim != 2:\n        raise ValueError(\n            \"'X' should be a 2D NumPy array, 2D sparse matrix or pandas \"\n            \"dataframe when indexing the columns (i.e. 'axis=1'). \"\n            \"Got {} instead with {} dimension(s).\".format(type(X), X.ndim)\n        )\n\n    if axis == 1 and indices_dtype == 'str' and not hasattr(X, 'loc'):\n        raise ValueError(\n            \"Specifying the columns using strings is only supported for \"\n            \"pandas DataFrames\"\n        )\n\n    if hasattr(X, \"iloc\"):\n        return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n    elif hasattr(X, \"shape\"):\n        return _array_indexing(X, indices, indices_dtype, axis=axis)\n    else:\n        return _list_indexing(X, indices, indices_dtype)\n\n\ndef _get_column_indices(X, key):\n    \"\"\"Get feature column indices for input data X and key.\n\n    For accepted values of `key`, see the docstring of\n    :func:`_safe_indexing_column`.\n    \"\"\"\n    n_columns = X.shape[1]\n\n    key_dtype = _determine_key_type(key)\n\n    if isinstance(key, (list, tuple)) and not key:\n        # we get an empty list\n        return []\n    elif key_dtype in ('bool', 'int'):\n        # Convert key into positive indexes\n        try:\n            idx = _safe_indexing(np.arange(n_columns), key)\n        except IndexError as e:\n            raise ValueError(\n                'all features must be in [0, {}] or [-{}, 0]'\n                .format(n_columns - 1, n_columns)\n            ) from e\n        return np.atleast_1d(idx).tolist()\n    elif key_dtype == 'str':\n        try:\n            all_columns = X.columns\n        except AttributeError:\n            raise ValueError(\"Specifying the columns using strings is only \"\n                             \"supported for pandas DataFrames\")\n        if isinstance(key, str):\n            columns = [key]\n        elif isinstance(key, slice):\n            start, stop = key.start, key.stop\n            if start is not None:\n                start = all_columns.get_loc(start)\n            if stop is not None:\n                # pandas indexing with strings is endpoint included\n                stop = all_columns.get_loc(stop) + 1\n            else:\n                stop = n_columns + 1\n            return list(range(n_columns)[slice(start, stop)])\n        else:\n            columns = list(key)\n\n        try:\n            column_indices = []\n            for col in columns:\n                col_idx = all_columns.get_loc(col)\n                if not isinstance(col_idx, numbers.Integral):\n                    raise ValueError(f\"Selected columns, {columns}, are not \"\n                                     \"unique in dataframe\")\n                column_indices.append(col_idx)\n\n        except KeyError as e:\n            raise ValueError(\n                \"A given column is not a column of the dataframe\"\n            ) from e\n\n        return column_indices\n    else:\n        raise ValueError(\"No valid specification of the columns. Only a \"\n                         \"scalar, list or slice of all integers or all \"\n                         \"strings, or boolean mask is allowed\")\n\n\ndef resample(*arrays,\n             replace=True,\n             n_samples=None,\n             random_state=None,\n             stratify=None):\n    \"\"\"Resample arrays or sparse matrices in a consistent way.\n\n    The default strategy implements one step of the bootstrapping\n    procedure.\n\n    Parameters\n    ----------\n    *arrays : sequence of array-like of shape (n_samples,) or \\\n            (n_samples, n_outputs)\n        Indexable data-structures can be arrays, lists, dataframes or scipy\n        sparse matrices with consistent first dimension.\n\n    replace : bool, default=True\n        Implements resampling with replacement. If False, this will implement\n        (sliced) random permutations.\n\n    n_samples : int, default=None\n        Number of samples to generate. If left to None this is\n        automatically set to the first dimension of the arrays.\n        If replace is False it should not be larger than the length of\n        arrays.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for shuffling\n        the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    stratify : array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        If not None, data is split in a stratified fashion, using this as\n        the class labels.\n\n    Returns\n    -------\n    resampled_arrays : sequence of array-like of shape (n_samples,) or \\\n            (n_samples, n_outputs)\n        Sequence of resampled copies of the collections. The original arrays\n        are not impacted.\n\n    Examples\n    --------\n    It is possible to mix sparse and dense arrays in the same run::\n\n      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n      >>> y = np.array([0, 1, 2])\n\n      >>> from scipy.sparse import coo_matrix\n      >>> X_sparse = coo_matrix(X)\n\n      >>> from sklearn.utils import resample\n      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n      >>> X\n      array([[1., 0.],\n             [2., 1.],\n             [1., 0.]])\n\n      >>> X_sparse\n      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n          with 4 stored elements in Compressed Sparse Row format>\n\n      >>> X_sparse.toarray()\n      array([[1., 0.],\n             [2., 1.],\n             [1., 0.]])\n\n      >>> y\n      array([0, 1, 0])\n\n      >>> resample(y, n_samples=2, random_state=0)\n      array([0, 1])\n\n    Example using stratification::\n\n      >>> y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n      >>> resample(y, n_samples=5, replace=False, stratify=y,\n      ...          random_state=0)\n      [1, 1, 1, 0, 1]\n\n    See Also\n    --------\n    shuffle\n    \"\"\"\n    max_n_samples = n_samples\n    random_state = check_random_state(random_state)\n\n    if len(arrays) == 0:\n        return None\n\n    first = arrays[0]\n    n_samples = first.shape[0] if hasattr(first, 'shape') else len(first)\n\n    if max_n_samples is None:\n        max_n_samples = n_samples\n    elif (max_n_samples > n_samples) and (not replace):\n        raise ValueError(\"Cannot sample %d out of arrays with dim %d \"\n                         \"when replace is False\" % (max_n_samples,\n                                                    n_samples))\n\n    check_consistent_length(*arrays)\n\n    if stratify is None:\n        if replace:\n            indices = random_state.randint(0, n_samples, size=(max_n_samples,))\n        else:\n            indices = np.arange(n_samples)\n            random_state.shuffle(indices)\n            indices = indices[:max_n_samples]\n    else:\n        # Code adapted from StratifiedShuffleSplit()\n        y = check_array(stratify, ensure_2d=False, dtype=None)\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([' '.join(row.astype('str')) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(np.argsort(y_indices, kind='mergesort'),\n                                 np.cumsum(class_counts)[:-1])\n\n        n_i = _approximate_mode(class_counts, max_n_samples, random_state)\n\n        indices = []\n\n        for i in range(n_classes):\n            indices_i = random_state.choice(class_indices[i], n_i[i],\n                                            replace=replace)\n            indices.extend(indices_i)\n\n        indices = random_state.permutation(indices)\n\n    # convert sparse matrices to CSR for row-based indexing\n    arrays = [a.tocsr() if issparse(a) else a for a in arrays]\n    resampled_arrays = [_safe_indexing(a, indices) for a in arrays]\n    if len(resampled_arrays) == 1:\n        # syntactic sugar for the unit argument case\n        return resampled_arrays[0]\n    else:\n        return resampled_arrays\n\n\ndef shuffle(*arrays, random_state=None, n_samples=None):\n    \"\"\"Shuffle arrays or sparse matrices in a consistent way.\n\n    This is a convenience alias to ``resample(*arrays, replace=False)`` to do\n    random permutations of the collections.\n\n    Parameters\n    ----------\n    *arrays : sequence of indexable data-structures\n        Indexable data-structures can be arrays, lists, dataframes or scipy\n        sparse matrices with consistent first dimension.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for shuffling\n        the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_samples : int, default=None\n        Number of samples to generate. If left to None this is\n        automatically set to the first dimension of the arrays.  It should\n        not be larger than the length of arrays.\n\n    Returns\n    -------\n    shuffled_arrays : sequence of indexable data-structures\n        Sequence of shuffled copies of the collections. The original arrays\n        are not impacted.\n\n    Examples\n    --------\n    It is possible to mix sparse and dense arrays in the same run::\n\n      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n      >>> y = np.array([0, 1, 2])\n\n      >>> from scipy.sparse import coo_matrix\n      >>> X_sparse = coo_matrix(X)\n\n      >>> from sklearn.utils import shuffle\n      >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n      >>> X\n      array([[0., 0.],\n             [2., 1.],\n             [1., 0.]])\n\n      >>> X_sparse\n      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n          with 3 stored elements in Compressed Sparse Row format>\n\n      >>> X_sparse.toarray()\n      array([[0., 0.],\n             [2., 1.],\n             [1., 0.]])\n\n      >>> y\n      array([2, 1, 0])\n\n      >>> shuffle(y, n_samples=2, random_state=0)\n      array([0, 1])\n\n    See Also\n    --------\n    resample\n    \"\"\"\n    return resample(*arrays, replace=False, n_samples=n_samples,\n                    random_state=random_state)\n\n\ndef safe_sqr(X, *, copy=True):\n    \"\"\"Element wise squaring of array-likes and sparse matrices.\n\n    Parameters\n    ----------\n    X : {array-like, ndarray, sparse matrix}\n\n    copy : bool, default=True\n        Whether to create a copy of X and operate on it or to perform\n        inplace computation (default behaviour).\n\n    Returns\n    -------\n    X ** 2 : element wise square\n    \"\"\"\n    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], ensure_2d=False)\n    if issparse(X):\n        if copy:\n            X = X.copy()\n        X.data **= 2\n    else:\n        if copy:\n            X = X ** 2\n        else:\n            X **= 2\n    return X\n\n\ndef _chunk_generator(gen, chunksize):\n    \"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\n    chunk may have a length less than ``chunksize``.\"\"\"\n    while True:\n        chunk = list(islice(gen, chunksize))\n        if chunk:\n            yield chunk\n        else:\n            return\n\n\ndef gen_batches(n, batch_size, *, min_batch_size=0):\n    \"\"\"Generator to create slices containing batch_size elements, from 0 to n.\n\n    The last slice may contain less than batch_size elements, when batch_size\n    does not divide n.\n\n    Parameters\n    ----------\n    n : int\n    batch_size : int\n        Number of element in each batch.\n    min_batch_size : int, default=0\n        Minimum batch size to produce.\n\n    Yields\n    ------\n    slice of batch_size elements\n\n    See Also\n    --------\n    gen_even_slices: Generator to create n_packs slices going up to n.\n\n    Examples\n    --------\n    >>> from sklearn.utils import gen_batches\n    >>> list(gen_batches(7, 3))\n    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n    >>> list(gen_batches(6, 3))\n    [slice(0, 3, None), slice(3, 6, None)]\n    >>> list(gen_batches(2, 3))\n    [slice(0, 2, None)]\n    >>> list(gen_batches(7, 3, min_batch_size=0))\n    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n    >>> list(gen_batches(7, 3, min_batch_size=2))\n    [slice(0, 3, None), slice(3, 7, None)]\n    \"\"\"\n    if not isinstance(batch_size, numbers.Integral):\n        raise TypeError(\"gen_batches got batch_size=%s, must be an\"\n                        \" integer\" % batch_size)\n    if batch_size <= 0:\n        raise ValueError(\"gen_batches got batch_size=%s, must be\"\n                         \" positive\" % batch_size)\n    start = 0\n    for _ in range(int(n // batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)\n\n\ndef gen_even_slices(n, n_packs, *, n_samples=None):\n    \"\"\"Generator to create n_packs slices going up to n.\n\n    Parameters\n    ----------\n    n : int\n    n_packs : int\n        Number of slices to generate.\n    n_samples : int, default=None\n        Number of samples. Pass n_samples when the slices are to be used for\n        sparse matrix indexing; slicing off-the-end raises an exception, while\n        it works for NumPy arrays.\n\n    Yields\n    ------\n    slice\n\n    See Also\n    --------\n    gen_batches: Generator to create slices containing batch_size elements\n        from 0 to n.\n\n    Examples\n    --------\n    >>> from sklearn.utils import gen_even_slices\n    >>> list(gen_even_slices(10, 1))\n    [slice(0, 10, None)]\n    >>> list(gen_even_slices(10, 10))\n    [slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]\n    >>> list(gen_even_slices(10, 5))\n    [slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]\n    >>> list(gen_even_slices(10, 3))\n    [slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]\n    \"\"\"\n    start = 0\n    if n_packs < 1:\n        raise ValueError(\"gen_even_slices got n_packs=%s, must be >=1\"\n                         % n_packs)\n    for pack_num in range(n_packs):\n        this_n = n // n_packs\n        if pack_num < n % n_packs:\n            this_n += 1\n        if this_n > 0:\n            end = start + this_n\n            if n_samples is not None:\n                end = min(n_samples, end)\n            yield slice(start, end, None)\n            start = end\n\n\ndef tosequence(x):\n    \"\"\"Cast iterable x to a Sequence, avoiding a copy if possible.\n\n    Parameters\n    ----------\n    x : iterable\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        return np.asarray(x)\n    elif isinstance(x, Sequence):\n        return x\n    else:\n        return list(x)\n\n\ndef _to_object_array(sequence):\n    \"\"\"Convert sequence to a 1-D NumPy array of object dtype.\n\n    numpy.array constructor has a similar use but it's output\n    is ambiguous. It can be 1-D NumPy array of object dtype if\n    the input is a ragged array, but if the input is a list of\n    equal length arrays, then the output is a 2D numpy.array.\n    _to_object_array solves this ambiguity by guarantying that\n    the output is a 1-D NumPy array of objects for any input.\n\n    Parameters\n    ----------\n    sequence : array-like of shape (n_elements,)\n        The sequence to be converted.\n\n    Returns\n    -------\n    out : ndarray of shape (n_elements,), dtype=object\n        The converted sequence into a 1-D NumPy array of object dtype.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils import _to_object_array\n    >>> _to_object_array([np.array([0]), np.array([1])])\n    array([array([0]), array([1])], dtype=object)\n    >>> _to_object_array([np.array([0]), np.array([1, 2])])\n    array([array([0]), array([1, 2])], dtype=object)\n    >>> _to_object_array([np.array([0]), np.array([1, 2])])\n    array([array([0]), array([1, 2])], dtype=object)\n    \"\"\"\n    out = np.empty(len(sequence), dtype=object)\n    out[:] = sequence\n    return out\n\n\ndef indices_to_mask(indices, mask_length):\n    \"\"\"Convert list of indices to boolean mask.\n\n    Parameters\n    ----------\n    indices : list-like\n        List of integers treated as indices.\n    mask_length : int\n        Length of boolean mask to be generated.\n        This parameter must be greater than max(indices).\n\n    Returns\n    -------\n    mask : 1d boolean nd-array\n        Boolean array that is True where indices are present, else False.\n\n    Examples\n    --------\n    >>> from sklearn.utils import indices_to_mask\n    >>> indices = [1, 2 , 3, 4]\n    >>> indices_to_mask(indices, 5)\n    array([False,  True,  True,  True,  True])\n    \"\"\"\n    if mask_length <= np.max(indices):\n        raise ValueError(\"mask_length must be greater than max(indices)\")\n\n    mask = np.zeros(mask_length, dtype=bool)\n    mask[indices] = True\n\n    return mask\n\n\ndef _message_with_time(source, message, time):\n    \"\"\"Create one line message for logging purposes.\n\n    Parameters\n    ----------\n    source : str\n        String indicating the source or the reference of the message.\n\n    message : str\n        Short message.\n\n    time : int\n        Time in seconds.\n    \"\"\"\n    start_message = \"[%s] \" % source\n\n    # adapted from joblib.logger.short_format_time without the Windows -.1s\n    # adjustment\n    if time > 60:\n        time_str = \"%4.1fmin\" % (time / 60)\n    else:\n        time_str = \" %5.1fs\" % time\n    end_message = \" %s, total=%s\" % (message, time_str)\n    dots_len = (70 - len(start_message) - len(end_message))\n    return \"%s%s%s\" % (start_message, dots_len * '.', end_message)\n\n\n@contextmanager\ndef _print_elapsed_time(source, message=None):\n    \"\"\"Log elapsed time to stdout when the context is exited.\n\n    Parameters\n    ----------\n    source : str\n        String indicating the source or the reference of the message.\n\n    message : str, default=None\n        Short message. If None, nothing will be printed.\n\n    Returns\n    -------\n    context_manager\n        Prints elapsed time upon exit if verbose.\n    \"\"\"\n    if message is None:\n        yield\n    else:\n        start = timeit.default_timer()\n        yield\n        print(\n            _message_with_time(source, message,\n                               timeit.default_timer() - start))\n\n\ndef get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):\n    \"\"\"Calculates how many rows can be processed within working_memory.\n\n    Parameters\n    ----------\n    row_bytes : int\n        The expected number of bytes of memory that will be consumed\n        during the processing of each row.\n    max_n_rows : int, default=None\n        The maximum return value.\n    working_memory : int or float, default=None\n        The number of rows to fit inside this number of MiB will be returned.\n        When None (default), the value of\n        ``sklearn.get_config()['working_memory']`` is used.\n\n    Returns\n    -------\n    int or the value of n_samples\n\n    Warns\n    -----\n    Issues a UserWarning if ``row_bytes`` exceeds ``working_memory`` MiB.\n    \"\"\"\n\n    if working_memory is None:\n        working_memory = get_config()['working_memory']\n\n    chunk_n_rows = int(working_memory * (2 ** 20) // row_bytes)\n    if max_n_rows is not None:\n        chunk_n_rows = min(chunk_n_rows, max_n_rows)\n    if chunk_n_rows < 1:\n        warnings.warn('Could not adhere to working_memory config. '\n                      'Currently %.0fMiB, %.0fMiB required.' %\n                      (working_memory, np.ceil(row_bytes * 2 ** -20)))\n        chunk_n_rows = 1\n    return chunk_n_rows\n\n\ndef is_scalar_nan(x):\n    \"\"\"Tests if x is NaN.\n\n    This function is meant to overcome the issue that np.isnan does not allow\n    non-numerical types as input, and that np.nan is not float('nan').\n\n    Parameters\n    ----------\n    x : any type\n\n    Returns\n    -------\n    boolean\n\n    Examples\n    --------\n    >>> is_scalar_nan(np.nan)\n    True\n    >>> is_scalar_nan(float(\"nan\"))\n    True\n    >>> is_scalar_nan(None)\n    False\n    >>> is_scalar_nan(\"\")\n    False\n    >>> is_scalar_nan([np.nan])\n    False\n    \"\"\"\n    # convert from numpy.bool_ to python bool to ensure that testing\n    # is_scalar_nan(x) is True does not fail.\n    return bool(isinstance(x, numbers.Real) and np.isnan(x))\n\n\ndef _approximate_mode(class_counts, n_draws, rng):\n    \"\"\"Computes approximate mode of multivariate hypergeometric.\n\n    This is an approximation to the mode of the multivariate\n    hypergeometric given by class_counts and n_draws.\n    It shouldn't be off by more than one.\n\n    It is the mostly likely outcome of drawing n_draws many\n    samples from the population given by class_counts.\n\n    Parameters\n    ----------\n    class_counts : ndarray of int\n        Population per class.\n    n_draws : int\n        Number of draws (samples to draw) from the overall population.\n    rng : random state\n        Used to break ties.\n\n    Returns\n    -------\n    sampled_classes : ndarray of int\n        Number of samples drawn from each class.\n        np.sum(sampled_classes) == n_draws\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils import _approximate_mode\n    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)\n    array([2, 1])\n    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)\n    array([3, 1])\n    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),\n    ...                   n_draws=2, rng=0)\n    array([0, 1, 1, 0])\n    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),\n    ...                   n_draws=2, rng=42)\n    array([1, 1, 0, 0])\n    \"\"\"\n    rng = check_random_state(rng)\n    # this computes a bad approximation to the mode of the\n    # multivariate hypergeometric given by class_counts and n_draws\n    continuous = n_draws * class_counts / class_counts.sum()\n    # floored means we don't overshoot n_samples, but probably undershoot\n    floored = np.floor(continuous)\n    # we add samples according to how much \"left over\" probability\n    # they had, until we arrive at n_samples\n    need_to_add = int(n_draws - floored.sum())\n    if need_to_add > 0:\n        remainder = continuous - floored\n        values = np.sort(np.unique(remainder))[::-1]\n        # add according to remainder, but break ties\n        # randomly to avoid biases\n        for value in values:\n            inds, = np.where(remainder == value)\n            # if we need_to_add less than what's in inds\n            # we draw randomly from them.\n            # if we need to add more, we add them all and\n            # go to the next value\n            add_now = min(len(inds), need_to_add)\n            inds = rng.choice(inds, size=add_now, replace=False)\n            floored[inds] += 1\n            need_to_add -= add_now\n            if need_to_add == 0:\n                break\n    return floored.astype(int)\n\n\ndef check_matplotlib_support(caller_name):\n    \"\"\"Raise ImportError with detailed error message if mpl is not installed.\n\n    Plot utilities like :func:`plot_partial_dependence` should lazily import\n    matplotlib and call this helper before any computation.\n\n    Parameters\n    ----------\n    caller_name : str\n        The name of the caller that requires matplotlib.\n    \"\"\"\n    try:\n        import matplotlib  # noqa\n    except ImportError as e:\n        raise ImportError(\n            \"{} requires matplotlib. You can install matplotlib with \"\n            \"`pip install matplotlib`\".format(caller_name)\n        ) from e\n\n\ndef check_pandas_support(caller_name):\n    \"\"\"Raise ImportError with detailed error message if pandas is not\n    installed.\n\n    Plot utilities like :func:`fetch_openml` should lazily import\n    pandas and call this helper before any computation.\n\n    Parameters\n    ----------\n    caller_name : str\n        The name of the caller that requires pandas.\n    \"\"\"\n    try:\n        import pandas  # noqa\n        return pandas\n    except ImportError as e:\n        raise ImportError(\n            \"{} requires pandas.\".format(caller_name)\n        ) from e\n\n\ndef all_estimators(type_filter=None):\n    \"\"\"Get a list of all estimators from sklearn.\n\n    This function crawls the module and gets all classes that inherit\n    from BaseEstimator. Classes that are defined in test-modules are not\n    included.\n\n    Parameters\n    ----------\n    type_filter : {\"classifier\", \"regressor\", \"cluster\", \"transformer\"} \\\n            or list of such str, default=None\n        Which kind of estimators should be returned. If None, no filter is\n        applied and all estimators are returned.  Possible values are\n        'classifier', 'regressor', 'cluster' and 'transformer' to get\n        estimators only of these specific types, or a list of these to\n        get the estimators that fit at least one of the types.\n\n    Returns\n    -------\n    estimators : list of tuples\n        List of (name, class), where ``name`` is the class name as string\n        and ``class`` is the actuall type of the class.\n    \"\"\"\n    # lazy import to avoid circular imports from sklearn.base\n    from ._testing import ignore_warnings\n    from ..base import (BaseEstimator, ClassifierMixin, RegressorMixin,\n                        TransformerMixin, ClusterMixin)\n\n    def is_abstract(c):\n        if not(hasattr(c, '__abstractmethods__')):\n            return False\n        if not len(c.__abstractmethods__):\n            return False\n        return True\n\n    all_classes = []\n    modules_to_ignore = {\"tests\", \"externals\", \"setup\", \"conftest\"}\n    root = str(Path(__file__).parent.parent)  # sklearn package\n    # Ignore deprecation warnings triggered at import time and from walking\n    # packages\n    with ignore_warnings(category=FutureWarning):\n        for importer, modname, ispkg in pkgutil.walk_packages(\n                path=[root], prefix='sklearn.'):\n            mod_parts = modname.split(\".\")\n            if (any(part in modules_to_ignore for part in mod_parts)\n                    or '._' in modname):\n                continue\n            module = import_module(modname)\n            classes = inspect.getmembers(module, inspect.isclass)\n            classes = [(name, est_cls) for name, est_cls in classes\n                       if not name.startswith(\"_\")]\n\n            # TODO: Remove when FeatureHasher is implemented in PYPY\n            # Skips FeatureHasher for PYPY\n            if IS_PYPY and 'feature_extraction' in modname:\n                classes = [(name, est_cls) for name, est_cls in classes\n                           if name == \"FeatureHasher\"]\n\n            all_classes.extend(classes)\n\n    all_classes = set(all_classes)\n\n    estimators = [c for c in all_classes\n                  if (issubclass(c[1], BaseEstimator) and\n                      c[0] != 'BaseEstimator')]\n    # get rid of abstract base classes\n    estimators = [c for c in estimators if not is_abstract(c[1])]\n\n    if type_filter is not None:\n        if not isinstance(type_filter, list):\n            type_filter = [type_filter]\n        else:\n            type_filter = list(type_filter)  # copy\n        filtered_estimators = []\n        filters = {'classifier': ClassifierMixin,\n                   'regressor': RegressorMixin,\n                   'transformer': TransformerMixin,\n                   'cluster': ClusterMixin}\n        for name, mixin in filters.items():\n            if name in type_filter:\n                type_filter.remove(name)\n                filtered_estimators.extend([est for est in estimators\n                                            if issubclass(est[1], mixin)])\n        estimators = filtered_estimators\n        if type_filter:\n            raise ValueError(\"Parameter type_filter must be 'classifier', \"\n                             \"'regressor', 'transformer', 'cluster' or \"\n                             \"None, got\"\n                             \" %s.\" % repr(type_filter))\n\n    # drop duplicates, sort for reproducibility\n    # itemgetter is used to ensure the sort does not extend to the 2nd item of\n    # the tuple\n    return sorted(set(estimators), key=itemgetter(0))\n",1186],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/functools.py":["\"\"\"functools.py - Tools for working with functions and callable objects\n\"\"\"\n# Python module wrapper for _functools C module\n# to allow utilities written in Python to be added\n# to the functools module.\n# Written by Nick Coghlan <ncoghlan at gmail.com>,\n# Raymond Hettinger <python at rcn.com>,\n# and Łukasz Langa <lukasz at langa.pl>.\n#   Copyright (C) 2006-2013 Python Software Foundation.\n# See C source code for _functools credits/copyright\n\n__all__ = ['update_wrapper', 'wraps', 'WRAPPER_ASSIGNMENTS', 'WRAPPER_UPDATES',\n           'total_ordering', 'cache', 'cmp_to_key', 'lru_cache', 'reduce',\n           'partial', 'partialmethod', 'singledispatch', 'singledispatchmethod',\n           'cached_property']\n\nfrom abc import get_cache_token\nfrom collections import namedtuple\n# import types, weakref  # Deferred to single_dispatch()\nfrom reprlib import recursive_repr\nfrom _thread import RLock\nfrom types import GenericAlias\n\n\n################################################################################\n### update_wrapper() and wraps() decorator\n################################################################################\n\n# update_wrapper() and wraps() are tools to help write\n# wrapper functions that can handle naive introspection\n\nWRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__qualname__', '__doc__',\n                       '__annotations__')\nWRAPPER_UPDATES = ('__dict__',)\ndef update_wrapper(wrapper,\n                   wrapped,\n                   assigned = WRAPPER_ASSIGNMENTS,\n                   updated = WRAPPER_UPDATES):\n    \"\"\"Update a wrapper function to look like the wrapped function\n\n       wrapper is the function to be updated\n       wrapped is the original function\n       assigned is a tuple naming the attributes assigned directly\n       from the wrapped function to the wrapper function (defaults to\n       functools.WRAPPER_ASSIGNMENTS)\n       updated is a tuple naming the attributes of the wrapper that\n       are updated with the corresponding attribute from the wrapped\n       function (defaults to functools.WRAPPER_UPDATES)\n    \"\"\"\n    for attr in assigned:\n        try:\n            value = getattr(wrapped, attr)\n        except AttributeError:\n            pass\n        else:\n            setattr(wrapper, attr, value)\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n    # Issue #17482: set __wrapped__ last so we don't inadvertently copy it\n    # from the wrapped function when updating __dict__\n    wrapper.__wrapped__ = wrapped\n    # Return the wrapper so this can be used as a decorator via partial()\n    return wrapper\n\ndef wraps(wrapped,\n          assigned = WRAPPER_ASSIGNMENTS,\n          updated = WRAPPER_UPDATES):\n    \"\"\"Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    \"\"\"\n    return partial(update_wrapper, wrapped=wrapped,\n                   assigned=assigned, updated=updated)\n\n\n################################################################################\n### total_ordering class decorator\n################################################################################\n\n# The total ordering functions all invoke the root magic method directly\n# rather than using the corresponding operator.  This avoids possible\n# infinite recursion that could occur when the operator dispatch logic\n# detects a NotImplemented result and then calls a reflected method.\n\ndef _gt_from_lt(self, other, NotImplemented=NotImplemented):\n    'Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).'\n    op_result = self.__lt__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result and self != other\n\ndef _le_from_lt(self, other, NotImplemented=NotImplemented):\n    'Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).'\n    op_result = self.__lt__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result or self == other\n\ndef _ge_from_lt(self, other, NotImplemented=NotImplemented):\n    'Return a >= b.  Computed by @total_ordering from (not a < b).'\n    op_result = self.__lt__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _ge_from_le(self, other, NotImplemented=NotImplemented):\n    'Return a >= b.  Computed by @total_ordering from (not a <= b) or (a == b).'\n    op_result = self.__le__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result or self == other\n\ndef _lt_from_le(self, other, NotImplemented=NotImplemented):\n    'Return a < b.  Computed by @total_ordering from (a <= b) and (a != b).'\n    op_result = self.__le__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result and self != other\n\ndef _gt_from_le(self, other, NotImplemented=NotImplemented):\n    'Return a > b.  Computed by @total_ordering from (not a <= b).'\n    op_result = self.__le__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _lt_from_gt(self, other, NotImplemented=NotImplemented):\n    'Return a < b.  Computed by @total_ordering from (not a > b) and (a != b).'\n    op_result = self.__gt__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result and self != other\n\ndef _ge_from_gt(self, other, NotImplemented=NotImplemented):\n    'Return a >= b.  Computed by @total_ordering from (a > b) or (a == b).'\n    op_result = self.__gt__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result or self == other\n\ndef _le_from_gt(self, other, NotImplemented=NotImplemented):\n    'Return a <= b.  Computed by @total_ordering from (not a > b).'\n    op_result = self.__gt__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _le_from_ge(self, other, NotImplemented=NotImplemented):\n    'Return a <= b.  Computed by @total_ordering from (not a >= b) or (a == b).'\n    op_result = self.__ge__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result or self == other\n\ndef _gt_from_ge(self, other, NotImplemented=NotImplemented):\n    'Return a > b.  Computed by @total_ordering from (a >= b) and (a != b).'\n    op_result = self.__ge__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result and self != other\n\ndef _lt_from_ge(self, other, NotImplemented=NotImplemented):\n    'Return a < b.  Computed by @total_ordering from (not a >= b).'\n    op_result = self.__ge__(other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\n_convert = {\n    '__lt__': [('__gt__', _gt_from_lt),\n               ('__le__', _le_from_lt),\n               ('__ge__', _ge_from_lt)],\n    '__le__': [('__ge__', _ge_from_le),\n               ('__lt__', _lt_from_le),\n               ('__gt__', _gt_from_le)],\n    '__gt__': [('__lt__', _lt_from_gt),\n               ('__ge__', _ge_from_gt),\n               ('__le__', _le_from_gt)],\n    '__ge__': [('__le__', _le_from_ge),\n               ('__gt__', _gt_from_ge),\n               ('__lt__', _lt_from_ge)]\n}\n\ndef total_ordering(cls):\n    \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n    # Find user-defined comparisons (not those inherited from object).\n    roots = {op for op in _convert if getattr(cls, op, None) is not getattr(object, op, None)}\n    if not roots:\n        raise ValueError('must define at least one ordering operation: < > <= >=')\n    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n    for opname, opfunc in _convert[root]:\n        if opname not in roots:\n            opfunc.__name__ = opname\n            setattr(cls, opname, opfunc)\n    return cls\n\n\n################################################################################\n### cmp_to_key() function converter\n################################################################################\n\ndef cmp_to_key(mycmp):\n    \"\"\"Convert a cmp= function into a key= function\"\"\"\n    class K(object):\n        __slots__ = ['obj']\n        def __init__(self, obj):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        __hash__ = None\n    return K\n\ntry:\n    from _functools import cmp_to_key\nexcept ImportError:\n    pass\n\n\n################################################################################\n### reduce() sequence to a single item\n################################################################################\n\n_initial_missing = object()\n\ndef reduce(function, sequence, initial=_initial_missing):\n    \"\"\"\n    reduce(function, sequence[, initial]) -> value\n\n    Apply a function of two arguments cumulatively to the items of a sequence,\n    from left to right, so as to reduce the sequence to a single value.\n    For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\n    of the sequence in the calculation, and serves as a default when the\n    sequence is empty.\n    \"\"\"\n\n    it = iter(sequence)\n\n    if initial is _initial_missing:\n        try:\n            value = next(it)\n        except StopIteration:\n            raise TypeError(\"reduce() of empty sequence with no initial value\") from None\n    else:\n        value = initial\n\n    for element in it:\n        value = function(value, element)\n\n    return value\n\ntry:\n    from _functools import reduce\nexcept ImportError:\n    pass\n\n\n################################################################################\n### partial() argument application\n################################################################################\n\n# Purely functional, no descriptor behaviour\nclass partial:\n    \"\"\"New function with partial application of the given arguments\n    and keywords.\n    \"\"\"\n\n    __slots__ = \"func\", \"args\", \"keywords\", \"__dict__\", \"__weakref__\"\n\n    def __new__(cls, func, /, *args, **keywords):\n        if not callable(func):\n            raise TypeError(\"the first argument must be callable\")\n\n        if hasattr(func, \"func\"):\n            args = func.args + args\n            keywords = {**func.keywords, **keywords}\n            func = func.func\n\n        self = super(partial, cls).__new__(cls)\n\n        self.func = func\n        self.args = args\n        self.keywords = keywords\n        return self\n\n    def __call__(self, /, *args, **keywords):\n        keywords = {**self.keywords, **keywords}\n        return self.func(*self.args, *args, **keywords)\n\n    @recursive_repr()\n    def __repr__(self):\n        qualname = type(self).__qualname__\n        args = [repr(self.func)]\n        args.extend(repr(x) for x in self.args)\n        args.extend(f\"{k}={v!r}\" for (k, v) in self.keywords.items())\n        if type(self).__module__ == \"functools\":\n            return f\"functools.{qualname}({', '.join(args)})\"\n        return f\"{qualname}({', '.join(args)})\"\n\n    def __reduce__(self):\n        return type(self), (self.func,), (self.func, self.args,\n               self.keywords or None, self.__dict__ or None)\n\n    def __setstate__(self, state):\n        if not isinstance(state, tuple):\n            raise TypeError(\"argument to __setstate__ must be a tuple\")\n        if len(state) != 4:\n            raise TypeError(f\"expected 4 items in state, got {len(state)}\")\n        func, args, kwds, namespace = state\n        if (not callable(func) or not isinstance(args, tuple) or\n           (kwds is not None and not isinstance(kwds, dict)) or\n           (namespace is not None and not isinstance(namespace, dict))):\n            raise TypeError(\"invalid partial state\")\n\n        args = tuple(args) # just in case it's a subclass\n        if kwds is None:\n            kwds = {}\n        elif type(kwds) is not dict: # XXX does it need to be *exactly* dict?\n            kwds = dict(kwds)\n        if namespace is None:\n            namespace = {}\n\n        self.__dict__ = namespace\n        self.func = func\n        self.args = args\n        self.keywords = kwds\n\ntry:\n    from _functools import partial\nexcept ImportError:\n    pass\n\n# Descriptor version\nclass partialmethod(object):\n    \"\"\"Method descriptor with partial application of the given arguments\n    and keywords.\n\n    Supports wrapping existing descriptors and handles non-descriptor\n    callables as instance methods.\n    \"\"\"\n\n    def __init__(self, func, /, *args, **keywords):\n        if not callable(func) and not hasattr(func, \"__get__\"):\n            raise TypeError(\"{!r} is not callable or a descriptor\"\n                                 .format(func))\n\n        # func could be a descriptor like classmethod which isn't callable,\n        # so we can't inherit from partial (it verifies func is callable)\n        if isinstance(func, partialmethod):\n            # flattening is mandatory in order to place cls/self before all\n            # other arguments\n            # it's also more efficient since only one function will be called\n            self.func = func.func\n            self.args = func.args + args\n            self.keywords = {**func.keywords, **keywords}\n        else:\n            self.func = func\n            self.args = args\n            self.keywords = keywords\n\n    def __repr__(self):\n        args = \", \".join(map(repr, self.args))\n        keywords = \", \".join(\"{}={!r}\".format(k, v)\n                                 for k, v in self.keywords.items())\n        format_string = \"{module}.{cls}({func}, {args}, {keywords})\"\n        return format_string.format(module=self.__class__.__module__,\n                                    cls=self.__class__.__qualname__,\n                                    func=self.func,\n                                    args=args,\n                                    keywords=keywords)\n\n    def _make_unbound_method(self):\n        def _method(cls_or_self, /, *args, **keywords):\n            keywords = {**self.keywords, **keywords}\n            return self.func(cls_or_self, *self.args, *args, **keywords)\n        _method.__isabstractmethod__ = self.__isabstractmethod__\n        _method._partialmethod = self\n        return _method\n\n    def __get__(self, obj, cls=None):\n        get = getattr(self.func, \"__get__\", None)\n        result = None\n        if get is not None:\n            new_func = get(obj, cls)\n            if new_func is not self.func:\n                # Assume __get__ returning something new indicates the\n                # creation of an appropriate callable\n                result = partial(new_func, *self.args, **self.keywords)\n                try:\n                    result.__self__ = new_func.__self__\n                except AttributeError:\n                    pass\n        if result is None:\n            # If the underlying descriptor didn't do anything, treat this\n            # like an instance method\n            result = self._make_unbound_method().__get__(obj, cls)\n        return result\n\n    @property\n    def __isabstractmethod__(self):\n        return getattr(self.func, \"__isabstractmethod__\", False)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\n# Helper functions\n\ndef _unwrap_partial(func):\n    while isinstance(func, partial):\n        func = func.func\n    return func\n\n################################################################################\n### LRU Cache function decorator\n################################################################################\n\n_CacheInfo = namedtuple(\"CacheInfo\", [\"hits\", \"misses\", \"maxsize\", \"currsize\"])\n\nclass _HashedSeq(list):\n    \"\"\" This class guarantees that hash() will be called no more than once\n        per element.  This is important because the lru_cache() will hash\n        the key multiple times on a cache miss.\n\n    \"\"\"\n\n    __slots__ = 'hashvalue'\n\n    def __init__(self, tup, hash=hash):\n        self[:] = tup\n        self.hashvalue = hash(tup)\n\n    def __hash__(self):\n        return self.hashvalue\n\ndef _make_key(args, kwds, typed,\n             kwd_mark = (object(),),\n             fasttypes = {int, str},\n             tuple=tuple, type=type, len=len):\n    \"\"\"Make a cache key from optionally typed positional and keyword arguments\n\n    The key is constructed in a way that is flat as possible rather than\n    as a nested structure that would take more memory.\n\n    If there is only a single argument and its data type is known to cache\n    its hash value, then that argument is returned without a wrapper.  This\n    saves space and improves lookup speed.\n\n    \"\"\"\n    # All of code below relies on kwds preserving the order input by the user.\n    # Formerly, we sorted() the kwds before looping.  The new way is *much*\n    # faster; however, it means that f(x=1, y=2) will now be treated as a\n    # distinct call from f(y=2, x=1) which will be cached separately.\n    key = args\n    if kwds:\n        key += kwd_mark\n        for item in kwds.items():\n            key += item\n    if typed:\n        key += tuple(type(v) for v in args)\n        if kwds:\n            key += tuple(type(v) for v in kwds.values())\n    elif len(key) == 1 and type(key[0]) in fasttypes:\n        return key[0]\n    return _HashedSeq(key)\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  http://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    \"\"\"\n\n    # Users should only access the lru_cache through its public API:\n    #       cache_info, cache_clear, and f.__wrapped__\n    # The internals of the lru_cache are encapsulated for thread safety and\n    # to allow the implementation to change (including a possible C version).\n\n    if isinstance(maxsize, int):\n        # Negative maxsize is treated as 0\n        if maxsize < 0:\n            maxsize = 0\n    elif callable(maxsize) and isinstance(typed, bool):\n        # The user_function was passed in directly via the maxsize argument\n        user_function, maxsize = maxsize, 128\n        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}\n        return update_wrapper(wrapper, user_function)\n    elif maxsize is not None:\n        raise TypeError(\n            'Expected first argument to be an integer, a callable, or None')\n\n    def decorating_function(user_function):\n        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}\n        return update_wrapper(wrapper, user_function)\n\n    return decorating_function\n\ndef _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo):\n    # Constants shared by all lru cache instances:\n    sentinel = object()          # unique object used to signal cache misses\n    make_key = _make_key         # build a key from the function arguments\n    PREV, NEXT, KEY, RESULT = 0, 1, 2, 3   # names for the link fields\n\n    cache = {}\n    hits = misses = 0\n    full = False\n    cache_get = cache.get    # bound method to lookup a key or return None\n    cache_len = cache.__len__  # get cache size without calling len()\n    lock = RLock()           # because linkedlist updates aren't threadsafe\n    root = []                # root of the circular doubly linked list\n    root[:] = [root, root, None, None]     # initialize by pointing to self\n\n    if maxsize == 0:\n\n        def wrapper(*args, **kwds):\n            # No caching -- just a statistics update\n            nonlocal misses\n            misses += 1\n            result = user_function(*args, **kwds)\n            return result\n\n    elif maxsize is None:\n\n        def wrapper(*args, **kwds):\n            # Simple caching without ordering or size limit\n            nonlocal hits, misses\n            key = make_key(args, kwds, typed)\n            result = cache_get(key, sentinel)\n            if result is not sentinel:\n                hits += 1\n                return result\n            misses += 1\n            result = user_function(*args, **kwds)\n            cache[key] = result\n            return result\n\n    else:\n\n        def wrapper(*args, **kwds):\n            # Size limited caching that tracks accesses by recency\n            nonlocal root, hits, misses, full\n            key = make_key(args, kwds, typed)\n            with lock:\n                link = cache_get(key)\n                if link is not None:\n                    # Move the link to the front of the circular queue\n                    link_prev, link_next, _key, result = link\n                    link_prev[NEXT] = link_next\n                    link_next[PREV] = link_prev\n                    last = root[PREV]\n                    last[NEXT] = root[PREV] = link\n                    link[PREV] = last\n                    link[NEXT] = root\n                    hits += 1\n                    return result\n                misses += 1\n            result = user_function(*args, **kwds)\n            with lock:\n                if key in cache:\n                    # Getting here means that this same key was added to the\n                    # cache while the lock was released.  Since the link\n                    # update is already done, we need only return the\n                    # computed result and update the count of misses.\n                    pass\n                elif full:\n                    # Use the old root to store the new key and result.\n                    oldroot = root\n                    oldroot[KEY] = key\n                    oldroot[RESULT] = result\n                    # Empty the oldest link and make it the new root.\n                    # Keep a reference to the old key and old result to\n                    # prevent their ref counts from going to zero during the\n                    # update. That will prevent potentially arbitrary object\n                    # clean-up code (i.e. __del__) from running while we're\n                    # still adjusting the links.\n                    root = oldroot[NEXT]\n                    oldkey = root[KEY]\n                    oldresult = root[RESULT]\n                    root[KEY] = root[RESULT] = None\n                    # Now update the cache dictionary.\n                    del cache[oldkey]\n                    # Save the potentially reentrant cache[key] assignment\n                    # for last, after the root and links have been put in\n                    # a consistent state.\n                    cache[key] = oldroot\n                else:\n                    # Put result in a new link at the front of the queue.\n                    last = root[PREV]\n                    link = [last, root, key, result]\n                    last[NEXT] = root[PREV] = cache[key] = link\n                    # Use the cache_len bound method instead of the len() function\n                    # which could potentially be wrapped in an lru_cache itself.\n                    full = (cache_len() >= maxsize)\n            return result\n\n    def cache_info():\n        \"\"\"Report cache statistics\"\"\"\n        with lock:\n            return _CacheInfo(hits, misses, maxsize, cache_len())\n\n    def cache_clear():\n        \"\"\"Clear the cache and cache statistics\"\"\"\n        nonlocal hits, misses, full\n        with lock:\n            cache.clear()\n            root[:] = [root, root, None, None]\n            hits = misses = 0\n            full = False\n\n    wrapper.cache_info = cache_info\n    wrapper.cache_clear = cache_clear\n    return wrapper\n\ntry:\n    from _functools import _lru_cache_wrapper\nexcept ImportError:\n    pass\n\n\n################################################################################\n### cache -- simplified access to the infinity cache\n################################################################################\n\ndef cache(user_function, /):\n    'Simple lightweight unbounded cache.  Sometimes called \"memoize\".'\n    return lru_cache(maxsize=None)(user_function)\n\n\n################################################################################\n### singledispatch() - single-dispatch generic function decorator\n################################################################################\n\ndef _c3_merge(sequences):\n    \"\"\"Merges MROs in *sequences* to a single MRO using the C3 algorithm.\n\n    Adapted from http://www.python.org/download/releases/2.3/mro/.\n\n    \"\"\"\n    result = []\n    while True:\n        sequences = [s for s in sequences if s]   # purge empty sequences\n        if not sequences:\n            return result\n        for s1 in sequences:   # find merge candidates among seq heads\n            candidate = s1[0]\n            for s2 in sequences:\n                if candidate in s2[1:]:\n                    candidate = None\n                    break      # reject the current head, it appears later\n            else:\n                break\n        if candidate is None:\n            raise RuntimeError(\"Inconsistent hierarchy\")\n        result.append(candidate)\n        # remove the chosen candidate\n        for seq in sequences:\n            if seq[0] == candidate:\n                del seq[0]\n\ndef _c3_mro(cls, abcs=None):\n    \"\"\"Computes the method resolution order using extended C3 linearization.\n\n    If no *abcs* are given, the algorithm works exactly like the built-in C3\n    linearization used for method resolution.\n\n    If given, *abcs* is a list of abstract base classes that should be inserted\n    into the resulting MRO. Unrelated ABCs are ignored and don't end up in the\n    result. The algorithm inserts ABCs where their functionality is introduced,\n    i.e. issubclass(cls, abc) returns True for the class itself but returns\n    False for all its direct base classes. Implicit ABCs for a given class\n    (either registered or inferred from the presence of a special method like\n    __len__) are inserted directly after the last ABC explicitly listed in the\n    MRO of said class. If two implicit ABCs end up next to each other in the\n    resulting MRO, their ordering depends on the order of types in *abcs*.\n\n    \"\"\"\n    for i, base in enumerate(reversed(cls.__bases__)):\n        if hasattr(base, '__abstractmethods__'):\n            boundary = len(cls.__bases__) - i\n            break   # Bases up to the last explicit ABC are considered first.\n    else:\n        boundary = 0\n    abcs = list(abcs) if abcs else []\n    explicit_bases = list(cls.__bases__[:boundary])\n    abstract_bases = []\n    other_bases = list(cls.__bases__[boundary:])\n    for base in abcs:\n        if issubclass(cls, base) and not any(\n                issubclass(b, base) for b in cls.__bases__\n            ):\n            # If *cls* is the class that introduces behaviour described by\n            # an ABC *base*, insert said ABC to its MRO.\n            abstract_bases.append(base)\n    for base in abstract_bases:\n        abcs.remove(base)\n    explicit_c3_mros = [_c3_mro(base, abcs=abcs) for base in explicit_bases]\n    abstract_c3_mros = [_c3_mro(base, abcs=abcs) for base in abstract_bases]\n    other_c3_mros = [_c3_mro(base, abcs=abcs) for base in other_bases]\n    return _c3_merge(\n        [[cls]] +\n        explicit_c3_mros + abstract_c3_mros + other_c3_mros +\n        [explicit_bases] + [abstract_bases] + [other_bases]\n    )\n\ndef _compose_mro(cls, types):\n    \"\"\"Calculates the method resolution order for a given class *cls*.\n\n    Includes relevant abstract base classes (with their respective bases) from\n    the *types* iterable. Uses a modified C3 linearization algorithm.\n\n    \"\"\"\n    bases = set(cls.__mro__)\n    # Remove entries which are already present in the __mro__ or unrelated.\n    def is_related(typ):\n        return (typ not in bases and hasattr(typ, '__mro__')\n                                 and issubclass(cls, typ))\n    types = [n for n in types if is_related(n)]\n    # Remove entries which are strict bases of other entries (they will end up\n    # in the MRO anyway.\n    def is_strict_base(typ):\n        for other in types:\n            if typ != other and typ in other.__mro__:\n                return True\n        return False\n    types = [n for n in types if not is_strict_base(n)]\n    # Subclasses of the ABCs in *types* which are also implemented by\n    # *cls* can be used to stabilize ABC ordering.\n    type_set = set(types)\n    mro = []\n    for typ in types:\n        found = []\n        for sub in typ.__subclasses__():\n            if sub not in bases and issubclass(cls, sub):\n                found.append([s for s in sub.__mro__ if s in type_set])\n        if not found:\n            mro.append(typ)\n            continue\n        # Favor subclasses with the biggest number of useful bases\n        found.sort(key=len, reverse=True)\n        for sub in found:\n            for subcls in sub:\n                if subcls not in mro:\n                    mro.append(subcls)\n    return _c3_mro(cls, abcs=mro)\n\ndef _find_impl(cls, registry):\n    \"\"\"Returns the best matching implementation from *registry* for type *cls*.\n\n    Where there is no registered implementation for a specific type, its method\n    resolution order is used to find a more generic implementation.\n\n    Note: if *registry* does not contain an implementation for the base\n    *object* type, this function may return None.\n\n    \"\"\"\n    mro = _compose_mro(cls, registry.keys())\n    match = None\n    for t in mro:\n        if match is not None:\n            # If *match* is an implicit ABC but there is another unrelated,\n            # equally matching implicit ABC, refuse the temptation to guess.\n            if (t in registry and t not in cls.__mro__\n                              and match not in cls.__mro__\n                              and not issubclass(match, t)):\n                raise RuntimeError(\"Ambiguous dispatch: {} or {}\".format(\n                    match, t))\n            break\n        if t in registry:\n            match = t\n    return registry.get(match)\n\ndef singledispatch(func):\n    \"\"\"Single-dispatch generic function decorator.\n\n    Transforms a function into a generic function, which can have different\n    behaviours depending upon the type of its first argument. The decorated\n    function acts as the default implementation, and additional\n    implementations can be registered using the register() attribute of the\n    generic function.\n    \"\"\"\n    # There are many programs that use functools without singledispatch, so we\n    # trade-off making singledispatch marginally slower for the benefit of\n    # making start-up of such applications slightly faster.\n    import types, weakref\n\n    registry = {}\n    dispatch_cache = weakref.WeakKeyDictionary()\n    cache_token = None\n\n    def dispatch(cls):\n        \"\"\"generic_func.dispatch(cls) -> <function implementation>\n\n        Runs the dispatch algorithm to return the best available implementation\n        for the given *cls* registered on *generic_func*.\n\n        \"\"\"\n        nonlocal cache_token\n        if cache_token is not None:\n            current_token = get_cache_token()\n            if cache_token != current_token:\n                dispatch_cache.clear()\n                cache_token = current_token\n        try:\n            impl = dispatch_cache[cls]\n        except KeyError:\n            try:\n                impl = registry[cls]\n            except KeyError:\n                impl = _find_impl(cls, registry)\n            dispatch_cache[cls] = impl\n        return impl\n\n    def register(cls, func=None):\n        \"\"\"generic_func.register(cls, func) -> func\n\n        Registers a new implementation for the given *cls* on a *generic_func*.\n\n        \"\"\"\n        nonlocal cache_token\n        if func is None:\n            if isinstance(cls, type):\n                return lambda f: register(cls, f)\n            ann = getattr(cls, '__annotations__', {})\n            if not ann:\n                raise TypeError(\n                    f\"Invalid first argument to `register()`: {cls!r}. \"\n                    f\"Use either `@register(some_class)` or plain `@register` \"\n                    f\"on an annotated function.\"\n                )\n            func = cls\n\n            # only import typing if annotation parsing is necessary\n            from typing import get_type_hints\n            argname, cls = next(iter(get_type_hints(func).items()))\n            if not isinstance(cls, type):\n                raise TypeError(\n                    f\"Invalid annotation for {argname!r}. \"\n                    f\"{cls!r} is not a class.\"\n                )\n        registry[cls] = func\n        if cache_token is None and hasattr(cls, '__abstractmethods__'):\n            cache_token = get_cache_token()\n        dispatch_cache.clear()\n        return func\n\n    def wrapper(*args, **kw):\n        if not args:\n            raise TypeError(f'{funcname} requires at least '\n                            '1 positional argument')\n\n        return dispatch(args[0].__class__)(*args, **kw)\n\n    funcname = getattr(func, '__name__', 'singledispatch function')\n    registry[object] = func\n    wrapper.register = register\n    wrapper.dispatch = dispatch\n    wrapper.registry = types.MappingProxyType(registry)\n    wrapper._clear_cache = dispatch_cache.clear\n    update_wrapper(wrapper, func)\n    return wrapper\n\n\n# Descriptor version\nclass singledispatchmethod:\n    \"\"\"Single-dispatch generic method descriptor.\n\n    Supports wrapping existing descriptors and handles non-descriptor\n    callables as instance methods.\n    \"\"\"\n\n    def __init__(self, func):\n        if not callable(func) and not hasattr(func, \"__get__\"):\n            raise TypeError(f\"{func!r} is not callable or a descriptor\")\n\n        self.dispatcher = singledispatch(func)\n        self.func = func\n\n    def register(self, cls, method=None):\n        \"\"\"generic_method.register(cls, func) -> func\n\n        Registers a new implementation for the given *cls* on a *generic_method*.\n        \"\"\"\n        return self.dispatcher.register(cls, func=method)\n\n    def __get__(self, obj, cls=None):\n        def _method(*args, **kwargs):\n            method = self.dispatcher.dispatch(args[0].__class__)\n            return method.__get__(obj, cls)(*args, **kwargs)\n\n        _method.__isabstractmethod__ = self.__isabstractmethod__\n        _method.register = self.register\n        update_wrapper(_method, self.func)\n        return _method\n\n    @property\n    def __isabstractmethod__(self):\n        return getattr(self.func, '__isabstractmethod__', False)\n\n\n################################################################################\n### cached_property() - computed once per instance, cached as attribute\n################################################################################\n\n_NOT_FOUND = object()\n\n\nclass cached_property:\n    def __init__(self, func):\n        self.func = func\n        self.attrname = None\n        self.__doc__ = func.__doc__\n        self.lock = RLock()\n\n    def __set_name__(self, owner, name):\n        if self.attrname is None:\n            self.attrname = name\n        elif name != self.attrname:\n            raise TypeError(\n                \"Cannot assign the same cached_property to two different names \"\n                f\"({self.attrname!r} and {name!r}).\"\n            )\n\n    def __get__(self, instance, owner=None):\n        if instance is None:\n            return self\n        if self.attrname is None:\n            raise TypeError(\n                \"Cannot use cached_property instance without calling __set_name__ on it.\")\n        try:\n            cache = instance.__dict__\n        except AttributeError:  # not all objects have __dict__ (e.g. class defines slots)\n            msg = (\n                f\"No '__dict__' attribute on {type(instance).__name__!r} \"\n                f\"instance to cache {self.attrname!r} property.\"\n            )\n            raise TypeError(msg) from None\n        val = cache.get(self.attrname, _NOT_FOUND)\n        if val is _NOT_FOUND:\n            with self.lock:\n                # check if another thread filled cache while we awaited lock\n                val = cache.get(self.attrname, _NOT_FOUND)\n                if val is _NOT_FOUND:\n                    val = self.func(instance)\n                    try:\n                        cache[self.attrname] = val\n                    except TypeError:\n                        msg = (\n                            f\"The '__dict__' attribute on {type(instance).__name__!r} instance \"\n                            f\"does not support item assignment for caching {self.attrname!r} property.\"\n                        )\n                        raise TypeError(msg) from None\n        return val\n\n    __class_getitem__ = classmethod(GenericAlias)\n",980],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py":["\"\"\"Compatibility fixes for older version of python, numpy and scipy\n\nIf you add content to this file, please give the version of the package\nat which the fixe is no longer needed.\n\"\"\"\n# Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>\n#          Gael Varoquaux <gael.varoquaux@normalesup.org>\n#          Fabian Pedregosa <fpedregosa@acm.org>\n#          Lars Buitinck\n#\n# License: BSD 3 clause\n\nfrom functools import update_wrapper\nimport functools\n\nimport numpy as np\nimport scipy.sparse as sp\nimport scipy\nimport scipy.stats\nfrom scipy.sparse.linalg import lsqr as sparse_lsqr  # noqa\nfrom .._config import config_context, get_config\nfrom ..externals._packaging.version import parse as parse_version\n\n\nnp_version = parse_version(np.__version__)\nsp_version = parse_version(scipy.__version__)\n\n\nif sp_version >= parse_version('1.4'):\n    from scipy.sparse.linalg import lobpcg\nelse:\n    # Backport of lobpcg functionality from scipy 1.4.0, can be removed\n    # once support for sp_version < parse_version('1.4') is dropped\n    # mypy error: Name 'lobpcg' already defined (possibly by an import)\n    from ..externals._lobpcg import lobpcg  # type: ignore  # noqa\n\n\ndef _object_dtype_isnan(X):\n    return X != X\n\n\n# TODO: replace by copy=False, when only scipy > 1.1 is supported.\ndef _astype_copy_false(X):\n    \"\"\"Returns the copy=False parameter for\n    {ndarray, csr_matrix, csc_matrix}.astype when possible,\n    otherwise don't specify\n    \"\"\"\n    if sp_version >= parse_version('1.1') or not sp.issparse(X):\n        return {'copy': False}\n    else:\n        return {}\n\n\ndef _joblib_parallel_args(**kwargs):\n    \"\"\"Set joblib.Parallel arguments in a compatible way for 0.11 and 0.12+\n\n    For joblib 0.11 this maps both ``prefer`` and ``require`` parameters to\n    a specific ``backend``.\n\n    Parameters\n    ----------\n\n    prefer : str in {'processes', 'threads'} or None\n        Soft hint to choose the default backend if no specific backend\n        was selected with the parallel_backend context manager.\n\n    require : 'sharedmem' or None\n        Hard condstraint to select the backend. If set to 'sharedmem',\n        the selected backend will be single-host and thread-based even\n        if the user asked for a non-thread based backend with\n        parallel_backend.\n\n    See joblib.Parallel documentation for more details\n    \"\"\"\n    import joblib\n\n    if parse_version(joblib.__version__) >= parse_version('0.12'):\n        return kwargs\n\n    extra_args = set(kwargs.keys()).difference({'prefer', 'require'})\n    if extra_args:\n        raise NotImplementedError('unhandled arguments %s with joblib %s'\n                                  % (list(extra_args), joblib.__version__))\n    args = {}\n    if 'prefer' in kwargs:\n        prefer = kwargs['prefer']\n        if prefer not in ['threads', 'processes', None]:\n            raise ValueError('prefer=%s is not supported' % prefer)\n        args['backend'] = {'threads': 'threading',\n                           'processes': 'multiprocessing',\n                           None: None}[prefer]\n\n    if 'require' in kwargs:\n        require = kwargs['require']\n        if require not in [None, 'sharedmem']:\n            raise ValueError('require=%s is not supported' % require)\n        if require == 'sharedmem':\n            args['backend'] = 'threading'\n    return args\n\n\nclass loguniform(scipy.stats.reciprocal):\n    \"\"\"A class supporting log-uniform random variables.\n\n    Parameters\n    ----------\n    low : float\n        The minimum value\n    high : float\n        The maximum value\n\n    Methods\n    -------\n    rvs(self, size=None, random_state=None)\n        Generate log-uniform random variables\n\n    The most useful method for Scikit-learn usage is highlighted here.\n    For a full list, see\n    `scipy.stats.reciprocal\n    <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html>`_.\n    This list includes all functions of ``scipy.stats`` continuous\n    distributions such as ``pdf``.\n\n    Notes\n    -----\n    This class generates values between ``low`` and ``high`` or\n\n        low <= loguniform(low, high).rvs() <= high\n\n    The logarithmic probability density function (PDF) is uniform. When\n    ``x`` is a uniformly distributed random variable between 0 and 1, ``10**x``\n    are random variables that are equally likely to be returned.\n\n    This class is an alias to ``scipy.stats.reciprocal``, which uses the\n    reciprocal distribution:\n    https://en.wikipedia.org/wiki/Reciprocal_distribution\n\n    Examples\n    --------\n\n    >>> from sklearn.utils.fixes import loguniform\n    >>> rv = loguniform(1e-3, 1e1)\n    >>> rvs = rv.rvs(random_state=42, size=1000)\n    >>> rvs.min()  # doctest: +SKIP\n    0.0010435856341129003\n    >>> rvs.max()  # doctest: +SKIP\n    9.97403052786026\n    \"\"\"\n\n\ndef _take_along_axis(arr, indices, axis):\n    \"\"\"Implements a simplified version of np.take_along_axis if numpy\n    version < 1.15\"\"\"\n    if np_version >= parse_version('1.15'):\n        return np.take_along_axis(arr=arr, indices=indices, axis=axis)\n    else:\n        if axis is None:\n            arr = arr.flatten()\n\n        if not np.issubdtype(indices.dtype, np.intp):\n            raise IndexError('`indices` must be an integer array')\n        if arr.ndim != indices.ndim:\n            raise ValueError(\n                \"`indices` and `arr` must have the same number of dimensions\")\n\n        shape_ones = (1,) * indices.ndim\n        dest_dims = (\n            list(range(axis)) +\n            [None] +\n            list(range(axis+1, indices.ndim))\n        )\n\n        # build a fancy index, consisting of orthogonal aranges, with the\n        # requested index inserted at the right location\n        fancy_index = []\n        for dim, n in zip(dest_dims, arr.shape):\n            if dim is None:\n                fancy_index.append(indices)\n            else:\n                ind_shape = shape_ones[:dim] + (-1,) + shape_ones[dim+1:]\n                fancy_index.append(np.arange(n).reshape(ind_shape))\n\n        fancy_index = tuple(fancy_index)\n        return arr[fancy_index]\n\n\n# remove when https://github.com/joblib/joblib/issues/1071 is fixed\ndef delayed(function):\n    \"\"\"Decorator used to capture the arguments of a function.\"\"\"\n    @functools.wraps(function)\n    def delayed_function(*args, **kwargs):\n        return _FuncWrapper(function), args, kwargs\n    return delayed_function\n\n\nclass _FuncWrapper:\n    \"\"\"\"Load the global configuration before calling the function.\"\"\"\n    def __init__(self, function):\n        self.function = function\n        self.config = get_config()\n        update_wrapper(self, self.function)\n\n    def __call__(self, *args, **kwargs):\n        with config_context(**self.config):\n            return self.function(*args, **kwargs)\n\n\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None,\n             axis=0):\n    \"\"\"Implements a simplified linspace function as of numpy verion >= 1.16.\n\n    As of numpy 1.16, the arguments start and stop can be array-like and\n    there is an optional argument `axis`.\n    For simplicity, we only allow 1d array-like to be passed to start and stop.\n    See: https://github.com/numpy/numpy/pull/12388 and numpy 1.16 release\n    notes about start and stop arrays for linspace logspace and geomspace.\n\n    Returns\n    -------\n    out : ndarray of shape (num, n_start) or (num,)\n        The output array with `n_start=start.shape[0]` columns.\n    \"\"\"\n    if np_version < parse_version('1.16'):\n        start = np.asanyarray(start) * 1.0\n        stop = np.asanyarray(stop) * 1.0\n        dt = np.result_type(start, stop, float(num))\n        if dtype is None:\n            dtype = dt\n\n        if start.ndim == 0 == stop.ndim:\n            return np.linspace(start=start, stop=stop, num=num,\n                               endpoint=endpoint, retstep=retstep, dtype=dtype)\n\n        if start.ndim != 1 or stop.ndim != 1 or start.shape != stop.shape:\n            raise ValueError(\"start and stop must be 1d array-like of same\"\n                             \" shape.\")\n        n_start = start.shape[0]\n        out = np.empty((num, n_start), dtype=dtype)\n        step = np.empty(n_start, dtype=np.float)\n        for i in range(n_start):\n            out[:, i], step[i] = np.linspace(start=start[i], stop=stop[i],\n                                             num=num, endpoint=endpoint,\n                                             retstep=True, dtype=dtype)\n        if axis != 0:\n            out = np.moveaxis(out, 0, axis)\n\n        if retstep:\n            return out, step\n        else:\n            return out\n    else:\n        return np.linspace(start=start, stop=stop, num=num, endpoint=endpoint,\n                           retstep=retstep, dtype=dtype, axis=axis)\n",253],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py":["\"\"\"Base and mixin classes for nearest neighbors\"\"\"\n# Authors: Jake Vanderplas <vanderplas@astro.washington.edu>\n#          Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Sparseness support by Lars Buitinck\n#          Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>\n#\n# License: BSD 3 clause (C) INRIA, University of Amsterdam\nfrom functools import partial\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nimport numbers\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, issparse\nimport joblib\nfrom joblib import Parallel, effective_n_jobs\n\nfrom ._ball_tree import BallTree\nfrom ._kd_tree import KDTree\nfrom ..base import BaseEstimator, MultiOutputMixin\nfrom ..base import is_classifier\nfrom ..metrics import pairwise_distances_chunked\nfrom ..metrics.pairwise import PAIRWISE_DISTANCE_FUNCTIONS\nfrom ..utils import (\n    check_array,\n    gen_even_slices,\n    _to_object_array,\n)\nfrom ..utils.deprecation import deprecated\nfrom ..utils.multiclass import check_classification_targets\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import check_non_negative\nfrom ..utils.fixes import delayed\nfrom ..utils.fixes import parse_version\nfrom ..exceptions import DataConversionWarning, EfficiencyWarning\n\nVALID_METRICS = dict(ball_tree=BallTree.valid_metrics,\n                     kd_tree=KDTree.valid_metrics,\n                     # The following list comes from the\n                     # sklearn.metrics.pairwise doc string\n                     brute=(list(PAIRWISE_DISTANCE_FUNCTIONS.keys()) +\n                            ['braycurtis', 'canberra', 'chebyshev',\n                             'correlation', 'cosine', 'dice', 'hamming',\n                             'jaccard', 'kulsinski', 'mahalanobis',\n                             'matching', 'minkowski', 'rogerstanimoto',\n                             'russellrao', 'seuclidean', 'sokalmichener',\n                             'sokalsneath', 'sqeuclidean',\n                             'yule', 'wminkowski']))\n\n\nVALID_METRICS_SPARSE = dict(ball_tree=[],\n                            kd_tree=[],\n                            brute=(PAIRWISE_DISTANCE_FUNCTIONS.keys() -\n                                   {'haversine', 'nan_euclidean'}))\n\n\ndef _check_weights(weights):\n    \"\"\"Check to make sure weights are valid\"\"\"\n    if (weights not in (None, 'uniform', 'distance') and\n            not callable(weights)):\n        raise ValueError(\"weights not recognized: should be 'uniform', \"\n                         \"'distance', or a callable function\")\n\n    return weights\n\n\ndef _get_weights(dist, weights):\n    \"\"\"Get the weights from an array of distances and a parameter ``weights``\n\n    Parameters\n    ----------\n    dist : ndarray\n        The input distances.\n\n    weights : {'uniform', 'distance' or a callable}\n        The kind of weighting used.\n\n    Returns\n    -------\n    weights_arr : array of the same shape as ``dist``\n        If ``weights == 'uniform'``, then returns None.\n    \"\"\"\n    if weights in (None, 'uniform'):\n        return None\n    elif weights == 'distance':\n        # if user attempts to classify a point that was zero distance from one\n        # or more training points, those training points are weighted as 1.0\n        # and the other points as 0.0\n        if dist.dtype is np.dtype(object):\n            for point_dist_i, point_dist in enumerate(dist):\n                # check if point_dist is iterable\n                # (ex: RadiusNeighborClassifier.predict may set an element of\n                # dist to 1e-6 to represent an 'outlier')\n                if hasattr(point_dist, '__contains__') and 0. in point_dist:\n                    dist[point_dist_i] = point_dist == 0.\n                else:\n                    dist[point_dist_i] = 1. / point_dist\n        else:\n            with np.errstate(divide='ignore'):\n                dist = 1. / dist\n            inf_mask = np.isinf(dist)\n            inf_row = np.any(inf_mask, axis=1)\n            dist[inf_row] = inf_mask[inf_row]\n        return dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized: should be 'uniform', \"\n                         \"'distance', or a callable function\")\n\n\ndef _is_sorted_by_data(graph):\n    \"\"\"Returns whether the graph's non-zero entries are sorted by data\n\n    The non-zero entries are stored in graph.data and graph.indices.\n    For each row (or sample), the non-zero entries can be either:\n        - sorted by indices, as after graph.sort_indices();\n        - sorted by data, as after _check_precomputed(graph);\n        - not sorted.\n\n    Parameters\n    ----------\n    graph : sparse matrix of shape (n_samples, n_samples)\n        Neighbors graph as given by `kneighbors_graph` or\n        `radius_neighbors_graph`. Matrix should be of format CSR format.\n\n    Returns\n    -------\n    res : bool\n        Whether input graph is sorted by data.\n    \"\"\"\n    assert graph.format == 'csr'\n    out_of_order = graph.data[:-1] > graph.data[1:]\n    line_change = np.unique(graph.indptr[1:-1] - 1)\n    line_change = line_change[line_change < out_of_order.shape[0]]\n    return (out_of_order.sum() == out_of_order[line_change].sum())\n\n\ndef _check_precomputed(X):\n    \"\"\"Check precomputed distance matrix\n\n    If the precomputed distance matrix is sparse, it checks that the non-zero\n    entries are sorted by distances. If not, the matrix is copied and sorted.\n\n    Parameters\n    ----------\n    X : {sparse matrix, array-like}, (n_samples, n_samples)\n        Distance matrix to other samples. X may be a sparse matrix, in which\n        case only non-zero elements may be considered neighbors.\n\n    Returns\n    -------\n    X : {sparse matrix, array-like}, (n_samples, n_samples)\n        Distance matrix to other samples. X may be a sparse matrix, in which\n        case only non-zero elements may be considered neighbors.\n    \"\"\"\n    if not issparse(X):\n        X = check_array(X)\n        check_non_negative(X, whom=\"precomputed distance matrix.\")\n        return X\n    else:\n        graph = X\n\n    if graph.format not in ('csr', 'csc', 'coo', 'lil'):\n        raise TypeError('Sparse matrix in {!r} format is not supported due to '\n                        'its handling of explicit zeros'.format(graph.format))\n    copied = graph.format != 'csr'\n    graph = check_array(graph, accept_sparse='csr')\n    check_non_negative(graph, whom=\"precomputed distance matrix.\")\n\n    if not _is_sorted_by_data(graph):\n        warnings.warn('Precomputed sparse input was not sorted by data.',\n                      EfficiencyWarning)\n        if not copied:\n            graph = graph.copy()\n\n        # if each sample has the same number of provided neighbors\n        row_nnz = np.diff(graph.indptr)\n        if row_nnz.max() == row_nnz.min():\n            n_samples = graph.shape[0]\n            distances = graph.data.reshape(n_samples, -1)\n\n            order = np.argsort(distances, kind='mergesort')\n            order += np.arange(n_samples)[:, None] * row_nnz[0]\n            order = order.ravel()\n            graph.data = graph.data[order]\n            graph.indices = graph.indices[order]\n\n        else:\n            for start, stop in zip(graph.indptr, graph.indptr[1:]):\n                order = np.argsort(graph.data[start:stop], kind='mergesort')\n                graph.data[start:stop] = graph.data[start:stop][order]\n                graph.indices[start:stop] = graph.indices[start:stop][order]\n    return graph\n\n\ndef _kneighbors_from_graph(graph, n_neighbors, return_distance):\n    \"\"\"Decompose a nearest neighbors sparse graph into distances and indices\n\n    Parameters\n    ----------\n    graph : sparse matrix of shape (n_samples, n_samples)\n        Neighbors graph as given by `kneighbors_graph` or\n        `radius_neighbors_graph`. Matrix should be of format CSR format.\n\n    n_neighbors : int\n        Number of neighbors required for each sample.\n\n    return_distance : bool\n        Whether or not to return the distances.\n\n    Returns\n    -------\n    neigh_dist : ndarray of shape (n_samples, n_neighbors)\n        Distances to nearest neighbors. Only present if `return_distance=True`.\n\n    neigh_ind : ndarray of shape (n_samples, n_neighbors)\n        Indices of nearest neighbors.\n    \"\"\"\n    n_samples = graph.shape[0]\n    assert graph.format == 'csr'\n\n    # number of neighbors by samples\n    row_nnz = np.diff(graph.indptr)\n    row_nnz_min = row_nnz.min()\n    if n_neighbors is not None and row_nnz_min < n_neighbors:\n        raise ValueError(\n            '%d neighbors per samples are required, but some samples have only'\n            ' %d neighbors in precomputed graph matrix. Decrease number of '\n            'neighbors used or recompute the graph with more neighbors.'\n            % (n_neighbors, row_nnz_min))\n\n    def extract(a):\n        # if each sample has the same number of provided neighbors\n        if row_nnz.max() == row_nnz_min:\n            return a.reshape(n_samples, -1)[:, :n_neighbors]\n        else:\n            idx = np.tile(np.arange(n_neighbors), (n_samples, 1))\n            idx += graph.indptr[:-1, None]\n            return a.take(idx, mode='clip').reshape(n_samples, n_neighbors)\n\n    if return_distance:\n        return extract(graph.data), extract(graph.indices)\n    else:\n        return extract(graph.indices)\n\n\ndef _radius_neighbors_from_graph(graph, radius, return_distance):\n    \"\"\"Decompose a nearest neighbors sparse graph into distances and indices\n\n    Parameters\n    ----------\n    graph : sparse matrix of shape (n_samples, n_samples)\n        Neighbors graph as given by `kneighbors_graph` or\n        `radius_neighbors_graph`. Matrix should be of format CSR format.\n\n    radius : float\n        Radius of neighborhoods which should be strictly positive.\n\n    return_distance : bool\n        Whether or not to return the distances.\n\n    Returns\n    -------\n    neigh_dist : ndarray of shape (n_samples,) of arrays\n        Distances to nearest neighbors. Only present if `return_distance=True`.\n\n    neigh_ind : ndarray of shape (n_samples,) of arrays\n        Indices of nearest neighbors.\n    \"\"\"\n    assert graph.format == 'csr'\n\n    no_filter_needed = bool(graph.data.max() <= radius)\n\n    if no_filter_needed:\n        data, indices, indptr = graph.data, graph.indices, graph.indptr\n    else:\n        mask = graph.data <= radius\n        if return_distance:\n            data = np.compress(mask, graph.data)\n        indices = np.compress(mask, graph.indices)\n        indptr = np.concatenate(([0], np.cumsum(mask)))[graph.indptr]\n\n    indices = indices.astype(np.intp, copy=no_filter_needed)\n\n    if return_distance:\n        neigh_dist = _to_object_array(np.split(data, indptr[1:-1]))\n    neigh_ind = _to_object_array(np.split(indices, indptr[1:-1]))\n\n    if return_distance:\n        return neigh_dist, neigh_ind\n    else:\n        return neigh_ind\n\n\nclass NeighborsBase(MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Base class for nearest neighbors estimators.\"\"\"\n\n    @abstractmethod\n    def __init__(self, n_neighbors=None, radius=None,\n                 algorithm='auto', leaf_size=30, metric='minkowski',\n                 p=2, metric_params=None, n_jobs=None):\n\n        self.n_neighbors = n_neighbors\n        self.radius = radius\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.metric = metric\n        self.metric_params = metric_params\n        self.p = p\n        self.n_jobs = n_jobs\n\n    def _check_algorithm_metric(self):\n        if self.algorithm not in ['auto', 'brute',\n                                  'kd_tree', 'ball_tree']:\n            raise ValueError(\"unrecognized algorithm: '%s'\" % self.algorithm)\n\n        if self.algorithm == 'auto':\n            if self.metric == 'precomputed':\n                alg_check = 'brute'\n            elif (callable(self.metric) or\n                  self.metric in VALID_METRICS['ball_tree']):\n                alg_check = 'ball_tree'\n            else:\n                alg_check = 'brute'\n        else:\n            alg_check = self.algorithm\n\n        if callable(self.metric):\n            if self.algorithm == 'kd_tree':\n                # callable metric is only valid for brute force and ball_tree\n                raise ValueError(\n                    \"kd_tree does not support callable metric '%s'\"\n                    \"Function call overhead will result\"\n                    \"in very poor performance.\"\n                    % self.metric)\n        elif self.metric not in VALID_METRICS[alg_check]:\n            raise ValueError(\"Metric '%s' not valid. Use \"\n                             \"sorted(sklearn.neighbors.VALID_METRICS['%s']) \"\n                             \"to get valid options. \"\n                             \"Metric can also be a callable function.\"\n                             % (self.metric, alg_check))\n\n        if self.metric_params is not None and 'p' in self.metric_params:\n            if self.p is not None:\n                warnings.warn(\"Parameter p is found in metric_params. \"\n                              \"The corresponding parameter from __init__ \"\n                              \"is ignored.\", SyntaxWarning, stacklevel=3)\n            effective_p = self.metric_params['p']\n        else:\n            effective_p = self.p\n\n        if self.metric in ['wminkowski', 'minkowski'] and effective_p < 1:\n            raise ValueError(\"p must be greater or equal to one for \"\n                             \"minkowski metric\")\n\n    def _fit(self, X, y=None):\n        if self._get_tags()[\"requires_y\"]:\n            if not isinstance(X, (KDTree, BallTree, NeighborsBase)):\n                X, y = self._validate_data(X, y, accept_sparse=\"csr\",\n                                           multi_output=True)\n\n            if is_classifier(self):\n                # Classification targets require a specific format\n                if y.ndim == 1 or y.ndim == 2 and y.shape[1] == 1:\n                    if y.ndim != 1:\n                        warnings.warn(\"A column-vector y was passed when a \"\n                                      \"1d array was expected. Please change \"\n                                      \"the shape of y to (n_samples,), for \"\n                                      \"example using ravel().\",\n                                      DataConversionWarning, stacklevel=2)\n\n                    self.outputs_2d_ = False\n                    y = y.reshape((-1, 1))\n                else:\n                    self.outputs_2d_ = True\n\n                check_classification_targets(y)\n                self.classes_ = []\n                self._y = np.empty(y.shape, dtype=int)\n                for k in range(self._y.shape[1]):\n                    classes, self._y[:, k] = np.unique(\n                        y[:, k], return_inverse=True)\n                    self.classes_.append(classes)\n\n                if not self.outputs_2d_:\n                    self.classes_ = self.classes_[0]\n                    self._y = self._y.ravel()\n            else:\n                self._y = y\n\n        else:\n            if not isinstance(X, (KDTree, BallTree, NeighborsBase)):\n                X = self._validate_data(X, accept_sparse='csr')\n\n        self._check_algorithm_metric()\n        if self.metric_params is None:\n            self.effective_metric_params_ = {}\n        else:\n            self.effective_metric_params_ = self.metric_params.copy()\n\n        effective_p = self.effective_metric_params_.get('p', self.p)\n        if self.metric in ['wminkowski', 'minkowski']:\n            self.effective_metric_params_['p'] = effective_p\n\n        self.effective_metric_ = self.metric\n        # For minkowski distance, use more efficient methods where available\n        if self.metric == 'minkowski':\n            p = self.effective_metric_params_.pop('p', 2)\n            if p < 1:\n                raise ValueError(\"p must be greater or equal to one for \"\n                                 \"minkowski metric\")\n            elif p == 1:\n                self.effective_metric_ = 'manhattan'\n            elif p == 2:\n                self.effective_metric_ = 'euclidean'\n            elif p == np.inf:\n                self.effective_metric_ = 'chebyshev'\n            else:\n                self.effective_metric_params_['p'] = p\n\n        if isinstance(X, NeighborsBase):\n            self._fit_X = X._fit_X\n            self._tree = X._tree\n            self._fit_method = X._fit_method\n            self.n_samples_fit_ = X.n_samples_fit_\n            return self\n\n        elif isinstance(X, BallTree):\n            self._fit_X = X.data\n            self._tree = X\n            self._fit_method = 'ball_tree'\n            self.n_samples_fit_ = X.data.shape[0]\n            return self\n\n        elif isinstance(X, KDTree):\n            self._fit_X = X.data\n            self._tree = X\n            self._fit_method = 'kd_tree'\n            self.n_samples_fit_ = X.data.shape[0]\n            return self\n\n        if self.metric == 'precomputed':\n            X = _check_precomputed(X)\n            # Precomputed matrix X must be squared\n            if X.shape[0] != X.shape[1]:\n                raise ValueError(\"Precomputed matrix must be square.\"\n                                 \" Input is a {}x{} matrix.\"\n                                 .format(X.shape[0], X.shape[1]))\n            self.n_features_in_ = X.shape[1]\n\n        n_samples = X.shape[0]\n        if n_samples == 0:\n            raise ValueError(\"n_samples must be greater than 0\")\n\n        if issparse(X):\n            if self.algorithm not in ('auto', 'brute'):\n                warnings.warn(\"cannot use tree with sparse input: \"\n                              \"using brute force\")\n            if self.effective_metric_ not in VALID_METRICS_SPARSE['brute'] \\\n                    and not callable(self.effective_metric_):\n                raise ValueError(\"Metric '%s' not valid for sparse input. \"\n                                 \"Use sorted(sklearn.neighbors.\"\n                                 \"VALID_METRICS_SPARSE['brute']) \"\n                                 \"to get valid options. \"\n                                 \"Metric can also be a callable function.\"\n                                 % (self.effective_metric_))\n            self._fit_X = X.copy()\n            self._tree = None\n            self._fit_method = 'brute'\n            self.n_samples_fit_ = X.shape[0]\n            return self\n\n        self._fit_method = self.algorithm\n        self._fit_X = X\n        self.n_samples_fit_ = X.shape[0]\n\n        if self._fit_method == 'auto':\n            # A tree approach is better for small number of neighbors or small\n            # number of features, with KDTree generally faster when available\n            if (self.metric == 'precomputed' or self._fit_X.shape[1] > 15 or\n                    (self.n_neighbors is not None and\n                     self.n_neighbors >= self._fit_X.shape[0] // 2)):\n                self._fit_method = 'brute'\n            else:\n                if self.effective_metric_ in VALID_METRICS['kd_tree']:\n                    self._fit_method = 'kd_tree'\n                elif (callable(self.effective_metric_) or\n                        self.effective_metric_ in VALID_METRICS['ball_tree']):\n                    self._fit_method = 'ball_tree'\n                else:\n                    self._fit_method = 'brute'\n\n        if self._fit_method == 'ball_tree':\n            self._tree = BallTree(X, self.leaf_size,\n                                  metric=self.effective_metric_,\n                                  **self.effective_metric_params_)\n        elif self._fit_method == 'kd_tree':\n            self._tree = KDTree(X, self.leaf_size,\n                                metric=self.effective_metric_,\n                                **self.effective_metric_params_)\n        elif self._fit_method == 'brute':\n            self._tree = None\n        else:\n            raise ValueError(\"algorithm = '%s' not recognized\"\n                             % self.algorithm)\n\n        if self.n_neighbors is not None:\n            if self.n_neighbors <= 0:\n                raise ValueError(\n                    \"Expected n_neighbors > 0. Got %d\" %\n                    self.n_neighbors)\n            elif not isinstance(self.n_neighbors, numbers.Integral):\n                raise TypeError(\n                    \"n_neighbors does not take %s value, \"\n                    \"enter integer value\" %\n                    type(self.n_neighbors))\n\n        return self\n\n    def _more_tags(self):\n        # For cross-validation routines to split data correctly\n        return {'pairwise': self.metric == 'precomputed'}\n\n    # TODO: Remove in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(\"Attribute _pairwise was deprecated in \"  # type: ignore\n                \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\")\n    @property\n    def _pairwise(self):\n        # For cross-validation routines to split data correctly\n        return self.metric == 'precomputed'\n\n\ndef _tree_query_parallel_helper(tree, *args, **kwargs):\n    \"\"\"Helper for the Parallel calls in KNeighborsMixin.kneighbors\n\n    The Cython method tree.query is not directly picklable by cloudpickle\n    under PyPy.\n    \"\"\"\n    return tree.query(*args, **kwargs)\n\n\nclass KNeighborsMixin:\n    \"\"\"Mixin for k-neighbors searches\"\"\"\n\n    def _kneighbors_reduce_func(self, dist, start,\n                                n_neighbors, return_distance):\n        \"\"\"Reduce a chunk of distances to the nearest neighbors\n\n        Callback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\n\n        Parameters\n        ----------\n        dist : ndarray of shape (n_samples_chunk, n_samples)\n            The distance matrix.\n\n        start : int\n            The index in X which the first row of dist corresponds to.\n\n        n_neighbors : int\n            Number of neighbors required for each sample.\n\n        return_distance : bool\n            Whether or not to return the distances.\n\n        Returns\n        -------\n        dist : array of shape (n_samples_chunk, n_neighbors)\n            Returned only if `return_distance=True`.\n\n        neigh : array of shape (n_samples_chunk, n_neighbors)\n            The neighbors indices.\n        \"\"\"\n        sample_range = np.arange(dist.shape[0])[:, None]\n        neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)\n        neigh_ind = neigh_ind[:, :n_neighbors]\n        # argpartition doesn't guarantee sorted order, so we sort again\n        neigh_ind = neigh_ind[\n            sample_range, np.argsort(dist[sample_range, neigh_ind])]\n        if return_distance:\n            if self.effective_metric_ == 'euclidean':\n                result = np.sqrt(dist[sample_range, neigh_ind]), neigh_ind\n            else:\n                result = dist[sample_range, neigh_ind], neigh_ind\n        else:\n            result = neigh_ind\n        return result\n\n    def kneighbors(self, X=None, n_neighbors=None, return_distance=True):\n        \"\"\"Finds the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_queries, n_features), \\\n            or (n_queries, n_indexed) if metric == 'precomputed', \\\n                default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int, default=None\n            Number of neighbors required for each sample. The default is the\n            value passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_queries, n_neighbors)\n            Array representing the lengths to points, only present if\n            return_distance=True\n\n        neigh_ind : ndarray of shape (n_queries, n_neighbors)\n            Indices of the nearest points in the population matrix.\n\n        Examples\n        --------\n        In the following example, we construct a NearestNeighbors\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples)\n        NearestNeighbors(n_neighbors=1)\n        >>> print(neigh.kneighbors([[1., 1., 1.]]))\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False)\n        array([[1],\n               [2]]...)\n        \"\"\"\n        check_is_fitted(self)\n\n        if n_neighbors is None:\n            n_neighbors = self.n_neighbors\n        elif n_neighbors <= 0:\n            raise ValueError(\n                \"Expected n_neighbors > 0. Got %d\" %\n                n_neighbors)\n        elif not isinstance(n_neighbors, numbers.Integral):\n            raise TypeError(\n                \"n_neighbors does not take %s value, \"\n                \"enter integer value\" %\n                type(n_neighbors))\n\n        if X is not None:\n            query_is_train = False\n            if self.metric == 'precomputed':\n                X = _check_precomputed(X)\n            else:\n                X = self._validate_data(X, accept_sparse='csr', reset=False)\n        else:\n            query_is_train = True\n            X = self._fit_X\n            # Include an extra neighbor to account for the sample itself being\n            # returned, which is removed later\n            n_neighbors += 1\n\n        n_samples_fit = self.n_samples_fit_\n        if n_neighbors > n_samples_fit:\n            raise ValueError(\n                \"Expected n_neighbors <= n_samples, \"\n                \" but n_samples = %d, n_neighbors = %d\" %\n                (n_samples_fit, n_neighbors)\n            )\n\n        n_jobs = effective_n_jobs(self.n_jobs)\n        chunked_results = None\n        if (self._fit_method == 'brute' and\n                self.metric == 'precomputed' and issparse(X)):\n            results = _kneighbors_from_graph(\n                X, n_neighbors=n_neighbors,\n                return_distance=return_distance)\n\n        elif self._fit_method == 'brute':\n            reduce_func = partial(self._kneighbors_reduce_func,\n                                  n_neighbors=n_neighbors,\n                                  return_distance=return_distance)\n\n            # for efficiency, use squared euclidean distances\n            if self.effective_metric_ == 'euclidean':\n                kwds = {'squared': True}\n            else:\n                kwds = self.effective_metric_params_\n\n            chunked_results = list(pairwise_distances_chunked(\n                X, self._fit_X, reduce_func=reduce_func,\n                metric=self.effective_metric_, n_jobs=n_jobs,\n                **kwds))\n\n        elif self._fit_method in ['ball_tree', 'kd_tree']:\n            if issparse(X):\n                raise ValueError(\n                    \"%s does not work with sparse matrices. Densify the data, \"\n                    \"or set algorithm='brute'\" % self._fit_method)\n            old_joblib = (\n                    parse_version(joblib.__version__) < parse_version('0.12'))\n            if old_joblib:\n                # Deal with change of API in joblib\n                parallel_kwargs = {\"backend\": \"threading\"}\n            else:\n                parallel_kwargs = {\"prefer\": \"threads\"}\n            chunked_results = Parallel(n_jobs, **parallel_kwargs)(\n                delayed(_tree_query_parallel_helper)(\n                    self._tree, X[s], n_neighbors, return_distance)\n                for s in gen_even_slices(X.shape[0], n_jobs)\n            )\n        else:\n            raise ValueError(\"internal: _fit_method not recognized\")\n\n        if chunked_results is not None:\n            if return_distance:\n                neigh_dist, neigh_ind = zip(*chunked_results)\n                results = np.vstack(neigh_dist), np.vstack(neigh_ind)\n            else:\n                results = np.vstack(chunked_results)\n\n        if not query_is_train:\n            return results\n        else:\n            # If the query data is the same as the indexed data, we would like\n            # to ignore the first nearest neighbor of every sample, i.e\n            # the sample itself.\n            if return_distance:\n                neigh_dist, neigh_ind = results\n            else:\n                neigh_ind = results\n\n            n_queries, _ = X.shape\n            sample_range = np.arange(n_queries)[:, None]\n            sample_mask = neigh_ind != sample_range\n\n            # Corner case: When the number of duplicates are more\n            # than the number of neighbors, the first NN will not\n            # be the sample, but a duplicate.\n            # In that case mask the first duplicate.\n            dup_gr_nbrs = np.all(sample_mask, axis=1)\n            sample_mask[:, 0][dup_gr_nbrs] = False\n            neigh_ind = np.reshape(\n                neigh_ind[sample_mask], (n_queries, n_neighbors - 1))\n\n            if return_distance:\n                neigh_dist = np.reshape(\n                    neigh_dist[sample_mask], (n_queries, n_neighbors - 1))\n                return neigh_dist, neigh_ind\n            return neigh_ind\n\n    def kneighbors_graph(self, X=None, n_neighbors=None,\n                         mode='connectivity'):\n        \"\"\"Computes the (weighted) graph of k-Neighbors for points in X\n\n        Parameters\n        ----------\n        X : array-like of shape (n_queries, n_features), \\\n                or (n_queries, n_indexed) if metric == 'precomputed', \\\n                default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n            For ``metric='precomputed'`` the shape should be\n            (n_queries, n_indexed). Otherwise the shape should be\n            (n_queries, n_features).\n\n        n_neighbors : int, default=None\n            Number of neighbors for each sample. The default is the value\n            passed to the constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are Euclidean distance between points.\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X)\n        NearestNeighbors(n_neighbors=2)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 1.],\n               [1., 0., 1.]])\n\n        See Also\n        --------\n        NearestNeighbors.radius_neighbors_graph\n        \"\"\"\n        check_is_fitted(self)\n        if n_neighbors is None:\n            n_neighbors = self.n_neighbors\n\n        # check the input only in self.kneighbors\n\n        # construct CSR matrix representation of the k-NN graph\n        if mode == 'connectivity':\n            A_ind = self.kneighbors(X, n_neighbors, return_distance=False)\n            n_queries = A_ind.shape[0]\n            A_data = np.ones(n_queries * n_neighbors)\n\n        elif mode == 'distance':\n            A_data, A_ind = self.kneighbors(\n                X, n_neighbors, return_distance=True)\n            A_data = np.ravel(A_data)\n\n        else:\n            raise ValueError(\n                'Unsupported mode, must be one of \"connectivity\" '\n                'or \"distance\" but got \"%s\" instead' % mode)\n\n        n_queries = A_ind.shape[0]\n        n_samples_fit = self.n_samples_fit_\n        n_nonzero = n_queries * n_neighbors\n        A_indptr = np.arange(0, n_nonzero + 1, n_neighbors)\n\n        kneighbors_graph = csr_matrix((A_data, A_ind.ravel(), A_indptr),\n                                      shape=(n_queries, n_samples_fit))\n\n        return kneighbors_graph\n\n\ndef _tree_query_radius_parallel_helper(tree, *args, **kwargs):\n    \"\"\"Helper for the Parallel calls in RadiusNeighborsMixin.radius_neighbors\n\n    The Cython method tree.query_radius is not directly picklable by\n    cloudpickle under PyPy.\n    \"\"\"\n    return tree.query_radius(*args, **kwargs)\n\n\nclass RadiusNeighborsMixin:\n    \"\"\"Mixin for radius-based neighbors searches\"\"\"\n\n    def _radius_neighbors_reduce_func(self, dist, start,\n                                      radius, return_distance):\n        \"\"\"Reduce a chunk of distances to the nearest neighbors\n\n        Callback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\n\n        Parameters\n        ----------\n        dist : ndarray of shape (n_samples_chunk, n_samples)\n            The distance matrix.\n\n        start : int\n            The index in X which the first row of dist corresponds to.\n\n        radius : float\n            The radius considered when making the nearest neighbors search.\n\n        return_distance : bool\n            Whether or not to return the distances.\n\n        Returns\n        -------\n        dist : list of ndarray of shape (n_samples_chunk,)\n            Returned only if `return_distance=True`.\n\n        neigh : list of ndarray of shape (n_samples_chunk,)\n            The neighbors indices.\n        \"\"\"\n        neigh_ind = [np.where(d <= radius)[0] for d in dist]\n\n        if return_distance:\n            if self.effective_metric_ == 'euclidean':\n                dist = [np.sqrt(d[neigh_ind[i]])\n                        for i, d in enumerate(dist)]\n            else:\n                dist = [d[neigh_ind[i]]\n                        for i, d in enumerate(dist)]\n            results = dist, neigh_ind\n        else:\n            results = neigh_ind\n        return results\n\n    def radius_neighbors(self, X=None, radius=None, return_distance=True,\n                         sort_results=False):\n        \"\"\"Finds the neighbors within a given radius of a point or points.\n\n        Return the indices and distances of each point from the dataset\n        lying in a ball with size ``radius`` around the points of the query\n        array. Points lying on the boundary are included in the results.\n\n        The result points are *not* necessarily sorted by distance to their\n        query point.\n\n        Parameters\n        ----------\n        X : array-like of (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Limiting distance of neighbors to return. The default is the value\n            passed to the constructor.\n\n        return_distance : bool, default=True\n            Whether or not to return the distances.\n\n        sort_results : bool, default=False\n            If True, the distances and indices will be sorted by increasing\n            distances before being returned. If False, the results may not\n            be sorted. If `return_distance=False`, setting `sort_results=True`\n            will result in an error.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        neigh_dist : ndarray of shape (n_samples,) of arrays\n            Array representing the distances to each point, only present if\n            `return_distance=True`. The distance values are computed according\n            to the ``metric`` constructor parameter.\n\n        neigh_ind : ndarray of shape (n_samples,) of arrays\n            An array of arrays of indices of the approximate nearest points\n            from the population matrix that lie within a ball of size\n            ``radius`` around the query points.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1, 1, 1]:\n\n        >>> import numpy as np\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.6)\n        >>> neigh.fit(samples)\n        NearestNeighbors(radius=1.6)\n        >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n        >>> print(np.asarray(rng[0][0]))\n        [1.5 0.5]\n        >>> print(np.asarray(rng[1][0]))\n        [1 2]\n\n        The first array returned contains the distances to all points which\n        are closer than 1.6, while the second array returned contains their\n        indices.  In general, multiple points can be queried at the same time.\n\n        Notes\n        -----\n        Because the number of neighbors of each point is not necessarily\n        equal, the results for multiple query points cannot be fit in a\n        standard data array.\n        For efficiency, `radius_neighbors` returns arrays of objects, where\n        each object is a 1D array of indices or distances.\n        \"\"\"\n        check_is_fitted(self)\n\n        if X is not None:\n            query_is_train = False\n            if self.metric == 'precomputed':\n                X = _check_precomputed(X)\n            else:\n                X = self._validate_data(X, accept_sparse='csr', reset=False)\n        else:\n            query_is_train = True\n            X = self._fit_X\n\n        if radius is None:\n            radius = self.radius\n\n        if (self._fit_method == 'brute' and\n                self.metric == 'precomputed' and issparse(X)):\n            results = _radius_neighbors_from_graph(\n                X, radius=radius, return_distance=return_distance)\n\n        elif self._fit_method == 'brute':\n            # for efficiency, use squared euclidean distances\n            if self.effective_metric_ == 'euclidean':\n                radius *= radius\n                kwds = {'squared': True}\n            else:\n                kwds = self.effective_metric_params_\n\n            reduce_func = partial(self._radius_neighbors_reduce_func,\n                                  radius=radius,\n                                  return_distance=return_distance)\n\n            chunked_results = pairwise_distances_chunked(\n                X, self._fit_X, reduce_func=reduce_func,\n                metric=self.effective_metric_, n_jobs=self.n_jobs,\n                **kwds)\n            if return_distance:\n                neigh_dist_chunks, neigh_ind_chunks = zip(*chunked_results)\n                neigh_dist_list = sum(neigh_dist_chunks, [])\n                neigh_ind_list = sum(neigh_ind_chunks, [])\n                neigh_dist = _to_object_array(neigh_dist_list)\n                neigh_ind = _to_object_array(neigh_ind_list)\n                results = neigh_dist, neigh_ind\n            else:\n                neigh_ind_list = sum(chunked_results, [])\n                results = _to_object_array(neigh_ind_list)\n\n            if sort_results:\n                if not return_distance:\n                    raise ValueError(\"return_distance must be True \"\n                                     \"if sort_results is True.\")\n                for ii in range(len(neigh_dist)):\n                    order = np.argsort(neigh_dist[ii], kind='mergesort')\n                    neigh_ind[ii] = neigh_ind[ii][order]\n                    neigh_dist[ii] = neigh_dist[ii][order]\n                results = neigh_dist, neigh_ind\n\n        elif self._fit_method in ['ball_tree', 'kd_tree']:\n            if issparse(X):\n                raise ValueError(\n                    \"%s does not work with sparse matrices. Densify the data, \"\n                    \"or set algorithm='brute'\" % self._fit_method)\n\n            n_jobs = effective_n_jobs(self.n_jobs)\n            delayed_query = delayed(_tree_query_radius_parallel_helper)\n            if parse_version(joblib.__version__) < parse_version('0.12'):\n                # Deal with change of API in joblib\n                parallel_kwargs = {\"backend\": \"threading\"}\n            else:\n                parallel_kwargs = {\"prefer\": \"threads\"}\n\n            chunked_results = Parallel(n_jobs, **parallel_kwargs)(\n                delayed_query(self._tree, X[s], radius, return_distance,\n                              sort_results=sort_results)\n\n                for s in gen_even_slices(X.shape[0], n_jobs)\n            )\n            if return_distance:\n                neigh_ind, neigh_dist = tuple(zip(*chunked_results))\n                results = np.hstack(neigh_dist), np.hstack(neigh_ind)\n            else:\n                results = np.hstack(chunked_results)\n        else:\n            raise ValueError(\"internal: _fit_method not recognized\")\n\n        if not query_is_train:\n            return results\n        else:\n            # If the query data is the same as the indexed data, we would like\n            # to ignore the first nearest neighbor of every sample, i.e\n            # the sample itself.\n            if return_distance:\n                neigh_dist, neigh_ind = results\n            else:\n                neigh_ind = results\n\n            for ind, ind_neighbor in enumerate(neigh_ind):\n                mask = ind_neighbor != ind\n\n                neigh_ind[ind] = ind_neighbor[mask]\n                if return_distance:\n                    neigh_dist[ind] = neigh_dist[ind][mask]\n\n            if return_distance:\n                return neigh_dist, neigh_ind\n            return neigh_ind\n\n    def radius_neighbors_graph(self, X=None, radius=None, mode='connectivity',\n                               sort_results=False):\n        \"\"\"Computes the (weighted) graph of Neighbors for points in X\n\n        Neighborhoods are restricted the points at a distance lower than\n        radius.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features), default=None\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        radius : float, default=None\n            Radius of neighborhoods. The default is the value passed to the\n            constructor.\n\n        mode : {'connectivity', 'distance'}, default='connectivity'\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are Euclidean distance between points.\n\n        sort_results : bool, default=False\n            If True, in each row of the result, the non-zero entries will be\n            sorted by increasing distances. If False, the non-zero entries may\n            not be sorted. Only used with mode='distance'.\n\n            .. versionadded:: 0.22\n\n        Returns\n        -------\n        A : sparse-matrix of shape (n_queries, n_samples_fit)\n            `n_samples_fit` is the number of samples in the fitted data.\n            `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n            The matrix is of CSR format.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(radius=1.5)\n        >>> neigh.fit(X)\n        NearestNeighbors(radius=1.5)\n        >>> A = neigh.radius_neighbors_graph(X)\n        >>> A.toarray()\n        array([[1., 0., 1.],\n               [0., 1., 0.],\n               [1., 0., 1.]])\n\n        See Also\n        --------\n        kneighbors_graph\n        \"\"\"\n        check_is_fitted(self)\n\n        # check the input only in self.radius_neighbors\n\n        if radius is None:\n            radius = self.radius\n\n        # construct CSR matrix representation of the NN graph\n        if mode == 'connectivity':\n            A_ind = self.radius_neighbors(X, radius,\n                                          return_distance=False)\n            A_data = None\n        elif mode == 'distance':\n            dist, A_ind = self.radius_neighbors(X, radius,\n                                                return_distance=True,\n                                                sort_results=sort_results)\n            A_data = np.concatenate(list(dist))\n        else:\n            raise ValueError(\n                'Unsupported mode, must be one of \"connectivity\", '\n                'or \"distance\" but got %s instead' % mode)\n\n        n_queries = A_ind.shape[0]\n        n_samples_fit = self.n_samples_fit_\n        n_neighbors = np.array([len(a) for a in A_ind])\n        A_ind = np.concatenate(list(A_ind))\n        if A_data is None:\n            A_data = np.ones(len(A_ind))\n        A_indptr = np.concatenate((np.zeros(1, dtype=int),\n                                   np.cumsum(n_neighbors)))\n\n        return csr_matrix((A_data, A_ind, A_indptr),\n                          shape=(n_queries, n_samples_fit))\n",1165],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py":["\"\"\"Utilities for with-statement contexts.  See PEP 343.\"\"\"\nimport abc\nimport sys\nimport _collections_abc\nfrom collections import deque\nfrom functools import wraps\nfrom types import MethodType, GenericAlias\n\n__all__ = [\"asynccontextmanager\", \"contextmanager\", \"closing\", \"nullcontext\",\n           \"AbstractContextManager\", \"AbstractAsyncContextManager\",\n           \"AsyncExitStack\", \"ContextDecorator\", \"ExitStack\",\n           \"redirect_stdout\", \"redirect_stderr\", \"suppress\"]\n\n\nclass AbstractContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for context managers.\"\"\"\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def __enter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractContextManager:\n            return _collections_abc._check_methods(C, \"__enter__\", \"__exit__\")\n        return NotImplemented\n\n\nclass AbstractAsyncContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for asynchronous context managers.\"\"\"\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    async def __aenter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractAsyncContextManager:\n            return _collections_abc._check_methods(C, \"__aenter__\",\n                                                   \"__aexit__\")\n        return NotImplemented\n\n\nclass ContextDecorator(object):\n    \"A base class or mixin that enables context managers to work as decorators.\"\n\n    def _recreate_cm(self):\n        \"\"\"Return a recreated instance of self.\n\n        Allows an otherwise one-shot context manager like\n        _GeneratorContextManager to support use as\n        a decorator via implicit recreation.\n\n        This is a private interface just for _GeneratorContextManager.\n        See issue #11647 for details.\n        \"\"\"\n        return self\n\n    def __call__(self, func):\n        @wraps(func)\n        def inner(*args, **kwds):\n            with self._recreate_cm():\n                return func(*args, **kwds)\n        return inner\n\n\nclass _GeneratorContextManagerBase:\n    \"\"\"Shared functionality for @contextmanager and @asynccontextmanager.\"\"\"\n\n    def __init__(self, func, args, kwds):\n        self.gen = func(*args, **kwds)\n        self.func, self.args, self.kwds = func, args, kwds\n        # Issue 19330: ensure context manager instances have good docstrings\n        doc = getattr(func, \"__doc__\", None)\n        if doc is None:\n            doc = type(self).__doc__\n        self.__doc__ = doc\n        # Unfortunately, this still doesn't provide good help output when\n        # inspecting the created context manager instances, since pydoc\n        # currently bypasses the instance docstring and shows the docstring\n        # for the class instead.\n        # See http://bugs.python.org/issue19404 for more details.\n\n\nclass _GeneratorContextManager(_GeneratorContextManagerBase,\n                               AbstractContextManager,\n                               ContextDecorator):\n    \"\"\"Helper for @contextmanager decorator.\"\"\"\n\n    def _recreate_cm(self):\n        # _GCM instances are one-shot context managers, so the\n        # CM must be recreated each time a decorated function is\n        # called\n        return self.__class__(self.func, self.args, self.kwds)\n\n    def __enter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n            return next(self.gen)\n        except StopIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    def __exit__(self, type, value, traceback):\n        if type is None:\n            try:\n                next(self.gen)\n            except StopIteration:\n                return False\n            else:\n                raise RuntimeError(\"generator didn't stop\")\n        else:\n            if value is None:\n                # Need to force instantiation so we can reliably\n                # tell if we get the same exception back\n                value = type()\n            try:\n                self.gen.throw(type, value, traceback)\n            except StopIteration as exc:\n                # Suppress StopIteration *unless* it's the same exception that\n                # was passed to throw().  This prevents a StopIteration\n                # raised inside the \"with\" statement from being suppressed.\n                return exc is not value\n            except RuntimeError as exc:\n                # Don't re-raise the passed in exception. (issue27122)\n                if exc is value:\n                    return False\n                # Likewise, avoid suppressing if a StopIteration exception\n                # was passed to throw() and later wrapped into a RuntimeError\n                # (see PEP 479).\n                if type is StopIteration and exc.__cause__ is value:\n                    return False\n                raise\n            except:\n                # only re-raise if it's *not* the exception that was\n                # passed to throw(), because __exit__() must not raise\n                # an exception unless __exit__() itself failed.  But throw()\n                # has to raise the exception to signal propagation, so this\n                # fixes the impedance mismatch between the throw() protocol\n                # and the __exit__() protocol.\n                #\n                # This cannot use 'except BaseException as exc' (as in the\n                # async implementation) to maintain compatibility with\n                # Python 2, where old-style class exceptions are not caught\n                # by 'except BaseException'.\n                if sys.exc_info()[1] is value:\n                    return False\n                raise\n            raise RuntimeError(\"generator didn't stop after throw()\")\n\n\nclass _AsyncGeneratorContextManager(_GeneratorContextManagerBase,\n                                    AbstractAsyncContextManager):\n    \"\"\"Helper for @asynccontextmanager.\"\"\"\n\n    async def __aenter__(self):\n        try:\n            return await self.gen.__anext__()\n        except StopAsyncIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    async def __aexit__(self, typ, value, traceback):\n        if typ is None:\n            try:\n                await self.gen.__anext__()\n            except StopAsyncIteration:\n                return\n            else:\n                raise RuntimeError(\"generator didn't stop\")\n        else:\n            if value is None:\n                value = typ()\n            # See _GeneratorContextManager.__exit__ for comments on subtleties\n            # in this implementation\n            try:\n                await self.gen.athrow(typ, value, traceback)\n                raise RuntimeError(\"generator didn't stop after athrow()\")\n            except StopAsyncIteration as exc:\n                return exc is not value\n            except RuntimeError as exc:\n                if exc is value:\n                    return False\n                # Avoid suppressing if a StopIteration exception\n                # was passed to throw() and later wrapped into a RuntimeError\n                # (see PEP 479 for sync generators; async generators also\n                # have this behavior). But do this only if the exception wrapped\n                # by the RuntimeError is actully Stop(Async)Iteration (see\n                # issue29692).\n                if isinstance(value, (StopIteration, StopAsyncIteration)):\n                    if exc.__cause__ is value:\n                        return False\n                raise\n            except BaseException as exc:\n                if exc is not value:\n                    raise\n\n\ndef contextmanager(func):\n    \"\"\"@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return _GeneratorContextManager(func, args, kwds)\n    return helper\n\n\ndef asynccontextmanager(func):\n    \"\"\"@asynccontextmanager decorator.\n\n    Typical usage:\n\n        @asynccontextmanager\n        async def some_async_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        async with some_async_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return _AsyncGeneratorContextManager(func, args, kwds)\n    return helper\n\n\nclass closing(AbstractContextManager):\n    \"\"\"Context to automatically close something at the end of a block.\n\n    Code like this:\n\n        with closing(<module>.open(<arguments>)) as f:\n            <block>\n\n    is equivalent to this:\n\n        f = <module>.open(<arguments>)\n        try:\n            <block>\n        finally:\n            f.close()\n\n    \"\"\"\n    def __init__(self, thing):\n        self.thing = thing\n    def __enter__(self):\n        return self.thing\n    def __exit__(self, *exc_info):\n        self.thing.close()\n\n\nclass _RedirectStream(AbstractContextManager):\n\n    _stream = None\n\n    def __init__(self, new_target):\n        self._new_target = new_target\n        # We use a list of old targets to make this CM re-entrant\n        self._old_targets = []\n\n    def __enter__(self):\n        self._old_targets.append(getattr(sys, self._stream))\n        setattr(sys, self._stream, self._new_target)\n        return self._new_target\n\n    def __exit__(self, exctype, excinst, exctb):\n        setattr(sys, self._stream, self._old_targets.pop())\n\n\nclass redirect_stdout(_RedirectStream):\n    \"\"\"Context manager for temporarily redirecting stdout to another file.\n\n        # How to send help() to stderr\n        with redirect_stdout(sys.stderr):\n            help(dir)\n\n        # How to write help() to a file\n        with open('help.txt', 'w') as f:\n            with redirect_stdout(f):\n                help(pow)\n    \"\"\"\n\n    _stream = \"stdout\"\n\n\nclass redirect_stderr(_RedirectStream):\n    \"\"\"Context manager for temporarily redirecting stderr to another file.\"\"\"\n\n    _stream = \"stderr\"\n\n\nclass suppress(AbstractContextManager):\n    \"\"\"Context manager to suppress specified exceptions\n\n    After the exception is suppressed, execution proceeds with the next\n    statement following the with statement.\n\n         with suppress(FileNotFoundError):\n             os.remove(somefile)\n         # Execution still resumes here if the file was already removed\n    \"\"\"\n\n    def __init__(self, *exceptions):\n        self._exceptions = exceptions\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exctype, excinst, exctb):\n        # Unlike isinstance and issubclass, CPython exception handling\n        # currently only looks at the concrete type hierarchy (ignoring\n        # the instance and subclass checking hooks). While Guido considers\n        # that a bug rather than a feature, it's a fairly hard one to fix\n        # due to various internal implementation details. suppress provides\n        # the simpler issubclass based semantics, rather than trying to\n        # exactly reproduce the limitations of the CPython interpreter.\n        #\n        # See http://bugs.python.org/issue12029 for more details\n        return exctype is not None and issubclass(exctype, self._exceptions)\n\n\nclass _BaseExitStack:\n    \"\"\"A base class for ExitStack and AsyncExitStack.\"\"\"\n\n    @staticmethod\n    def _create_exit_wrapper(cm, cm_exit):\n        return MethodType(cm_exit, cm)\n\n    @staticmethod\n    def _create_cb_wrapper(callback, /, *args, **kwds):\n        def _exit_wrapper(exc_type, exc, tb):\n            callback(*args, **kwds)\n        return _exit_wrapper\n\n    def __init__(self):\n        self._exit_callbacks = deque()\n\n    def pop_all(self):\n        \"\"\"Preserve the context stack by transferring it to a new instance.\"\"\"\n        new_stack = type(self)()\n        new_stack._exit_callbacks = self._exit_callbacks\n        self._exit_callbacks = deque()\n        return new_stack\n\n    def push(self, exit):\n        \"\"\"Registers a callback with the standard __exit__ method signature.\n\n        Can suppress exceptions the same way __exit__ method can.\n        Also accepts any object with an __exit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n        # We use an unbound method rather than a bound method to follow\n        # the standard lookup behaviour for special methods.\n        _cb_type = type(exit)\n\n        try:\n            exit_method = _cb_type.__exit__\n        except AttributeError:\n            # Not a context manager, so assume it's a callable.\n            self._push_exit_callback(exit)\n        else:\n            self._push_cm_exit(exit, exit_method)\n        return exit  # Allow use as a decorator.\n\n    def enter_context(self, cm):\n        \"\"\"Enters the supplied context manager.\n\n        If successful, also pushes its __exit__ method as a callback and\n        returns the result of the __enter__ method.\n        \"\"\"\n        # We look up the special methods on the type to match the with\n        # statement.\n        _cm_type = type(cm)\n        _exit = _cm_type.__exit__\n        result = _cm_type.__enter__(cm)\n        self._push_cm_exit(cm, _exit)\n        return result\n\n    def callback(self, callback, /, *args, **kwds):\n        \"\"\"Registers an arbitrary callback and arguments.\n\n        Cannot suppress exceptions.\n        \"\"\"\n        _exit_wrapper = self._create_cb_wrapper(callback, *args, **kwds)\n\n        # We changed the signature, so using @wraps is not appropriate, but\n        # setting __wrapped__ may still help with introspection.\n        _exit_wrapper.__wrapped__ = callback\n        self._push_exit_callback(_exit_wrapper)\n        return callback  # Allow use as a decorator\n\n    def _push_cm_exit(self, cm, cm_exit):\n        \"\"\"Helper to correctly register callbacks to __exit__ methods.\"\"\"\n        _exit_wrapper = self._create_exit_wrapper(cm, cm_exit)\n        self._push_exit_callback(_exit_wrapper, True)\n\n    def _push_exit_callback(self, callback, is_sync=True):\n        self._exit_callbacks.append((is_sync, callback))\n\n\n# Inspired by discussions on http://bugs.python.org/issue13585\nclass ExitStack(_BaseExitStack, AbstractContextManager):\n    \"\"\"Context manager for dynamic management of a stack of exit callbacks.\n\n    For example:\n        with ExitStack() as stack:\n            files = [stack.enter_context(open(fname)) for fname in filenames]\n            # All opened files will automatically be closed at the end of\n            # the with statement, even if attempts to open files later\n            # in the list raise an exception.\n    \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_details):\n        received_exc = exc_details[0] is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exc_info()[1]\n        def _fix_exception_context(new_exc, old_exc):\n            # Context may not be correct, so find the end of the chain\n            while 1:\n                exc_context = new_exc.__context__\n                if exc_context is old_exc:\n                    # Context is already set correctly (see issue 20317)\n                    return\n                if exc_context is None or exc_context is frame_exc:\n                    break\n                new_exc = exc_context\n            # Change the end of the chain to point to the exception\n            # we expect it to reference\n            new_exc.__context__ = old_exc\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            is_sync, cb = self._exit_callbacks.pop()\n            assert is_sync\n            try:\n                if cb(*exc_details):\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc_details = (None, None, None)\n            except:\n                new_exc_details = sys.exc_info()\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc_details[1], exc_details[1])\n                pending_raise = True\n                exc_details = new_exc_details\n        if pending_raise:\n            try:\n                # bare \"raise exc_details[1]\" replaces our carefully\n                # set-up context\n                fixed_ctx = exc_details[1].__context__\n                raise exc_details[1]\n            except BaseException:\n                exc_details[1].__context__ = fixed_ctx\n                raise\n        return received_exc and suppressed_exc\n\n    def close(self):\n        \"\"\"Immediately unwind the context stack.\"\"\"\n        self.__exit__(None, None, None)\n\n\n# Inspired by discussions on https://bugs.python.org/issue29302\nclass AsyncExitStack(_BaseExitStack, AbstractAsyncContextManager):\n    \"\"\"Async context manager for dynamic management of a stack of exit\n    callbacks.\n\n    For example:\n        async with AsyncExitStack() as stack:\n            connections = [await stack.enter_async_context(get_connection())\n                for i in range(5)]\n            # All opened connections will automatically be released at the\n            # end of the async with statement, even if attempts to open a\n            # connection later in the list raise an exception.\n    \"\"\"\n\n    @staticmethod\n    def _create_async_exit_wrapper(cm, cm_exit):\n        return MethodType(cm_exit, cm)\n\n    @staticmethod\n    def _create_async_cb_wrapper(callback, /, *args, **kwds):\n        async def _exit_wrapper(exc_type, exc, tb):\n            await callback(*args, **kwds)\n        return _exit_wrapper\n\n    async def enter_async_context(self, cm):\n        \"\"\"Enters the supplied async context manager.\n\n        If successful, also pushes its __aexit__ method as a callback and\n        returns the result of the __aenter__ method.\n        \"\"\"\n        _cm_type = type(cm)\n        _exit = _cm_type.__aexit__\n        result = await _cm_type.__aenter__(cm)\n        self._push_async_cm_exit(cm, _exit)\n        return result\n\n    def push_async_exit(self, exit):\n        \"\"\"Registers a coroutine function with the standard __aexit__ method\n        signature.\n\n        Can suppress exceptions the same way __aexit__ method can.\n        Also accepts any object with an __aexit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n        _cb_type = type(exit)\n        try:\n            exit_method = _cb_type.__aexit__\n        except AttributeError:\n            # Not an async context manager, so assume it's a coroutine function\n            self._push_exit_callback(exit, False)\n        else:\n            self._push_async_cm_exit(exit, exit_method)\n        return exit  # Allow use as a decorator\n\n    def push_async_callback(self, callback, /, *args, **kwds):\n        \"\"\"Registers an arbitrary coroutine function and arguments.\n\n        Cannot suppress exceptions.\n        \"\"\"\n        _exit_wrapper = self._create_async_cb_wrapper(callback, *args, **kwds)\n\n        # We changed the signature, so using @wraps is not appropriate, but\n        # setting __wrapped__ may still help with introspection.\n        _exit_wrapper.__wrapped__ = callback\n        self._push_exit_callback(_exit_wrapper, False)\n        return callback  # Allow use as a decorator\n\n    async def aclose(self):\n        \"\"\"Immediately unwind the context stack.\"\"\"\n        await self.__aexit__(None, None, None)\n\n    def _push_async_cm_exit(self, cm, cm_exit):\n        \"\"\"Helper to correctly register coroutine function to __aexit__\n        method.\"\"\"\n        _exit_wrapper = self._create_async_exit_wrapper(cm, cm_exit)\n        self._push_exit_callback(_exit_wrapper, False)\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc_details):\n        received_exc = exc_details[0] is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exc_info()[1]\n        def _fix_exception_context(new_exc, old_exc):\n            # Context may not be correct, so find the end of the chain\n            while 1:\n                exc_context = new_exc.__context__\n                if exc_context is old_exc:\n                    # Context is already set correctly (see issue 20317)\n                    return\n                if exc_context is None or exc_context is frame_exc:\n                    break\n                new_exc = exc_context\n            # Change the end of the chain to point to the exception\n            # we expect it to reference\n            new_exc.__context__ = old_exc\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            is_sync, cb = self._exit_callbacks.pop()\n            try:\n                if is_sync:\n                    cb_suppress = cb(*exc_details)\n                else:\n                    cb_suppress = await cb(*exc_details)\n\n                if cb_suppress:\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc_details = (None, None, None)\n            except:\n                new_exc_details = sys.exc_info()\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc_details[1], exc_details[1])\n                pending_raise = True\n                exc_details = new_exc_details\n        if pending_raise:\n            try:\n                # bare \"raise exc_details[1]\" replaces our carefully\n                # set-up context\n                fixed_ctx = exc_details[1].__context__\n                raise exc_details[1]\n            except BaseException:\n                exc_details[1].__context__ = fixed_ctx\n                raise\n        return received_exc and suppressed_exc\n\n\nclass nullcontext(AbstractContextManager):\n    \"\"\"Context manager that does no additional processing.\n\n    Used as a stand-in for a normal context manager, when a particular\n    block of code is only sometimes used with a normal context manager:\n\n    cm = optional_cm if condition else nullcontext()\n    with cm:\n        # Perform operation, using optional_cm if condition is True\n    \"\"\"\n\n    def __init__(self, enter_result=None):\n        self.enter_result = enter_result\n\n    def __enter__(self):\n        return self.enter_result\n\n    def __exit__(self, *excinfo):\n        pass\n",676],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py":["\"\"\"\nCreate the numpy.core.multiarray namespace for backward compatibility. In v1.16\nthe multiarray and umath c-extension modules were merged into a single\n_multiarray_umath extension module. So we replicate the old namespace\nby importing from the extension module.\n\n\"\"\"\n\nimport functools\nimport warnings\n\nfrom . import overrides\nfrom . import _multiarray_umath\nfrom ._multiarray_umath import *  # noqa: F403\n# These imports are needed for backward compatibility,\n# do not change them. issue gh-15518\n# _get_ndarray_c_version is semi-public, on purpose not added to __all__\nfrom ._multiarray_umath import (\n    _fastCopyAndTranspose, _flagdict, _insert, _reconstruct, _vec_string,\n    _ARRAY_API, _monotonicity, _get_ndarray_c_version, _set_madvise_hugepage,\n    )\n\n__all__ = [\n    '_ARRAY_API', 'ALLOW_THREADS', 'BUFSIZE', 'CLIP', 'DATETIMEUNITS',\n    'ITEM_HASOBJECT', 'ITEM_IS_POINTER', 'LIST_PICKLE', 'MAXDIMS',\n    'MAY_SHARE_BOUNDS', 'MAY_SHARE_EXACT', 'NEEDS_INIT', 'NEEDS_PYAPI',\n    'RAISE', 'USE_GETITEM', 'USE_SETITEM', 'WRAP', '_fastCopyAndTranspose',\n    '_flagdict', '_insert', '_reconstruct', '_vec_string', '_monotonicity',\n    'add_docstring', 'arange', 'array', 'bincount', 'broadcast',\n    'busday_count', 'busday_offset', 'busdaycalendar', 'can_cast',\n    'compare_chararrays', 'concatenate', 'copyto', 'correlate', 'correlate2',\n    'count_nonzero', 'c_einsum', 'datetime_as_string', 'datetime_data',\n    'digitize', 'dot', 'dragon4_positional', 'dragon4_scientific', 'dtype',\n    'empty', 'empty_like', 'error', 'flagsobj', 'flatiter', 'format_longfloat',\n    'frombuffer', 'fromfile', 'fromiter', 'fromstring', 'inner',\n    'interp', 'interp_complex', 'is_busday', 'lexsort',\n    'matmul', 'may_share_memory', 'min_scalar_type', 'ndarray', 'nditer',\n    'nested_iters', 'normalize_axis_index', 'packbits',\n    'promote_types', 'putmask', 'ravel_multi_index', 'result_type', 'scalar',\n    'set_datetimeparse_function', 'set_legacy_print_mode', 'set_numeric_ops',\n    'set_string_function', 'set_typeDict', 'shares_memory',\n    'tracemalloc_domain', 'typeinfo', 'unpackbits', 'unravel_index', 'vdot',\n    'where', 'zeros']\n\n# For backward compatibility, make sure pickle imports these functions from here\n_reconstruct.__module__ = 'numpy.core.multiarray'\nscalar.__module__ = 'numpy.core.multiarray'\n\n\narange.__module__ = 'numpy'\narray.__module__ = 'numpy'\ndatetime_data.__module__ = 'numpy'\nempty.__module__ = 'numpy'\nfrombuffer.__module__ = 'numpy'\nfromfile.__module__ = 'numpy'\nfromiter.__module__ = 'numpy'\nfrompyfunc.__module__ = 'numpy'\nfromstring.__module__ = 'numpy'\ngeterrobj.__module__ = 'numpy'\nmay_share_memory.__module__ = 'numpy'\nnested_iters.__module__ = 'numpy'\npromote_types.__module__ = 'numpy'\nset_numeric_ops.__module__ = 'numpy'\nseterrobj.__module__ = 'numpy'\nzeros.__module__ = 'numpy'\n\n\n# We can't verify dispatcher signatures because NumPy's C functions don't\n# support introspection.\narray_function_from_c_func_and_dispatcher = functools.partial(\n    overrides.array_function_from_dispatcher,\n    module='numpy', docs_from_dispatcher=True, verify=False)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.empty_like)\ndef empty_like(prototype, dtype=None, order=None, subok=None, shape=None):\n    \"\"\"\n    empty_like(prototype, dtype=None, order='K', subok=True, shape=None)\n\n    Return a new array with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    prototype : array_like\n        The shape and data-type of `prototype` define these same attributes\n        of the returned array.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n\n        .. versionadded:: 1.6.0\n    order : {'C', 'F', 'A', or 'K'}, optional\n        Overrides the memory layout of the result. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `prototype` is Fortran\n        contiguous, 'C' otherwise. 'K' means match the layout of `prototype`\n        as closely as possible.\n\n        .. versionadded:: 1.6.0\n    subok : bool, optional.\n        If True, then the newly created array will use the sub-class\n        type of `prototype`, otherwise it will be a base-class array. Defaults\n        to True.\n    shape : int or sequence of ints, optional.\n        Overrides the shape of the result. If order='K' and the number of\n        dimensions is unchanged, will try to keep order, otherwise,\n        order='C' is implied.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of uninitialized (arbitrary) data with the same\n        shape and type as `prototype`.\n\n    See Also\n    --------\n    ones_like : Return an array of ones with shape and type of input.\n    zeros_like : Return an array of zeros with shape and type of input.\n    full_like : Return a new array with shape of input filled with value.\n    empty : Return a new uninitialized array.\n\n    Notes\n    -----\n    This function does *not* initialize the returned array; to do that use\n    `zeros_like` or `ones_like` instead.  It may be marginally faster than\n    the functions that do set the array values.\n\n    Examples\n    --------\n    >>> a = ([1,2,3], [4,5,6])                         # a is array-like\n    >>> np.empty_like(a)\n    array([[-1073741821, -1073741821,           3],    # uninitialized\n           [          0,           0, -1073741821]])\n    >>> a = np.array([[1., 2., 3.],[4.,5.,6.]])\n    >>> np.empty_like(a)\n    array([[ -2.00000715e+000,   1.48219694e-323,  -2.00000572e+000], # uninitialized\n           [  4.38791518e-305,  -2.00000715e+000,   4.17269252e-309]])\n\n    \"\"\"\n    return (prototype,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.concatenate)\ndef concatenate(arrays, axis=None, out=None, *, dtype=None, casting=None):\n    \"\"\"\n    concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n\n    Join a sequence of arrays along an existing axis.\n\n    Parameters\n    ----------\n    a1, a2, ... : sequence of array_like\n        The arrays must have the same shape, except in the dimension\n        corresponding to `axis` (the first, by default).\n    axis : int, optional\n        The axis along which the arrays will be joined.  If axis is None,\n        arrays are flattened before use.  Default is 0.\n    out : ndarray, optional\n        If provided, the destination to place the result. The shape must be\n        correct, matching that of what concatenate would have returned if no\n        out argument were specified.\n    dtype : str or dtype\n        If provided, the destination array will have this dtype. Cannot be\n        provided together with `out`.\n\n        .. versionadded:: 1.20.0\n\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n        Controls what kind of data casting may occur. Defaults to 'same_kind'.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    res : ndarray\n        The concatenated array.\n\n    See Also\n    --------\n    ma.concatenate : Concatenate function that preserves input masks.\n    array_split : Split an array into multiple sub-arrays of equal or\n                  near-equal size.\n    split : Split array into a list of multiple sub-arrays of equal size.\n    hsplit : Split array into multiple sub-arrays horizontally (column wise).\n    vsplit : Split array into multiple sub-arrays vertically (row wise).\n    dsplit : Split array into multiple sub-arrays along the 3rd axis (depth).\n    stack : Stack a sequence of arrays along a new axis.\n    block : Assemble arrays from blocks.\n    hstack : Stack arrays in sequence horizontally (column wise).\n    vstack : Stack arrays in sequence vertically (row wise).\n    dstack : Stack arrays in sequence depth wise (along third dimension).\n    column_stack : Stack 1-D arrays as columns into a 2-D array.\n\n    Notes\n    -----\n    When one or more of the arrays to be concatenated is a MaskedArray,\n    this function will return a MaskedArray object instead of an ndarray,\n    but the input masks are *not* preserved. In cases where a MaskedArray\n    is expected as input, use the ma.concatenate function from the masked\n    array module instead.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> b = np.array([[5, 6]])\n    >>> np.concatenate((a, b), axis=0)\n    array([[1, 2],\n           [3, 4],\n           [5, 6]])\n    >>> np.concatenate((a, b.T), axis=1)\n    array([[1, 2, 5],\n           [3, 4, 6]])\n    >>> np.concatenate((a, b), axis=None)\n    array([1, 2, 3, 4, 5, 6])\n\n    This function will not preserve masking of MaskedArray inputs.\n\n    >>> a = np.ma.arange(3)\n    >>> a[1] = np.ma.masked\n    >>> b = np.arange(2, 5)\n    >>> a\n    masked_array(data=[0, --, 2],\n                 mask=[False,  True, False],\n           fill_value=999999)\n    >>> b\n    array([2, 3, 4])\n    >>> np.concatenate([a, b])\n    masked_array(data=[0, 1, 2, 2, 3, 4],\n                 mask=False,\n           fill_value=999999)\n    >>> np.ma.concatenate([a, b])\n    masked_array(data=[0, --, 2, 2, 3, 4],\n                 mask=[False,  True, False, False, False, False],\n           fill_value=999999)\n\n    \"\"\"\n    if out is not None:\n        # optimize for the typical case where only arrays is provided\n        arrays = list(arrays)\n        arrays.append(out)\n    return arrays\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.inner)\ndef inner(a, b):\n    \"\"\"\n    inner(a, b)\n\n    Inner product of two arrays.\n\n    Ordinary inner product of vectors for 1-D arrays (without complex\n    conjugation), in higher dimensions a sum product over the last axes.\n\n    Parameters\n    ----------\n    a, b : array_like\n        If `a` and `b` are nonscalar, their last dimensions must match.\n\n    Returns\n    -------\n    out : ndarray\n        `out.shape = a.shape[:-1] + b.shape[:-1]`\n\n    Raises\n    ------\n    ValueError\n        If the last dimension of `a` and `b` has different size.\n\n    See Also\n    --------\n    tensordot : Sum products over arbitrary axes.\n    dot : Generalised matrix product, using second last dimension of `b`.\n    einsum : Einstein summation convention.\n\n    Notes\n    -----\n    For vectors (1-D arrays) it computes the ordinary inner-product::\n\n        np.inner(a, b) = sum(a[:]*b[:])\n\n    More generally, if `ndim(a) = r > 0` and `ndim(b) = s > 0`::\n\n        np.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n\n    or explicitly::\n\n        np.inner(a, b)[i0,...,ir-1,j0,...,js-1]\n             = sum(a[i0,...,ir-1,:]*b[j0,...,js-1,:])\n\n    In addition `a` or `b` may be scalars, in which case::\n\n       np.inner(a,b) = a*b\n\n    Examples\n    --------\n    Ordinary inner product for vectors:\n\n    >>> a = np.array([1,2,3])\n    >>> b = np.array([0,1,0])\n    >>> np.inner(a, b)\n    2\n\n    A multidimensional example:\n\n    >>> a = np.arange(24).reshape((2,3,4))\n    >>> b = np.arange(4)\n    >>> np.inner(a, b)\n    array([[ 14,  38,  62],\n           [ 86, 110, 134]])\n\n    An example where `b` is a scalar:\n\n    >>> np.inner(np.eye(2), 7)\n    array([[7., 0.],\n           [0., 7.]])\n\n    \"\"\"\n    return (a, b)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.where)\ndef where(condition, x=None, y=None):\n    \"\"\"\n    where(condition, [x, y])\n\n    Return elements chosen from `x` or `y` depending on `condition`.\n\n    .. note::\n        When only `condition` is provided, this function is a shorthand for\n        ``np.asarray(condition).nonzero()``. Using `nonzero` directly should be\n        preferred, as it behaves correctly for subclasses. The rest of this\n        documentation covers only the case where all three arguments are\n        provided.\n\n    Parameters\n    ----------\n    condition : array_like, bool\n        Where True, yield `x`, otherwise yield `y`.\n    x, y : array_like\n        Values from which to choose. `x`, `y` and `condition` need to be\n        broadcastable to some shape.\n\n    Returns\n    -------\n    out : ndarray\n        An array with elements from `x` where `condition` is True, and elements\n        from `y` elsewhere.\n\n    See Also\n    --------\n    choose\n    nonzero : The function that is called when x and y are omitted\n\n    Notes\n    -----\n    If all the arrays are 1-D, `where` is equivalent to::\n\n        [xv if c else yv\n         for c, xv, yv in zip(condition, x, y)]\n\n    Examples\n    --------\n    >>> a = np.arange(10)\n    >>> a\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> np.where(a < 5, a, 10*a)\n    array([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n\n    This can be used on multidimensional arrays too:\n\n    >>> np.where([[True, False], [True, True]],\n    ...          [[1, 2], [3, 4]],\n    ...          [[9, 8], [7, 6]])\n    array([[1, 8],\n           [3, 4]])\n\n    The shapes of x, y, and the condition are broadcast together:\n\n    >>> x, y = np.ogrid[:3, :4]\n    >>> np.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\n    array([[10,  0,  0,  0],\n           [10, 11,  1,  1],\n           [10, 11, 12,  2]])\n\n    >>> a = np.array([[0, 1, 2],\n    ...               [0, 2, 4],\n    ...               [0, 3, 6]])\n    >>> np.where(a < 4, a, -1)  # -1 is broadcast\n    array([[ 0,  1,  2],\n           [ 0,  2, -1],\n           [ 0,  3, -1]])\n    \"\"\"\n    return (condition, x, y)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.lexsort)\ndef lexsort(keys, axis=None):\n    \"\"\"\n    lexsort(keys, axis=-1)\n\n    Perform an indirect stable sort using a sequence of keys.\n\n    Given multiple sorting keys, which can be interpreted as columns in a\n    spreadsheet, lexsort returns an array of integer indices that describes\n    the sort order by multiple columns. The last key in the sequence is used\n    for the primary sort order, the second-to-last key for the secondary sort\n    order, and so on. The keys argument must be a sequence of objects that\n    can be converted to arrays of the same shape. If a 2D array is provided\n    for the keys argument, its rows are interpreted as the sorting keys and\n    sorting is according to the last row, second last row etc.\n\n    Parameters\n    ----------\n    keys : (k, N) array or tuple containing k (N,)-shaped sequences\n        The `k` different \"columns\" to be sorted.  The last column (or row if\n        `keys` is a 2D array) is the primary sort key.\n    axis : int, optional\n        Axis to be indirectly sorted.  By default, sort over the last axis.\n\n    Returns\n    -------\n    indices : (N,) ndarray of ints\n        Array of indices that sort the keys along the specified axis.\n\n    See Also\n    --------\n    argsort : Indirect sort.\n    ndarray.sort : In-place sort.\n    sort : Return a sorted copy of an array.\n\n    Examples\n    --------\n    Sort names: first by surname, then by name.\n\n    >>> surnames =    ('Hertz',    'Galilei', 'Hertz')\n    >>> first_names = ('Heinrich', 'Galileo', 'Gustav')\n    >>> ind = np.lexsort((first_names, surnames))\n    >>> ind\n    array([1, 2, 0])\n\n    >>> [surnames[i] + \", \" + first_names[i] for i in ind]\n    ['Galilei, Galileo', 'Hertz, Gustav', 'Hertz, Heinrich']\n\n    Sort two columns of numbers:\n\n    >>> a = [1,5,1,4,3,4,4] # First column\n    >>> b = [9,4,0,4,0,2,1] # Second column\n    >>> ind = np.lexsort((b,a)) # Sort by a, then by b\n    >>> ind\n    array([2, 0, 4, 6, 5, 3, 1])\n\n    >>> [(a[i],b[i]) for i in ind]\n    [(1, 0), (1, 9), (3, 0), (4, 1), (4, 2), (4, 4), (5, 4)]\n\n    Note that sorting is first according to the elements of ``a``.\n    Secondary sorting is according to the elements of ``b``.\n\n    A normal ``argsort`` would have yielded:\n\n    >>> [(a[i],b[i]) for i in np.argsort(a)]\n    [(1, 9), (1, 0), (3, 0), (4, 4), (4, 2), (4, 1), (5, 4)]\n\n    Structured arrays are sorted lexically by ``argsort``:\n\n    >>> x = np.array([(1,9), (5,4), (1,0), (4,4), (3,0), (4,2), (4,1)],\n    ...              dtype=np.dtype([('x', int), ('y', int)]))\n\n    >>> np.argsort(x) # or np.argsort(x, order=('x', 'y'))\n    array([2, 0, 4, 6, 5, 3, 1])\n\n    \"\"\"\n    if isinstance(keys, tuple):\n        return keys\n    else:\n        return (keys,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.can_cast)\ndef can_cast(from_, to, casting=None):\n    \"\"\"\n    can_cast(from_, to, casting='safe')\n\n    Returns True if cast between data types can occur according to the\n    casting rule.  If from is a scalar or array scalar, also returns\n    True if the scalar value can be cast without overflow or truncation\n    to an integer.\n\n    Parameters\n    ----------\n    from_ : dtype, dtype specifier, scalar, or array\n        Data type, scalar, or array to cast from.\n    to : dtype or dtype specifier\n        Data type to cast to.\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n        Controls what kind of data casting may occur.\n\n          * 'no' means the data types should not be cast at all.\n          * 'equiv' means only byte-order changes are allowed.\n          * 'safe' means only casts which can preserve values are allowed.\n          * 'same_kind' means only safe casts or casts within a kind,\n            like float64 to float32, are allowed.\n          * 'unsafe' means any data conversions may be done.\n\n    Returns\n    -------\n    out : bool\n        True if cast can occur according to the casting rule.\n\n    Notes\n    -----\n    .. versionchanged:: 1.17.0\n       Casting between a simple data type and a structured one is possible only\n       for \"unsafe\" casting.  Casting to multiple fields is allowed, but\n       casting from multiple fields is not.\n\n    .. versionchanged:: 1.9.0\n       Casting from numeric to string types in 'safe' casting mode requires\n       that the string dtype length is long enough to store the maximum\n       integer/float value converted.\n\n    See also\n    --------\n    dtype, result_type\n\n    Examples\n    --------\n    Basic examples\n\n    >>> np.can_cast(np.int32, np.int64)\n    True\n    >>> np.can_cast(np.float64, complex)\n    True\n    >>> np.can_cast(complex, float)\n    False\n\n    >>> np.can_cast('i8', 'f8')\n    True\n    >>> np.can_cast('i8', 'f4')\n    False\n    >>> np.can_cast('i4', 'S4')\n    False\n\n    Casting scalars\n\n    >>> np.can_cast(100, 'i1')\n    True\n    >>> np.can_cast(150, 'i1')\n    False\n    >>> np.can_cast(150, 'u1')\n    True\n\n    >>> np.can_cast(3.5e100, np.float32)\n    False\n    >>> np.can_cast(1000.0, np.float32)\n    True\n\n    Array scalar checks the value, array does not\n\n    >>> np.can_cast(np.array(1000.0), np.float32)\n    True\n    >>> np.can_cast(np.array([1000.0]), np.float32)\n    False\n\n    Using the casting rules\n\n    >>> np.can_cast('i8', 'i8', 'no')\n    True\n    >>> np.can_cast('<i8', '>i8', 'no')\n    False\n\n    >>> np.can_cast('<i8', '>i8', 'equiv')\n    True\n    >>> np.can_cast('<i4', '>i8', 'equiv')\n    False\n\n    >>> np.can_cast('<i4', '>i8', 'safe')\n    True\n    >>> np.can_cast('<i8', '>i4', 'safe')\n    False\n\n    >>> np.can_cast('<i8', '>i4', 'same_kind')\n    True\n    >>> np.can_cast('<i8', '>u4', 'same_kind')\n    False\n\n    >>> np.can_cast('<i8', '>u4', 'unsafe')\n    True\n\n    \"\"\"\n    return (from_,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.min_scalar_type)\ndef min_scalar_type(a):\n    \"\"\"\n    min_scalar_type(a)\n\n    For scalar ``a``, returns the data type with the smallest size\n    and smallest scalar kind which can hold its value.  For non-scalar\n    array ``a``, returns the vector's dtype unmodified.\n\n    Floating point values are not demoted to integers,\n    and complex values are not demoted to floats.\n\n    Parameters\n    ----------\n    a : scalar or array_like\n        The value whose minimal data type is to be found.\n\n    Returns\n    -------\n    out : dtype\n        The minimal data type.\n\n    Notes\n    -----\n    .. versionadded:: 1.6.0\n\n    See Also\n    --------\n    result_type, promote_types, dtype, can_cast\n\n    Examples\n    --------\n    >>> np.min_scalar_type(10)\n    dtype('uint8')\n\n    >>> np.min_scalar_type(-260)\n    dtype('int16')\n\n    >>> np.min_scalar_type(3.1)\n    dtype('float16')\n\n    >>> np.min_scalar_type(1e50)\n    dtype('float64')\n\n    >>> np.min_scalar_type(np.arange(4,dtype='f8'))\n    dtype('float64')\n\n    \"\"\"\n    return (a,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.result_type)\ndef result_type(*arrays_and_dtypes):\n    \"\"\"\n    result_type(*arrays_and_dtypes)\n\n    Returns the type that results from applying the NumPy\n    type promotion rules to the arguments.\n\n    Type promotion in NumPy works similarly to the rules in languages\n    like C++, with some slight differences.  When both scalars and\n    arrays are used, the array's type takes precedence and the actual value\n    of the scalar is taken into account.\n\n    For example, calculating 3*a, where a is an array of 32-bit floats,\n    intuitively should result in a 32-bit float output.  If the 3 is a\n    32-bit integer, the NumPy rules indicate it can't convert losslessly\n    into a 32-bit float, so a 64-bit float should be the result type.\n    By examining the value of the constant, '3', we see that it fits in\n    an 8-bit integer, which can be cast losslessly into the 32-bit float.\n\n    Parameters\n    ----------\n    arrays_and_dtypes : list of arrays and dtypes\n        The operands of some operation whose result type is needed.\n\n    Returns\n    -------\n    out : dtype\n        The result type.\n\n    See also\n    --------\n    dtype, promote_types, min_scalar_type, can_cast\n\n    Notes\n    -----\n    .. versionadded:: 1.6.0\n\n    The specific algorithm used is as follows.\n\n    Categories are determined by first checking which of boolean,\n    integer (int/uint), or floating point (float/complex) the maximum\n    kind of all the arrays and the scalars are.\n\n    If there are only scalars or the maximum category of the scalars\n    is higher than the maximum category of the arrays,\n    the data types are combined with :func:`promote_types`\n    to produce the return value.\n\n    Otherwise, `min_scalar_type` is called on each array, and\n    the resulting data types are all combined with :func:`promote_types`\n    to produce the return value.\n\n    The set of int values is not a subset of the uint values for types\n    with the same number of bits, something not reflected in\n    :func:`min_scalar_type`, but handled as a special case in `result_type`.\n\n    Examples\n    --------\n    >>> np.result_type(3, np.arange(7, dtype='i1'))\n    dtype('int8')\n\n    >>> np.result_type('i4', 'c8')\n    dtype('complex128')\n\n    >>> np.result_type(3.0, -2)\n    dtype('float64')\n\n    \"\"\"\n    return arrays_and_dtypes\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.dot)\ndef dot(a, b, out=None):\n    \"\"\"\n    dot(a, b, out=None)\n\n    Dot product of two arrays. Specifically,\n\n    - If both `a` and `b` are 1-D arrays, it is inner product of vectors\n      (without complex conjugation).\n\n    - If both `a` and `b` are 2-D arrays, it is matrix multiplication,\n      but using :func:`matmul` or ``a @ b`` is preferred.\n\n    - If either `a` or `b` is 0-D (scalar), it is equivalent to :func:`multiply`\n      and using ``numpy.multiply(a, b)`` or ``a * b`` is preferred.\n\n    - If `a` is an N-D array and `b` is a 1-D array, it is a sum product over\n      the last axis of `a` and `b`.\n\n    - If `a` is an N-D array and `b` is an M-D array (where ``M>=2``), it is a\n      sum product over the last axis of `a` and the second-to-last axis of `b`::\n\n        dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n\n    Parameters\n    ----------\n    a : array_like\n        First argument.\n    b : array_like\n        Second argument.\n    out : ndarray, optional\n        Output argument. This must have the exact kind that would be returned\n        if it was not used. In particular, it must have the right type, must be\n        C-contiguous, and its dtype must be the dtype that would be returned\n        for `dot(a,b)`. This is a performance feature. Therefore, if these\n        conditions are not met, an exception is raised, instead of attempting\n        to be flexible.\n\n    Returns\n    -------\n    output : ndarray\n        Returns the dot product of `a` and `b`.  If `a` and `b` are both\n        scalars or both 1-D arrays then a scalar is returned; otherwise\n        an array is returned.\n        If `out` is given, then it is returned.\n\n    Raises\n    ------\n    ValueError\n        If the last dimension of `a` is not the same size as\n        the second-to-last dimension of `b`.\n\n    See Also\n    --------\n    vdot : Complex-conjugating dot product.\n    tensordot : Sum products over arbitrary axes.\n    einsum : Einstein summation convention.\n    matmul : '@' operator as method with out parameter.\n    linalg.multi_dot : Chained dot product.\n\n    Examples\n    --------\n    >>> np.dot(3, 4)\n    12\n\n    Neither argument is complex-conjugated:\n\n    >>> np.dot([2j, 3j], [2j, 3j])\n    (-13+0j)\n\n    For 2-D arrays it is the matrix product:\n\n    >>> a = [[1, 0], [0, 1]]\n    >>> b = [[4, 1], [2, 2]]\n    >>> np.dot(a, b)\n    array([[4, 1],\n           [2, 2]])\n\n    >>> a = np.arange(3*4*5*6).reshape((3,4,5,6))\n    >>> b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))\n    >>> np.dot(a, b)[2,3,2,1,2,2]\n    499128\n    >>> sum(a[2,3,2,:] * b[1,2,:,2])\n    499128\n\n    \"\"\"\n    return (a, b, out)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.vdot)\ndef vdot(a, b):\n    \"\"\"\n    vdot(a, b)\n\n    Return the dot product of two vectors.\n\n    The vdot(`a`, `b`) function handles complex numbers differently than\n    dot(`a`, `b`).  If the first argument is complex the complex conjugate\n    of the first argument is used for the calculation of the dot product.\n\n    Note that `vdot` handles multidimensional arrays differently than `dot`:\n    it does *not* perform a matrix product, but flattens input arguments\n    to 1-D vectors first. Consequently, it should only be used for vectors.\n\n    Parameters\n    ----------\n    a : array_like\n        If `a` is complex the complex conjugate is taken before calculation\n        of the dot product.\n    b : array_like\n        Second argument to the dot product.\n\n    Returns\n    -------\n    output : ndarray\n        Dot product of `a` and `b`.  Can be an int, float, or\n        complex depending on the types of `a` and `b`.\n\n    See Also\n    --------\n    dot : Return the dot product without using the complex conjugate of the\n          first argument.\n\n    Examples\n    --------\n    >>> a = np.array([1+2j,3+4j])\n    >>> b = np.array([5+6j,7+8j])\n    >>> np.vdot(a, b)\n    (70-8j)\n    >>> np.vdot(b, a)\n    (70+8j)\n\n    Note that higher-dimensional arrays are flattened!\n\n    >>> a = np.array([[1, 4], [5, 6]])\n    >>> b = np.array([[4, 1], [2, 2]])\n    >>> np.vdot(a, b)\n    30\n    >>> np.vdot(b, a)\n    30\n    >>> 1*4 + 4*1 + 5*2 + 6*2\n    30\n\n    \"\"\"\n    return (a, b)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.bincount)\ndef bincount(x, weights=None, minlength=None):\n    \"\"\"\n    bincount(x, weights=None, minlength=0)\n\n    Count number of occurrences of each value in array of non-negative ints.\n\n    The number of bins (of size 1) is one larger than the largest value in\n    `x`. If `minlength` is specified, there will be at least this number\n    of bins in the output array (though it will be longer if necessary,\n    depending on the contents of `x`).\n    Each bin gives the number of occurrences of its index value in `x`.\n    If `weights` is specified the input array is weighted by it, i.e. if a\n    value ``n`` is found at position ``i``, ``out[n] += weight[i]`` instead\n    of ``out[n] += 1``.\n\n    Parameters\n    ----------\n    x : array_like, 1 dimension, nonnegative ints\n        Input array.\n    weights : array_like, optional\n        Weights, array of the same shape as `x`.\n    minlength : int, optional\n        A minimum number of bins for the output array.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    out : ndarray of ints\n        The result of binning the input array.\n        The length of `out` is equal to ``np.amax(x)+1``.\n\n    Raises\n    ------\n    ValueError\n        If the input is not 1-dimensional, or contains elements with negative\n        values, or if `minlength` is negative.\n    TypeError\n        If the type of the input is float or complex.\n\n    See Also\n    --------\n    histogram, digitize, unique\n\n    Examples\n    --------\n    >>> np.bincount(np.arange(5))\n    array([1, 1, 1, 1, 1])\n    >>> np.bincount(np.array([0, 1, 1, 3, 2, 1, 7]))\n    array([1, 3, 1, 1, 0, 0, 0, 1])\n\n    >>> x = np.array([0, 1, 1, 3, 2, 1, 7, 23])\n    >>> np.bincount(x).size == np.amax(x)+1\n    True\n\n    The input array needs to be of integer dtype, otherwise a\n    TypeError is raised:\n\n    >>> np.bincount(np.arange(5, dtype=float))\n    Traceback (most recent call last):\n      ...\n    TypeError: Cannot cast array data from dtype('float64') to dtype('int64')\n    according to the rule 'safe'\n\n    A possible use of ``bincount`` is to perform sums over\n    variable-size chunks of an array, using the ``weights`` keyword.\n\n    >>> w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights\n    >>> x = np.array([0, 1, 1, 2, 2, 2])\n    >>> np.bincount(x,  weights=w)\n    array([ 0.3,  0.7,  1.1])\n\n    \"\"\"\n    return (x, weights)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.ravel_multi_index)\ndef ravel_multi_index(multi_index, dims, mode=None, order=None):\n    \"\"\"\n    ravel_multi_index(multi_index, dims, mode='raise', order='C')\n\n    Converts a tuple of index arrays into an array of flat\n    indices, applying boundary modes to the multi-index.\n\n    Parameters\n    ----------\n    multi_index : tuple of array_like\n        A tuple of integer arrays, one array for each dimension.\n    dims : tuple of ints\n        The shape of array into which the indices from ``multi_index`` apply.\n    mode : {'raise', 'wrap', 'clip'}, optional\n        Specifies how out-of-bounds indices are handled.  Can specify\n        either one mode or a tuple of modes, one mode per index.\n\n        * 'raise' -- raise an error (default)\n        * 'wrap' -- wrap around\n        * 'clip' -- clip to the range\n\n        In 'clip' mode, a negative index which would normally\n        wrap will clip to 0 instead.\n    order : {'C', 'F'}, optional\n        Determines whether the multi-index should be viewed as\n        indexing in row-major (C-style) or column-major\n        (Fortran-style) order.\n\n    Returns\n    -------\n    raveled_indices : ndarray\n        An array of indices into the flattened version of an array\n        of dimensions ``dims``.\n\n    See Also\n    --------\n    unravel_index\n\n    Notes\n    -----\n    .. versionadded:: 1.6.0\n\n    Examples\n    --------\n    >>> arr = np.array([[3,6,6],[4,5,1]])\n    >>> np.ravel_multi_index(arr, (7,6))\n    array([22, 41, 37])\n    >>> np.ravel_multi_index(arr, (7,6), order='F')\n    array([31, 41, 13])\n    >>> np.ravel_multi_index(arr, (4,6), mode='clip')\n    array([22, 23, 19])\n    >>> np.ravel_multi_index(arr, (4,4), mode=('clip','wrap'))\n    array([12, 13, 13])\n\n    >>> np.ravel_multi_index((3,1,4,1), (6,7,8,9))\n    1621\n    \"\"\"\n    return multi_index\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.unravel_index)\ndef unravel_index(indices, shape=None, order=None, dims=None):\n    \"\"\"\n    unravel_index(indices, shape, order='C')\n\n    Converts a flat index or array of flat indices into a tuple\n    of coordinate arrays.\n\n    Parameters\n    ----------\n    indices : array_like\n        An integer array whose elements are indices into the flattened\n        version of an array of dimensions ``shape``. Before version 1.6.0,\n        this function accepted just one index value.\n    shape : tuple of ints\n        The shape of the array to use for unraveling ``indices``.\n\n        .. versionchanged:: 1.16.0\n            Renamed from ``dims`` to ``shape``.\n\n    order : {'C', 'F'}, optional\n        Determines whether the indices should be viewed as indexing in\n        row-major (C-style) or column-major (Fortran-style) order.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    unraveled_coords : tuple of ndarray\n        Each array in the tuple has the same shape as the ``indices``\n        array.\n\n    See Also\n    --------\n    ravel_multi_index\n\n    Examples\n    --------\n    >>> np.unravel_index([22, 41, 37], (7,6))\n    (array([3, 6, 6]), array([4, 5, 1]))\n    >>> np.unravel_index([31, 41, 13], (7,6), order='F')\n    (array([3, 6, 6]), array([4, 5, 1]))\n\n    >>> np.unravel_index(1621, (6,7,8,9))\n    (3, 1, 4, 1)\n\n    \"\"\"\n    if dims is not None:\n        warnings.warn(\"'shape' argument should be used instead of 'dims'\",\n                      DeprecationWarning, stacklevel=3)\n    return (indices,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.copyto)\ndef copyto(dst, src, casting=None, where=None):\n    \"\"\"\n    copyto(dst, src, casting='same_kind', where=True)\n\n    Copies values from one array to another, broadcasting as necessary.\n\n    Raises a TypeError if the `casting` rule is violated, and if\n    `where` is provided, it selects which elements to copy.\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    ----------\n    dst : ndarray\n        The array into which values are copied.\n    src : array_like\n        The array from which values are copied.\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n        Controls what kind of data casting may occur when copying.\n\n          * 'no' means the data types should not be cast at all.\n          * 'equiv' means only byte-order changes are allowed.\n          * 'safe' means only casts which can preserve values are allowed.\n          * 'same_kind' means only safe casts or casts within a kind,\n            like float64 to float32, are allowed.\n          * 'unsafe' means any data conversions may be done.\n    where : array_like of bool, optional\n        A boolean array which is broadcasted to match the dimensions\n        of `dst`, and selects elements to copy from `src` to `dst`\n        wherever it contains the value True.\n    \"\"\"\n    return (dst, src, where)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.putmask)\ndef putmask(a, mask, values):\n    \"\"\"\n    putmask(a, mask, values)\n\n    Changes elements of an array based on conditional and input values.\n\n    Sets ``a.flat[n] = values[n]`` for each n where ``mask.flat[n]==True``.\n\n    If `values` is not the same size as `a` and `mask` then it will repeat.\n    This gives behavior different from ``a[mask] = values``.\n\n    Parameters\n    ----------\n    a : ndarray\n        Target array.\n    mask : array_like\n        Boolean mask array. It has to be the same shape as `a`.\n    values : array_like\n        Values to put into `a` where `mask` is True. If `values` is smaller\n        than `a` it will be repeated.\n\n    See Also\n    --------\n    place, put, take, copyto\n\n    Examples\n    --------\n    >>> x = np.arange(6).reshape(2, 3)\n    >>> np.putmask(x, x>2, x**2)\n    >>> x\n    array([[ 0,  1,  2],\n           [ 9, 16, 25]])\n\n    If `values` is smaller than `a` it is repeated:\n\n    >>> x = np.arange(5)\n    >>> np.putmask(x, x>1, [-33, -44])\n    >>> x\n    array([  0,   1, -33, -44, -33])\n\n    \"\"\"\n    return (a, mask, values)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.packbits)\ndef packbits(a, axis=None, bitorder='big'):\n    \"\"\"\n    packbits(a, axis=None, bitorder='big')\n\n    Packs the elements of a binary-valued array into bits in a uint8 array.\n\n    The result is padded to full bytes by inserting zero bits at the end.\n\n    Parameters\n    ----------\n    a : array_like\n        An array of integers or booleans whose elements should be packed to\n        bits.\n    axis : int, optional\n        The dimension over which bit-packing is done.\n        ``None`` implies packing the flattened array.\n    bitorder : {'big', 'little'}, optional\n        The order of the input bits. 'big' will mimic bin(val),\n        ``[0, 0, 0, 0, 0, 0, 1, 1] => 3 = 0b00000011``, 'little' will\n        reverse the order so ``[1, 1, 0, 0, 0, 0, 0, 0] => 3``.\n        Defaults to 'big'.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    packed : ndarray\n        Array of type uint8 whose elements represent bits corresponding to the\n        logical (0 or nonzero) value of the input elements. The shape of\n        `packed` has the same number of dimensions as the input (unless `axis`\n        is None, in which case the output is 1-D).\n\n    See Also\n    --------\n    unpackbits: Unpacks elements of a uint8 array into a binary-valued output\n                array.\n\n    Examples\n    --------\n    >>> a = np.array([[[1,0,1],\n    ...                [0,1,0]],\n    ...               [[1,1,0],\n    ...                [0,0,1]]])\n    >>> b = np.packbits(a, axis=-1)\n    >>> b\n    array([[[160],\n            [ 64]],\n           [[192],\n            [ 32]]], dtype=uint8)\n\n    Note that in binary 160 = 1010 0000, 64 = 0100 0000, 192 = 1100 0000,\n    and 32 = 0010 0000.\n\n    \"\"\"\n    return (a,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.unpackbits)\ndef unpackbits(a, axis=None, count=None, bitorder='big'):\n    \"\"\"\n    unpackbits(a, axis=None, count=None, bitorder='big')\n\n    Unpacks elements of a uint8 array into a binary-valued output array.\n\n    Each element of `a` represents a bit-field that should be unpacked\n    into a binary-valued output array. The shape of the output array is\n    either 1-D (if `axis` is ``None``) or the same shape as the input\n    array with unpacking done along the axis specified.\n\n    Parameters\n    ----------\n    a : ndarray, uint8 type\n       Input array.\n    axis : int, optional\n        The dimension over which bit-unpacking is done.\n        ``None`` implies unpacking the flattened array.\n    count : int or None, optional\n        The number of elements to unpack along `axis`, provided as a way\n        of undoing the effect of packing a size that is not a multiple\n        of eight. A non-negative number means to only unpack `count`\n        bits. A negative number means to trim off that many bits from\n        the end. ``None`` means to unpack the entire array (the\n        default). Counts larger than the available number of bits will\n        add zero padding to the output. Negative counts must not\n        exceed the available number of bits.\n\n        .. versionadded:: 1.17.0\n\n    bitorder : {'big', 'little'}, optional\n        The order of the returned bits. 'big' will mimic bin(val),\n        ``3 = 0b00000011 => [0, 0, 0, 0, 0, 0, 1, 1]``, 'little' will reverse\n        the order to ``[1, 1, 0, 0, 0, 0, 0, 0]``.\n        Defaults to 'big'.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    unpacked : ndarray, uint8 type\n       The elements are binary-valued (0 or 1).\n\n    See Also\n    --------\n    packbits : Packs the elements of a binary-valued array into bits in\n               a uint8 array.\n\n    Examples\n    --------\n    >>> a = np.array([[2], [7], [23]], dtype=np.uint8)\n    >>> a\n    array([[ 2],\n           [ 7],\n           [23]], dtype=uint8)\n    >>> b = np.unpackbits(a, axis=1)\n    >>> b\n    array([[0, 0, 0, 0, 0, 0, 1, 0],\n           [0, 0, 0, 0, 0, 1, 1, 1],\n           [0, 0, 0, 1, 0, 1, 1, 1]], dtype=uint8)\n    >>> c = np.unpackbits(a, axis=1, count=-3)\n    >>> c\n    array([[0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0],\n           [0, 0, 0, 1, 0]], dtype=uint8)\n\n    >>> p = np.packbits(b, axis=0)\n    >>> np.unpackbits(p, axis=0)\n    array([[0, 0, 0, 0, 0, 0, 1, 0],\n           [0, 0, 0, 0, 0, 1, 1, 1],\n           [0, 0, 0, 1, 0, 1, 1, 1],\n           [0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n    >>> np.array_equal(b, np.unpackbits(p, axis=0, count=b.shape[0]))\n    True\n\n    \"\"\"\n    return (a,)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.shares_memory)\ndef shares_memory(a, b, max_work=None):\n    \"\"\"\n    shares_memory(a, b, max_work=None)\n\n    Determine if two arrays share memory.\n\n    .. warning::\n\n       This function can be exponentially slow for some inputs, unless\n       `max_work` is set to a finite number or ``MAY_SHARE_BOUNDS``.\n       If in doubt, use `numpy.may_share_memory` instead.\n\n    Parameters\n    ----------\n    a, b : ndarray\n        Input arrays\n    max_work : int, optional\n        Effort to spend on solving the overlap problem (maximum number\n        of candidate solutions to consider). The following special\n        values are recognized:\n\n        max_work=MAY_SHARE_EXACT  (default)\n            The problem is solved exactly. In this case, the function returns\n            True only if there is an element shared between the arrays. Finding\n            the exact solution may take extremely long in some cases.\n        max_work=MAY_SHARE_BOUNDS\n            Only the memory bounds of a and b are checked.\n\n    Raises\n    ------\n    numpy.TooHardError\n        Exceeded max_work.\n\n    Returns\n    -------\n    out : bool\n\n    See Also\n    --------\n    may_share_memory\n\n    Examples\n    --------\n    >>> x = np.array([1, 2, 3, 4])\n    >>> np.shares_memory(x, np.array([5, 6, 7]))\n    False\n    >>> np.shares_memory(x[::2], x)\n    True\n    >>> np.shares_memory(x[::2], x[1::2])\n    False\n\n    Checking whether two arrays share memory is NP-complete, and\n    runtime may increase exponentially in the number of\n    dimensions. Hence, `max_work` should generally be set to a finite\n    number, as it is possible to construct examples that take\n    extremely long to run:\n\n    >>> from numpy.lib.stride_tricks import as_strided\n    >>> x = np.zeros([192163377], dtype=np.int8)\n    >>> x1 = as_strided(x, strides=(36674, 61119, 85569), shape=(1049, 1049, 1049))\n    >>> x2 = as_strided(x[64023025:], strides=(12223, 12224, 1), shape=(1049, 1049, 1))\n    >>> np.shares_memory(x1, x2, max_work=1000)\n    Traceback (most recent call last):\n    ...\n    numpy.TooHardError: Exceeded max_work\n\n    Running ``np.shares_memory(x1, x2)`` without `max_work` set takes\n    around 1 minute for this case. It is possible to find problems\n    that take still significantly longer.\n\n    \"\"\"\n    return (a, b)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.may_share_memory)\ndef may_share_memory(a, b, max_work=None):\n    \"\"\"\n    may_share_memory(a, b, max_work=None)\n\n    Determine if two arrays might share memory\n\n    A return of True does not necessarily mean that the two arrays\n    share any element.  It just means that they *might*.\n\n    Only the memory bounds of a and b are checked by default.\n\n    Parameters\n    ----------\n    a, b : ndarray\n        Input arrays\n    max_work : int, optional\n        Effort to spend on solving the overlap problem.  See\n        `shares_memory` for details.  Default for ``may_share_memory``\n        is to do a bounds check.\n\n    Returns\n    -------\n    out : bool\n\n    See Also\n    --------\n    shares_memory\n\n    Examples\n    --------\n    >>> np.may_share_memory(np.array([1,2]), np.array([5,8,9]))\n    False\n    >>> x = np.zeros([3, 4])\n    >>> np.may_share_memory(x[:,0], x[:,1])\n    True\n\n    \"\"\"\n    return (a, b)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.is_busday)\ndef is_busday(dates, weekmask=None, holidays=None, busdaycal=None, out=None):\n    \"\"\"\n    is_busday(dates, weekmask='1111100', holidays=None, busdaycal=None, out=None)\n\n    Calculates which of the given dates are valid days, and which are not.\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    ----------\n    dates : array_like of datetime64[D]\n        The array of dates to process.\n    weekmask : str or array_like of bool, optional\n        A seven-element array indicating which of Monday through Sunday are\n        valid days. May be specified as a length-seven list or array, like\n        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n        weekdays, optionally separated by white space. Valid abbreviations\n        are: Mon Tue Wed Thu Fri Sat Sun\n    holidays : array_like of datetime64[D], optional\n        An array of dates to consider as invalid dates.  They may be\n        specified in any order, and NaT (not-a-time) dates are ignored.\n        This list is saved in a normalized form that is suited for\n        fast calculations of valid days.\n    busdaycal : busdaycalendar, optional\n        A `busdaycalendar` object which specifies the valid days. If this\n        parameter is provided, neither weekmask nor holidays may be\n        provided.\n    out : array of bool, optional\n        If provided, this array is filled with the result.\n\n    Returns\n    -------\n    out : array of bool\n        An array with the same shape as ``dates``, containing True for\n        each valid day, and False for each invalid day.\n\n    See Also\n    --------\n    busdaycalendar: An object that specifies a custom set of valid days.\n    busday_offset : Applies an offset counted in valid days.\n    busday_count : Counts how many valid days are in a half-open date range.\n\n    Examples\n    --------\n    >>> # The weekdays are Friday, Saturday, and Monday\n    ... np.is_busday(['2011-07-01', '2011-07-02', '2011-07-18'],\n    ...                 holidays=['2011-07-01', '2011-07-04', '2011-07-17'])\n    array([False, False,  True])\n    \"\"\"\n    return (dates, weekmask, holidays, out)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.busday_offset)\ndef busday_offset(dates, offsets, roll=None, weekmask=None, holidays=None,\n                  busdaycal=None, out=None):\n    \"\"\"\n    busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)\n\n    First adjusts the date to fall on a valid day according to\n    the ``roll`` rule, then applies offsets to the given dates\n    counted in valid days.\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    ----------\n    dates : array_like of datetime64[D]\n        The array of dates to process.\n    offsets : array_like of int\n        The array of offsets, which is broadcast with ``dates``.\n    roll : {'raise', 'nat', 'forward', 'following', 'backward', 'preceding', 'modifiedfollowing', 'modifiedpreceding'}, optional\n        How to treat dates that do not fall on a valid day. The default\n        is 'raise'.\n\n          * 'raise' means to raise an exception for an invalid day.\n          * 'nat' means to return a NaT (not-a-time) for an invalid day.\n          * 'forward' and 'following' mean to take the first valid day\n            later in time.\n          * 'backward' and 'preceding' mean to take the first valid day\n            earlier in time.\n          * 'modifiedfollowing' means to take the first valid day\n            later in time unless it is across a Month boundary, in which\n            case to take the first valid day earlier in time.\n          * 'modifiedpreceding' means to take the first valid day\n            earlier in time unless it is across a Month boundary, in which\n            case to take the first valid day later in time.\n    weekmask : str or array_like of bool, optional\n        A seven-element array indicating which of Monday through Sunday are\n        valid days. May be specified as a length-seven list or array, like\n        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n        weekdays, optionally separated by white space. Valid abbreviations\n        are: Mon Tue Wed Thu Fri Sat Sun\n    holidays : array_like of datetime64[D], optional\n        An array of dates to consider as invalid dates.  They may be\n        specified in any order, and NaT (not-a-time) dates are ignored.\n        This list is saved in a normalized form that is suited for\n        fast calculations of valid days.\n    busdaycal : busdaycalendar, optional\n        A `busdaycalendar` object which specifies the valid days. If this\n        parameter is provided, neither weekmask nor holidays may be\n        provided.\n    out : array of datetime64[D], optional\n        If provided, this array is filled with the result.\n\n    Returns\n    -------\n    out : array of datetime64[D]\n        An array with a shape from broadcasting ``dates`` and ``offsets``\n        together, containing the dates with offsets applied.\n\n    See Also\n    --------\n    busdaycalendar: An object that specifies a custom set of valid days.\n    is_busday : Returns a boolean array indicating valid days.\n    busday_count : Counts how many valid days are in a half-open date range.\n\n    Examples\n    --------\n    >>> # First business day in October 2011 (not accounting for holidays)\n    ... np.busday_offset('2011-10', 0, roll='forward')\n    numpy.datetime64('2011-10-03')\n    >>> # Last business day in February 2012 (not accounting for holidays)\n    ... np.busday_offset('2012-03', -1, roll='forward')\n    numpy.datetime64('2012-02-29')\n    >>> # Third Wednesday in January 2011\n    ... np.busday_offset('2011-01', 2, roll='forward', weekmask='Wed')\n    numpy.datetime64('2011-01-19')\n    >>> # 2012 Mother's Day in Canada and the U.S.\n    ... np.busday_offset('2012-05', 1, roll='forward', weekmask='Sun')\n    numpy.datetime64('2012-05-13')\n\n    >>> # First business day on or after a date\n    ... np.busday_offset('2011-03-20', 0, roll='forward')\n    numpy.datetime64('2011-03-21')\n    >>> np.busday_offset('2011-03-22', 0, roll='forward')\n    numpy.datetime64('2011-03-22')\n    >>> # First business day after a date\n    ... np.busday_offset('2011-03-20', 1, roll='backward')\n    numpy.datetime64('2011-03-21')\n    >>> np.busday_offset('2011-03-22', 1, roll='backward')\n    numpy.datetime64('2011-03-23')\n    \"\"\"\n    return (dates, offsets, weekmask, holidays, out)\n\n\n@array_function_from_c_func_and_dispatcher(_multiarray_umath.busday_count)\ndef busday_count(begindates, enddates, weekmask=None, holidays=None,\n                 busdaycal=None, out=None):\n    \"\"\"\n    busday_count(begindates, enddates, weekmask='1111100', holidays=[], busdaycal=None, out=None)\n\n    Counts the number of valid days between `begindates` and\n    `enddates`, not including the day of `enddates`.\n\n    If ``enddates`` specifies a date value that is earlier than the\n    corresponding ``begindates`` date value, the count will be negative.\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    ----------\n    begindates : array_like of datetime64[D]\n        The array of the first dates for counting.\n    enddates : array_like of datetime64[D]\n        The array of the end dates for counting, which are excluded\n        from the count themselves.\n    weekmask : str or array_like of bool, optional\n        A seven-element array indicating which of Monday through Sunday are\n        valid days. May be specified as a length-seven list or array, like\n        [1,1,1,1,1,0,0]; a length-seven string, like '1111100'; or a string\n        like \"Mon Tue Wed Thu Fri\", made up of 3-character abbreviations for\n        weekdays, optionally separated by white space. Valid abbreviations\n        are: Mon Tue Wed Thu Fri Sat Sun\n    holidays : array_like of datetime64[D], optional\n        An array of dates to consider as invalid dates.  They may be\n        specified in any order, and NaT (not-a-time) dates are ignored.\n        This list is saved in a normalized form that is suited for\n        fast calculations of valid days.\n    busdaycal : busdaycalendar, optional\n        A `busdaycalendar` object which specifies the valid days. If this\n        parameter is provided, neither weekmask nor holidays may be\n        provided.\n    out : array of int, optional\n        If provided, this array is filled with the result.\n\n    Returns\n    -------\n    out : array of int\n        An array with a shape from broadcasting ``begindates`` and ``enddates``\n        together, containing the number of valid days between\n        the begin and end dates.\n\n    See Also\n    --------\n    busdaycalendar: An object that specifies a custom set of valid days.\n    is_busday : Returns a boolean array indicating valid days.\n    busday_offset : Applies an offset counted in valid days.\n\n    Examples\n    --------\n    >>> # Number of weekdays in January 2011\n    ... np.busday_count('2011-01', '2011-02')\n    21\n    >>> # Number of weekdays in 2011\n    >>> np.busday_count('2011', '2012')\n    260\n    >>> # Number of Saturdays in 2011\n    ... np.busday_count('2011', '2012', weekmask='Sat')\n    53\n    \"\"\"\n    return (begindates, enddates, weekmask, holidays, out)\n\n\n@array_function_from_c_func_and_dispatcher(\n    _multiarray_umath.datetime_as_string)\ndef datetime_as_string(arr, unit=None, timezone=None, casting=None):\n    \"\"\"\n    datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind')\n\n    Convert an array of datetimes into an array of strings.\n\n    Parameters\n    ----------\n    arr : array_like of datetime64\n        The array of UTC timestamps to format.\n    unit : str\n        One of None, 'auto', or a :ref:`datetime unit <arrays.dtypes.dateunits>`.\n    timezone : {'naive', 'UTC', 'local'} or tzinfo\n        Timezone information to use when displaying the datetime. If 'UTC', end\n        with a Z to indicate UTC time. If 'local', convert to the local timezone\n        first, and suffix with a +-#### timezone offset. If a tzinfo object,\n        then do as with 'local', but use the specified timezone.\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}\n        Casting to allow when changing between datetime units.\n\n    Returns\n    -------\n    str_arr : ndarray\n        An array of strings the same shape as `arr`.\n\n    Examples\n    --------\n    >>> import pytz\n    >>> d = np.arange('2002-10-27T04:30', 4*60, 60, dtype='M8[m]')\n    >>> d\n    array(['2002-10-27T04:30', '2002-10-27T05:30', '2002-10-27T06:30',\n           '2002-10-27T07:30'], dtype='datetime64[m]')\n\n    Setting the timezone to UTC shows the same information, but with a Z suffix\n\n    >>> np.datetime_as_string(d, timezone='UTC')\n    array(['2002-10-27T04:30Z', '2002-10-27T05:30Z', '2002-10-27T06:30Z',\n           '2002-10-27T07:30Z'], dtype='<U35')\n\n    Note that we picked datetimes that cross a DST boundary. Passing in a\n    ``pytz`` timezone object will print the appropriate offset\n\n    >>> np.datetime_as_string(d, timezone=pytz.timezone('US/Eastern'))\n    array(['2002-10-27T00:30-0400', '2002-10-27T01:30-0400',\n           '2002-10-27T01:30-0500', '2002-10-27T02:30-0500'], dtype='<U39')\n\n    Passing in a unit will change the precision\n\n    >>> np.datetime_as_string(d, unit='h')\n    array(['2002-10-27T04', '2002-10-27T05', '2002-10-27T06', '2002-10-27T07'],\n          dtype='<U32')\n    >>> np.datetime_as_string(d, unit='s')\n    array(['2002-10-27T04:30:00', '2002-10-27T05:30:00', '2002-10-27T06:30:00',\n           '2002-10-27T07:30:00'], dtype='<U38')\n\n    'casting' can be used to specify whether precision can be changed\n\n    >>> np.datetime_as_string(d, unit='h', casting='safe')\n    Traceback (most recent call last):\n        ...\n    TypeError: Cannot create a datetime string as units 'h' from a NumPy\n    datetime with units 'm' according to the rule 'safe'\n    \"\"\"\n    return (arr,)\n",1673],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py":["import functools\nimport itertools\nimport operator\nimport sys\nimport warnings\nimport numbers\n\nimport numpy as np\nfrom . import multiarray\nfrom .multiarray import (\n    _fastCopyAndTranspose as fastCopyAndTranspose, ALLOW_THREADS,\n    BUFSIZE, CLIP, MAXDIMS, MAY_SHARE_BOUNDS, MAY_SHARE_EXACT, RAISE,\n    WRAP, arange, array, broadcast, can_cast, compare_chararrays,\n    concatenate, copyto, dot, dtype, empty,\n    empty_like, flatiter, frombuffer, fromfile, fromiter, fromstring,\n    inner, lexsort, matmul, may_share_memory,\n    min_scalar_type, ndarray, nditer, nested_iters, promote_types,\n    putmask, result_type, set_numeric_ops, shares_memory, vdot, where,\n    zeros, normalize_axis_index)\n\nfrom . import overrides\nfrom . import umath\nfrom . import shape_base\nfrom .overrides import set_array_function_like_doc, set_module\nfrom .umath import (multiply, invert, sin, PINF, NAN)\nfrom . import numerictypes\nfrom .numerictypes import longlong, intc, int_, float_, complex_, bool_\nfrom ._exceptions import TooHardError, AxisError\nfrom ._asarray import asarray, asanyarray\nfrom ._ufunc_config import errstate\n\nbitwise_not = invert\nufunc = type(sin)\nnewaxis = None\n\narray_function_dispatch = functools.partial(\n    overrides.array_function_dispatch, module='numpy')\n\n\n__all__ = [\n    'newaxis', 'ndarray', 'flatiter', 'nditer', 'nested_iters', 'ufunc',\n    'arange', 'array', 'zeros', 'count_nonzero', 'empty', 'broadcast', 'dtype',\n    'fromstring', 'fromfile', 'frombuffer', 'where',\n    'argwhere', 'copyto', 'concatenate', 'fastCopyAndTranspose', 'lexsort',\n    'set_numeric_ops', 'can_cast', 'promote_types', 'min_scalar_type',\n    'result_type', 'isfortran', 'empty_like', 'zeros_like', 'ones_like',\n    'correlate', 'convolve', 'inner', 'dot', 'outer', 'vdot', 'roll',\n    'rollaxis', 'moveaxis', 'cross', 'tensordot', 'little_endian',\n    'fromiter', 'array_equal', 'array_equiv', 'indices', 'fromfunction',\n    'isclose', 'isscalar', 'binary_repr', 'base_repr', 'ones',\n    'identity', 'allclose', 'compare_chararrays', 'putmask',\n    'flatnonzero', 'Inf', 'inf', 'infty', 'Infinity', 'nan', 'NaN',\n    'False_', 'True_', 'bitwise_not', 'CLIP', 'RAISE', 'WRAP', 'MAXDIMS',\n    'BUFSIZE', 'ALLOW_THREADS', 'ComplexWarning', 'full', 'full_like',\n    'matmul', 'shares_memory', 'may_share_memory', 'MAY_SHARE_BOUNDS',\n    'MAY_SHARE_EXACT', 'TooHardError', 'AxisError']\n\n\n@set_module('numpy')\nclass ComplexWarning(RuntimeWarning):\n    \"\"\"\n    The warning raised when casting a complex dtype to a real dtype.\n\n    As implemented, casting a complex number to a real discards its imaginary\n    part, but this behavior may not be what the user actually wants.\n\n    \"\"\"\n    pass\n\n\ndef _zeros_like_dispatcher(a, dtype=None, order=None, subok=None, shape=None):\n    return (a,)\n\n\n@array_function_dispatch(_zeros_like_dispatcher)\ndef zeros_like(a, dtype=None, order='K', subok=True, shape=None):\n    \"\"\"\n    Return an array of zeros with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of `a` define these same attributes of\n        the returned array.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n\n        .. versionadded:: 1.6.0\n    order : {'C', 'F', 'A', or 'K'}, optional\n        Overrides the memory layout of the result. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n        'C' otherwise. 'K' means match the layout of `a` as closely\n        as possible.\n\n        .. versionadded:: 1.6.0\n    subok : bool, optional.\n        If True, then the newly created array will use the sub-class\n        type of `a`, otherwise it will be a base-class array. Defaults\n        to True.\n    shape : int or sequence of ints, optional.\n        Overrides the shape of the result. If order='K' and the number of\n        dimensions is unchanged, will try to keep order, otherwise,\n        order='C' is implied.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of zeros with the same shape and type as `a`.\n\n    See Also\n    --------\n    empty_like : Return an empty array with shape and type of input.\n    ones_like : Return an array of ones with shape and type of input.\n    full_like : Return a new array with shape of input filled with value.\n    zeros : Return a new array setting values to zero.\n\n    Examples\n    --------\n    >>> x = np.arange(6)\n    >>> x = x.reshape((2, 3))\n    >>> x\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.zeros_like(x)\n    array([[0, 0, 0],\n           [0, 0, 0]])\n\n    >>> y = np.arange(3, dtype=float)\n    >>> y\n    array([0., 1., 2.])\n    >>> np.zeros_like(y)\n    array([0.,  0.,  0.])\n\n    \"\"\"\n    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n    # needed instead of a 0 to get same result as zeros for for string dtypes\n    z = zeros(1, dtype=res.dtype)\n    multiarray.copyto(res, z, casting='unsafe')\n    return res\n\n\ndef _ones_dispatcher(shape, dtype=None, order=None, *, like=None):\n    return(like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef ones(shape, dtype=None, order='C', *, like=None):\n    \"\"\"\n    Return a new array of given shape and type, filled with ones.\n\n    Parameters\n    ----------\n    shape : int or sequence of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    dtype : data-type, optional\n        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n        `numpy.float64`.\n    order : {'C', 'F'}, optional, default: C\n        Whether to store multi-dimensional data in row-major\n        (C-style) or column-major (Fortran-style) order in\n        memory.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of ones with the given shape, dtype, and order.\n\n    See Also\n    --------\n    ones_like : Return an array of ones with shape and type of input.\n    empty : Return a new uninitialized array.\n    zeros : Return a new array setting values to zero.\n    full : Return a new array of given shape filled with value.\n\n\n    Examples\n    --------\n    >>> np.ones(5)\n    array([1., 1., 1., 1., 1.])\n\n    >>> np.ones((5,), dtype=int)\n    array([1, 1, 1, 1, 1])\n\n    >>> np.ones((2, 1))\n    array([[1.],\n           [1.]])\n\n    >>> s = (2,2)\n    >>> np.ones(s)\n    array([[1.,  1.],\n           [1.,  1.]])\n\n    \"\"\"\n    if like is not None:\n        return _ones_with_like(shape, dtype=dtype, order=order, like=like)\n\n    a = empty(shape, dtype, order)\n    multiarray.copyto(a, 1, casting='unsafe')\n    return a\n\n\n_ones_with_like = array_function_dispatch(\n    _ones_dispatcher\n)(ones)\n\n\ndef _ones_like_dispatcher(a, dtype=None, order=None, subok=None, shape=None):\n    return (a,)\n\n\n@array_function_dispatch(_ones_like_dispatcher)\ndef ones_like(a, dtype=None, order='K', subok=True, shape=None):\n    \"\"\"\n    Return an array of ones with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of `a` define these same attributes of\n        the returned array.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n\n        .. versionadded:: 1.6.0\n    order : {'C', 'F', 'A', or 'K'}, optional\n        Overrides the memory layout of the result. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n        'C' otherwise. 'K' means match the layout of `a` as closely\n        as possible.\n\n        .. versionadded:: 1.6.0\n    subok : bool, optional.\n        If True, then the newly created array will use the sub-class\n        type of `a`, otherwise it will be a base-class array. Defaults\n        to True.\n    shape : int or sequence of ints, optional.\n        Overrides the shape of the result. If order='K' and the number of\n        dimensions is unchanged, will try to keep order, otherwise,\n        order='C' is implied.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of ones with the same shape and type as `a`.\n\n    See Also\n    --------\n    empty_like : Return an empty array with shape and type of input.\n    zeros_like : Return an array of zeros with shape and type of input.\n    full_like : Return a new array with shape of input filled with value.\n    ones : Return a new array setting values to one.\n\n    Examples\n    --------\n    >>> x = np.arange(6)\n    >>> x = x.reshape((2, 3))\n    >>> x\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.ones_like(x)\n    array([[1, 1, 1],\n           [1, 1, 1]])\n\n    >>> y = np.arange(3, dtype=float)\n    >>> y\n    array([0., 1., 2.])\n    >>> np.ones_like(y)\n    array([1.,  1.,  1.])\n\n    \"\"\"\n    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n    multiarray.copyto(res, 1, casting='unsafe')\n    return res\n\n\ndef _full_dispatcher(shape, fill_value, dtype=None, order=None, *, like=None):\n    return(like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef full(shape, fill_value, dtype=None, order='C', *, like=None):\n    \"\"\"\n    Return a new array of given shape and type, filled with `fill_value`.\n\n    Parameters\n    ----------\n    shape : int or sequence of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    fill_value : scalar or array_like\n        Fill value.\n    dtype : data-type, optional\n        The desired data-type for the array  The default, None, means\n         `np.array(fill_value).dtype`.\n    order : {'C', 'F'}, optional\n        Whether to store multidimensional data in C- or Fortran-contiguous\n        (row- or column-wise) order in memory.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of `fill_value` with the given shape, dtype, and order.\n\n    See Also\n    --------\n    full_like : Return a new array with shape of input filled with value.\n    empty : Return a new uninitialized array.\n    ones : Return a new array setting values to one.\n    zeros : Return a new array setting values to zero.\n\n    Examples\n    --------\n    >>> np.full((2, 2), np.inf)\n    array([[inf, inf],\n           [inf, inf]])\n    >>> np.full((2, 2), 10)\n    array([[10, 10],\n           [10, 10]])\n\n    >>> np.full((2, 2), [1, 2])\n    array([[1, 2],\n           [1, 2]])\n\n    \"\"\"\n    if like is not None:\n        return _full_with_like(shape, fill_value, dtype=dtype, order=order, like=like)\n\n    if dtype is None:\n        fill_value = asarray(fill_value)\n        dtype = fill_value.dtype\n    a = empty(shape, dtype, order)\n    multiarray.copyto(a, fill_value, casting='unsafe')\n    return a\n\n\n_full_with_like = array_function_dispatch(\n    _full_dispatcher\n)(full)\n\n\ndef _full_like_dispatcher(a, fill_value, dtype=None, order=None, subok=None, shape=None):\n    return (a,)\n\n\n@array_function_dispatch(_full_like_dispatcher)\ndef full_like(a, fill_value, dtype=None, order='K', subok=True, shape=None):\n    \"\"\"\n    Return a full array with the same shape and type as a given array.\n\n    Parameters\n    ----------\n    a : array_like\n        The shape and data-type of `a` define these same attributes of\n        the returned array.\n    fill_value : scalar\n        Fill value.\n    dtype : data-type, optional\n        Overrides the data type of the result.\n    order : {'C', 'F', 'A', or 'K'}, optional\n        Overrides the memory layout of the result. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n        'C' otherwise. 'K' means match the layout of `a` as closely\n        as possible.\n    subok : bool, optional.\n        If True, then the newly created array will use the sub-class\n        type of `a`, otherwise it will be a base-class array. Defaults\n        to True.\n    shape : int or sequence of ints, optional.\n        Overrides the shape of the result. If order='K' and the number of\n        dimensions is unchanged, will try to keep order, otherwise,\n        order='C' is implied.\n\n        .. versionadded:: 1.17.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of `fill_value` with the same shape and type as `a`.\n\n    See Also\n    --------\n    empty_like : Return an empty array with shape and type of input.\n    ones_like : Return an array of ones with shape and type of input.\n    zeros_like : Return an array of zeros with shape and type of input.\n    full : Return a new array of given shape filled with value.\n\n    Examples\n    --------\n    >>> x = np.arange(6, dtype=int)\n    >>> np.full_like(x, 1)\n    array([1, 1, 1, 1, 1, 1])\n    >>> np.full_like(x, 0.1)\n    array([0, 0, 0, 0, 0, 0])\n    >>> np.full_like(x, 0.1, dtype=np.double)\n    array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n    >>> np.full_like(x, np.nan, dtype=np.double)\n    array([nan, nan, nan, nan, nan, nan])\n\n    >>> y = np.arange(6, dtype=np.double)\n    >>> np.full_like(y, 0.1)\n    array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n\n    \"\"\"\n    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)\n    multiarray.copyto(res, fill_value, casting='unsafe')\n    return res\n\n\ndef _count_nonzero_dispatcher(a, axis=None, *, keepdims=None):\n    return (a,)\n\n\n@array_function_dispatch(_count_nonzero_dispatcher)\ndef count_nonzero(a, axis=None, *, keepdims=False):\n    \"\"\"\n    Counts the number of non-zero values in the array ``a``.\n\n    The word \"non-zero\" is in reference to the Python 2.x\n    built-in method ``__nonzero__()`` (renamed ``__bool__()``\n    in Python 3.x) of Python objects that tests an object's\n    \"truthfulness\". For example, any number is considered\n    truthful if it is nonzero, whereas any string is considered\n    truthful if it is not the empty string. Thus, this function\n    (recursively) counts how many elements in ``a`` (and in\n    sub-arrays thereof) have their ``__nonzero__()`` or ``__bool__()``\n    method evaluated to ``True``.\n\n    Parameters\n    ----------\n    a : array_like\n        The array for which to count non-zeros.\n    axis : int or tuple, optional\n        Axis or tuple of axes along which to count non-zeros.\n        Default is None, meaning that non-zeros will be counted\n        along a flattened version of ``a``.\n\n        .. versionadded:: 1.12.0\n\n    keepdims : bool, optional\n        If this is set to True, the axes that are counted are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n        .. versionadded:: 1.19.0\n\n    Returns\n    -------\n    count : int or array of int\n        Number of non-zero values in the array along a given axis.\n        Otherwise, the total number of non-zero values in the array\n        is returned.\n\n    See Also\n    --------\n    nonzero : Return the coordinates of all the non-zero values.\n\n    Examples\n    --------\n    >>> np.count_nonzero(np.eye(4))\n    4\n    >>> a = np.array([[0, 1, 7, 0],\n    ...               [3, 0, 2, 19]])\n    >>> np.count_nonzero(a)\n    5\n    >>> np.count_nonzero(a, axis=0)\n    array([1, 1, 2, 1])\n    >>> np.count_nonzero(a, axis=1)\n    array([2, 3])\n    >>> np.count_nonzero(a, axis=1, keepdims=True)\n    array([[2],\n           [3]])\n    \"\"\"\n    if axis is None and not keepdims:\n        return multiarray.count_nonzero(a)\n\n    a = asanyarray(a)\n\n    # TODO: this works around .astype(bool) not working properly (gh-9847)\n    if np.issubdtype(a.dtype, np.character):\n        a_bool = a != a.dtype.type()\n    else:\n        a_bool = a.astype(np.bool_, copy=False)\n\n    return a_bool.sum(axis=axis, dtype=np.intp, keepdims=keepdims)\n\n\n@set_module('numpy')\ndef isfortran(a):\n    \"\"\"\n    Check if the array is Fortran contiguous but *not* C contiguous.\n\n    This function is obsolete and, because of changes due to relaxed stride\n    checking, its return value for the same array may differ for versions\n    of NumPy >= 1.10.0 and previous versions. If you only want to check if an\n    array is Fortran contiguous use ``a.flags.f_contiguous`` instead.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n\n    Returns\n    -------\n    isfortran : bool\n        Returns True if the array is Fortran contiguous but *not* C contiguous.\n\n\n    Examples\n    --------\n\n    np.array allows to specify whether the array is written in C-contiguous\n    order (last index varies the fastest), or FORTRAN-contiguous order in\n    memory (first index varies the fastest).\n\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(a)\n    False\n\n    >>> b = np.array([[1, 2, 3], [4, 5, 6]], order='F')\n    >>> b\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(b)\n    True\n\n\n    The transpose of a C-ordered array is a FORTRAN-ordered array.\n\n    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')\n    >>> a\n    array([[1, 2, 3],\n           [4, 5, 6]])\n    >>> np.isfortran(a)\n    False\n    >>> b = a.T\n    >>> b\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n    >>> np.isfortran(b)\n    True\n\n    C-ordered arrays evaluate as False even if they are also FORTRAN-ordered.\n\n    >>> np.isfortran(np.array([1, 2], order='F'))\n    False\n\n    \"\"\"\n    return a.flags.fnc\n\n\ndef _argwhere_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_argwhere_dispatcher)\ndef argwhere(a):\n    \"\"\"\n    Find the indices of array elements that are non-zero, grouped by element.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    Returns\n    -------\n    index_array : (N, a.ndim) ndarray\n        Indices of elements that are non-zero. Indices are grouped by element.\n        This array will have shape ``(N, a.ndim)`` where ``N`` is the number of\n        non-zero items.\n\n    See Also\n    --------\n    where, nonzero\n\n    Notes\n    -----\n    ``np.argwhere(a)`` is almost the same as ``np.transpose(np.nonzero(a))``,\n    but produces a result of the correct shape for a 0D array.\n\n    The output of ``argwhere`` is not suitable for indexing arrays.\n    For this purpose use ``nonzero(a)`` instead.\n\n    Examples\n    --------\n    >>> x = np.arange(6).reshape(2,3)\n    >>> x\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.argwhere(x>1)\n    array([[0, 2],\n           [1, 0],\n           [1, 1],\n           [1, 2]])\n\n    \"\"\"\n    # nonzero does not behave well on 0d, so promote to 1d\n    if np.ndim(a) == 0:\n        a = shape_base.atleast_1d(a)\n        # then remove the added dimension\n        return argwhere(a)[:,:0]\n    return transpose(nonzero(a))\n\n\ndef _flatnonzero_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_flatnonzero_dispatcher)\ndef flatnonzero(a):\n    \"\"\"\n    Return indices that are non-zero in the flattened version of a.\n\n    This is equivalent to np.nonzero(np.ravel(a))[0].\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, containing the indices of the elements of `a.ravel()`\n        that are non-zero.\n\n    See Also\n    --------\n    nonzero : Return the indices of the non-zero elements of the input array.\n    ravel : Return a 1-D array containing the elements of the input array.\n\n    Examples\n    --------\n    >>> x = np.arange(-2, 3)\n    >>> x\n    array([-2, -1,  0,  1,  2])\n    >>> np.flatnonzero(x)\n    array([0, 1, 3, 4])\n\n    Use the indices of the non-zero elements as an index array to extract\n    these elements:\n\n    >>> x.ravel()[np.flatnonzero(x)]\n    array([-2, -1,  1,  2])\n\n    \"\"\"\n    return np.nonzero(np.ravel(a))[0]\n\n\n_mode_from_name_dict = {'v': 0,\n                        's': 1,\n                        'f': 2}\n\n\ndef _mode_from_name(mode):\n    if isinstance(mode, str):\n        return _mode_from_name_dict[mode.lower()[0]]\n    return mode\n\n\ndef _correlate_dispatcher(a, v, mode=None):\n    return (a, v)\n\n\n@array_function_dispatch(_correlate_dispatcher)\ndef correlate(a, v, mode='valid'):\n    \"\"\"\n    Cross-correlation of two 1-dimensional sequences.\n\n    This function computes the correlation as generally defined in signal\n    processing texts::\n\n        c_{av}[k] = sum_n a[n+k] * conj(v[n])\n\n    with a and v sequences being zero-padded where necessary and conj being\n    the conjugate.\n\n    Parameters\n    ----------\n    a, v : array_like\n        Input sequences.\n    mode : {'valid', 'same', 'full'}, optional\n        Refer to the `convolve` docstring.  Note that the default\n        is 'valid', unlike `convolve`, which uses 'full'.\n    old_behavior : bool\n        `old_behavior` was removed in NumPy 1.10. If you need the old\n        behavior, use `multiarray.correlate`.\n\n    Returns\n    -------\n    out : ndarray\n        Discrete cross-correlation of `a` and `v`.\n\n    See Also\n    --------\n    convolve : Discrete, linear convolution of two one-dimensional sequences.\n    multiarray.correlate : Old, no conjugate, version of correlate.\n\n    Notes\n    -----\n    The definition of correlation above is not unique and sometimes correlation\n    may be defined differently. Another common definition is::\n\n        c'_{av}[k] = sum_n a[n] conj(v[n+k])\n\n    which is related to ``c_{av}[k]`` by ``c'_{av}[k] = c_{av}[-k]``.\n\n    Examples\n    --------\n    >>> np.correlate([1, 2, 3], [0, 1, 0.5])\n    array([3.5])\n    >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"same\")\n    array([2. ,  3.5,  3. ])\n    >>> np.correlate([1, 2, 3], [0, 1, 0.5], \"full\")\n    array([0.5,  2. ,  3.5,  3. ,  0. ])\n\n    Using complex sequences:\n\n    >>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full')\n    array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])\n\n    Note that you get the time reversed, complex conjugated result\n    when the two input sequences change places, i.e.,\n    ``c_{va}[k] = c^{*}_{av}[-k]``:\n\n    >>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full')\n    array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])\n\n    \"\"\"\n    mode = _mode_from_name(mode)\n    return multiarray.correlate2(a, v, mode)\n\n\ndef _convolve_dispatcher(a, v, mode=None):\n    return (a, v)\n\n\n@array_function_dispatch(_convolve_dispatcher)\ndef convolve(a, v, mode='full'):\n    \"\"\"\n    Returns the discrete, linear convolution of two one-dimensional sequences.\n\n    The convolution operator is often seen in signal processing, where it\n    models the effect of a linear time-invariant system on a signal [1]_.  In\n    probability theory, the sum of two independent random variables is\n    distributed according to the convolution of their individual\n    distributions.\n\n    If `v` is longer than `a`, the arrays are swapped before computation.\n\n    Parameters\n    ----------\n    a : (N,) array_like\n        First one-dimensional input array.\n    v : (M,) array_like\n        Second one-dimensional input array.\n    mode : {'full', 'valid', 'same'}, optional\n        'full':\n          By default, mode is 'full'.  This returns the convolution\n          at each point of overlap, with an output shape of (N+M-1,). At\n          the end-points of the convolution, the signals do not overlap\n          completely, and boundary effects may be seen.\n\n        'same':\n          Mode 'same' returns output of length ``max(M, N)``.  Boundary\n          effects are still visible.\n\n        'valid':\n          Mode 'valid' returns output of length\n          ``max(M, N) - min(M, N) + 1``.  The convolution product is only given\n          for points where the signals overlap completely.  Values outside\n          the signal boundary have no effect.\n\n    Returns\n    -------\n    out : ndarray\n        Discrete, linear convolution of `a` and `v`.\n\n    See Also\n    --------\n    scipy.signal.fftconvolve : Convolve two arrays using the Fast Fourier\n                               Transform.\n    scipy.linalg.toeplitz : Used to construct the convolution operator.\n    polymul : Polynomial multiplication. Same output as convolve, but also\n              accepts poly1d objects as input.\n\n    Notes\n    -----\n    The discrete convolution operation is defined as\n\n    .. math:: (a * v)[n] = \\\\sum_{m = -\\\\infty}^{\\\\infty} a[m] v[n - m]\n\n    It can be shown that a convolution :math:`x(t) * y(t)` in time/space\n    is equivalent to the multiplication :math:`X(f) Y(f)` in the Fourier\n    domain, after appropriate padding (padding is necessary to prevent\n    circular convolution).  Since multiplication is more efficient (faster)\n    than convolution, the function `scipy.signal.fftconvolve` exploits the\n    FFT to calculate the convolution of large data-sets.\n\n    References\n    ----------\n    .. [1] Wikipedia, \"Convolution\",\n        https://en.wikipedia.org/wiki/Convolution\n\n    Examples\n    --------\n    Note how the convolution operator flips the second array\n    before \"sliding\" the two across one another:\n\n    >>> np.convolve([1, 2, 3], [0, 1, 0.5])\n    array([0. , 1. , 2.5, 4. , 1.5])\n\n    Only return the middle values of the convolution.\n    Contains boundary effects, where zeros are taken\n    into account:\n\n    >>> np.convolve([1,2,3],[0,1,0.5], 'same')\n    array([1. ,  2.5,  4. ])\n\n    The two arrays are of the same length, so there\n    is only one position where they completely overlap:\n\n    >>> np.convolve([1,2,3],[0,1,0.5], 'valid')\n    array([2.5])\n\n    \"\"\"\n    a, v = array(a, copy=False, ndmin=1), array(v, copy=False, ndmin=1)\n    if (len(v) > len(a)):\n        a, v = v, a\n    if len(a) == 0:\n        raise ValueError('a cannot be empty')\n    if len(v) == 0:\n        raise ValueError('v cannot be empty')\n    mode = _mode_from_name(mode)\n    return multiarray.correlate(a, v[::-1], mode)\n\n\ndef _outer_dispatcher(a, b, out=None):\n    return (a, b, out)\n\n\n@array_function_dispatch(_outer_dispatcher)\ndef outer(a, b, out=None):\n    \"\"\"\n    Compute the outer product of two vectors.\n\n    Given two vectors, ``a = [a0, a1, ..., aM]`` and\n    ``b = [b0, b1, ..., bN]``,\n    the outer product [1]_ is::\n\n      [[a0*b0  a0*b1 ... a0*bN ]\n       [a1*b0    .\n       [ ...          .\n       [aM*b0            aM*bN ]]\n\n    Parameters\n    ----------\n    a : (M,) array_like\n        First input vector.  Input is flattened if\n        not already 1-dimensional.\n    b : (N,) array_like\n        Second input vector.  Input is flattened if\n        not already 1-dimensional.\n    out : (M, N) ndarray, optional\n        A location where the result is stored\n\n        .. versionadded:: 1.9.0\n\n    Returns\n    -------\n    out : (M, N) ndarray\n        ``out[i, j] = a[i] * b[j]``\n\n    See also\n    --------\n    inner\n    einsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.\n    ufunc.outer : A generalization to dimensions other than 1D and other\n                  operations. ``np.multiply.outer(a.ravel(), b.ravel())``\n                  is the equivalent.\n    tensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``\n                is the equivalent.\n\n    References\n    ----------\n    .. [1] : G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd\n             ed., Baltimore, MD, Johns Hopkins University Press, 1996,\n             pg. 8.\n\n    Examples\n    --------\n    Make a (*very* coarse) grid for computing a Mandelbrot set:\n\n    >>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n    >>> rl\n    array([[-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.]])\n    >>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))\n    >>> im\n    array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],\n           [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\n           [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n           [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],\n           [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])\n    >>> grid = rl + im\n    >>> grid\n    array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],\n           [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],\n           [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],\n           [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],\n           [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])\n\n    An example using a \"vector\" of letters:\n\n    >>> x = np.array(['a', 'b', 'c'], dtype=object)\n    >>> np.outer(x, [1, 2, 3])\n    array([['a', 'aa', 'aaa'],\n           ['b', 'bb', 'bbb'],\n           ['c', 'cc', 'ccc']], dtype=object)\n\n    \"\"\"\n    a = asarray(a)\n    b = asarray(b)\n    return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n\n\ndef _tensordot_dispatcher(a, b, axes=None):\n    return (a, b)\n\n\n@array_function_dispatch(_tensordot_dispatcher)\ndef tensordot(a, b, axes=2):\n    \"\"\"\n    Compute tensor dot product along specified axes.\n\n    Given two tensors, `a` and `b`, and an array_like object containing\n    two array_like objects, ``(a_axes, b_axes)``, sum the products of\n    `a`'s and `b`'s elements (components) over the axes specified by\n    ``a_axes`` and ``b_axes``. The third argument can be a single non-negative\n    integer_like scalar, ``N``; if it is such, then the last ``N`` dimensions\n    of `a` and the first ``N`` dimensions of `b` are summed over.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Tensors to \"dot\".\n\n    axes : int or (2,) array_like\n        * integer_like\n          If an int N, sum over the last N axes of `a` and the first N axes\n          of `b` in order. The sizes of the corresponding axes must match.\n        * (2,) array_like\n          Or, a list of axes to be summed over, first sequence applying to `a`,\n          second to `b`. Both elements array_like must be of the same length.\n\n    Returns\n    -------\n    output : ndarray\n        The tensor dot product of the input.\n\n    See Also\n    --------\n    dot, einsum\n\n    Notes\n    -----\n    Three common use cases are:\n        * ``axes = 0`` : tensor product :math:`a\\\\otimes b`\n        * ``axes = 1`` : tensor dot product :math:`a\\\\cdot b`\n        * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n\n    When `axes` is integer_like, the sequence for evaluation will be: first\n    the -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and\n    Nth axis in `b` last.\n\n    When there is more than one axis to sum over - and they are not the last\n    (first) axes of `a` (`b`) - the argument `axes` should consist of\n    two sequences of the same length, with the first axis to sum over given\n    first in both sequences, the second axis second, and so forth.\n\n    The shape of the result consists of the non-contracted axes of the\n    first tensor, followed by the non-contracted axes of the second.\n\n    Examples\n    --------\n    A \"traditional\" example:\n\n    >>> a = np.arange(60.).reshape(3,4,5)\n    >>> b = np.arange(24.).reshape(4,3,2)\n    >>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n    >>> c.shape\n    (5, 2)\n    >>> c\n    array([[4400., 4730.],\n           [4532., 4874.],\n           [4664., 5018.],\n           [4796., 5162.],\n           [4928., 5306.]])\n    >>> # A slower but equivalent way of computing the same...\n    >>> d = np.zeros((5,2))\n    >>> for i in range(5):\n    ...   for j in range(2):\n    ...     for k in range(3):\n    ...       for n in range(4):\n    ...         d[i,j] += a[k,n,i] * b[n,k,j]\n    >>> c == d\n    array([[ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True]])\n\n    An extended example taking advantage of the overloading of + and \\\\*:\n\n    >>> a = np.array(range(1, 9))\n    >>> a.shape = (2, 2, 2)\n    >>> A = np.array(('a', 'b', 'c', 'd'), dtype=object)\n    >>> A.shape = (2, 2)\n    >>> a; A\n    array([[[1, 2],\n            [3, 4]],\n           [[5, 6],\n            [7, 8]]])\n    array([['a', 'b'],\n           ['c', 'd']], dtype=object)\n\n    >>> np.tensordot(a, A) # third argument default is 2 for double-contraction\n    array(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, 1)\n    array([[['acc', 'bdd'],\n            ['aaacccc', 'bbbdddd']],\n           [['aaaaacccccc', 'bbbbbdddddd'],\n            ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\n    array([[[[['a', 'b'],\n              ['c', 'd']],\n              ...\n\n    >>> np.tensordot(a, A, (0, 1))\n    array([[['abbbbb', 'cddddd'],\n            ['aabbbbbb', 'ccdddddd']],\n           [['aaabbbbbbb', 'cccddddddd'],\n            ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, (2, 1))\n    array([[['abb', 'cdd'],\n            ['aaabbbb', 'cccdddd']],\n           [['aaaaabbbbbb', 'cccccdddddd'],\n            ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, ((0, 1), (0, 1)))\n    array(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, ((2, 1), (1, 0)))\n    array(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)\n\n    \"\"\"\n    try:\n        iter(axes)\n    except Exception:\n        axes_a = list(range(-axes, 0))\n        axes_b = list(range(0, axes))\n    else:\n        axes_a, axes_b = axes\n    try:\n        na = len(axes_a)\n        axes_a = list(axes_a)\n    except TypeError:\n        axes_a = [axes_a]\n        na = 1\n    try:\n        nb = len(axes_b)\n        axes_b = list(axes_b)\n    except TypeError:\n        axes_b = [axes_b]\n        nb = 1\n\n    a, b = asarray(a), asarray(b)\n    as_ = a.shape\n    nda = a.ndim\n    bs = b.shape\n    ndb = b.ndim\n    equal = True\n    if na != nb:\n        equal = False\n    else:\n        for k in range(na):\n            if as_[axes_a[k]] != bs[axes_b[k]]:\n                equal = False\n                break\n            if axes_a[k] < 0:\n                axes_a[k] += nda\n            if axes_b[k] < 0:\n                axes_b[k] += ndb\n    if not equal:\n        raise ValueError(\"shape-mismatch for sum\")\n\n    # Move the axes to sum over to the end of \"a\"\n    # and to the front of \"b\"\n    notin = [k for k in range(nda) if k not in axes_a]\n    newaxes_a = notin + axes_a\n    N2 = 1\n    for axis in axes_a:\n        N2 *= as_[axis]\n    newshape_a = (int(multiply.reduce([as_[ax] for ax in notin])), N2)\n    olda = [as_[axis] for axis in notin]\n\n    notin = [k for k in range(ndb) if k not in axes_b]\n    newaxes_b = axes_b + notin\n    N2 = 1\n    for axis in axes_b:\n        N2 *= bs[axis]\n    newshape_b = (N2, int(multiply.reduce([bs[ax] for ax in notin])))\n    oldb = [bs[axis] for axis in notin]\n\n    at = a.transpose(newaxes_a).reshape(newshape_a)\n    bt = b.transpose(newaxes_b).reshape(newshape_b)\n    res = dot(at, bt)\n    return res.reshape(olda + oldb)\n\n\ndef _roll_dispatcher(a, shift, axis=None):\n    return (a,)\n\n\n@array_function_dispatch(_roll_dispatcher)\ndef roll(a, shift, axis=None):\n    \"\"\"\n    Roll array elements along a given axis.\n\n    Elements that roll beyond the last position are re-introduced at\n    the first.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    shift : int or tuple of ints\n        The number of places by which elements are shifted.  If a tuple,\n        then `axis` must be a tuple of the same size, and each of the\n        given axes is shifted by the corresponding number.  If an int\n        while `axis` is a tuple of ints, then the same value is used for\n        all given axes.\n    axis : int or tuple of ints, optional\n        Axis or axes along which elements are shifted.  By default, the\n        array is flattened before shifting, after which the original\n        shape is restored.\n\n    Returns\n    -------\n    res : ndarray\n        Output array, with the same shape as `a`.\n\n    See Also\n    --------\n    rollaxis : Roll the specified axis backwards, until it lies in a\n               given position.\n\n    Notes\n    -----\n    .. versionadded:: 1.12.0\n\n    Supports rolling over multiple dimensions simultaneously.\n\n    Examples\n    --------\n    >>> x = np.arange(10)\n    >>> np.roll(x, 2)\n    array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n    >>> np.roll(x, -2)\n    array([2, 3, 4, 5, 6, 7, 8, 9, 0, 1])\n\n    >>> x2 = np.reshape(x, (2,5))\n    >>> x2\n    array([[0, 1, 2, 3, 4],\n           [5, 6, 7, 8, 9]])\n    >>> np.roll(x2, 1)\n    array([[9, 0, 1, 2, 3],\n           [4, 5, 6, 7, 8]])\n    >>> np.roll(x2, -1)\n    array([[1, 2, 3, 4, 5],\n           [6, 7, 8, 9, 0]])\n    >>> np.roll(x2, 1, axis=0)\n    array([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> np.roll(x2, -1, axis=0)\n    array([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> np.roll(x2, 1, axis=1)\n    array([[4, 0, 1, 2, 3],\n           [9, 5, 6, 7, 8]])\n    >>> np.roll(x2, -1, axis=1)\n    array([[1, 2, 3, 4, 0],\n           [6, 7, 8, 9, 5]])\n\n    \"\"\"\n    a = asanyarray(a)\n    if axis is None:\n        return roll(a.ravel(), shift, 0).reshape(a.shape)\n\n    else:\n        axis = normalize_axis_tuple(axis, a.ndim, allow_duplicate=True)\n        broadcasted = broadcast(shift, axis)\n        if broadcasted.ndim > 1:\n            raise ValueError(\n                \"'shift' and 'axis' should be scalars or 1D sequences\")\n        shifts = {ax: 0 for ax in range(a.ndim)}\n        for sh, ax in broadcasted:\n            shifts[ax] += sh\n\n        rolls = [((slice(None), slice(None)),)] * a.ndim\n        for ax, offset in shifts.items():\n            offset %= a.shape[ax] or 1  # If `a` is empty, nothing matters.\n            if offset:\n                # (original, result), (original, result)\n                rolls[ax] = ((slice(None, -offset), slice(offset, None)),\n                             (slice(-offset, None), slice(None, offset)))\n\n        result = empty_like(a)\n        for indices in itertools.product(*rolls):\n            arr_index, res_index = zip(*indices)\n            result[res_index] = a[arr_index]\n\n        return result\n\n\ndef _rollaxis_dispatcher(a, axis, start=None):\n    return (a,)\n\n\n@array_function_dispatch(_rollaxis_dispatcher)\ndef rollaxis(a, axis, start=0):\n    \"\"\"\n    Roll the specified axis backwards, until it lies in a given position.\n\n    This function continues to be supported for backward compatibility, but you\n    should prefer `moveaxis`. The `moveaxis` function was added in NumPy\n    1.11.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n    axis : int\n        The axis to be rolled. The positions of the other axes do not\n        change relative to one another.\n    start : int, optional\n        When ``start <= axis``, the axis is rolled back until it lies in\n        this position. When ``start > axis``, the axis is rolled until it\n        lies before this position. The default, 0, results in a \"complete\"\n        roll. The following table describes how negative values of ``start``\n        are interpreted:\n\n        .. table::\n           :align: left\n\n           +-------------------+----------------------+\n           |     ``start``     | Normalized ``start`` |\n           +===================+======================+\n           | ``-(arr.ndim+1)`` | raise ``AxisError``  |\n           +-------------------+----------------------+\n           | ``-arr.ndim``     | 0                    |\n           +-------------------+----------------------+\n           | |vdots|           | |vdots|              |\n           +-------------------+----------------------+\n           | ``-1``            | ``arr.ndim-1``       |\n           +-------------------+----------------------+\n           | ``0``             | ``0``                |\n           +-------------------+----------------------+\n           | |vdots|           | |vdots|              |\n           +-------------------+----------------------+\n           | ``arr.ndim``      | ``arr.ndim``         |\n           +-------------------+----------------------+\n           | ``arr.ndim + 1``  | raise ``AxisError``  |\n           +-------------------+----------------------+\n           \n        .. |vdots|   unicode:: U+22EE .. Vertical Ellipsis\n\n    Returns\n    -------\n    res : ndarray\n        For NumPy >= 1.10.0 a view of `a` is always returned. For earlier\n        NumPy versions a view of `a` is returned only if the order of the\n        axes is changed, otherwise the input array is returned.\n\n    See Also\n    --------\n    moveaxis : Move array axes to new positions.\n    roll : Roll the elements of an array by a number of positions along a\n        given axis.\n\n    Examples\n    --------\n    >>> a = np.ones((3,4,5,6))\n    >>> np.rollaxis(a, 3, 1).shape\n    (3, 6, 4, 5)\n    >>> np.rollaxis(a, 2).shape\n    (5, 3, 4, 6)\n    >>> np.rollaxis(a, 1, 4).shape\n    (3, 5, 6, 4)\n\n    \"\"\"\n    n = a.ndim\n    axis = normalize_axis_index(axis, n)\n    if start < 0:\n        start += n\n    msg = \"'%s' arg requires %d <= %s < %d, but %d was passed in\"\n    if not (0 <= start < n + 1):\n        raise AxisError(msg % ('start', -n, 'start', n + 1, start))\n    if axis < start:\n        # it's been removed\n        start -= 1\n    if axis == start:\n        return a[...]\n    axes = list(range(0, n))\n    axes.remove(axis)\n    axes.insert(start, axis)\n    return a.transpose(axes)\n\n\ndef normalize_axis_tuple(axis, ndim, argname=None, allow_duplicate=False):\n    \"\"\"\n    Normalizes an axis argument into a tuple of non-negative integer axes.\n\n    This handles shorthands such as ``1`` and converts them to ``(1,)``,\n    as well as performing the handling of negative indices covered by\n    `normalize_axis_index`.\n\n    By default, this forbids axes from being specified multiple times.\n\n    Used internally by multi-axis-checking logic.\n\n    .. versionadded:: 1.13.0\n\n    Parameters\n    ----------\n    axis : int, iterable of int\n        The un-normalized index or indices of the axis.\n    ndim : int\n        The number of dimensions of the array that `axis` should be normalized\n        against.\n    argname : str, optional\n        A prefix to put before the error message, typically the name of the\n        argument.\n    allow_duplicate : bool, optional\n        If False, the default, disallow an axis from being specified twice.\n\n    Returns\n    -------\n    normalized_axes : tuple of int\n        The normalized axis index, such that `0 <= normalized_axis < ndim`\n\n    Raises\n    ------\n    AxisError\n        If any axis provided is out of range\n    ValueError\n        If an axis is repeated\n\n    See also\n    --------\n    normalize_axis_index : normalizing a single scalar axis\n    \"\"\"\n    # Optimization to speed-up the most common cases.\n    if type(axis) not in (tuple, list):\n        try:\n            axis = [operator.index(axis)]\n        except TypeError:\n            pass\n    # Going via an iterator directly is slower than via list comprehension.\n    axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis])\n    if not allow_duplicate and len(set(axis)) != len(axis):\n        if argname:\n            raise ValueError('repeated axis in `{}` argument'.format(argname))\n        else:\n            raise ValueError('repeated axis')\n    return axis\n\n\ndef _moveaxis_dispatcher(a, source, destination):\n    return (a,)\n\n\n@array_function_dispatch(_moveaxis_dispatcher)\ndef moveaxis(a, source, destination):\n    \"\"\"\n    Move axes of an array to new positions.\n\n    Other axes remain in their original order.\n\n    .. versionadded:: 1.11.0\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The array whose axes should be reordered.\n    source : int or sequence of int\n        Original positions of the axes to move. These must be unique.\n    destination : int or sequence of int\n        Destination positions for each of the original axes. These must also be\n        unique.\n\n    Returns\n    -------\n    result : np.ndarray\n        Array with moved axes. This array is a view of the input array.\n\n    See Also\n    --------\n    transpose: Permute the dimensions of an array.\n    swapaxes: Interchange two axes of an array.\n\n    Examples\n    --------\n\n    >>> x = np.zeros((3, 4, 5))\n    >>> np.moveaxis(x, 0, -1).shape\n    (4, 5, 3)\n    >>> np.moveaxis(x, -1, 0).shape\n    (5, 3, 4)\n\n    These all achieve the same result:\n\n    >>> np.transpose(x).shape\n    (5, 4, 3)\n    >>> np.swapaxes(x, 0, -1).shape\n    (5, 4, 3)\n    >>> np.moveaxis(x, [0, 1], [-1, -2]).shape\n    (5, 4, 3)\n    >>> np.moveaxis(x, [0, 1, 2], [-1, -2, -3]).shape\n    (5, 4, 3)\n\n    \"\"\"\n    try:\n        # allow duck-array types if they define transpose\n        transpose = a.transpose\n    except AttributeError:\n        a = asarray(a)\n        transpose = a.transpose\n\n    source = normalize_axis_tuple(source, a.ndim, 'source')\n    destination = normalize_axis_tuple(destination, a.ndim, 'destination')\n    if len(source) != len(destination):\n        raise ValueError('`source` and `destination` arguments must have '\n                         'the same number of elements')\n\n    order = [n for n in range(a.ndim) if n not in source]\n\n    for dest, src in sorted(zip(destination, source)):\n        order.insert(dest, src)\n\n    result = transpose(order)\n    return result\n\n\n# fix hack in scipy which imports this function\ndef _move_axis_to_0(a, axis):\n    return moveaxis(a, axis, 0)\n\n\ndef _cross_dispatcher(a, b, axisa=None, axisb=None, axisc=None, axis=None):\n    return (a, b)\n\n\n@array_function_dispatch(_cross_dispatcher)\ndef cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):\n    \"\"\"\n    Return the cross product of two (arrays of) vectors.\n\n    The cross product of `a` and `b` in :math:`R^3` is a vector perpendicular\n    to both `a` and `b`.  If `a` and `b` are arrays of vectors, the vectors\n    are defined by the last axis of `a` and `b` by default, and these axes\n    can have dimensions 2 or 3.  Where the dimension of either `a` or `b` is\n    2, the third component of the input vector is assumed to be zero and the\n    cross product calculated accordingly.  In cases where both input vectors\n    have dimension 2, the z-component of the cross product is returned.\n\n    Parameters\n    ----------\n    a : array_like\n        Components of the first vector(s).\n    b : array_like\n        Components of the second vector(s).\n    axisa : int, optional\n        Axis of `a` that defines the vector(s).  By default, the last axis.\n    axisb : int, optional\n        Axis of `b` that defines the vector(s).  By default, the last axis.\n    axisc : int, optional\n        Axis of `c` containing the cross product vector(s).  Ignored if\n        both input vectors have dimension 2, as the return is scalar.\n        By default, the last axis.\n    axis : int, optional\n        If defined, the axis of `a`, `b` and `c` that defines the vector(s)\n        and cross product(s).  Overrides `axisa`, `axisb` and `axisc`.\n\n    Returns\n    -------\n    c : ndarray\n        Vector cross product(s).\n\n    Raises\n    ------\n    ValueError\n        When the dimension of the vector(s) in `a` and/or `b` does not\n        equal 2 or 3.\n\n    See Also\n    --------\n    inner : Inner product\n    outer : Outer product.\n    ix_ : Construct index arrays.\n\n    Notes\n    -----\n    .. versionadded:: 1.9.0\n\n    Supports full broadcasting of the inputs.\n\n    Examples\n    --------\n    Vector cross-product.\n\n    >>> x = [1, 2, 3]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([-3,  6, -3])\n\n    One vector with dimension 2.\n\n    >>> x = [1, 2]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([12, -6, -3])\n\n    Equivalently:\n\n    >>> x = [1, 2, 0]\n    >>> y = [4, 5, 6]\n    >>> np.cross(x, y)\n    array([12, -6, -3])\n\n    Both vectors with dimension 2.\n\n    >>> x = [1,2]\n    >>> y = [4,5]\n    >>> np.cross(x, y)\n    array(-3)\n\n    Multiple vector cross-products. Note that the direction of the cross\n    product vector is defined by the `right-hand rule`.\n\n    >>> x = np.array([[1,2,3], [4,5,6]])\n    >>> y = np.array([[4,5,6], [1,2,3]])\n    >>> np.cross(x, y)\n    array([[-3,  6, -3],\n           [ 3, -6,  3]])\n\n    The orientation of `c` can be changed using the `axisc` keyword.\n\n    >>> np.cross(x, y, axisc=0)\n    array([[-3,  3],\n           [ 6, -6],\n           [-3,  3]])\n\n    Change the vector definition of `x` and `y` using `axisa` and `axisb`.\n\n    >>> x = np.array([[1,2,3], [4,5,6], [7, 8, 9]])\n    >>> y = np.array([[7, 8, 9], [4,5,6], [1,2,3]])\n    >>> np.cross(x, y)\n    array([[ -6,  12,  -6],\n           [  0,   0,   0],\n           [  6, -12,   6]])\n    >>> np.cross(x, y, axisa=0, axisb=0)\n    array([[-24,  48, -24],\n           [-30,  60, -30],\n           [-36,  72, -36]])\n\n    \"\"\"\n    if axis is not None:\n        axisa, axisb, axisc = (axis,) * 3\n    a = asarray(a)\n    b = asarray(b)\n    # Check axisa and axisb are within bounds\n    axisa = normalize_axis_index(axisa, a.ndim, msg_prefix='axisa')\n    axisb = normalize_axis_index(axisb, b.ndim, msg_prefix='axisb')\n\n    # Move working axis to the end of the shape\n    a = moveaxis(a, axisa, -1)\n    b = moveaxis(b, axisb, -1)\n    msg = (\"incompatible dimensions for cross product\\n\"\n           \"(dimension must be 2 or 3)\")\n    if a.shape[-1] not in (2, 3) or b.shape[-1] not in (2, 3):\n        raise ValueError(msg)\n\n    # Create the output array\n    shape = broadcast(a[..., 0], b[..., 0]).shape\n    if a.shape[-1] == 3 or b.shape[-1] == 3:\n        shape += (3,)\n        # Check axisc is within bounds\n        axisc = normalize_axis_index(axisc, len(shape), msg_prefix='axisc')\n    dtype = promote_types(a.dtype, b.dtype)\n    cp = empty(shape, dtype)\n\n    # create local aliases for readability\n    a0 = a[..., 0]\n    a1 = a[..., 1]\n    if a.shape[-1] == 3:\n        a2 = a[..., 2]\n    b0 = b[..., 0]\n    b1 = b[..., 1]\n    if b.shape[-1] == 3:\n        b2 = b[..., 2]\n    if cp.ndim != 0 and cp.shape[-1] == 3:\n        cp0 = cp[..., 0]\n        cp1 = cp[..., 1]\n        cp2 = cp[..., 2]\n\n    if a.shape[-1] == 2:\n        if b.shape[-1] == 2:\n            # a0 * b1 - a1 * b0\n            multiply(a0, b1, out=cp)\n            cp -= a1 * b0\n            return cp\n        else:\n            assert b.shape[-1] == 3\n            # cp0 = a1 * b2 - 0  (a2 = 0)\n            # cp1 = 0 - a0 * b2  (a2 = 0)\n            # cp2 = a0 * b1 - a1 * b0\n            multiply(a1, b2, out=cp0)\n            multiply(a0, b2, out=cp1)\n            negative(cp1, out=cp1)\n            multiply(a0, b1, out=cp2)\n            cp2 -= a1 * b0\n    else:\n        assert a.shape[-1] == 3\n        if b.shape[-1] == 3:\n            # cp0 = a1 * b2 - a2 * b1\n            # cp1 = a2 * b0 - a0 * b2\n            # cp2 = a0 * b1 - a1 * b0\n            multiply(a1, b2, out=cp0)\n            tmp = array(a2 * b1)\n            cp0 -= tmp\n            multiply(a2, b0, out=cp1)\n            multiply(a0, b2, out=tmp)\n            cp1 -= tmp\n            multiply(a0, b1, out=cp2)\n            multiply(a1, b0, out=tmp)\n            cp2 -= tmp\n        else:\n            assert b.shape[-1] == 2\n            # cp0 = 0 - a2 * b1  (b2 = 0)\n            # cp1 = a2 * b0 - 0  (b2 = 0)\n            # cp2 = a0 * b1 - a1 * b0\n            multiply(a2, b1, out=cp0)\n            negative(cp0, out=cp0)\n            multiply(a2, b0, out=cp1)\n            multiply(a0, b1, out=cp2)\n            cp2 -= a1 * b0\n\n    return moveaxis(cp, -1, axisc)\n\n\nlittle_endian = (sys.byteorder == 'little')\n\n\n@set_module('numpy')\ndef indices(dimensions, dtype=int, sparse=False):\n    \"\"\"\n    Return an array representing the indices of a grid.\n\n    Compute an array where the subarrays contain index values 0, 1, ...\n    varying only along the corresponding axis.\n\n    Parameters\n    ----------\n    dimensions : sequence of ints\n        The shape of the grid.\n    dtype : dtype, optional\n        Data type of the result.\n    sparse : boolean, optional\n        Return a sparse representation of the grid instead of a dense\n        representation. Default is False.\n\n        .. versionadded:: 1.17\n\n    Returns\n    -------\n    grid : one ndarray or tuple of ndarrays\n        If sparse is False:\n            Returns one array of grid indices,\n            ``grid.shape = (len(dimensions),) + tuple(dimensions)``.\n        If sparse is True:\n            Returns a tuple of arrays, with\n            ``grid[i].shape = (1, ..., 1, dimensions[i], 1, ..., 1)`` with\n            dimensions[i] in the ith place\n\n    See Also\n    --------\n    mgrid, ogrid, meshgrid\n\n    Notes\n    -----\n    The output shape in the dense case is obtained by prepending the number\n    of dimensions in front of the tuple of dimensions, i.e. if `dimensions`\n    is a tuple ``(r0, ..., rN-1)`` of length ``N``, the output shape is\n    ``(N, r0, ..., rN-1)``.\n\n    The subarrays ``grid[k]`` contains the N-D array of indices along the\n    ``k-th`` axis. Explicitly::\n\n        grid[k, i0, i1, ..., iN-1] = ik\n\n    Examples\n    --------\n    >>> grid = np.indices((2, 3))\n    >>> grid.shape\n    (2, 2, 3)\n    >>> grid[0]        # row indices\n    array([[0, 0, 0],\n           [1, 1, 1]])\n    >>> grid[1]        # column indices\n    array([[0, 1, 2],\n           [0, 1, 2]])\n\n    The indices can be used as an index into an array.\n\n    >>> x = np.arange(20).reshape(5, 4)\n    >>> row, col = np.indices((2, 3))\n    >>> x[row, col]\n    array([[0, 1, 2],\n           [4, 5, 6]])\n\n    Note that it would be more straightforward in the above example to\n    extract the required elements directly with ``x[:2, :3]``.\n\n    If sparse is set to true, the grid will be returned in a sparse\n    representation.\n\n    >>> i, j = np.indices((2, 3), sparse=True)\n    >>> i.shape\n    (2, 1)\n    >>> j.shape\n    (1, 3)\n    >>> i        # row indices\n    array([[0],\n           [1]])\n    >>> j        # column indices\n    array([[0, 1, 2]])\n\n    \"\"\"\n    dimensions = tuple(dimensions)\n    N = len(dimensions)\n    shape = (1,)*N\n    if sparse:\n        res = tuple()\n    else:\n        res = empty((N,)+dimensions, dtype=dtype)\n    for i, dim in enumerate(dimensions):\n        idx = arange(dim, dtype=dtype).reshape(\n            shape[:i] + (dim,) + shape[i+1:]\n        )\n        if sparse:\n            res = res + (idx,)\n        else:\n            res[i] = idx\n    return res\n\n\ndef _fromfunction_dispatcher(function, shape, *, dtype=None, like=None, **kwargs):\n    return (like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef fromfunction(function, shape, *, dtype=float, like=None, **kwargs):\n    \"\"\"\n    Construct an array by executing a function over each coordinate.\n\n    The resulting array therefore has a value ``fn(x, y, z)`` at\n    coordinate ``(x, y, z)``.\n\n    Parameters\n    ----------\n    function : callable\n        The function is called with N parameters, where N is the rank of\n        `shape`.  Each parameter represents the coordinates of the array\n        varying along a specific axis.  For example, if `shape`\n        were ``(2, 2)``, then the parameters would be\n        ``array([[0, 0], [1, 1]])`` and ``array([[0, 1], [0, 1]])``\n    shape : (N,) tuple of ints\n        Shape of the output array, which also determines the shape of\n        the coordinate arrays passed to `function`.\n    dtype : data-type, optional\n        Data-type of the coordinate arrays passed to `function`.\n        By default, `dtype` is float.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    fromfunction : any\n        The result of the call to `function` is passed back directly.\n        Therefore the shape of `fromfunction` is completely determined by\n        `function`.  If `function` returns a scalar value, the shape of\n        `fromfunction` would not match the `shape` parameter.\n\n    See Also\n    --------\n    indices, meshgrid\n\n    Notes\n    -----\n    Keywords other than `dtype` are passed to `function`.\n\n    Examples\n    --------\n    >>> np.fromfunction(lambda i, j: i == j, (3, 3), dtype=int)\n    array([[ True, False, False],\n           [False,  True, False],\n           [False, False,  True]])\n\n    >>> np.fromfunction(lambda i, j: i + j, (3, 3), dtype=int)\n    array([[0, 1, 2],\n           [1, 2, 3],\n           [2, 3, 4]])\n\n    \"\"\"\n    if like is not None:\n        return _fromfunction_with_like(function, shape, dtype=dtype, like=like, **kwargs)\n\n    args = indices(shape, dtype=dtype)\n    return function(*args, **kwargs)\n\n\n_fromfunction_with_like = array_function_dispatch(\n    _fromfunction_dispatcher\n)(fromfunction)\n\n\ndef _frombuffer(buf, dtype, shape, order):\n    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)\n\n\n@set_module('numpy')\ndef isscalar(element):\n    \"\"\"\n    Returns True if the type of `element` is a scalar type.\n\n    Parameters\n    ----------\n    element : any\n        Input argument, can be of any type and shape.\n\n    Returns\n    -------\n    val : bool\n        True if `element` is a scalar type, False if it is not.\n\n    See Also\n    --------\n    ndim : Get the number of dimensions of an array\n\n    Notes\n    -----\n    If you need a stricter way to identify a *numerical* scalar, use\n    ``isinstance(x, numbers.Number)``, as that returns ``False`` for most\n    non-numerical elements such as strings.\n\n    In most cases ``np.ndim(x) == 0`` should be used instead of this function,\n    as that will also return true for 0d arrays. This is how numpy overloads\n    functions in the style of the ``dx`` arguments to `gradient` and the ``bins``\n    argument to `histogram`. Some key differences:\n\n    +--------------------------------------+---------------+-------------------+\n    | x                                    |``isscalar(x)``|``np.ndim(x) == 0``|\n    +======================================+===============+===================+\n    | PEP 3141 numeric objects (including  | ``True``      | ``True``          |\n    | builtins)                            |               |                   |\n    +--------------------------------------+---------------+-------------------+\n    | builtin string and buffer objects    | ``True``      | ``True``          |\n    +--------------------------------------+---------------+-------------------+\n    | other builtin objects, like          | ``False``     | ``True``          |\n    | `pathlib.Path`, `Exception`,         |               |                   |\n    | the result of `re.compile`           |               |                   |\n    +--------------------------------------+---------------+-------------------+\n    | third-party objects like             | ``False``     | ``True``          |\n    | `matplotlib.figure.Figure`           |               |                   |\n    +--------------------------------------+---------------+-------------------+\n    | zero-dimensional numpy arrays        | ``False``     | ``True``          |\n    +--------------------------------------+---------------+-------------------+\n    | other numpy arrays                   | ``False``     | ``False``         |\n    +--------------------------------------+---------------+-------------------+\n    | `list`, `tuple`, and other sequence  | ``False``     | ``False``         |\n    | objects                              |               |                   |\n    +--------------------------------------+---------------+-------------------+\n\n    Examples\n    --------\n    >>> np.isscalar(3.1)\n    True\n    >>> np.isscalar(np.array(3.1))\n    False\n    >>> np.isscalar([3.1])\n    False\n    >>> np.isscalar(False)\n    True\n    >>> np.isscalar('numpy')\n    True\n\n    NumPy supports PEP 3141 numbers:\n\n    >>> from fractions import Fraction\n    >>> np.isscalar(Fraction(5, 17))\n    True\n    >>> from numbers import Number\n    >>> np.isscalar(Number())\n    True\n\n    \"\"\"\n    return (isinstance(element, generic)\n            or type(element) in ScalarType\n            or isinstance(element, numbers.Number))\n\n\n@set_module('numpy')\ndef binary_repr(num, width=None):\n    \"\"\"\n    Return the binary representation of the input number as a string.\n\n    For negative numbers, if width is not given, a minus sign is added to the\n    front. If width is given, the two's complement of the number is\n    returned, with respect to that width.\n\n    In a two's-complement system negative numbers are represented by the two's\n    complement of the absolute value. This is the most common method of\n    representing signed integers on computers [1]_. A N-bit two's-complement\n    system can represent every integer in the range\n    :math:`-2^{N-1}` to :math:`+2^{N-1}-1`.\n\n    Parameters\n    ----------\n    num : int\n        Only an integer decimal number can be used.\n    width : int, optional\n        The length of the returned string if `num` is positive, or the length\n        of the two's complement if `num` is negative, provided that `width` is\n        at least a sufficient number of bits for `num` to be represented in the\n        designated form.\n\n        If the `width` value is insufficient, it will be ignored, and `num` will\n        be returned in binary (`num` > 0) or two's complement (`num` < 0) form\n        with its width equal to the minimum number of bits needed to represent\n        the number in the designated form. This behavior is deprecated and will\n        later raise an error.\n\n        .. deprecated:: 1.12.0\n\n    Returns\n    -------\n    bin : str\n        Binary representation of `num` or two's complement of `num`.\n\n    See Also\n    --------\n    base_repr: Return a string representation of a number in the given base\n               system.\n    bin: Python's built-in binary representation generator of an integer.\n\n    Notes\n    -----\n    `binary_repr` is equivalent to using `base_repr` with base 2, but about 25x\n    faster.\n\n    References\n    ----------\n    .. [1] Wikipedia, \"Two's complement\",\n        https://en.wikipedia.org/wiki/Two's_complement\n\n    Examples\n    --------\n    >>> np.binary_repr(3)\n    '11'\n    >>> np.binary_repr(-3)\n    '-11'\n    >>> np.binary_repr(3, width=4)\n    '0011'\n\n    The two's complement is returned when the input number is negative and\n    width is specified:\n\n    >>> np.binary_repr(-3, width=3)\n    '101'\n    >>> np.binary_repr(-3, width=5)\n    '11101'\n\n    \"\"\"\n    def warn_if_insufficient(width, binwidth):\n        if width is not None and width < binwidth:\n            warnings.warn(\n                \"Insufficient bit width provided. This behavior \"\n                \"will raise an error in the future.\", DeprecationWarning,\n                stacklevel=3)\n\n    # Ensure that num is a Python integer to avoid overflow or unwanted\n    # casts to floating point.\n    num = operator.index(num)\n\n    if num == 0:\n        return '0' * (width or 1)\n\n    elif num > 0:\n        binary = bin(num)[2:]\n        binwidth = len(binary)\n        outwidth = (binwidth if width is None\n                    else max(binwidth, width))\n        warn_if_insufficient(width, binwidth)\n        return binary.zfill(outwidth)\n\n    else:\n        if width is None:\n            return '-' + bin(-num)[2:]\n\n        else:\n            poswidth = len(bin(-num)[2:])\n\n            # See gh-8679: remove extra digit\n            # for numbers at boundaries.\n            if 2**(poswidth - 1) == -num:\n                poswidth -= 1\n\n            twocomp = 2**(poswidth + 1) + num\n            binary = bin(twocomp)[2:]\n            binwidth = len(binary)\n\n            outwidth = max(binwidth, width)\n            warn_if_insufficient(width, binwidth)\n            return '1' * (outwidth - binwidth) + binary\n\n\n@set_module('numpy')\ndef base_repr(number, base=2, padding=0):\n    \"\"\"\n    Return a string representation of a number in the given base system.\n\n    Parameters\n    ----------\n    number : int\n        The value to convert. Positive and negative values are handled.\n    base : int, optional\n        Convert `number` to the `base` number system. The valid range is 2-36,\n        the default value is 2.\n    padding : int, optional\n        Number of zeros padded on the left. Default is 0 (no padding).\n\n    Returns\n    -------\n    out : str\n        String representation of `number` in `base` system.\n\n    See Also\n    --------\n    binary_repr : Faster version of `base_repr` for base 2.\n\n    Examples\n    --------\n    >>> np.base_repr(5)\n    '101'\n    >>> np.base_repr(6, 5)\n    '11'\n    >>> np.base_repr(7, base=5, padding=3)\n    '00012'\n\n    >>> np.base_repr(10, base=16)\n    'A'\n    >>> np.base_repr(32, base=16)\n    '20'\n\n    \"\"\"\n    digits = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if base > len(digits):\n        raise ValueError(\"Bases greater than 36 not handled in base_repr.\")\n    elif base < 2:\n        raise ValueError(\"Bases less than 2 not handled in base_repr.\")\n\n    num = abs(number)\n    res = []\n    while num:\n        res.append(digits[num % base])\n        num //= base\n    if padding:\n        res.append('0' * padding)\n    if number < 0:\n        res.append('-')\n    return ''.join(reversed(res or '0'))\n\n\n# These are all essentially abbreviations\n# These might wind up in a special abbreviations module\n\n\ndef _maketup(descr, val):\n    dt = dtype(descr)\n    # Place val in all scalar tuples:\n    fields = dt.fields\n    if fields is None:\n        return val\n    else:\n        res = [_maketup(fields[name][0], val) for name in dt.names]\n        return tuple(res)\n\n\ndef _identity_dispatcher(n, dtype=None, *, like=None):\n    return (like,)\n\n\n@set_array_function_like_doc\n@set_module('numpy')\ndef identity(n, dtype=None, *, like=None):\n    \"\"\"\n    Return the identity array.\n\n    The identity array is a square array with ones on\n    the main diagonal.\n\n    Parameters\n    ----------\n    n : int\n        Number of rows (and columns) in `n` x `n` output.\n    dtype : data-type, optional\n        Data-type of the output.  Defaults to ``float``.\n    ${ARRAY_FUNCTION_LIKE}\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        `n` x `n` array with its main diagonal set to one,\n        and all other elements 0.\n\n    Examples\n    --------\n    >>> np.identity(3)\n    array([[1.,  0.,  0.],\n           [0.,  1.,  0.],\n           [0.,  0.,  1.]])\n\n    \"\"\"\n    if like is not None:\n        return _identity_with_like(n, dtype=dtype, like=like)\n\n    from numpy import eye\n    return eye(n, dtype=dtype, like=like)\n\n\n_identity_with_like = array_function_dispatch(\n    _identity_dispatcher\n)(identity)\n\n\ndef _allclose_dispatcher(a, b, rtol=None, atol=None, equal_nan=None):\n    return (a, b)\n\n\n@array_function_dispatch(_allclose_dispatcher)\ndef allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    \"\"\"\n    Returns True if two arrays are element-wise equal within a tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    NaNs are treated as equal if they are in the same place and if\n    ``equal_nan=True``.  Infs are treated as equal if they are in the same\n    place and of the same sign in both arrays.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : float\n        The relative tolerance parameter (see Notes).\n    atol : float\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n        .. versionadded:: 1.10.0\n\n    Returns\n    -------\n    allclose : bool\n        Returns True if the two arrays are equal within the given\n        tolerance; False otherwise.\n\n    See Also\n    --------\n    isclose, all, any, equal\n\n    Notes\n    -----\n    If the following equation is element-wise True, then allclose returns\n    True.\n\n     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\n    The above equation is not symmetric in `a` and `b`, so that\n    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in\n    some rare cases.\n\n    The comparison of `a` and `b` uses standard broadcasting, which\n    means that `a` and `b` need not have the same shape in order for\n    ``allclose(a, b)`` to evaluate to True.  The same is true for\n    `equal` but not `array_equal`.\n\n    `allclose` is not defined for non-numeric data types.\n\n    Examples\n    --------\n    >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])\n    False\n    >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])\n    True\n    >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])\n    False\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan])\n    False\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    True\n\n    \"\"\"\n    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n    return bool(res)\n\n\ndef _isclose_dispatcher(a, b, rtol=None, atol=None, equal_nan=None):\n    return (a, b)\n\n\n@array_function_dispatch(_isclose_dispatcher)\ndef isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    \"\"\"\n    Returns a boolean array where two arrays are element-wise equal within a\n    tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    .. warning:: The default `atol` is not appropriate for comparing numbers\n                 that are much smaller than one (see Notes).\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : float\n        The relative tolerance parameter (see Notes).\n    atol : float\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n    Returns\n    -------\n    y : array_like\n        Returns a boolean array of where `a` and `b` are equal within the\n        given tolerance. If both `a` and `b` are scalars, returns a single\n        boolean value.\n\n    See Also\n    --------\n    allclose\n    math.isclose\n\n    Notes\n    -----\n    .. versionadded:: 1.7.0\n\n    For finite values, isclose uses the following equation to test whether\n    two floating point values are equivalent.\n\n     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\n    Unlike the built-in `math.isclose`, the above equation is not symmetric\n    in `a` and `b` -- it assumes `b` is the reference value -- so that\n    `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,\n    the default value of atol is not zero, and is used to determine what\n    small values should be considered close to zero. The default value is\n    appropriate for expected values of order unity: if the expected values\n    are significantly smaller than one, it can result in false positives.\n    `atol` should be carefully selected for the use case at hand. A zero value\n    for `atol` will result in `False` if either `a` or `b` is zero.\n\n    `isclose` is not defined for non-numeric data types.\n\n    Examples\n    --------\n    >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])\n    array([ True, False])\n    >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])\n    array([ True, True])\n    >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])\n    array([False,  True])\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan])\n    array([ True, False])\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    array([ True, True])\n    >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])\n    array([ True, False])\n    >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)\n    array([False, False])\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])\n    array([ True,  True])\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)\n    array([False,  True])\n    \"\"\"\n    def within_tol(x, y, atol, rtol):\n        with errstate(invalid='ignore'):\n            return less_equal(abs(x-y), atol + rtol * abs(y))\n\n    x = asanyarray(a)\n    y = asanyarray(b)\n\n    # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).\n    # This will cause casting of x later. Also, make sure to allow subclasses\n    # (e.g., for numpy.ma).\n    # NOTE: We explicitly allow timedelta, which used to work. This could\n    #       possibly be deprecated. See also gh-18286.\n    #       timedelta works if `atol` is an integer or also a timedelta.\n    #       Although, the default tolerances are unlikely to be useful\n    if y.dtype.kind != \"m\":\n        dt = multiarray.result_type(y, 1.)\n        y = array(y, dtype=dt, copy=False, subok=True)\n\n    xfin = isfinite(x)\n    yfin = isfinite(y)\n    if all(xfin) and all(yfin):\n        return within_tol(x, y, atol, rtol)\n    else:\n        finite = xfin & yfin\n        cond = zeros_like(finite, subok=True)\n        # Because we're using boolean indexing, x & y must be the same shape.\n        # Ideally, we'd just do x, y = broadcast_arrays(x, y). It's in\n        # lib.stride_tricks, though, so we can't import it here.\n        x = x * ones_like(cond)\n        y = y * ones_like(cond)\n        # Avoid subtraction with infinite/nan values...\n        cond[finite] = within_tol(x[finite], y[finite], atol, rtol)\n        # Check for equality of infinite values...\n        cond[~finite] = (x[~finite] == y[~finite])\n        if equal_nan:\n            # Make NaN == NaN\n            both_nan = isnan(x) & isnan(y)\n\n            # Needed to treat masked arrays correctly. = True would not work.\n            cond[both_nan] = both_nan[both_nan]\n\n        return cond[()]  # Flatten 0d arrays to scalars\n\n\ndef _array_equal_dispatcher(a1, a2, equal_nan=None):\n    return (a1, a2)\n\n\n@array_function_dispatch(_array_equal_dispatcher)\ndef array_equal(a1, a2, equal_nan=False):\n    \"\"\"\n    True if two arrays have the same shape and elements, False otherwise.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n    equal_nan : bool\n        Whether to compare NaN's as equal. If the dtype of a1 and a2 is\n        complex, values will be considered equal if either the real or the\n        imaginary component of a given value is ``nan``.\n\n        .. versionadded:: 1.19.0\n\n    Returns\n    -------\n    b : bool\n        Returns True if the arrays are equal.\n\n    See Also\n    --------\n    allclose: Returns True if two arrays are element-wise equal within a\n              tolerance.\n    array_equiv: Returns True if input arrays are shape consistent and all\n                 elements equal.\n\n    Examples\n    --------\n    >>> np.array_equal([1, 2], [1, 2])\n    True\n    >>> np.array_equal(np.array([1, 2]), np.array([1, 2]))\n    True\n    >>> np.array_equal([1, 2], [1, 2, 3])\n    False\n    >>> np.array_equal([1, 2], [1, 4])\n    False\n    >>> a = np.array([1, np.nan])\n    >>> np.array_equal(a, a)\n    False\n    >>> np.array_equal(a, a, equal_nan=True)\n    True\n\n    When ``equal_nan`` is True, complex values with nan components are\n    considered equal if either the real *or* the imaginary components are nan.\n\n    >>> a = np.array([1 + 1j])\n    >>> b = a.copy()\n    >>> a.real = np.nan\n    >>> b.imag = np.nan\n    >>> np.array_equal(a, b, equal_nan=True)\n    True\n    \"\"\"\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    if a1.shape != a2.shape:\n        return False\n    if not equal_nan:\n        return bool(asarray(a1 == a2).all())\n    # Handling NaN values if equal_nan is True\n    a1nan, a2nan = isnan(a1), isnan(a2)\n    # NaN's occur at different locations\n    if not (a1nan == a2nan).all():\n        return False\n    # Shapes of a1, a2 and masks are guaranteed to be consistent by this point\n    return bool(asarray(a1[~a1nan] == a2[~a1nan]).all())\n\n\ndef _array_equiv_dispatcher(a1, a2):\n    return (a1, a2)\n\n\n@array_function_dispatch(_array_equiv_dispatcher)\ndef array_equiv(a1, a2):\n    \"\"\"\n    Returns True if input arrays are shape consistent and all elements equal.\n\n    Shape consistent means they are either the same shape, or one input array\n    can be broadcasted to create the same shape as the other one.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n\n    Returns\n    -------\n    out : bool\n        True if equivalent, False otherwise.\n\n    Examples\n    --------\n    >>> np.array_equiv([1, 2], [1, 2])\n    True\n    >>> np.array_equiv([1, 2], [1, 3])\n    False\n\n    Showing the shape equivalence:\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 2]])\n    True\n    >>> np.array_equiv([1, 2], [[1, 2, 1, 2], [1, 2, 1, 2]])\n    False\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 3]])\n    False\n\n    \"\"\"\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    try:\n        multiarray.broadcast(a1, a2)\n    except Exception:\n        return False\n\n    return bool(asarray(a1 == a2).all())\n\n\nInf = inf = infty = Infinity = PINF\nnan = NaN = NAN\nFalse_ = bool_(False)\nTrue_ = bool_(True)\n\n\ndef extend_all(module):\n    existing = set(__all__)\n    mall = getattr(module, '__all__')\n    for a in mall:\n        if a not in existing:\n            __all__.append(a)\n\n\nfrom .umath import *\nfrom .numerictypes import *\nfrom . import fromnumeric\nfrom .fromnumeric import *\nfrom . import arrayprint\nfrom .arrayprint import *\nfrom . import _asarray\nfrom ._asarray import *\nfrom . import _ufunc_config\nfrom ._ufunc_config import *\nextend_all(fromnumeric)\nextend_all(umath)\nextend_all(numerictypes)\nextend_all(arrayprint)\nextend_all(_asarray)\nextend_all(_ufunc_config)\n",2544],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/logger.py":["\"\"\"\nHelpers for logging.\n\nThis module needs much love to become useful.\n\"\"\"\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# Copyright (c) 2008 Gael Varoquaux\n# License: BSD Style, 3 clauses.\n\nfrom __future__ import print_function\n\nimport time\nimport sys\nimport os\nimport shutil\nimport logging\nimport pprint\n\nfrom .disk import mkdirp\n\n\ndef _squeeze_time(t):\n    \"\"\"Remove .1s to the time under Windows: this is the time it take to\n    stat files. This is needed to make results similar to timings under\n    Unix, for tests\n    \"\"\"\n    if sys.platform.startswith('win'):\n        return max(0, t - .1)\n    else:\n        return t\n\n\ndef format_time(t):\n    t = _squeeze_time(t)\n    return \"%.1fs, %.1fmin\" % (t, t / 60.)\n\n\ndef short_format_time(t):\n    t = _squeeze_time(t)\n    if t > 60:\n        return \"%4.1fmin\" % (t / 60.)\n    else:\n        return \" %5.1fs\" % (t)\n\n\ndef pformat(obj, indent=0, depth=3):\n    if 'numpy' in sys.modules:\n        import numpy as np\n        print_options = np.get_printoptions()\n        np.set_printoptions(precision=6, threshold=64, edgeitems=1)\n    else:\n        print_options = None\n    out = pprint.pformat(obj, depth=depth, indent=indent)\n    if print_options:\n        np.set_printoptions(**print_options)\n    return out\n\n\n###############################################################################\n# class `Logger`\n###############################################################################\nclass Logger(object):\n    \"\"\" Base class for logging messages.\n    \"\"\"\n\n    def __init__(self, depth=3):\n        \"\"\"\n            Parameters\n            ----------\n            depth: int, optional\n                The depth of objects printed.\n        \"\"\"\n        self.depth = depth\n\n    def warn(self, msg):\n        logging.warning(\"[%s]: %s\" % (self, msg))\n\n    def debug(self, msg):\n        # XXX: This conflicts with the debug flag used in children class\n        logging.debug(\"[%s]: %s\" % (self, msg))\n\n    def format(self, obj, indent=0):\n        \"\"\"Return the formatted representation of the object.\"\"\"\n        return pformat(obj, indent=indent, depth=self.depth)\n\n\n###############################################################################\n# class `PrintTime`\n###############################################################################\nclass PrintTime(object):\n    \"\"\" Print and log messages while keeping track of time.\n    \"\"\"\n\n    def __init__(self, logfile=None, logdir=None):\n        if logfile is not None and logdir is not None:\n            raise ValueError('Cannot specify both logfile and logdir')\n        # XXX: Need argument docstring\n        self.last_time = time.time()\n        self.start_time = self.last_time\n        if logdir is not None:\n            logfile = os.path.join(logdir, 'joblib.log')\n        self.logfile = logfile\n        if logfile is not None:\n            mkdirp(os.path.dirname(logfile))\n            if os.path.exists(logfile):\n                # Rotate the logs\n                for i in range(1, 9):\n                    try:\n                        shutil.move(logfile + '.%i' % i,\n                                    logfile + '.%i' % (i + 1))\n                    except:\n                        \"No reason failing here\"\n                # Use a copy rather than a move, so that a process\n                # monitoring this file does not get lost.\n                try:\n                    shutil.copy(logfile, logfile + '.1')\n                except:\n                    \"No reason failing here\"\n            try:\n                with open(logfile, 'w') as logfile:\n                    logfile.write('\\nLogging joblib python script\\n')\n                    logfile.write('\\n---%s---\\n' % time.ctime(self.last_time))\n            except:\n                \"\"\" Multiprocessing writing to files can create race\n                    conditions. Rather fail silently than crash the\n                    computation.\n                \"\"\"\n                # XXX: We actually need a debug flag to disable this\n                # silent failure.\n\n    def __call__(self, msg='', total=False):\n        \"\"\" Print the time elapsed between the last call and the current\n            call, with an optional message.\n        \"\"\"\n        if not total:\n            time_lapse = time.time() - self.last_time\n            full_msg = \"%s: %s\" % (msg, format_time(time_lapse))\n        else:\n            # FIXME: Too much logic duplicated\n            time_lapse = time.time() - self.start_time\n            full_msg = \"%s: %.2fs, %.1f min\" % (msg, time_lapse,\n                                                time_lapse / 60)\n        print(full_msg, file=sys.stderr)\n        if self.logfile is not None:\n            try:\n                with open(self.logfile, 'a') as f:\n                    print(full_msg, file=f)\n            except:\n                \"\"\" Multiprocessing writing to files can create race\n                    conditions. Rather fail silently than crash the\n                    calculation.\n                \"\"\"\n                # XXX: We actually need a debug flag to disable this\n                # silent failure.\n        self.last_time = time.time()\n",156],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py":["__all__ = ['atleast_1d', 'atleast_2d', 'atleast_3d', 'block', 'hstack',\n           'stack', 'vstack']\n\nimport functools\nimport itertools\nimport operator\nimport warnings\n\nfrom . import numeric as _nx\nfrom . import overrides\nfrom ._asarray import array, asanyarray\nfrom .multiarray import normalize_axis_index\nfrom . import fromnumeric as _from_nx\n\n\narray_function_dispatch = functools.partial(\n    overrides.array_function_dispatch, module='numpy')\n\n\ndef _atleast_1d_dispatcher(*arys):\n    return arys\n\n\n@array_function_dispatch(_atleast_1d_dispatcher)\ndef atleast_1d(*arys):\n    \"\"\"\n    Convert inputs to arrays with at least one dimension.\n\n    Scalar inputs are converted to 1-dimensional arrays, whilst\n    higher-dimensional inputs are preserved.\n\n    Parameters\n    ----------\n    arys1, arys2, ... : array_like\n        One or more input arrays.\n\n    Returns\n    -------\n    ret : ndarray\n        An array, or list of arrays, each with ``a.ndim >= 1``.\n        Copies are made only if necessary.\n\n    See Also\n    --------\n    atleast_2d, atleast_3d\n\n    Examples\n    --------\n    >>> np.atleast_1d(1.0)\n    array([1.])\n\n    >>> x = np.arange(9.0).reshape(3,3)\n    >>> np.atleast_1d(x)\n    array([[0., 1., 2.],\n           [3., 4., 5.],\n           [6., 7., 8.]])\n    >>> np.atleast_1d(x) is x\n    True\n\n    >>> np.atleast_1d(1, [3, 4])\n    [array([1]), array([3, 4])]\n\n    \"\"\"\n    res = []\n    for ary in arys:\n        ary = asanyarray(ary)\n        if ary.ndim == 0:\n            result = ary.reshape(1)\n        else:\n            result = ary\n        res.append(result)\n    if len(res) == 1:\n        return res[0]\n    else:\n        return res\n\n\ndef _atleast_2d_dispatcher(*arys):\n    return arys\n\n\n@array_function_dispatch(_atleast_2d_dispatcher)\ndef atleast_2d(*arys):\n    \"\"\"\n    View inputs as arrays with at least two dimensions.\n\n    Parameters\n    ----------\n    arys1, arys2, ... : array_like\n        One or more array-like sequences.  Non-array inputs are converted\n        to arrays.  Arrays that already have two or more dimensions are\n        preserved.\n\n    Returns\n    -------\n    res, res2, ... : ndarray\n        An array, or list of arrays, each with ``a.ndim >= 2``.\n        Copies are avoided where possible, and views with two or more\n        dimensions are returned.\n\n    See Also\n    --------\n    atleast_1d, atleast_3d\n\n    Examples\n    --------\n    >>> np.atleast_2d(3.0)\n    array([[3.]])\n\n    >>> x = np.arange(3.0)\n    >>> np.atleast_2d(x)\n    array([[0., 1., 2.]])\n    >>> np.atleast_2d(x).base is x\n    True\n\n    >>> np.atleast_2d(1, [1, 2], [[1, 2]])\n    [array([[1]]), array([[1, 2]]), array([[1, 2]])]\n\n    \"\"\"\n    res = []\n    for ary in arys:\n        ary = asanyarray(ary)\n        if ary.ndim == 0:\n            result = ary.reshape(1, 1)\n        elif ary.ndim == 1:\n            result = ary[_nx.newaxis, :]\n        else:\n            result = ary\n        res.append(result)\n    if len(res) == 1:\n        return res[0]\n    else:\n        return res\n\n\ndef _atleast_3d_dispatcher(*arys):\n    return arys\n\n\n@array_function_dispatch(_atleast_3d_dispatcher)\ndef atleast_3d(*arys):\n    \"\"\"\n    View inputs as arrays with at least three dimensions.\n\n    Parameters\n    ----------\n    arys1, arys2, ... : array_like\n        One or more array-like sequences.  Non-array inputs are converted to\n        arrays.  Arrays that already have three or more dimensions are\n        preserved.\n\n    Returns\n    -------\n    res1, res2, ... : ndarray\n        An array, or list of arrays, each with ``a.ndim >= 3``.  Copies are\n        avoided where possible, and views with three or more dimensions are\n        returned.  For example, a 1-D array of shape ``(N,)`` becomes a view\n        of shape ``(1, N, 1)``, and a 2-D array of shape ``(M, N)`` becomes a\n        view of shape ``(M, N, 1)``.\n\n    See Also\n    --------\n    atleast_1d, atleast_2d\n\n    Examples\n    --------\n    >>> np.atleast_3d(3.0)\n    array([[[3.]]])\n\n    >>> x = np.arange(3.0)\n    >>> np.atleast_3d(x).shape\n    (1, 3, 1)\n\n    >>> x = np.arange(12.0).reshape(4,3)\n    >>> np.atleast_3d(x).shape\n    (4, 3, 1)\n    >>> np.atleast_3d(x).base is x.base  # x is a reshape, so not base itself\n    True\n\n    >>> for arr in np.atleast_3d([1, 2], [[1, 2]], [[[1, 2]]]):\n    ...     print(arr, arr.shape) # doctest: +SKIP\n    ...\n    [[[1]\n      [2]]] (1, 2, 1)\n    [[[1]\n      [2]]] (1, 2, 1)\n    [[[1 2]]] (1, 1, 2)\n\n    \"\"\"\n    res = []\n    for ary in arys:\n        ary = asanyarray(ary)\n        if ary.ndim == 0:\n            result = ary.reshape(1, 1, 1)\n        elif ary.ndim == 1:\n            result = ary[_nx.newaxis, :, _nx.newaxis]\n        elif ary.ndim == 2:\n            result = ary[:, :, _nx.newaxis]\n        else:\n            result = ary\n        res.append(result)\n    if len(res) == 1:\n        return res[0]\n    else:\n        return res\n\n\ndef _arrays_for_stack_dispatcher(arrays, stacklevel=4):\n    if not hasattr(arrays, '__getitem__') and hasattr(arrays, '__iter__'):\n        warnings.warn('arrays to stack must be passed as a \"sequence\" type '\n                      'such as list or tuple. Support for non-sequence '\n                      'iterables such as generators is deprecated as of '\n                      'NumPy 1.16 and will raise an error in the future.',\n                      FutureWarning, stacklevel=stacklevel)\n        return ()\n    return arrays\n\n\ndef _vhstack_dispatcher(tup):\n    return _arrays_for_stack_dispatcher(tup)\n\n\n@array_function_dispatch(_vhstack_dispatcher)\ndef vstack(tup):\n    \"\"\"\n    Stack arrays in sequence vertically (row wise).\n\n    This is equivalent to concatenation along the first axis after 1-D arrays\n    of shape `(N,)` have been reshaped to `(1,N)`. Rebuilds arrays divided by\n    `vsplit`.\n\n    This function makes most sense for arrays with up to 3 dimensions. For\n    instance, for pixel-data with a height (first axis), width (second axis),\n    and r/g/b channels (third axis). The functions `concatenate`, `stack` and\n    `block` provide more general stacking and concatenation operations.\n\n    Parameters\n    ----------\n    tup : sequence of ndarrays\n        The arrays must have the same shape along all but the first axis.\n        1-D arrays must have the same length.\n\n    Returns\n    -------\n    stacked : ndarray\n        The array formed by stacking the given arrays, will be at least 2-D.\n\n    See Also\n    --------\n    concatenate : Join a sequence of arrays along an existing axis.\n    stack : Join a sequence of arrays along a new axis.\n    block : Assemble an nd-array from nested lists of blocks.\n    hstack : Stack arrays in sequence horizontally (column wise).\n    dstack : Stack arrays in sequence depth wise (along third axis).\n    column_stack : Stack 1-D arrays as columns into a 2-D array.\n    vsplit : Split an array into multiple sub-arrays vertically (row-wise).\n\n    Examples\n    --------\n    >>> a = np.array([1, 2, 3])\n    >>> b = np.array([2, 3, 4])\n    >>> np.vstack((a,b))\n    array([[1, 2, 3],\n           [2, 3, 4]])\n\n    >>> a = np.array([[1], [2], [3]])\n    >>> b = np.array([[2], [3], [4]])\n    >>> np.vstack((a,b))\n    array([[1],\n           [2],\n           [3],\n           [2],\n           [3],\n           [4]])\n\n    \"\"\"\n    if not overrides.ARRAY_FUNCTION_ENABLED:\n        # raise warning if necessary\n        _arrays_for_stack_dispatcher(tup, stacklevel=2)\n    arrs = atleast_2d(*tup)\n    if not isinstance(arrs, list):\n        arrs = [arrs]\n    return _nx.concatenate(arrs, 0)\n\n\n@array_function_dispatch(_vhstack_dispatcher)\ndef hstack(tup):\n    \"\"\"\n    Stack arrays in sequence horizontally (column wise).\n\n    This is equivalent to concatenation along the second axis, except for 1-D\n    arrays where it concatenates along the first axis. Rebuilds arrays divided\n    by `hsplit`.\n\n    This function makes most sense for arrays with up to 3 dimensions. For\n    instance, for pixel-data with a height (first axis), width (second axis),\n    and r/g/b channels (third axis). The functions `concatenate`, `stack` and\n    `block` provide more general stacking and concatenation operations.\n\n    Parameters\n    ----------\n    tup : sequence of ndarrays\n        The arrays must have the same shape along all but the second axis,\n        except 1-D arrays which can be any length.\n\n    Returns\n    -------\n    stacked : ndarray\n        The array formed by stacking the given arrays.\n\n    See Also\n    --------\n    concatenate : Join a sequence of arrays along an existing axis.\n    stack : Join a sequence of arrays along a new axis.\n    block : Assemble an nd-array from nested lists of blocks.\n    vstack : Stack arrays in sequence vertically (row wise).\n    dstack : Stack arrays in sequence depth wise (along third axis).\n    column_stack : Stack 1-D arrays as columns into a 2-D array.\n    hsplit : Split an array into multiple sub-arrays horizontally (column-wise).\n\n    Examples\n    --------\n    >>> a = np.array((1,2,3))\n    >>> b = np.array((2,3,4))\n    >>> np.hstack((a,b))\n    array([1, 2, 3, 2, 3, 4])\n    >>> a = np.array([[1],[2],[3]])\n    >>> b = np.array([[2],[3],[4]])\n    >>> np.hstack((a,b))\n    array([[1, 2],\n           [2, 3],\n           [3, 4]])\n\n    \"\"\"\n    if not overrides.ARRAY_FUNCTION_ENABLED:\n        # raise warning if necessary\n        _arrays_for_stack_dispatcher(tup, stacklevel=2)\n\n    arrs = atleast_1d(*tup)\n    if not isinstance(arrs, list):\n        arrs = [arrs]\n    # As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\n    if arrs and arrs[0].ndim == 1:\n        return _nx.concatenate(arrs, 0)\n    else:\n        return _nx.concatenate(arrs, 1)\n\n\ndef _stack_dispatcher(arrays, axis=None, out=None):\n    arrays = _arrays_for_stack_dispatcher(arrays, stacklevel=6)\n    if out is not None:\n        # optimize for the typical case where only arrays is provided\n        arrays = list(arrays)\n        arrays.append(out)\n    return arrays\n\n\n@array_function_dispatch(_stack_dispatcher)\ndef stack(arrays, axis=0, out=None):\n    \"\"\"\n    Join a sequence of arrays along a new axis.\n\n    The ``axis`` parameter specifies the index of the new axis in the\n    dimensions of the result. For example, if ``axis=0`` it will be the first\n    dimension and if ``axis=-1`` it will be the last dimension.\n\n    .. versionadded:: 1.10.0\n\n    Parameters\n    ----------\n    arrays : sequence of array_like\n        Each array must have the same shape.\n\n    axis : int, optional\n        The axis in the result array along which the input arrays are stacked.\n\n    out : ndarray, optional\n        If provided, the destination to place the result. The shape must be\n        correct, matching that of what stack would have returned if no\n        out argument were specified.\n\n    Returns\n    -------\n    stacked : ndarray\n        The stacked array has one more dimension than the input arrays.\n\n    See Also\n    --------\n    concatenate : Join a sequence of arrays along an existing axis.\n    block : Assemble an nd-array from nested lists of blocks.\n    split : Split array into a list of multiple sub-arrays of equal size.\n\n    Examples\n    --------\n    >>> arrays = [np.random.randn(3, 4) for _ in range(10)]\n    >>> np.stack(arrays, axis=0).shape\n    (10, 3, 4)\n\n    >>> np.stack(arrays, axis=1).shape\n    (3, 10, 4)\n\n    >>> np.stack(arrays, axis=2).shape\n    (3, 4, 10)\n\n    >>> a = np.array([1, 2, 3])\n    >>> b = np.array([2, 3, 4])\n    >>> np.stack((a, b))\n    array([[1, 2, 3],\n           [2, 3, 4]])\n\n    >>> np.stack((a, b), axis=-1)\n    array([[1, 2],\n           [2, 3],\n           [3, 4]])\n\n    \"\"\"\n    if not overrides.ARRAY_FUNCTION_ENABLED:\n        # raise warning if necessary\n        _arrays_for_stack_dispatcher(arrays, stacklevel=2)\n\n    arrays = [asanyarray(arr) for arr in arrays]\n    if not arrays:\n        raise ValueError('need at least one array to stack')\n\n    shapes = {arr.shape for arr in arrays}\n    if len(shapes) != 1:\n        raise ValueError('all input arrays must have the same shape')\n\n    result_ndim = arrays[0].ndim + 1\n    axis = normalize_axis_index(axis, result_ndim)\n\n    sl = (slice(None),) * axis + (_nx.newaxis,)\n    expanded_arrays = [arr[sl] for arr in arrays]\n    return _nx.concatenate(expanded_arrays, axis=axis, out=out)\n\n\n# Internal functions to eliminate the overhead of repeated dispatch in one of\n# the two possible paths inside np.block.\n# Use getattr to protect against __array_function__ being disabled.\n_size = getattr(_from_nx.size, '__wrapped__', _from_nx.size)\n_ndim = getattr(_from_nx.ndim, '__wrapped__', _from_nx.ndim)\n_concatenate = getattr(_from_nx.concatenate, '__wrapped__', _from_nx.concatenate)\n\n\ndef _block_format_index(index):\n    \"\"\"\n    Convert a list of indices ``[0, 1, 2]`` into ``\"arrays[0][1][2]\"``.\n    \"\"\"\n    idx_str = ''.join('[{}]'.format(i) for i in index if i is not None)\n    return 'arrays' + idx_str\n\n\ndef _block_check_depths_match(arrays, parent_index=[]):\n    \"\"\"\n    Recursive function checking that the depths of nested lists in `arrays`\n    all match. Mismatch raises a ValueError as described in the block\n    docstring below.\n\n    The entire index (rather than just the depth) needs to be calculated\n    for each innermost list, in case an error needs to be raised, so that\n    the index of the offending list can be printed as part of the error.\n\n    Parameters\n    ----------\n    arrays : nested list of arrays\n        The arrays to check\n    parent_index : list of int\n        The full index of `arrays` within the nested lists passed to\n        `_block_check_depths_match` at the top of the recursion.\n\n    Returns\n    -------\n    first_index : list of int\n        The full index of an element from the bottom of the nesting in\n        `arrays`. If any element at the bottom is an empty list, this will\n        refer to it, and the last index along the empty axis will be None.\n    max_arr_ndim : int\n        The maximum of the ndims of the arrays nested in `arrays`.\n    final_size: int\n        The number of elements in the final array. This is used the motivate\n        the choice of algorithm used using benchmarking wisdom.\n\n    \"\"\"\n    if type(arrays) is tuple:\n        # not strictly necessary, but saves us from:\n        #  - more than one way to do things - no point treating tuples like\n        #    lists\n        #  - horribly confusing behaviour that results when tuples are\n        #    treated like ndarray\n        raise TypeError(\n            '{} is a tuple. '\n            'Only lists can be used to arrange blocks, and np.block does '\n            'not allow implicit conversion from tuple to ndarray.'.format(\n                _block_format_index(parent_index)\n            )\n        )\n    elif type(arrays) is list and len(arrays) > 0:\n        idxs_ndims = (_block_check_depths_match(arr, parent_index + [i])\n                      for i, arr in enumerate(arrays))\n\n        first_index, max_arr_ndim, final_size = next(idxs_ndims)\n        for index, ndim, size in idxs_ndims:\n            final_size += size\n            if ndim > max_arr_ndim:\n                max_arr_ndim = ndim\n            if len(index) != len(first_index):\n                raise ValueError(\n                    \"List depths are mismatched. First element was at depth \"\n                    \"{}, but there is an element at depth {} ({})\".format(\n                        len(first_index),\n                        len(index),\n                        _block_format_index(index)\n                    )\n                )\n            # propagate our flag that indicates an empty list at the bottom\n            if index[-1] is None:\n                first_index = index\n\n        return first_index, max_arr_ndim, final_size\n    elif type(arrays) is list and len(arrays) == 0:\n        # We've 'bottomed out' on an empty list\n        return parent_index + [None], 0, 0\n    else:\n        # We've 'bottomed out' - arrays is either a scalar or an array\n        size = _size(arrays)\n        return parent_index, _ndim(arrays), size\n\n\ndef _atleast_nd(a, ndim):\n    # Ensures `a` has at least `ndim` dimensions by prepending\n    # ones to `a.shape` as necessary\n    return array(a, ndmin=ndim, copy=False, subok=True)\n\n\ndef _accumulate(values):\n    return list(itertools.accumulate(values))\n\n\ndef _concatenate_shapes(shapes, axis):\n    \"\"\"Given array shapes, return the resulting shape and slices prefixes.\n\n    These help in nested concatenation.\n    \n    Returns\n    -------\n    shape: tuple of int\n        This tuple satisfies:\n        ```\n        shape, _ = _concatenate_shapes([arr.shape for shape in arrs], axis)\n        shape == concatenate(arrs, axis).shape\n        ```\n\n    slice_prefixes: tuple of (slice(start, end), )\n        For a list of arrays being concatenated, this returns the slice\n        in the larger array at axis that needs to be sliced into.\n\n        For example, the following holds:\n        ```\n        ret = concatenate([a, b, c], axis)\n        _, (sl_a, sl_b, sl_c) = concatenate_slices([a, b, c], axis)\n\n        ret[(slice(None),) * axis + sl_a] == a\n        ret[(slice(None),) * axis + sl_b] == b\n        ret[(slice(None),) * axis + sl_c] == c\n        ```\n\n        These are called slice prefixes since they are used in the recursive\n        blocking algorithm to compute the left-most slices during the\n        recursion. Therefore, they must be prepended to rest of the slice\n        that was computed deeper in the recursion.\n\n        These are returned as tuples to ensure that they can quickly be added\n        to existing slice tuple without creating a new tuple every time.\n\n    \"\"\"\n    # Cache a result that will be reused.\n    shape_at_axis = [shape[axis] for shape in shapes]\n\n    # Take a shape, any shape\n    first_shape = shapes[0]\n    first_shape_pre = first_shape[:axis]\n    first_shape_post = first_shape[axis+1:]\n\n    if any(shape[:axis] != first_shape_pre or\n           shape[axis+1:] != first_shape_post for shape in shapes):\n        raise ValueError(\n            'Mismatched array shapes in block along axis {}.'.format(axis))\n\n    shape = (first_shape_pre + (sum(shape_at_axis),) + first_shape[axis+1:])\n\n    offsets_at_axis = _accumulate(shape_at_axis)\n    slice_prefixes = [(slice(start, end),)\n                      for start, end in zip([0] + offsets_at_axis,\n                                            offsets_at_axis)]\n    return shape, slice_prefixes\n\n\ndef _block_info_recursion(arrays, max_depth, result_ndim, depth=0):\n    \"\"\"\n    Returns the shape of the final array, along with a list\n    of slices and a list of arrays that can be used for assignment inside the\n    new array\n\n    Parameters\n    ----------\n    arrays : nested list of arrays\n        The arrays to check\n    max_depth : list of int\n        The number of nested lists\n    result_ndim: int\n        The number of dimensions in thefinal array.\n\n    Returns\n    -------\n    shape : tuple of int\n        The shape that the final array will take on.\n    slices: list of tuple of slices\n        The slices into the full array required for assignment. These are\n        required to be prepended with ``(Ellipsis, )`` to obtain to correct\n        final index.\n    arrays: list of ndarray\n        The data to assign to each slice of the full array\n\n    \"\"\"\n    if depth < max_depth:\n        shapes, slices, arrays = zip(\n            *[_block_info_recursion(arr, max_depth, result_ndim, depth+1)\n              for arr in arrays])\n\n        axis = result_ndim - max_depth + depth\n        shape, slice_prefixes = _concatenate_shapes(shapes, axis)\n\n        # Prepend the slice prefix and flatten the slices\n        slices = [slice_prefix + the_slice\n                  for slice_prefix, inner_slices in zip(slice_prefixes, slices)\n                  for the_slice in inner_slices]\n\n        # Flatten the array list\n        arrays = functools.reduce(operator.add, arrays)\n\n        return shape, slices, arrays\n    else:\n        # We've 'bottomed out' - arrays is either a scalar or an array\n        # type(arrays) is not list\n        # Return the slice and the array inside a list to be consistent with\n        # the recursive case.\n        arr = _atleast_nd(arrays, result_ndim)\n        return arr.shape, [()], [arr]\n\n\ndef _block(arrays, max_depth, result_ndim, depth=0):\n    \"\"\"\n    Internal implementation of block based on repeated concatenation.\n    `arrays` is the argument passed to\n    block. `max_depth` is the depth of nested lists within `arrays` and\n    `result_ndim` is the greatest of the dimensions of the arrays in\n    `arrays` and the depth of the lists in `arrays` (see block docstring\n    for details).\n    \"\"\"\n    if depth < max_depth:\n        arrs = [_block(arr, max_depth, result_ndim, depth+1)\n                for arr in arrays]\n        return _concatenate(arrs, axis=-(max_depth-depth))\n    else:\n        # We've 'bottomed out' - arrays is either a scalar or an array\n        # type(arrays) is not list\n        return _atleast_nd(arrays, result_ndim)\n\n\ndef _block_dispatcher(arrays):\n    # Use type(...) is list to match the behavior of np.block(), which special\n    # cases list specifically rather than allowing for generic iterables or\n    # tuple. Also, we know that list.__array_function__ will never exist.\n    if type(arrays) is list:\n        for subarrays in arrays:\n            yield from _block_dispatcher(subarrays)\n    else:\n        yield arrays\n\n\n@array_function_dispatch(_block_dispatcher)\ndef block(arrays):\n    \"\"\"\n    Assemble an nd-array from nested lists of blocks.\n\n    Blocks in the innermost lists are concatenated (see `concatenate`) along\n    the last dimension (-1), then these are concatenated along the\n    second-last dimension (-2), and so on until the outermost list is reached.\n\n    Blocks can be of any dimension, but will not be broadcasted using the normal\n    rules. Instead, leading axes of size 1 are inserted, to make ``block.ndim``\n    the same for all blocks. This is primarily useful for working with scalars,\n    and means that code like ``np.block([v, 1])`` is valid, where\n    ``v.ndim == 1``.\n\n    When the nested list is two levels deep, this allows block matrices to be\n    constructed from their components.\n\n    .. versionadded:: 1.13.0\n\n    Parameters\n    ----------\n    arrays : nested list of array_like or scalars (but not tuples)\n        If passed a single ndarray or scalar (a nested list of depth 0), this\n        is returned unmodified (and not copied).\n\n        Elements shapes must match along the appropriate axes (without\n        broadcasting), but leading 1s will be prepended to the shape as\n        necessary to make the dimensions match.\n\n    Returns\n    -------\n    block_array : ndarray\n        The array assembled from the given blocks.\n\n        The dimensionality of the output is equal to the greatest of:\n        * the dimensionality of all the inputs\n        * the depth to which the input list is nested\n\n    Raises\n    ------\n    ValueError\n        * If list depths are mismatched - for instance, ``[[a, b], c]`` is\n          illegal, and should be spelt ``[[a, b], [c]]``\n        * If lists are empty - for instance, ``[[a, b], []]``\n\n    See Also\n    --------\n    concatenate : Join a sequence of arrays along an existing axis.\n    stack : Join a sequence of arrays along a new axis.\n    vstack : Stack arrays in sequence vertically (row wise).\n    hstack : Stack arrays in sequence horizontally (column wise).\n    dstack : Stack arrays in sequence depth wise (along third axis).\n    column_stack : Stack 1-D arrays as columns into a 2-D array.\n    vsplit : Split an array into multiple sub-arrays vertically (row-wise).\n\n    Notes\n    -----\n\n    When called with only scalars, ``np.block`` is equivalent to an ndarray\n    call. So ``np.block([[1, 2], [3, 4]])`` is equivalent to\n    ``np.array([[1, 2], [3, 4]])``.\n\n    This function does not enforce that the blocks lie on a fixed grid.\n    ``np.block([[a, b], [c, d]])`` is not restricted to arrays of the form::\n\n        AAAbb\n        AAAbb\n        cccDD\n\n    But is also allowed to produce, for some ``a, b, c, d``::\n\n        AAAbb\n        AAAbb\n        cDDDD\n\n    Since concatenation happens along the last axis first, `block` is _not_\n    capable of producing the following directly::\n\n        AAAbb\n        cccbb\n        cccDD\n\n    Matlab's \"square bracket stacking\", ``[A, B, ...; p, q, ...]``, is\n    equivalent to ``np.block([[A, B, ...], [p, q, ...]])``.\n\n    Examples\n    --------\n    The most common use of this function is to build a block matrix\n\n    >>> A = np.eye(2) * 2\n    >>> B = np.eye(3) * 3\n    >>> np.block([\n    ...     [A,               np.zeros((2, 3))],\n    ...     [np.ones((3, 2)), B               ]\n    ... ])\n    array([[2., 0., 0., 0., 0.],\n           [0., 2., 0., 0., 0.],\n           [1., 1., 3., 0., 0.],\n           [1., 1., 0., 3., 0.],\n           [1., 1., 0., 0., 3.]])\n\n    With a list of depth 1, `block` can be used as `hstack`\n\n    >>> np.block([1, 2, 3])              # hstack([1, 2, 3])\n    array([1, 2, 3])\n\n    >>> a = np.array([1, 2, 3])\n    >>> b = np.array([2, 3, 4])\n    >>> np.block([a, b, 10])             # hstack([a, b, 10])\n    array([ 1,  2,  3,  2,  3,  4, 10])\n\n    >>> A = np.ones((2, 2), int)\n    >>> B = 2 * A\n    >>> np.block([A, B])                 # hstack([A, B])\n    array([[1, 1, 2, 2],\n           [1, 1, 2, 2]])\n\n    With a list of depth 2, `block` can be used in place of `vstack`:\n\n    >>> a = np.array([1, 2, 3])\n    >>> b = np.array([2, 3, 4])\n    >>> np.block([[a], [b]])             # vstack([a, b])\n    array([[1, 2, 3],\n           [2, 3, 4]])\n\n    >>> A = np.ones((2, 2), int)\n    >>> B = 2 * A\n    >>> np.block([[A], [B]])             # vstack([A, B])\n    array([[1, 1],\n           [1, 1],\n           [2, 2],\n           [2, 2]])\n\n    It can also be used in places of `atleast_1d` and `atleast_2d`\n\n    >>> a = np.array(0)\n    >>> b = np.array([1])\n    >>> np.block([a])                    # atleast_1d(a)\n    array([0])\n    >>> np.block([b])                    # atleast_1d(b)\n    array([1])\n\n    >>> np.block([[a]])                  # atleast_2d(a)\n    array([[0]])\n    >>> np.block([[b]])                  # atleast_2d(b)\n    array([[1]])\n\n\n    \"\"\"\n    arrays, list_ndim, result_ndim, final_size = _block_setup(arrays)\n\n    # It was found through benchmarking that making an array of final size\n    # around 256x256 was faster by straight concatenation on a\n    # i7-7700HQ processor and dual channel ram 2400MHz.\n    # It didn't seem to matter heavily on the dtype used.\n    #\n    # A 2D array using repeated concatenation requires 2 copies of the array.\n    #\n    # The fastest algorithm will depend on the ratio of CPU power to memory\n    # speed.\n    # One can monitor the results of the benchmark\n    # https://pv.github.io/numpy-bench/#bench_shape_base.Block2D.time_block2d\n    # to tune this parameter until a C version of the `_block_info_recursion`\n    # algorithm is implemented which would likely be faster than the python\n    # version.\n    if list_ndim * final_size > (2 * 512 * 512):\n        return _block_slicing(arrays, list_ndim, result_ndim)\n    else:\n        return _block_concatenate(arrays, list_ndim, result_ndim)\n\n\n# These helper functions are mostly used for testing.\n# They allow us to write tests that directly call `_block_slicing`\n# or `_block_concatenate` without blocking large arrays to force the wisdom\n# to trigger the desired path.\ndef _block_setup(arrays):\n    \"\"\"\n    Returns\n    (`arrays`, list_ndim, result_ndim, final_size)\n    \"\"\"\n    bottom_index, arr_ndim, final_size = _block_check_depths_match(arrays)\n    list_ndim = len(bottom_index)\n    if bottom_index and bottom_index[-1] is None:\n        raise ValueError(\n            'List at {} cannot be empty'.format(\n                _block_format_index(bottom_index)\n            )\n        )\n    result_ndim = max(arr_ndim, list_ndim)\n    return arrays, list_ndim, result_ndim, final_size\n\n\ndef _block_slicing(arrays, list_ndim, result_ndim):\n    shape, slices, arrays = _block_info_recursion(\n        arrays, list_ndim, result_ndim)\n    dtype = _nx.result_type(*[arr.dtype for arr in arrays])\n\n    # Test preferring F only in the case that all input arrays are F\n    F_order = all(arr.flags['F_CONTIGUOUS'] for arr in arrays)\n    C_order = all(arr.flags['C_CONTIGUOUS'] for arr in arrays)\n    order = 'F' if F_order and not C_order else 'C'\n    result = _nx.empty(shape=shape, dtype=dtype, order=order)\n    # Note: In a c implementation, the function\n    # PyArray_CreateMultiSortedStridePerm could be used for more advanced\n    # guessing of the desired order.\n\n    for the_slice, arr in zip(slices, arrays):\n        result[(Ellipsis,) + the_slice] = arr\n    return result\n\n\ndef _block_concatenate(arrays, list_ndim, result_ndim):\n    result = _block(arrays, list_ndim, result_ndim)\n    if list_ndim == 0:\n        # Catch an edge case where _block returns a view because\n        # `arrays` is a single numpy array and not a list of numpy arrays.\n        # This might copy scalars or lists twice, but this isn't a likely\n        # usecase for those interested in performance\n        result = result.copy()\n    return result\n",901],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py":["# Copyright 2002 Gary Strangman.  All rights reserved\n# Copyright 2002-2016 The SciPy Developers\n#\n# The original code from Gary Strangman was heavily adapted for\n# use in SciPy by Travis Oliphant.  The original code came with the\n# following disclaimer:\n#\n# This software is provided \"as-is\".  There are no expressed or implied\n# warranties of any kind, including, but not limited to, the warranties\n# of merchantability and fitness for a given application.  In no event\n# shall Gary Strangman be liable for any direct, indirect, incidental,\n# special, exemplary or consequential damages (including, but not limited\n# to, loss of use, data or profits, or business interruption) however\n# caused and on any theory of liability, whether in contract, strict\n# liability or tort (including negligence or otherwise) arising in any way\n# out of the use of this software, even if advised of the possibility of\n# such damage.\n\n\"\"\"\nA collection of basic statistical functions for Python.  The function\nnames appear below.\n\n Some scalar functions defined here are also available in the scipy.special\n package where they work on arbitrary sized arrays.\n\nDisclaimers:  The function list is obviously incomplete and, worse, the\nfunctions are not optimized.  All functions have been tested (some more\nso than others), but they are far from bulletproof.  Thus, as with any\nfree software, no warranty or guarantee is expressed or implied. :-)  A\nfew extra functions that don't appear in the list below can be found by\ninterested treasure-hunters.  These functions don't necessarily have\nboth list and array versions but were deemed useful.\n\nCentral Tendency\n----------------\n.. autosummary::\n   :toctree: generated/\n\n    gmean\n    hmean\n    mode\n\nMoments\n-------\n.. autosummary::\n   :toctree: generated/\n\n    moment\n    variation\n    skew\n    kurtosis\n    normaltest\n\nAltered Versions\n----------------\n.. autosummary::\n   :toctree: generated/\n\n    tmean\n    tvar\n    tstd\n    tsem\n    describe\n\nFrequency Stats\n---------------\n.. autosummary::\n   :toctree: generated/\n\n    itemfreq\n    scoreatpercentile\n    percentileofscore\n    cumfreq\n    relfreq\n\nVariability\n-----------\n.. autosummary::\n   :toctree: generated/\n\n    obrientransform\n    sem\n    zmap\n    zscore\n    gstd\n    iqr\n    median_abs_deviation\n\nTrimming Functions\n------------------\n.. autosummary::\n   :toctree: generated/\n\n   trimboth\n   trim1\n\nCorrelation Functions\n---------------------\n.. autosummary::\n   :toctree: generated/\n\n   pearsonr\n   fisher_exact\n   spearmanr\n   pointbiserialr\n   kendalltau\n   weightedtau\n   linregress\n   theilslopes\n   multiscale_graphcorr\n\nInferential Stats\n-----------------\n.. autosummary::\n   :toctree: generated/\n\n   ttest_1samp\n   ttest_ind\n   ttest_ind_from_stats\n   ttest_rel\n   chisquare\n   power_divergence\n   kstest\n   ks_1samp\n   ks_2samp\n   epps_singleton_2samp\n   mannwhitneyu\n   ranksums\n   wilcoxon\n   kruskal\n   friedmanchisquare\n   brunnermunzel\n   combine_pvalues\n\nStatistical Distances\n---------------------\n.. autosummary::\n   :toctree: generated/\n\n   wasserstein_distance\n   energy_distance\n\nANOVA Functions\n---------------\n.. autosummary::\n   :toctree: generated/\n\n   f_oneway\n\nSupport Functions\n-----------------\n.. autosummary::\n   :toctree: generated/\n\n   rankdata\n   rvs_ratio_uniforms\n\nReferences\n----------\n.. [CRCProbStat2000] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n   Probability and Statistics Tables and Formulae. Chapman & Hall: New\n   York. 2000.\n\n\"\"\"\n\nimport warnings\nimport math\nfrom math import gcd\nfrom collections import namedtuple\n\nimport numpy as np\nfrom numpy import array, asarray, ma\n\nfrom scipy.spatial.distance import cdist\nfrom scipy.ndimage import measurements\nfrom scipy._lib._util import (_lazywhere, check_random_state, MapWrapper,\n                              rng_integers, float_factorial)\nimport scipy.special as special\nfrom scipy import linalg\nfrom . import distributions\nfrom . import mstats_basic\nfrom ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n                                   siegelslopes)\nfrom ._stats import (_kendall_dis, _toint64, _weightedrankedtau,\n                     _local_correlations)\nfrom ._rvs_sampling import rvs_ratio_uniforms\nfrom ._hypotests import epps_singleton_2samp, cramervonmises\n\n\n__all__ = ['find_repeats', 'gmean', 'hmean', 'mode', 'tmean', 'tvar',\n           'tmin', 'tmax', 'tstd', 'tsem', 'moment', 'variation',\n           'skew', 'kurtosis', 'describe', 'skewtest', 'kurtosistest',\n           'normaltest', 'jarque_bera', 'itemfreq',\n           'scoreatpercentile', 'percentileofscore',\n           'cumfreq', 'relfreq', 'obrientransform',\n           'sem', 'zmap', 'zscore', 'iqr', 'gstd', 'median_absolute_deviation',\n           'median_abs_deviation',\n           'sigmaclip', 'trimboth', 'trim1', 'trim_mean',\n           'f_oneway', 'F_onewayConstantInputWarning',\n           'F_onewayBadInputSizesWarning',\n           'PearsonRConstantInputWarning', 'PearsonRNearConstantInputWarning',\n           'pearsonr', 'fisher_exact', 'SpearmanRConstantInputWarning',\n           'spearmanr', 'pointbiserialr',\n           'kendalltau', 'weightedtau', 'multiscale_graphcorr',\n           'linregress', 'siegelslopes', 'theilslopes', 'ttest_1samp',\n           'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel',\n           'kstest', 'ks_1samp', 'ks_2samp',\n           'chisquare', 'power_divergence', 'mannwhitneyu',\n           'tiecorrect', 'ranksums', 'kruskal', 'friedmanchisquare',\n           'rankdata', 'rvs_ratio_uniforms',\n           'combine_pvalues', 'wasserstein_distance', 'energy_distance',\n           'brunnermunzel', 'epps_singleton_2samp', 'cramervonmises']\n\n\ndef _contains_nan(a, nan_policy='propagate'):\n    policies = ['propagate', 'raise', 'omit']\n    if nan_policy not in policies:\n        raise ValueError(\"nan_policy must be one of {%s}\" %\n                         ', '.join(\"'%s'\" % s for s in policies))\n    try:\n        # Calling np.sum to avoid creating a huge array into memory\n        # e.g. np.isnan(a).any()\n        with np.errstate(invalid='ignore'):\n            contains_nan = np.isnan(np.sum(a))\n    except TypeError:\n        # This can happen when attempting to sum things which are not\n        # numbers (e.g. as in the function `mode`). Try an alternative method:\n        try:\n            contains_nan = np.nan in set(a.ravel())\n        except TypeError:\n            # Don't know what to do. Fall back to omitting nan values and\n            # issue a warning.\n            contains_nan = False\n            nan_policy = 'omit'\n            warnings.warn(\"The input array could not be properly checked for nan \"\n                          \"values. nan values will be ignored.\", RuntimeWarning)\n\n    if contains_nan and nan_policy == 'raise':\n        raise ValueError(\"The input contains nan values\")\n\n    return contains_nan, nan_policy\n\n\ndef _chk_asarray(a, axis):\n    if axis is None:\n        a = np.ravel(a)\n        outaxis = 0\n    else:\n        a = np.asarray(a)\n        outaxis = axis\n\n    if a.ndim == 0:\n        a = np.atleast_1d(a)\n\n    return a, outaxis\n\n\ndef _chk2_asarray(a, b, axis):\n    if axis is None:\n        a = np.ravel(a)\n        b = np.ravel(b)\n        outaxis = 0\n    else:\n        a = np.asarray(a)\n        b = np.asarray(b)\n        outaxis = axis\n\n    if a.ndim == 0:\n        a = np.atleast_1d(a)\n    if b.ndim == 0:\n        b = np.atleast_1d(b)\n\n    return a, b, outaxis\n\n\ndef _shape_with_dropped_axis(a, axis):\n    \"\"\"\n    Given an array `a` and an integer `axis`, return the shape\n    of `a` with the `axis` dimension removed.\n\n    Examples\n    --------\n    >>> a = np.zeros((3, 5, 2))\n    >>> _shape_with_dropped_axis(a, 1)\n    (3, 2)\n    \"\"\"\n    shp = list(a.shape)\n    try:\n        del shp[axis]\n    except IndexError:\n        raise np.AxisError(axis, a.ndim) from None\n    return tuple(shp)\n\n\ndef _broadcast_shapes(shape1, shape2):\n    \"\"\"\n    Given two shapes (i.e. tuples of integers), return the shape\n    that would result from broadcasting two arrays with the given\n    shapes.\n\n    Examples\n    --------\n    >>> _broadcast_shapes((2, 1), (4, 1, 3))\n    (4, 2, 3)\n    \"\"\"\n    d = len(shape1) - len(shape2)\n    if d <= 0:\n        shp1 = (1,)*(-d) + shape1\n        shp2 = shape2\n    elif d > 0:\n        shp1 = shape1\n        shp2 = (1,)*d + shape2\n    shape = []\n    for n1, n2 in zip(shp1, shp2):\n        if n1 == 1:\n            n = n2\n        elif n2 == 1 or n1 == n2:\n            n = n1\n        else:\n            raise ValueError(f'shapes {shape1} and {shape2} could not be '\n                             'broadcast together')\n        shape.append(n)\n    return tuple(shape)\n\n\ndef _broadcast_shapes_with_dropped_axis(a, b, axis):\n    \"\"\"\n    Given two arrays `a` and `b` and an integer `axis`, find the\n    shape of the broadcast result after dropping `axis` from the\n    shapes of `a` and `b`.\n\n    Examples\n    --------\n    >>> a = np.zeros((5, 2, 1))\n    >>> b = np.zeros((1, 9, 3))\n    >>> _broadcast_shapes_with_dropped_axis(a, b, 1)\n    (5, 3)\n    \"\"\"\n    shp1 = _shape_with_dropped_axis(a, axis)\n    shp2 = _shape_with_dropped_axis(b, axis)\n    try:\n        shp = _broadcast_shapes(shp1, shp2)\n    except ValueError:\n        raise ValueError(f'non-axis shapes {shp1} and {shp2} could not be '\n                         'broadcast together') from None\n    return shp\n\n\ndef gmean(a, axis=0, dtype=None):\n    \"\"\"\n    Compute the geometric mean along the specified axis.\n\n    Return the geometric average of the array elements.\n    That is:  n-th root of (x1 * x2 * ... * xn)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the geometric mean is computed. Default is 0.\n        If None, compute over the whole array `a`.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed. If dtype is not specified, it defaults to the\n        dtype of a, unless a has an integer dtype with a precision less than\n        that of the default platform integer. In that case, the default\n        platform integer is used.\n\n    Returns\n    -------\n    gmean : ndarray\n        See `dtype` parameter above.\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.average : Weighted average\n    hmean : Harmonic mean\n\n    Notes\n    -----\n    The geometric average is computed over a single dimension of the input\n    array, axis=0 by default, or all values in the array if axis=None.\n    float64 intermediate and return values are used for integer inputs.\n\n    Use masked arrays to ignore any non-finite values in the input or that\n    arise in the calculations such as Not a Number and infinity because masked\n    arrays automatically mask any non-finite values.\n\n    Examples\n    --------\n    >>> from scipy.stats import gmean\n    >>> gmean([1, 4])\n    2.0\n    >>> gmean([1, 2, 3, 4, 5, 6, 7])\n    3.3800151591412964\n\n    \"\"\"\n    if not isinstance(a, np.ndarray):\n        # if not an ndarray object attempt to convert it\n        log_a = np.log(np.array(a, dtype=dtype))\n    elif dtype:\n        # Must change the default dtype allowing array type\n        if isinstance(a, np.ma.MaskedArray):\n            log_a = np.log(np.ma.asarray(a, dtype=dtype))\n        else:\n            log_a = np.log(np.asarray(a, dtype=dtype))\n    else:\n        log_a = np.log(a)\n    return np.exp(log_a.mean(axis=axis))\n\n\ndef hmean(a, axis=0, dtype=None):\n    \"\"\"\n    Calculate the harmonic mean along the specified axis.\n\n    That is:  n / (1/x1 + 1/x2 + ... + 1/xn)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array, masked array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the harmonic mean is computed. Default is 0.\n        If None, compute over the whole array `a`.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed. If `dtype` is not specified, it defaults to the\n        dtype of `a`, unless `a` has an integer `dtype` with a precision less\n        than that of the default platform integer. In that case, the default\n        platform integer is used.\n\n    Returns\n    -------\n    hmean : ndarray\n        See `dtype` parameter above.\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.average : Weighted average\n    gmean : Geometric mean\n\n    Notes\n    -----\n    The harmonic mean is computed over a single dimension of the input\n    array, axis=0 by default, or all values in the array if axis=None.\n    float64 intermediate and return values are used for integer inputs.\n\n    Use masked arrays to ignore any non-finite values in the input or that\n    arise in the calculations such as Not a Number and infinity.\n\n    Examples\n    --------\n    >>> from scipy.stats import hmean\n    >>> hmean([1, 4])\n    1.6000000000000001\n    >>> hmean([1, 2, 3, 4, 5, 6, 7])\n    2.6997245179063363\n\n    \"\"\"\n    if not isinstance(a, np.ndarray):\n        a = np.array(a, dtype=dtype)\n    if np.all(a >= 0):\n        # Harmonic mean only defined if greater than or equal to to zero.\n        if isinstance(a, np.ma.MaskedArray):\n            size = a.count(axis)\n        else:\n            if axis is None:\n                a = a.ravel()\n                size = a.shape[0]\n            else:\n                size = a.shape[axis]\n        with np.errstate(divide='ignore'):\n            return size / np.sum(1.0 / a, axis=axis, dtype=dtype)\n    else:\n        raise ValueError(\"Harmonic mean only defined if all elements greater \"\n                         \"than or equal to zero\")\n\n\nModeResult = namedtuple('ModeResult', ('mode', 'count'))\n\n\ndef mode(a, axis=0, nan_policy='propagate'):\n    \"\"\"\n    Return an array of the modal (most common) value in the passed array.\n\n    If there is more than one such value, only the smallest is returned.\n    The bin-count for the modal bins is also returned.\n\n    Parameters\n    ----------\n    a : array_like\n        n-dimensional array of which to find mode(s).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    mode : ndarray\n        Array of modal values.\n    count : ndarray\n        Array of counts for each mode.\n\n    Examples\n    --------\n    >>> a = np.array([[6, 8, 3, 0],\n    ...               [3, 2, 1, 7],\n    ...               [8, 1, 8, 4],\n    ...               [5, 3, 0, 5],\n    ...               [4, 7, 5, 9]])\n    >>> from scipy import stats\n    >>> stats.mode(a)\n    ModeResult(mode=array([[3, 1, 0, 0]]), count=array([[1, 1, 1, 1]]))\n\n    To get mode of whole array, specify ``axis=None``:\n\n    >>> stats.mode(a, axis=None)\n    ModeResult(mode=array([3]), count=array([3]))\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n    if a.size == 0:\n        return ModeResult(np.array([]), np.array([]))\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.mode(a, axis)\n\n    if a.dtype == object and np.nan in set(a.ravel()):\n        # Fall back to a slower method since np.unique does not work with NaN\n        scores = set(np.ravel(a))  # get ALL unique values\n        testshape = list(a.shape)\n        testshape[axis] = 1\n        oldmostfreq = np.zeros(testshape, dtype=a.dtype)\n        oldcounts = np.zeros(testshape, dtype=int)\n\n        for score in scores:\n            template = (a == score)\n            counts = np.expand_dims(np.sum(template, axis), axis)\n            mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)\n            oldcounts = np.maximum(counts, oldcounts)\n            oldmostfreq = mostfrequent\n\n        return ModeResult(mostfrequent, oldcounts)\n\n    def _mode1D(a):\n        vals, cnts = np.unique(a, return_counts=True)\n        return vals[cnts.argmax()], cnts.max()\n\n    # np.apply_along_axis will convert the _mode1D tuples to a numpy array, casting types in the process\n    # This recreates the results without that issue\n    # View of a, rotated so the requested axis is last\n    in_dims = list(range(a.ndim))\n    a_view = np.transpose(a, in_dims[:axis] + in_dims[axis+1:] + [axis])\n\n    inds = np.ndindex(a_view.shape[:-1])\n    modes = np.empty(a_view.shape[:-1], dtype=a.dtype)\n    counts = np.empty(a_view.shape[:-1], dtype=np.int_)\n    for ind in inds:\n        modes[ind], counts[ind] = _mode1D(a_view[ind])\n    newshape = list(a.shape)\n    newshape[axis] = 1\n    return ModeResult(modes.reshape(newshape), counts.reshape(newshape))\n\n\ndef _mask_to_limits(a, limits, inclusive):\n    \"\"\"Mask an array for values outside of given limits.\n\n    This is primarily a utility function.\n\n    Parameters\n    ----------\n    a : array\n    limits : (float or None, float or None)\n        A tuple consisting of the (lower limit, upper limit).  Values in the\n        input array less than the lower limit or greater than the upper limit\n        will be masked out. None implies no limit.\n    inclusive : (bool, bool)\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to lower or upper are allowed.\n\n    Returns\n    -------\n    A MaskedArray.\n\n    Raises\n    ------\n    A ValueError if there are no values within the given limits.\n\n    \"\"\"\n    lower_limit, upper_limit = limits\n    lower_include, upper_include = inclusive\n    am = ma.MaskedArray(a)\n    if lower_limit is not None:\n        if lower_include:\n            am = ma.masked_less(am, lower_limit)\n        else:\n            am = ma.masked_less_equal(am, lower_limit)\n\n    if upper_limit is not None:\n        if upper_include:\n            am = ma.masked_greater(am, upper_limit)\n        else:\n            am = ma.masked_greater_equal(am, upper_limit)\n\n    if am.count() == 0:\n        raise ValueError(\"No array values within given limits\")\n\n    return am\n\n\ndef tmean(a, limits=None, inclusive=(True, True), axis=None):\n    \"\"\"\n    Compute the trimmed mean.\n\n    This function finds the arithmetic mean of given values, ignoring values\n    outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored.  When limits is None (default), then all\n        values are used.  Either of the limit values in the tuple can also be\n        None representing a half-open interval.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to compute test. Default is None.\n\n    Returns\n    -------\n    tmean : float\n        Trimmed mean.\n\n    See Also\n    --------\n    trim_mean : Returns mean after trimming a proportion from both tails.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmean(x)\n    9.5\n    >>> stats.tmean(x, (3,17))\n    10.0\n\n    \"\"\"\n    a = asarray(a)\n    if limits is None:\n        return np.mean(a, None)\n\n    am = _mask_to_limits(a.ravel(), limits, inclusive)\n    return am.mean(axis=axis)\n\n\ndef tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    \"\"\"\n    Compute the trimmed variance.\n\n    This function computes the sample variance of an array of values,\n    while ignoring values which are outside of given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tvar : float\n        Trimmed variance.\n\n    Notes\n    -----\n    `tvar` computes the unbiased sample variance, i.e. it uses a correction\n    factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tvar(x)\n    35.0\n    >>> stats.tvar(x, (3,17))\n    20.0\n\n    \"\"\"\n    a = asarray(a)\n    a = a.astype(float)\n    if limits is None:\n        return a.var(ddof=ddof, axis=axis)\n    am = _mask_to_limits(a, limits, inclusive)\n    amnan = am.filled(fill_value=np.nan)\n    return np.nanvar(amnan, ddof=ddof, axis=axis)\n\n\ndef tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    \"\"\"\n    Compute the trimmed minimum.\n\n    This function finds the miminum value of an array `a` along the\n    specified axis, but only considering values greater than a specified\n    lower limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    lowerlimit : None or float, optional\n        Values in the input array less than the given limit will be ignored.\n        When lowerlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the lower limit\n        are included.  The default value is True.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    tmin : float, int or ndarray\n        Trimmed minimum.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmin(x)\n    0\n\n    >>> stats.tmin(x, 13)\n    13\n\n    >>> stats.tmin(x, 13, inclusive=False)\n    14\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n    am = _mask_to_limits(a, (lowerlimit, None), (inclusive, False))\n\n    contains_nan, nan_policy = _contains_nan(am, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        am = ma.masked_invalid(am)\n\n    res = ma.minimum.reduce(am, axis).data\n    if res.ndim == 0:\n        return res[()]\n    return res\n\n\ndef tmax(a, upperlimit=None, axis=0, inclusive=True, nan_policy='propagate'):\n    \"\"\"\n    Compute the trimmed maximum.\n\n    This function computes the maximum value of an array along a given axis,\n    while ignoring values larger than a specified upper limit.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    upperlimit : None or float, optional\n        Values in the input array greater than the given limit will be ignored.\n        When upperlimit is None, then all values are used. The default value\n        is None.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    inclusive : {True, False}, optional\n        This flag determines whether values exactly equal to the upper limit\n        are included.  The default value is True.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    tmax : float, int or ndarray\n        Trimmed maximum.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tmax(x)\n    19\n\n    >>> stats.tmax(x, 13)\n    13\n\n    >>> stats.tmax(x, 13, inclusive=False)\n    12\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n    am = _mask_to_limits(a, (None, upperlimit), (False, inclusive))\n\n    contains_nan, nan_policy = _contains_nan(am, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        am = ma.masked_invalid(am)\n\n    res = ma.maximum.reduce(am, axis).data\n    if res.ndim == 0:\n        return res[()]\n    return res\n\n\ndef tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    \"\"\"\n    Compute the trimmed sample standard deviation.\n\n    This function finds the sample standard deviation of given values,\n    ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tstd : float\n        Trimmed sample standard deviation.\n\n    Notes\n    -----\n    `tstd` computes the unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tstd(x)\n    5.9160797830996161\n    >>> stats.tstd(x, (3,17))\n    4.4721359549995796\n\n    \"\"\"\n    return np.sqrt(tvar(a, limits, inclusive, axis, ddof))\n\n\ndef tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n    \"\"\"\n    Compute the trimmed standard error of the mean.\n\n    This function finds the standard error of the mean for given\n    values, ignoring values outside the given `limits`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of values.\n    limits : None or (lower limit, upper limit), optional\n        Values in the input array less than the lower limit or greater than the\n        upper limit will be ignored. When limits is None, then all values are\n        used. Either of the limit values in the tuple can also be None\n        representing a half-open interval.  The default value is None.\n    inclusive : (bool, bool), optional\n        A tuple consisting of the (lower flag, upper flag).  These flags\n        determine whether values exactly equal to the lower or upper limits\n        are included.  The default value is (True, True).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over the\n        whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom.  Default is 1.\n\n    Returns\n    -------\n    tsem : float\n        Trimmed standard error of the mean.\n\n    Notes\n    -----\n    `tsem` uses unbiased sample standard deviation, i.e. it uses a\n    correction factor ``n / (n - 1)``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.tsem(x)\n    1.3228756555322954\n    >>> stats.tsem(x, (3,17))\n    1.1547005383792515\n\n    \"\"\"\n    a = np.asarray(a).ravel()\n    if limits is None:\n        return a.std(ddof=ddof) / np.sqrt(a.size)\n\n    am = _mask_to_limits(a, limits, inclusive)\n    sd = np.sqrt(np.ma.var(am, ddof=ddof, axis=axis))\n    return sd / np.sqrt(am.count())\n\n\n#####################################\n#              MOMENTS              #\n#####################################\n\ndef moment(a, moment=1, axis=0, nan_policy='propagate'):\n    r\"\"\"\n    Calculate the nth moment about the mean for a sample.\n\n    A moment is a specific quantitative measure of the shape of a set of\n    points. It is often used to calculate coefficients of skewness and kurtosis\n    due to its close relationship with them.\n\n    Parameters\n    ----------\n    a : array_like\n       Input array.\n    moment : int or array_like of ints, optional\n       Order of central moment that is returned. Default is 1.\n    axis : int or None, optional\n       Axis along which the central moment is computed. Default is 0.\n       If None, compute over the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    n-th central moment : ndarray or float\n       The appropriate moment along the given axis or over all values if axis\n       is None. The denominator for the moment calculation is the number of\n       observations, no degrees of freedom correction is done.\n\n    See Also\n    --------\n    kurtosis, skew, describe\n\n    Notes\n    -----\n    The k-th central moment of a data sample is:\n\n    .. math::\n\n        m_k = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^k\n\n    Where n is the number of samples and x-bar is the mean. This function uses\n    exponentiation by squares [1]_ for efficiency.\n\n    References\n    ----------\n    .. [1] https://eli.thegreenplace.net/2009/03/21/efficient-integer-exponentiation-algorithms\n\n    Examples\n    --------\n    >>> from scipy.stats import moment\n    >>> moment([1, 2, 3, 4, 5], moment=1)\n    0.0\n    >>> moment([1, 2, 3, 4, 5], moment=2)\n    2.0\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.moment(a, moment, axis)\n\n    if a.size == 0:\n        # empty array, return nan(s) with shape matching `moment`\n        if np.isscalar(moment):\n            return np.nan\n        else:\n            return np.full(np.asarray(moment).shape, np.nan, dtype=np.float64)\n\n    # for array_like moment input, return a value for each.\n    if not np.isscalar(moment):\n        mmnt = [_moment(a, i, axis) for i in moment]\n        return np.array(mmnt)\n    else:\n        return _moment(a, moment, axis)\n\n\ndef _moment(a, moment, axis):\n    if np.abs(moment - np.round(moment)) > 0:\n        raise ValueError(\"All moment parameters must be integers\")\n\n    if moment == 0:\n        # When moment equals 0, the result is 1, by definition.\n        shape = list(a.shape)\n        del shape[axis]\n        if shape:\n            # return an actual array of the appropriate shape\n            return np.ones(shape, dtype=float)\n        else:\n            # the input was 1D, so return a scalar instead of a rank-0 array\n            return 1.0\n\n    elif moment == 1:\n        # By definition the first moment about the mean is 0.\n        shape = list(a.shape)\n        del shape[axis]\n        if shape:\n            # return an actual array of the appropriate shape\n            return np.zeros(shape, dtype=float)\n        else:\n            # the input was 1D, so return a scalar instead of a rank-0 array\n            return np.float64(0.0)\n    else:\n        # Exponentiation by squares: form exponent sequence\n        n_list = [moment]\n        current_n = moment\n        while current_n > 2:\n            if current_n % 2:\n                current_n = (current_n - 1) / 2\n            else:\n                current_n /= 2\n            n_list.append(current_n)\n\n        # Starting point for exponentiation by squares\n        a_zero_mean = a - np.expand_dims(np.mean(a, axis), axis)\n        if n_list[-1] == 1:\n            s = a_zero_mean.copy()\n        else:\n            s = a_zero_mean**2\n\n        # Perform multiplications\n        for n in n_list[-2::-1]:\n            s = s**2\n            if n % 2:\n                s *= a_zero_mean\n        return np.mean(s, axis)\n\n\ndef variation(a, axis=0, nan_policy='propagate'):\n    \"\"\"\n    Compute the coefficient of variation.\n\n    The coefficient of variation is the ratio of the biased standard\n    deviation to the mean.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int or None, optional\n        Axis along which to calculate the coefficient of variation. Default\n        is 0. If None, compute over the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    variation : ndarray\n        The calculated variation along the requested axis.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n\n    Examples\n    --------\n    >>> from scipy.stats import variation\n    >>> variation([1, 2, 3, 4, 5])\n    0.47140452079103173\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.variation(a, axis)\n\n    return a.std(axis) / a.mean(axis)\n\n\ndef skew(a, axis=0, bias=True, nan_policy='propagate'):\n    r\"\"\"\n    Compute the sample skewness of a data set.\n\n    For normally distributed data, the skewness should be about zero. For\n    unimodal continuous distributions, a skewness value greater than zero means\n    that there is more weight in the right tail of the distribution. The\n    function `skewtest` can be used to determine if the skewness value\n    is close enough to zero, statistically speaking.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array.\n    axis : int or None, optional\n        Axis along which skewness is calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    bias : bool, optional\n        If False, then the calculations are corrected for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    skewness : ndarray\n        The skewness of values along an axis, returning 0 where all values are\n        equal.\n\n    Notes\n    -----\n    The sample skewness is computed as the Fisher-Pearson coefficient\n    of skewness, i.e.\n\n    .. math::\n\n        g_1=\\frac{m_3}{m_2^{3/2}}\n\n    where\n\n    .. math::\n\n        m_i=\\frac{1}{N}\\sum_{n=1}^N(x[n]-\\bar{x})^i\n\n    is the biased sample :math:`i\\texttt{th}` central moment, and :math:`\\bar{x}` is\n    the sample mean.  If ``bias`` is False, the calculations are\n    corrected for bias and the value computed is the adjusted\n    Fisher-Pearson standardized moment coefficient, i.e.\n\n    .. math::\n\n        G_1=\\frac{k_3}{k_2^{3/2}}=\n            \\frac{\\sqrt{N(N-1)}}{N-2}\\frac{m_3}{m_2^{3/2}}.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n       Section 2.2.24.1\n\n    Examples\n    --------\n    >>> from scipy.stats import skew\n    >>> skew([1, 2, 3, 4, 5])\n    0.0\n    >>> skew([2, 8, 0, 4, 1, 9, 9, 0])\n    0.2650554122698573\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n    n = a.shape[axis]\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.skew(a, axis, bias)\n\n    m2 = moment(a, 2, axis)\n    m3 = moment(a, 3, axis)\n    zero = (m2 == 0)\n    vals = _lazywhere(~zero, (m2, m3),\n                      lambda m2, m3: m3 / m2**1.5,\n                      0.)\n    if not bias:\n        can_correct = (n > 2) & (m2 > 0)\n        if can_correct.any():\n            m2 = np.extract(can_correct, m2)\n            m3 = np.extract(can_correct, m3)\n            nval = np.sqrt((n - 1.0) * n) / (n - 2.0) * m3 / m2**1.5\n            np.place(vals, can_correct, nval)\n\n    if vals.ndim == 0:\n        return vals.item()\n\n    return vals\n\n\ndef kurtosis(a, axis=0, fisher=True, bias=True, nan_policy='propagate'):\n    \"\"\"\n    Compute the kurtosis (Fisher or Pearson) of a dataset.\n\n    Kurtosis is the fourth central moment divided by the square of the\n    variance. If Fisher's definition is used, then 3.0 is subtracted from\n    the result to give 0.0 for a normal distribution.\n\n    If bias is False then the kurtosis is calculated using k statistics to\n    eliminate bias coming from biased moment estimators\n\n    Use `kurtosistest` to see if result is close enough to normal.\n\n    Parameters\n    ----------\n    a : array\n        Data for which the kurtosis is calculated.\n    axis : int or None, optional\n        Axis along which the kurtosis is calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    fisher : bool, optional\n        If True, Fisher's definition is used (normal ==> 0.0). If False,\n        Pearson's definition is used (normal ==> 3.0).\n    bias : bool, optional\n        If False, then the calculations are corrected for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.\n\n    Returns\n    -------\n    kurtosis : array\n        The kurtosis of values along an axis. If all values are equal,\n        return -3 for Fisher's definition and 0 for Pearson's definition.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n\n    Examples\n    --------\n    In Fisher's definiton, the kurtosis of the normal distribution is zero.\n    In the following example, the kurtosis is close to zero, because it was\n    calculated from the dataset, not from the continuous distribution.\n\n    >>> from scipy.stats import norm, kurtosis\n    >>> data = norm.rvs(size=1000, random_state=3)\n    >>> kurtosis(data)\n    -0.06928694200380558\n\n    The distribution with a higher kurtosis has a heavier tail.\n    The zero valued kurtosis of the normal distribution in Fisher's definition\n    can serve as a reference point.\n\n    >>> import matplotlib.pyplot as plt\n    >>> import scipy.stats as stats\n    >>> from scipy.stats import kurtosis\n\n    >>> x = np.linspace(-5, 5, 100)\n    >>> ax = plt.subplot()\n    >>> distnames = ['laplace', 'norm', 'uniform']\n\n    >>> for distname in distnames:\n    ...     if distname == 'uniform':\n    ...         dist = getattr(stats, distname)(loc=-2, scale=4)\n    ...     else:\n    ...         dist = getattr(stats, distname)\n    ...     data = dist.rvs(size=1000)\n    ...     kur = kurtosis(data, fisher=True)\n    ...     y = dist.pdf(x)\n    ...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n    ...     ax.legend()\n\n    The Laplace distribution has a heavier tail than the normal distribution.\n    The uniform distribution (which has negative kurtosis) has the thinnest\n    tail.\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.kurtosis(a, axis, fisher, bias)\n\n    n = a.shape[axis]\n    m2 = moment(a, 2, axis)\n    m4 = moment(a, 4, axis)\n    zero = (m2 == 0)\n    with np.errstate(all='ignore'):\n        vals = np.where(zero, 0, m4 / m2**2.0)\n\n    if not bias:\n        can_correct = (n > 3) & (m2 > 0)\n        if can_correct.any():\n            m2 = np.extract(can_correct, m2)\n            m4 = np.extract(can_correct, m4)\n            nval = 1.0/(n-2)/(n-3) * ((n**2-1.0)*m4/m2**2.0 - 3*(n-1)**2.0)\n            np.place(vals, can_correct, nval + 3.0)\n\n    if vals.ndim == 0:\n        vals = vals.item()  # array scalar\n\n    return vals - 3 if fisher else vals\n\n\nDescribeResult = namedtuple('DescribeResult',\n                            ('nobs', 'minmax', 'mean', 'variance', 'skewness',\n                             'kurtosis'))\n\n\ndef describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate'):\n    \"\"\"\n    Compute several descriptive statistics of the passed array.\n\n    Parameters\n    ----------\n    a : array_like\n       Input data.\n    axis : int or None, optional\n       Axis along which statistics are calculated. Default is 0.\n       If None, compute over the whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom (only for variance).  Default is 1.\n    bias : bool, optional\n        If False, then the skewness and kurtosis calculations are corrected for\n        statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    nobs : int or ndarray of ints\n       Number of observations (length of data along `axis`).\n       When 'omit' is chosen as nan_policy, each column is counted separately.\n    minmax: tuple of ndarrays or floats\n       Minimum and maximum value of data array.\n    mean : ndarray or float\n       Arithmetic mean of data along axis.\n    variance : ndarray or float\n       Unbiased variance of the data along axis, denominator is number of\n       observations minus one.\n    skewness : ndarray or float\n       Skewness, based on moment calculations with denominator equal to\n       the number of observations, i.e. no degrees of freedom correction.\n    kurtosis : ndarray or float\n       Kurtosis (Fisher).  The kurtosis is normalized so that it is\n       zero for the normal distribution.  No degrees of freedom are used.\n\n    See Also\n    --------\n    skew, kurtosis\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> a = np.arange(10)\n    >>> stats.describe(a)\n    DescribeResult(nobs=10, minmax=(0, 9), mean=4.5, variance=9.166666666666666,\n                   skewness=0.0, kurtosis=-1.2242424242424244)\n    >>> b = [[1, 2], [3, 4]]\n    >>> stats.describe(b)\n    DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n                   mean=array([2., 3.]), variance=array([2., 2.]),\n                   skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.describe(a, axis, ddof, bias)\n\n    if a.size == 0:\n        raise ValueError(\"The input must not be empty.\")\n    n = a.shape[axis]\n    mm = (np.min(a, axis=axis), np.max(a, axis=axis))\n    m = np.mean(a, axis=axis)\n    v = np.var(a, axis=axis, ddof=ddof)\n    sk = skew(a, axis, bias=bias)\n    kurt = kurtosis(a, axis, bias=bias)\n\n    return DescribeResult(n, mm, m, v, sk, kurt)\n\n#####################################\n#         NORMALITY TESTS           #\n#####################################\n\n\nSkewtestResult = namedtuple('SkewtestResult', ('statistic', 'pvalue'))\n\n\ndef skewtest(a, axis=0, nan_policy='propagate'):\n    \"\"\"\n    Test whether the skew is different from the normal distribution.\n\n    This function tests the null hypothesis that the skewness of\n    the population that the sample was drawn from is the same\n    as that of a corresponding normal distribution.\n\n    Parameters\n    ----------\n    a : array\n        The data to be tested.\n    axis : int or None, optional\n       Axis along which statistics are calculated. Default is 0.\n       If None, compute over the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    statistic : float\n        The computed z-score for this test.\n    pvalue : float\n        Two-sided p-value for the hypothesis test.\n\n    Notes\n    -----\n    The sample size must be at least 8.\n\n    References\n    ----------\n    .. [1] R. B. D'Agostino, A. J. Belanger and R. B. D'Agostino Jr.,\n            \"A suggestion for using powerful and informative tests of\n            normality\", American Statistician 44, pp. 316-321, 1990.\n\n    Examples\n    --------\n    >>> from scipy.stats import skewtest\n    >>> skewtest([1, 2, 3, 4, 5, 6, 7, 8])\n    SkewtestResult(statistic=1.0108048609177787, pvalue=0.3121098361421897)\n    >>> skewtest([2, 8, 0, 4, 1, 9, 9, 0])\n    SkewtestResult(statistic=0.44626385374196975, pvalue=0.6554066631275459)\n    >>> skewtest([1, 2, 3, 4, 5, 6, 7, 8000])\n    SkewtestResult(statistic=3.571773510360407, pvalue=0.0003545719905823133)\n    >>> skewtest([100, 100, 100, 100, 100, 100, 100, 101])\n    SkewtestResult(statistic=3.5717766638478072, pvalue=0.000354567720281634)\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.skewtest(a, axis)\n\n    if axis is None:\n        a = np.ravel(a)\n        axis = 0\n    b2 = skew(a, axis)\n    n = a.shape[axis]\n    if n < 8:\n        raise ValueError(\n            \"skewtest is not valid with less than 8 samples; %i samples\"\n            \" were given.\" % int(n))\n    y = b2 * math.sqrt(((n + 1) * (n + 3)) / (6.0 * (n - 2)))\n    beta2 = (3.0 * (n**2 + 27*n - 70) * (n+1) * (n+3) /\n             ((n-2.0) * (n+5) * (n+7) * (n+9)))\n    W2 = -1 + math.sqrt(2 * (beta2 - 1))\n    delta = 1 / math.sqrt(0.5 * math.log(W2))\n    alpha = math.sqrt(2.0 / (W2 - 1))\n    y = np.where(y == 0, 1, y)\n    Z = delta * np.log(y / alpha + np.sqrt((y / alpha)**2 + 1))\n\n    return SkewtestResult(Z, 2 * distributions.norm.sf(np.abs(Z)))\n\n\nKurtosistestResult = namedtuple('KurtosistestResult', ('statistic', 'pvalue'))\n\n\ndef kurtosistest(a, axis=0, nan_policy='propagate'):\n    \"\"\"\n    Test whether a dataset has normal kurtosis.\n\n    This function tests the null hypothesis that the kurtosis\n    of the population from which the sample was drawn is that\n    of the normal distribution: ``kurtosis = 3(n-1)/(n+1)``.\n\n    Parameters\n    ----------\n    a : array\n        Array of the sample data.\n    axis : int or None, optional\n       Axis along which to compute test. Default is 0. If None,\n       compute over the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    statistic : float\n        The computed z-score for this test.\n    pvalue : float\n        The two-sided p-value for the hypothesis test.\n\n    Notes\n    -----\n    Valid only for n>20. This function uses the method described in [1]_.\n\n    References\n    ----------\n    .. [1] see e.g. F. J. Anscombe, W. J. Glynn, \"Distribution of the kurtosis\n       statistic b2 for normal samples\", Biometrika, vol. 70, pp. 227-234, 1983.\n\n    Examples\n    --------\n    >>> from scipy.stats import kurtosistest\n    >>> kurtosistest(list(range(20)))\n    KurtosistestResult(statistic=-1.7058104152122062, pvalue=0.08804338332528348)\n\n    >>> np.random.seed(28041990)\n    >>> s = np.random.normal(0, 1, 1000)\n    >>> kurtosistest(s)\n    KurtosistestResult(statistic=1.2317590987707365, pvalue=0.21803908613450895)\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.kurtosistest(a, axis)\n\n    n = a.shape[axis]\n    if n < 5:\n        raise ValueError(\n            \"kurtosistest requires at least 5 observations; %i observations\"\n            \" were given.\" % int(n))\n    if n < 20:\n        warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n                      \"anyway, n=%i\" % int(n))\n    b2 = kurtosis(a, axis, fisher=False)\n\n    E = 3.0*(n-1) / (n+1)\n    varb2 = 24.0*n*(n-2)*(n-3) / ((n+1)*(n+1.)*(n+3)*(n+5))  # [1]_ Eq. 1\n    x = (b2-E) / np.sqrt(varb2)  # [1]_ Eq. 4\n    # [1]_ Eq. 2:\n    sqrtbeta1 = 6.0*(n*n-5*n+2)/((n+7)*(n+9)) * np.sqrt((6.0*(n+3)*(n+5)) /\n                                                        (n*(n-2)*(n-3)))\n    # [1]_ Eq. 3:\n    A = 6.0 + 8.0/sqrtbeta1 * (2.0/sqrtbeta1 + np.sqrt(1+4.0/(sqrtbeta1**2)))\n    term1 = 1 - 2/(9.0*A)\n    denom = 1 + x*np.sqrt(2/(A-4.0))\n    term2 = np.sign(denom) * np.where(denom == 0.0, np.nan,\n                                      np.power((1-2.0/A)/np.abs(denom), 1/3.0))\n    if np.any(denom == 0):\n        msg = \"Test statistic not defined in some cases due to division by \" \\\n              \"zero. Return nan in that case...\"\n        warnings.warn(msg, RuntimeWarning)\n\n    Z = (term1 - term2) / np.sqrt(2/(9.0*A))  # [1]_ Eq. 5\n    if Z.ndim == 0:\n        Z = Z[()]\n\n    # zprob uses upper tail, so Z needs to be positive\n    return KurtosistestResult(Z, 2 * distributions.norm.sf(np.abs(Z)))\n\n\nNormaltestResult = namedtuple('NormaltestResult', ('statistic', 'pvalue'))\n\n\ndef normaltest(a, axis=0, nan_policy='propagate'):\n    \"\"\"\n    Test whether a sample differs from a normal distribution.\n\n    This function tests the null hypothesis that a sample comes\n    from a normal distribution.  It is based on D'Agostino and\n    Pearson's [1]_, [2]_ test that combines skew and kurtosis to\n    produce an omnibus test of normality.\n\n    Parameters\n    ----------\n    a : array_like\n        The array containing the sample to be tested.\n    axis : int or None, optional\n        Axis along which to compute test. Default is 0. If None,\n        compute over the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    statistic : float or array\n        ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and\n        ``k`` is the z-score returned by `kurtosistest`.\n    pvalue : float or array\n       A 2-sided chi squared probability for the hypothesis test.\n\n    References\n    ----------\n    .. [1] D'Agostino, R. B. (1971), \"An omnibus test of normality for\n           moderate and large sample size\", Biometrika, 58, 341-348\n\n    .. [2] D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\n           normality\", Biometrika, 60, 613-622\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> pts = 1000\n    >>> np.random.seed(28041990)\n    >>> a = np.random.normal(0, 1, size=pts)\n    >>> b = np.random.normal(2, 1, size=pts)\n    >>> x = np.concatenate((a, b))\n    >>> k2, p = stats.normaltest(x)\n    >>> alpha = 1e-3\n    >>> print(\"p = {:g}\".format(p))\n    p = 3.27207e-11\n    >>> if p < alpha:  # null hypothesis: x comes from a normal distribution\n    ...     print(\"The null hypothesis can be rejected\")\n    ... else:\n    ...     print(\"The null hypothesis cannot be rejected\")\n    The null hypothesis can be rejected\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.normaltest(a, axis)\n\n    s, _ = skewtest(a, axis)\n    k, _ = kurtosistest(a, axis)\n    k2 = s*s + k*k\n\n    return NormaltestResult(k2, distributions.chi2.sf(k2, 2))\n\n\nJarque_beraResult = namedtuple('Jarque_beraResult', ('statistic', 'pvalue'))\n\n\ndef jarque_bera(x):\n    \"\"\"\n    Perform the Jarque-Bera goodness of fit test on sample data.\n\n    The Jarque-Bera test tests whether the sample data has the skewness and\n    kurtosis matching a normal distribution.\n\n    Note that this test only works for a large enough number of data samples\n    (>2000) as the test statistic asymptotically has a Chi-squared distribution\n    with 2 degrees of freedom.\n\n    Parameters\n    ----------\n    x : array_like\n        Observations of a random variable.\n\n    Returns\n    -------\n    jb_value : float\n        The test statistic.\n    p : float\n        The p-value for the hypothesis test.\n\n    References\n    ----------\n    .. [1] Jarque, C. and Bera, A. (1980) \"Efficient tests for normality,\n           homoscedasticity and serial independence of regression residuals\",\n           6 Econometric Letters 255-259.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> np.random.seed(987654321)\n    >>> x = np.random.normal(0, 1, 100000)\n    >>> jarque_bera_test = stats.jarque_bera(x)\n    >>> jarque_bera_test\n    Jarque_beraResult(statistic=4.716570798957913, pvalue=0.0945822550304295)\n    >>> jarque_bera_test.statistic\n    4.716570798957913\n    >>> jarque_bera_test.pvalue\n    0.0945822550304295\n\n    \"\"\"\n    x = np.asarray(x)\n    n = x.size\n    if n == 0:\n        raise ValueError('At least one observation is required.')\n\n    mu = x.mean()\n    diffx = x - mu\n    skewness = (1 / n * np.sum(diffx**3)) / (1 / n * np.sum(diffx**2))**(3 / 2.)\n    kurtosis = (1 / n * np.sum(diffx**4)) / (1 / n * np.sum(diffx**2))**2\n    jb_value = n / 6 * (skewness**2 + (kurtosis - 3)**2 / 4)\n    p = 1 - distributions.chi2.cdf(jb_value, 2)\n\n    return Jarque_beraResult(jb_value, p)\n\n\n#####################################\n#        FREQUENCY FUNCTIONS        #\n#####################################\n\n# deindent to work around numpy/gh-16202\n@np.deprecate(\n    message=\"`itemfreq` is deprecated and will be removed in a \"\n            \"future version. Use instead `np.unique(..., return_counts=True)`\")\ndef itemfreq(a):\n    \"\"\"\nReturn a 2-D array of item frequencies.\n\nParameters\n----------\na : (N,) array_like\n    Input array.\n\nReturns\n-------\nitemfreq : (K, 2) ndarray\n    A 2-D frequency table.  Column 1 contains sorted, unique values from\n    `a`, column 2 contains their respective counts.\n\nExamples\n--------\n>>> from scipy import stats\n>>> a = np.array([1, 1, 5, 0, 1, 2, 2, 0, 1, 4])\n>>> stats.itemfreq(a)\narray([[ 0.,  2.],\n       [ 1.,  4.],\n       [ 2.,  2.],\n       [ 4.,  1.],\n       [ 5.,  1.]])\n>>> np.bincount(a)\narray([2, 4, 2, 0, 1, 1])\n\n>>> stats.itemfreq(a/10.)\narray([[ 0. ,  2. ],\n       [ 0.1,  4. ],\n       [ 0.2,  2. ],\n       [ 0.4,  1. ],\n       [ 0.5,  1. ]])\n\"\"\"\n    items, inv = np.unique(a, return_inverse=True)\n    freq = np.bincount(inv)\n    return np.array([items, freq]).T\n\n\ndef scoreatpercentile(a, per, limit=(), interpolation_method='fraction',\n                      axis=None):\n    \"\"\"\n    Calculate the score at a given percentile of the input sequence.\n\n    For example, the score at `per=50` is the median. If the desired quantile\n    lies between two data points, we interpolate between them, according to\n    the value of `interpolation`. If the parameter `limit` is provided, it\n    should be a tuple (lower, upper) of two values.\n\n    Parameters\n    ----------\n    a : array_like\n        A 1-D array of values from which to extract score.\n    per : array_like\n        Percentile(s) at which to extract score.  Values should be in range\n        [0,100].\n    limit : tuple, optional\n        Tuple of two scalars, the lower and upper limits within which to\n        compute the percentile. Values of `a` outside\n        this (closed) interval will be ignored.\n    interpolation_method : {'fraction', 'lower', 'higher'}, optional\n        Specifies the interpolation method to use,\n        when the desired quantile lies between two data points `i` and `j`\n        The following options are available (default is 'fraction'):\n\n          * 'fraction': ``i + (j - i) * fraction`` where ``fraction`` is the\n            fractional part of the index surrounded by ``i`` and ``j``\n          * 'lower': ``i``\n          * 'higher': ``j``\n\n    axis : int, optional\n        Axis along which the percentiles are computed. Default is None. If\n        None, compute over the whole array `a`.\n\n    Returns\n    -------\n    score : float or ndarray\n        Score at percentile(s).\n\n    See Also\n    --------\n    percentileofscore, numpy.percentile\n\n    Notes\n    -----\n    This function will become obsolete in the future.\n    For NumPy 1.9 and higher, `numpy.percentile` provides all the functionality\n    that `scoreatpercentile` provides.  And it's significantly faster.\n    Therefore it's recommended to use `numpy.percentile` for users that have\n    numpy >= 1.9.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> a = np.arange(100)\n    >>> stats.scoreatpercentile(a, 50)\n    49.5\n\n    \"\"\"\n    # adapted from NumPy's percentile function.  When we require numpy >= 1.8,\n    # the implementation of this function can be replaced by np.percentile.\n    a = np.asarray(a)\n    if a.size == 0:\n        # empty array, return nan(s) with shape matching `per`\n        if np.isscalar(per):\n            return np.nan\n        else:\n            return np.full(np.asarray(per).shape, np.nan, dtype=np.float64)\n\n    if limit:\n        a = a[(limit[0] <= a) & (a <= limit[1])]\n\n    sorted_ = np.sort(a, axis=axis)\n    if axis is None:\n        axis = 0\n\n    return _compute_qth_percentile(sorted_, per, interpolation_method, axis)\n\n\n# handle sequence of per's without calling sort multiple times\ndef _compute_qth_percentile(sorted_, per, interpolation_method, axis):\n    if not np.isscalar(per):\n        score = [_compute_qth_percentile(sorted_, i,\n                                         interpolation_method, axis)\n                 for i in per]\n        return np.array(score)\n\n    if not (0 <= per <= 100):\n        raise ValueError(\"percentile must be in the range [0, 100]\")\n\n    indexer = [slice(None)] * sorted_.ndim\n    idx = per / 100. * (sorted_.shape[axis] - 1)\n\n    if int(idx) != idx:\n        # round fractional indices according to interpolation method\n        if interpolation_method == 'lower':\n            idx = int(np.floor(idx))\n        elif interpolation_method == 'higher':\n            idx = int(np.ceil(idx))\n        elif interpolation_method == 'fraction':\n            pass  # keep idx as fraction and interpolate\n        else:\n            raise ValueError(\"interpolation_method can only be 'fraction', \"\n                             \"'lower' or 'higher'\")\n\n    i = int(idx)\n    if i == idx:\n        indexer[axis] = slice(i, i + 1)\n        weights = array(1)\n        sumval = 1.0\n    else:\n        indexer[axis] = slice(i, i + 2)\n        j = i + 1\n        weights = array([(j - idx), (idx - i)], float)\n        wshape = [1] * sorted_.ndim\n        wshape[axis] = 2\n        weights.shape = wshape\n        sumval = weights.sum()\n\n    # Use np.add.reduce (== np.sum but a little faster) to coerce data type\n    return np.add.reduce(sorted_[tuple(indexer)] * weights, axis=axis) / sumval\n\n\ndef percentileofscore(a, score, kind='rank'):\n    \"\"\"\n    Compute the percentile rank of a score relative to a list of scores.\n\n    A `percentileofscore` of, for example, 80% means that 80% of the\n    scores in `a` are below the given score. In the case of gaps or\n    ties, the exact definition depends on the optional keyword, `kind`.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of scores to which `score` is compared.\n    score : int or float\n        Score that is compared to the elements in `a`.\n    kind : {'rank', 'weak', 'strict', 'mean'}, optional\n        Specifies the interpretation of the resulting score.\n        The following options are available (default is 'rank'):\n\n          * 'rank': Average percentage ranking of score.  In case of multiple\n            matches, average the percentage rankings of all matching scores.\n          * 'weak': This kind corresponds to the definition of a cumulative\n            distribution function.  A percentileofscore of 80% means that 80%\n            of values are less than or equal to the provided score.\n          * 'strict': Similar to \"weak\", except that only values that are\n            strictly less than the given score are counted.\n          * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n            in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n\n    Returns\n    -------\n    pcos : float\n        Percentile-position of score (0-100) relative to `a`.\n\n    See Also\n    --------\n    numpy.percentile\n\n    Examples\n    --------\n    Three-quarters of the given values lie below a given score:\n\n    >>> from scipy import stats\n    >>> stats.percentileofscore([1, 2, 3, 4], 3)\n    75.0\n\n    With multiple matches, note how the scores of the two matches, 0.6\n    and 0.8 respectively, are averaged:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n    70.0\n\n    Only 2/5 values are strictly less than 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n    40.0\n\n    But 4/5 values are less than or equal to 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n    80.0\n\n    The average between the weak and the strict scores is:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n    60.0\n\n    \"\"\"\n    if np.isnan(score):\n        return np.nan\n    a = np.asarray(a)\n    n = len(a)\n    if n == 0:\n        return 100.0\n\n    if kind == 'rank':\n        left = np.count_nonzero(a < score)\n        right = np.count_nonzero(a <= score)\n        pct = (right + left + (1 if right > left else 0)) * 50.0/n\n        return pct\n    elif kind == 'strict':\n        return np.count_nonzero(a < score) / n * 100\n    elif kind == 'weak':\n        return np.count_nonzero(a <= score) / n * 100\n    elif kind == 'mean':\n        pct = (np.count_nonzero(a < score) + np.count_nonzero(a <= score)) / n * 50\n        return pct\n    else:\n        raise ValueError(\"kind can only be 'rank', 'strict', 'weak' or 'mean'\")\n\n\nHistogramResult = namedtuple('HistogramResult',\n                             ('count', 'lowerlimit', 'binsize', 'extrapoints'))\n\n\ndef _histogram(a, numbins=10, defaultlimits=None, weights=None, printextras=False):\n    \"\"\"\n    Create a histogram.\n\n    Separate the range into several bins and return the number of instances\n    in each bin.\n\n    Parameters\n    ----------\n    a : array_like\n        Array of scores which will be put into bins.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultlimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n    printextras : bool, optional\n        If True, if there are extra points (i.e. the points that fall outside\n        the bin limits) a warning is raised saying how many of those points\n        there are.  Default is False.\n\n    Returns\n    -------\n    count : ndarray\n        Number of points (or sum of weights) in each bin.\n    lowerlimit : float\n        Lowest value of histogram, the lower limit of the first bin.\n    binsize : float\n        The size of the bins (all bins have the same size).\n    extrapoints : int\n        The number of points outside the range of the histogram.\n\n    See Also\n    --------\n    numpy.histogram\n\n    Notes\n    -----\n    This histogram is based on numpy's histogram but has a larger range by\n    default if default limits is not set.\n\n    \"\"\"\n    a = np.ravel(a)\n    if defaultlimits is None:\n        if a.size == 0:\n            # handle empty arrays. Undetermined range, so use 0-1.\n            defaultlimits = (0, 1)\n        else:\n            # no range given, so use values in `a`\n            data_min = a.min()\n            data_max = a.max()\n            # Have bins extend past min and max values slightly\n            s = (data_max - data_min) / (2. * (numbins - 1.))\n            defaultlimits = (data_min - s, data_max + s)\n\n    # use numpy's histogram method to compute bins\n    hist, bin_edges = np.histogram(a, bins=numbins, range=defaultlimits,\n                                   weights=weights)\n    # hist are not always floats, convert to keep with old output\n    hist = np.array(hist, dtype=float)\n    # fixed width for bins is assumed, as numpy's histogram gives\n    # fixed width bins for int values for 'bins'\n    binsize = bin_edges[1] - bin_edges[0]\n    # calculate number of extra points\n    extrapoints = len([v for v in a\n                       if defaultlimits[0] > v or v > defaultlimits[1]])\n    if extrapoints > 0 and printextras:\n        warnings.warn(\"Points outside given histogram range = %s\"\n                      % extrapoints)\n\n    return HistogramResult(hist, defaultlimits[0], binsize, extrapoints)\n\n\nCumfreqResult = namedtuple('CumfreqResult',\n                           ('cumcount', 'lowerlimit', 'binsize',\n                            'extrapoints'))\n\n\ndef cumfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    \"\"\"\n    Return a cumulative frequency histogram, using the histogram function.\n\n    A cumulative histogram is a mapping that counts the cumulative number of\n    observations in all of the bins up to the specified bin.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    cumcount : ndarray\n        Binned values of cumulative frequency.\n    lowerlimit : float\n        Lower real limit\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> x = [1, 4, 2, 1, 3, 1]\n    >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n    >>> res.cumcount\n    array([ 1.,  2.,  3.,  3.])\n    >>> res.extrapoints\n    3\n\n    Create a normal distribution with 1000 random values\n\n    >>> rng = np.random.RandomState(seed=12345)\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate cumulative frequencies\n\n    >>> res = stats.cumfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n    ...                                  res.cumcount.size)\n\n    Plot histogram and cumulative histogram\n\n    >>> fig = plt.figure(figsize=(10, 4))\n    >>> ax1 = fig.add_subplot(1, 2, 1)\n    >>> ax2 = fig.add_subplot(1, 2, 2)\n    >>> ax1.hist(samples, bins=25)\n    >>> ax1.set_title('Histogram')\n    >>> ax2.bar(x, res.cumcount, width=res.binsize)\n    >>> ax2.set_title('Cumulative histogram')\n    >>> ax2.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\"\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    cumhist = np.cumsum(h * 1, axis=0)\n    return CumfreqResult(cumhist, l, b, e)\n\n\nRelfreqResult = namedtuple('RelfreqResult',\n                           ('frequency', 'lowerlimit', 'binsize',\n                            'extrapoints'))\n\n\ndef relfreq(a, numbins=10, defaultreallimits=None, weights=None):\n    \"\"\"\n    Return a relative frequency histogram, using the histogram function.\n\n    A relative frequency  histogram is a mapping of the number of\n    observations in each of the bins relative to the total of observations.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    frequency : ndarray\n        Binned values of relative frequency.\n    lowerlimit : float\n        Lower real limit.\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> a = np.array([2, 4, 1, 2, 3, 2])\n    >>> res = stats.relfreq(a, numbins=4)\n    >>> res.frequency\n    array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n    >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n    1.0\n\n    Create a normal distribution with 1000 random values\n\n    >>> rng = np.random.RandomState(seed=12345)\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate relative frequencies\n\n    >>> res = stats.relfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n    ...                                  res.frequency.size)\n\n    Plot relative frequency histogram\n\n    >>> fig = plt.figure(figsize=(5, 4))\n    >>> ax = fig.add_subplot(1, 1, 1)\n    >>> ax.bar(x, res.frequency, width=res.binsize)\n    >>> ax.set_title('Relative frequency histogram')\n    >>> ax.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    \"\"\"\n    a = np.asanyarray(a)\n    h, l, b, e = _histogram(a, numbins, defaultreallimits, weights=weights)\n    h = h / a.shape[0]\n\n    return RelfreqResult(h, l, b, e)\n\n\n#####################################\n#        VARIABILITY FUNCTIONS      #\n#####################################\n\ndef obrientransform(*args):\n    \"\"\"\n    Compute the O'Brien transform on input data (any number of arrays).\n\n    Used to test for homogeneity of variance prior to running one-way stats.\n    Each array in ``*args`` is one level of a factor.\n    If `f_oneway` is run on the transformed data and found significant,\n    the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n\n    Parameters\n    ----------\n    args : tuple of array_like\n        Any number of arrays.\n\n    Returns\n    -------\n    obrientransform : ndarray\n        Transformed data for use in an ANOVA.  The first dimension\n        of the result corresponds to the sequence of transformed\n        arrays.  If the arrays given are all 1-D of the same length,\n        the return value is a 2-D array; otherwise it is a 1-D array\n        of type object, with each element being an ndarray.\n\n    References\n    ----------\n    .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n           Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n\n    Examples\n    --------\n    We'll test the following data sets for differences in their variance.\n\n    >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n    >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n\n    Apply the O'Brien transform to the data.\n\n    >>> from scipy.stats import obrientransform\n    >>> tx, ty = obrientransform(x, y)\n\n    Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n    transformed data.\n\n    >>> from scipy.stats import f_oneway\n    >>> F, p = f_oneway(tx, ty)\n    >>> p\n    0.1314139477040335\n\n    If we require that ``p < 0.05`` for significance, we cannot conclude\n    that the variances are different.\n\n    \"\"\"\n    TINY = np.sqrt(np.finfo(float).eps)\n\n    # `arrays` will hold the transformed arguments.\n    arrays = []\n    sLast = None\n\n    for arg in args:\n        a = np.asarray(arg)\n        n = len(a)\n        mu = np.mean(a)\n        sq = (a - mu)**2\n        sumsq = sq.sum()\n\n        # The O'Brien transform.\n        t = ((n - 1.5) * n * sq - 0.5 * sumsq) / ((n - 1) * (n - 2))\n\n        # Check that the mean of the transformed data is equal to the\n        # original variance.\n        var = sumsq / (n - 1)\n        if abs(var - np.mean(t)) > TINY:\n            raise ValueError('Lack of convergence in obrientransform.')\n\n        arrays.append(t)\n        sLast = a.shape\n\n    if sLast:\n        for arr in arrays[:-1]:\n            if sLast != arr.shape:\n                return np.array(arrays, dtype=object)\n    return np.array(arrays)\n\n\ndef sem(a, axis=0, ddof=1, nan_policy='propagate'):\n    \"\"\"\n    Compute standard error of the mean.\n\n    Calculate the standard error of the mean (or standard error of\n    measurement) of the values in the input array.\n\n    Parameters\n    ----------\n    a : array_like\n        An array containing the values for which the standard error is\n        returned.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Delta degrees-of-freedom. How many degrees of freedom to adjust\n        for bias in limited samples relative to the population estimate\n        of variance. Defaults to 1.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    s : ndarray or float\n        The standard error of the mean in the sample(s), along the input axis.\n\n    Notes\n    -----\n    The default value for `ddof` is different to the default (0) used by other\n    ddof containing routines, such as np.std and np.nanstd.\n\n    Examples\n    --------\n    Find standard error along the first axis:\n\n    >>> from scipy import stats\n    >>> a = np.arange(20).reshape(5,4)\n    >>> stats.sem(a)\n    array([ 2.8284,  2.8284,  2.8284,  2.8284])\n\n    Find standard error across the whole array, using n degrees of freedom:\n\n    >>> stats.sem(a, axis=None, ddof=0)\n    1.2893796958227628\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        a = ma.masked_invalid(a)\n        return mstats_basic.sem(a, axis, ddof)\n\n    n = a.shape[axis]\n    s = np.std(a, axis=axis, ddof=ddof) / np.sqrt(n)\n    return s\n\n\ndef _isconst(x):\n    \"\"\"\n    Check if all values in x are the same.  nans are ignored.\n\n    x must be a 1d array.\n\n    The return value is a 1d array with length 1, so it can be used\n    in np.apply_along_axis.\n    \"\"\"\n    y = x[~np.isnan(x)]\n    if y.size == 0:\n        return np.array([True])\n    else:\n        return (y[0] == y).all(keepdims=True)\n\n\ndef _quiet_nanmean(x):\n    \"\"\"\n    Compute nanmean for the 1d array x, but quietly return nan if x is all nan.\n\n    The return value is a 1d array with length 1, so it can be used\n    in np.apply_along_axis.\n    \"\"\"\n    y = x[~np.isnan(x)]\n    if y.size == 0:\n        return np.array([np.nan])\n    else:\n        return np.mean(y, keepdims=True)\n\n\ndef _quiet_nanstd(x):\n    \"\"\"\n    Compute nanstd for the 1d array x, but quietly return nan if x is all nan.\n\n    The return value is a 1d array with length 1, so it can be used\n    in np.apply_along_axis.\n    \"\"\"\n    y = x[~np.isnan(x)]\n    if y.size == 0:\n        return np.array([np.nan])\n    else:\n        return np.std(y, keepdims=True)\n\n\ndef zscore(a, axis=0, ddof=0, nan_policy='propagate'):\n    \"\"\"\n    Compute the z score.\n\n    Compute the z score of each value in the sample, relative to the\n    sample mean and standard deviation.\n\n    Parameters\n    ----------\n    a : array_like\n        An array like object containing the sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the z-scores computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        The z-scores, standardized by mean and standard deviation of\n        input array `a`.\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    Examples\n    --------\n    >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n    ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n    >>> from scipy import stats\n    >>> stats.zscore(a)\n    array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n            0.6748, -1.1488, -1.3324])\n\n    Computing along a specified axis, using n-1 degrees of freedom\n    (``ddof=1``) to calculate the standard deviation:\n\n    >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n    ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n    ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n    ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n    ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n    >>> stats.zscore(b, axis=1, ddof=1)\n    array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n           [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n           [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n           [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n           [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\n    An example with `nan_policy='omit'`:\n\n    >>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n    ...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n    >>> stats.zscore(x, axis=1, nan_policy='omit')\n    array([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\n    \"\"\"\n    a = np.asanyarray(a)\n\n    if a.size == 0:\n        return np.empty(a.shape)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        if axis is None:\n            mn = _quiet_nanmean(a.ravel())\n            std = _quiet_nanstd(a.ravel())\n            isconst = _isconst(a.ravel())\n        else:\n            mn = np.apply_along_axis(_quiet_nanmean, axis, a)\n            std = np.apply_along_axis(_quiet_nanstd, axis, a)\n            isconst = np.apply_along_axis(_isconst, axis, a)\n    else:\n        mn = a.mean(axis=axis, keepdims=True)\n        std = a.std(axis=axis, ddof=ddof, keepdims=True)\n        if axis is None:\n            isconst = (a.item(0) == a).all()\n        else:\n            isconst = (_first(a, axis) == a).all(axis=axis, keepdims=True)\n\n    # Set std deviations that are 0 to 1 to avoid division by 0.\n    std[isconst] = 1.0\n    z = (a - mn) / std\n    # Set the outputs associated with a constant input to nan.\n    z[np.broadcast_to(isconst, z.shape)] = np.nan\n    return z\n\n\ndef zmap(scores, compare, axis=0, ddof=0):\n    \"\"\"\n    Calculate the relative z-scores.\n\n    Return an array of z-scores, i.e., scores that are standardized to\n    zero mean and unit variance, where mean and variance are calculated\n    from the comparison array.\n\n    Parameters\n    ----------\n    scores : array_like\n        The input for which z-scores are calculated.\n    compare : array_like\n        The input from which the mean and standard deviation of the\n        normalization are taken; assumed to have the same dimension as\n        `scores`.\n    axis : int or None, optional\n        Axis over which mean and variance of `compare` are calculated.\n        Default is 0. If None, compute over the whole array `scores`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n\n    Returns\n    -------\n    zscore : array_like\n        Z-scores, in the same shape as `scores`.\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    Examples\n    --------\n    >>> from scipy.stats import zmap\n    >>> a = [0.5, 2.0, 2.5, 3]\n    >>> b = [0, 1, 2, 3, 4]\n    >>> zmap(a, b)\n    array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n\n    \"\"\"\n    scores, compare = map(np.asanyarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\n\n\ndef gstd(a, axis=0, ddof=1):\n    \"\"\"\n    Calculate the geometric standard deviation of an array.\n\n    The geometric standard deviation describes the spread of a set of numbers\n    where the geometric mean is preferred. It is a multiplicative factor, and\n    so a dimensionless quantity.\n\n    It is defined as the exponent of the standard deviation of ``log(a)``.\n    Mathematically the population geometric standard deviation can be\n    evaluated as::\n\n        gstd = exp(std(log(a)))\n\n    .. versionadded:: 1.3.0\n\n    Parameters\n    ----------\n    a : array_like\n        An array like object containing the sample data.\n    axis : int, tuple or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degree of freedom correction in the calculation of the\n        geometric standard deviation. Default is 1.\n\n    Returns\n    -------\n    ndarray or float\n        An array of the geometric standard deviation. If `axis` is None or `a`\n        is a 1d array a float is returned.\n\n    Notes\n    -----\n    As the calculation requires the use of logarithms the geometric standard\n    deviation only supports strictly positive values. Any non-positive or\n    infinite values will raise a `ValueError`.\n    The geometric standard deviation is sometimes confused with the exponent of\n    the standard deviation, ``exp(std(a))``. Instead the geometric standard\n    deviation is ``exp(std(log(a)))``.\n    The default value for `ddof` is different to the default value (0) used\n    by other ddof containing functions, such as ``np.std`` and ``np.nanstd``.\n\n    Examples\n    --------\n    Find the geometric standard deviation of a log-normally distributed sample.\n    Note that the standard deviation of the distribution is one, on a\n    log scale this evaluates to approximately ``exp(1)``.\n\n    >>> from scipy.stats import gstd\n    >>> np.random.seed(123)\n    >>> sample = np.random.lognormal(mean=0, sigma=1, size=1000)\n    >>> gstd(sample)\n    2.7217860664589946\n\n    Compute the geometric standard deviation of a multidimensional array and\n    of a given axis.\n\n    >>> a = np.arange(1, 25).reshape(2, 3, 4)\n    >>> gstd(a, axis=None)\n    2.2944076136018947\n    >>> gstd(a, axis=2)\n    array([[1.82424757, 1.22436866, 1.13183117],\n           [1.09348306, 1.07244798, 1.05914985]])\n    >>> gstd(a, axis=(1,2))\n    array([2.12939215, 1.22120169])\n\n    The geometric standard deviation further handles masked arrays.\n\n    >>> a = np.arange(1, 25).reshape(2, 3, 4)\n    >>> ma = np.ma.masked_where(a > 16, a)\n    >>> ma\n    masked_array(\n      data=[[[1, 2, 3, 4],\n             [5, 6, 7, 8],\n             [9, 10, 11, 12]],\n            [[13, 14, 15, 16],\n             [--, --, --, --],\n             [--, --, --, --]]],\n      mask=[[[False, False, False, False],\n             [False, False, False, False],\n             [False, False, False, False]],\n            [[False, False, False, False],\n             [ True,  True,  True,  True],\n             [ True,  True,  True,  True]]],\n      fill_value=999999)\n    >>> gstd(ma, axis=2)\n    masked_array(\n      data=[[1.8242475707663655, 1.2243686572447428, 1.1318311657788478],\n            [1.0934830582350938, --, --]],\n      mask=[[False, False, False],\n            [False,  True,  True]],\n      fill_value=999999)\n\n    \"\"\"\n    a = np.asanyarray(a)\n    log = ma.log if isinstance(a, ma.MaskedArray) else np.log\n\n    try:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", RuntimeWarning)\n            return np.exp(np.std(log(a), axis=axis, ddof=ddof))\n    except RuntimeWarning as w:\n        if np.isinf(a).any():\n            raise ValueError(\n                'Infinite value encountered. The geometric standard deviation '\n                'is defined for strictly positive values only.'\n            ) from w\n        a_nan = np.isnan(a)\n        a_nan_any = a_nan.any()\n        # exclude NaN's from negativity check, but\n        # avoid expensive masking for arrays with no NaN\n        if ((a_nan_any and np.less_equal(np.nanmin(a), 0)) or\n              (not a_nan_any and np.less_equal(a, 0).any())):\n            raise ValueError(\n                'Non positive value encountered. The geometric standard '\n                'deviation is defined for strictly positive values only.'\n            ) from w\n        elif 'Degrees of freedom <= 0 for slice' == str(w):\n            raise ValueError(w) from w\n        else:\n            #  Remaining warnings don't need to be exceptions.\n            return np.exp(np.std(log(a, where=~a_nan), axis=axis, ddof=ddof))\n    except TypeError as e:\n        raise ValueError(\n            'Invalid array input. The inputs could not be '\n            'safely coerced to any supported types') from e\n\n\n# Private dictionary initialized only once at module level\n# See https://en.wikipedia.org/wiki/Robust_measures_of_scale\n_scale_conversions = {'raw': 1.0,\n                      'normal': special.erfinv(0.5) * 2.0 * math.sqrt(2.0)}\n\n\ndef iqr(x, axis=None, rng=(25, 75), scale=1.0, nan_policy='propagate',\n        interpolation='linear', keepdims=False):\n    r\"\"\"\n    Compute the interquartile range of the data along the specified axis.\n\n    The interquartile range (IQR) is the difference between the 75th and\n    25th percentile of the data. It is a measure of the dispersion\n    similar to standard deviation or variance, but is much more robust\n    against outliers [2]_.\n\n    The ``rng`` parameter allows this function to compute other\n    percentile ranges than the actual IQR. For example, setting\n    ``rng=(0, 100)`` is equivalent to `numpy.ptp`.\n\n    The IQR of an empty array is `np.nan`.\n\n    .. versionadded:: 0.18.0\n\n    Parameters\n    ----------\n    x : array_like\n        Input array or object that can be converted to an array.\n    axis : int or sequence of int, optional\n        Axis along which the range is computed. The default is to\n        compute the IQR for the entire array.\n    rng : Two-element sequence containing floats in range of [0,100] optional\n        Percentiles over which to compute the range. Each must be\n        between 0 and 100, inclusive. The default is the true IQR:\n        `(25, 75)`. The order of the elements is not important.\n    scale : scalar or str, optional\n        The numerical value of scale will be divided out of the final\n        result. The following string values are recognized:\n\n          * 'raw' : No scaling, just return the raw IQR.\n            **Deprecated!**  Use `scale=1` instead.\n          * 'normal' : Scale by\n            :math:`2 \\sqrt{2} erf^{-1}(\\frac{1}{2}) \\approx 1.349`.\n\n        The default is 1.0. The use of scale='raw' is deprecated.\n        Array-like scale is also allowed, as long\n        as it broadcasts correctly to the output such that\n        ``out / scale`` is a valid operation. The output dimensions\n        depend on the input array, `x`, the `axis` argument, and the\n        `keepdims` flag.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n    interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}, optional\n        Specifies the interpolation method to use when the percentile\n        boundaries lie between two data points `i` and `j`.\n        The following options are available (default is 'linear'):\n\n          * 'linear': `i + (j - i) * fraction`, where `fraction` is the\n            fractional part of the index surrounded by `i` and `j`.\n          * 'lower': `i`.\n          * 'higher': `j`.\n          * 'nearest': `i` or `j` whichever is nearest.\n          * 'midpoint': `(i + j) / 2`.\n\n    keepdims : bool, optional\n        If this is set to `True`, the reduced axes are left in the\n        result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original array `x`.\n\n    Returns\n    -------\n    iqr : scalar or ndarray\n        If ``axis=None``, a scalar is returned. If the input contains\n        integers or floats of smaller precision than ``np.float64``, then the\n        output data-type is ``np.float64``. Otherwise, the output data-type is\n        the same as that of the input.\n\n    See Also\n    --------\n    numpy.std, numpy.var\n\n    Notes\n    -----\n    This function is heavily dependent on the version of `numpy` that is\n    installed. Versions greater than 1.11.0b3 are highly recommended, as they\n    include a number of enhancements and fixes to `numpy.percentile` and\n    `numpy.nanpercentile` that affect the operation of this function. The\n    following modifications apply:\n\n    Below 1.10.0 : `nan_policy` is poorly defined.\n        The default behavior of `numpy.percentile` is used for 'propagate'. This\n        is a hybrid of 'omit' and 'propagate' that mostly yields a skewed\n        version of 'omit' since NaNs are sorted to the end of the data. A\n        warning is raised if there are NaNs in the data.\n    Below 1.9.0: `numpy.nanpercentile` does not exist.\n        This means that `numpy.percentile` is used regardless of `nan_policy`\n        and a warning is issued. See previous item for a description of the\n        behavior.\n    Below 1.9.0: `keepdims` and `interpolation` are not supported.\n        The keywords get ignored with a warning if supplied with non-default\n        values. However, multiple axes are still supported.\n\n    References\n    ----------\n    .. [1] \"Interquartile range\" https://en.wikipedia.org/wiki/Interquartile_range\n    .. [2] \"Robust measures of scale\" https://en.wikipedia.org/wiki/Robust_measures_of_scale\n    .. [3] \"Quantile\" https://en.wikipedia.org/wiki/Quantile\n\n    Examples\n    --------\n    >>> from scipy.stats import iqr\n    >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n    >>> x\n    array([[10,  7,  4],\n           [ 3,  2,  1]])\n    >>> iqr(x)\n    4.0\n    >>> iqr(x, axis=0)\n    array([ 3.5,  2.5,  1.5])\n    >>> iqr(x, axis=1)\n    array([ 3.,  1.])\n    >>> iqr(x, axis=1, keepdims=True)\n    array([[ 3.],\n           [ 1.]])\n\n    \"\"\"\n    x = asarray(x)\n\n    # This check prevents percentile from raising an error later. Also, it is\n    # consistent with `np.var` and `np.std`.\n    if not x.size:\n        return np.nan\n\n    # An error may be raised here, so fail-fast, before doing lengthy\n    # computations, even though `scale` is not used until later\n    if isinstance(scale, str):\n        scale_key = scale.lower()\n        if scale_key not in _scale_conversions:\n            raise ValueError(\"{0} not a valid scale for `iqr`\".format(scale))\n        if scale_key == 'raw':\n            warnings.warn(\n                \"use of scale='raw' is deprecated, use scale=1.0 instead\",\n                np.VisibleDeprecationWarning\n                )\n        scale = _scale_conversions[scale_key]\n\n    # Select the percentile function to use based on nans and policy\n    contains_nan, nan_policy = _contains_nan(x, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        percentile_func = np.nanpercentile\n    else:\n        percentile_func = np.percentile\n\n    if len(rng) != 2:\n        raise TypeError(\"quantile range must be two element sequence\")\n\n    if np.isnan(rng).any():\n        raise ValueError(\"range must not contain NaNs\")\n\n    rng = sorted(rng)\n    pct = percentile_func(x, rng, axis=axis, interpolation=interpolation,\n                          keepdims=keepdims)\n    out = np.subtract(pct[1], pct[0])\n\n    if scale != 1.0:\n        out /= scale\n\n    return out\n\n\ndef _mad_1d(x, center, nan_policy):\n    # Median absolute deviation for 1-d array x.\n    # This is a helper function for `median_abs_deviation`; it assumes its\n    # arguments have been validated already.  In particular,  x must be a\n    # 1-d numpy array, center must be callable, and if nan_policy is not\n    # 'propagate', it is assumed to be 'omit', because 'raise' is handled\n    # in `median_abs_deviation`.\n    # No warning is generated if x is empty or all nan.\n    isnan = np.isnan(x)\n    if isnan.any():\n        if nan_policy == 'propagate':\n            return np.nan\n        x = x[~isnan]\n    if x.size == 0:\n        # MAD of an empty array is nan.\n        return np.nan\n    # Edge cases have been handled, so do the basic MAD calculation.\n    med = center(x)\n    mad = np.median(np.abs(x - med))\n    return mad\n\n\ndef median_abs_deviation(x, axis=0, center=np.median, scale=1.0,\n                         nan_policy='propagate'):\n    r\"\"\"\n    Compute the median absolute deviation of the data along the given axis.\n\n    The median absolute deviation (MAD, [1]_) computes the median over the\n    absolute deviations from the median. It is a measure of dispersion\n    similar to the standard deviation but more robust to outliers [2]_.\n\n    The MAD of an empty array is ``np.nan``.\n\n    .. versionadded:: 1.5.0\n\n    Parameters\n    ----------\n    x : array_like\n        Input array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the range is computed. Default is 0. If None, compute\n        the MAD over the entire array.\n    center : callable, optional\n        A function that will return the central value. The default is to use\n        np.median. Any user defined function used will need to have the\n        function signature ``func(arr, axis)``.\n    scale : scalar or str, optional\n        The numerical value of scale will be divided out of the final\n        result. The default is 1.0. The string \"normal\" is also accepted,\n        and results in `scale` being the inverse of the standard normal\n        quantile function at 0.75, which is approximately 0.67449.\n        Array-like scale is also allowed, as long as it broadcasts correctly\n        to the output such that ``out / scale`` is a valid operation. The\n        output dimensions depend on the input array, `x`, and the `axis`\n        argument.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    mad : scalar or ndarray\n        If ``axis=None``, a scalar is returned. If the input contains\n        integers or floats of smaller precision than ``np.float64``, then the\n        output data-type is ``np.float64``. Otherwise, the output data-type is\n        the same as that of the input.\n\n    See Also\n    --------\n    numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,\n    scipy.stats.tstd, scipy.stats.tvar\n\n    Notes\n    -----\n    The `center` argument only affects the calculation of the central value\n    around which the MAD is calculated. That is, passing in ``center=np.mean``\n    will calculate the MAD around the mean - it will not calculate the *mean*\n    absolute deviation.\n\n    The input array may contain `inf`, but if `center` returns `inf`, the\n    corresponding MAD for that data will be `nan`.\n\n    References\n    ----------\n    .. [1] \"Median absolute deviation\",\n           https://en.wikipedia.org/wiki/Median_absolute_deviation\n    .. [2] \"Robust measures of scale\",\n           https://en.wikipedia.org/wiki/Robust_measures_of_scale\n\n    Examples\n    --------\n    When comparing the behavior of `median_abs_deviation` with ``np.std``,\n    the latter is affected when we change a single value of an array to have an\n    outlier value while the MAD hardly changes:\n\n    >>> from scipy import stats\n    >>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)\n    >>> x.std()\n    0.9973906394005013\n    >>> stats.median_abs_deviation(x)\n    0.82832610097857\n    >>> x[0] = 345.6\n    >>> x.std()\n    34.42304872314415\n    >>> stats.median_abs_deviation(x)\n    0.8323442311590675\n\n    Axis handling example:\n\n    >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n    >>> x\n    array([[10,  7,  4],\n           [ 3,  2,  1]])\n    >>> stats.median_abs_deviation(x)\n    array([3.5, 2.5, 1.5])\n    >>> stats.median_abs_deviation(x, axis=None)\n    2.0\n\n    Scale normal example:\n\n    >>> x = stats.norm.rvs(size=1000000, scale=2, random_state=123456)\n    >>> stats.median_abs_deviation(x)\n    1.3487398527041636\n    >>> stats.median_abs_deviation(x, scale='normal')\n    1.9996446978061115\n\n    \"\"\"\n    if not callable(center):\n        raise TypeError(\"The argument 'center' must be callable. The given \"\n                        f\"value {repr(center)} is not callable.\")\n\n    # An error may be raised here, so fail-fast, before doing lengthy\n    # computations, even though `scale` is not used until later\n    if isinstance(scale, str):\n        if scale.lower() == 'normal':\n            scale = 0.6744897501960817  # special.ndtri(0.75)\n        else:\n            raise ValueError(f\"{scale} is not a valid scale value.\")\n\n    x = asarray(x)\n\n    # Consistent with `np.var` and `np.std`.\n    if not x.size:\n        if axis is None:\n            return np.nan\n        nan_shape = tuple(item for i, item in enumerate(x.shape) if i != axis)\n        if nan_shape == ():\n            # Return nan, not array(nan)\n            return np.nan\n        return np.full(nan_shape, np.nan)\n\n    contains_nan, nan_policy = _contains_nan(x, nan_policy)\n\n    if contains_nan:\n        if axis is None:\n            mad = _mad_1d(x.ravel(), center, nan_policy)\n        else:\n            mad = np.apply_along_axis(_mad_1d, axis, x, center, nan_policy)\n    else:\n        if axis is None:\n            med = center(x, axis=None)\n            mad = np.median(np.abs(x - med))\n        else:\n            # Wrap the call to center() in expand_dims() so it acts like\n            # keepdims=True was used.\n            med = np.expand_dims(center(x, axis=axis), axis)\n            mad = np.median(np.abs(x - med), axis=axis)\n\n    return mad / scale\n\n\n# Keep the top newline so that the message does not show up on the stats page\n_median_absolute_deviation_deprec_msg = \"\"\"\nTo preserve the existing default behavior, use\n`scipy.stats.median_abs_deviation(..., scale=1/1.4826)`.\nThe value 1.4826 is not numerically precise for scaling\nwith a normal distribution. For a numerically precise value, use\n`scipy.stats.median_abs_deviation(..., scale='normal')`.\n\"\"\"\n\n\n# Due to numpy/gh-16349 we need to unindent the entire docstring\n@np.deprecate(old_name='median_absolute_deviation',\n              new_name='median_abs_deviation',\n              message=_median_absolute_deviation_deprec_msg)\ndef median_absolute_deviation(x, axis=0, center=np.median, scale=1.4826,\n                              nan_policy='propagate'):\n    r\"\"\"\nCompute the median absolute deviation of the data along the given axis.\n\nThe median absolute deviation (MAD, [1]_) computes the median over the\nabsolute deviations from the median. It is a measure of dispersion\nsimilar to the standard deviation but more robust to outliers [2]_.\n\nThe MAD of an empty array is ``np.nan``.\n\n.. versionadded:: 1.3.0\n\nParameters\n----------\nx : array_like\n    Input array or object that can be converted to an array.\naxis : int or None, optional\n    Axis along which the range is computed. Default is 0. If None, compute\n    the MAD over the entire array.\ncenter : callable, optional\n    A function that will return the central value. The default is to use\n    np.median. Any user defined function used will need to have the function\n    signature ``func(arr, axis)``.\nscale : int, optional\n    The scaling factor applied to the MAD. The default scale (1.4826)\n    ensures consistency with the standard deviation for normally distributed\n    data.\nnan_policy : {'propagate', 'raise', 'omit'}, optional\n    Defines how to handle when input contains nan.\n    The following options are available (default is 'propagate'):\n\n    * 'propagate': returns nan\n    * 'raise': throws an error\n    * 'omit': performs the calculations ignoring nan values\n\nReturns\n-------\nmad : scalar or ndarray\n    If ``axis=None``, a scalar is returned. If the input contains\n    integers or floats of smaller precision than ``np.float64``, then the\n    output data-type is ``np.float64``. Otherwise, the output data-type is\n    the same as that of the input.\n\nSee Also\n--------\nnumpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,\nscipy.stats.tstd, scipy.stats.tvar\n\nNotes\n-----\nThe `center` argument only affects the calculation of the central value\naround which the MAD is calculated. That is, passing in ``center=np.mean``\nwill calculate the MAD around the mean - it will not calculate the *mean*\nabsolute deviation.\n\nReferences\n----------\n.. [1] \"Median absolute deviation\",\n       https://en.wikipedia.org/wiki/Median_absolute_deviation\n.. [2] \"Robust measures of scale\",\n       https://en.wikipedia.org/wiki/Robust_measures_of_scale\n\nExamples\n--------\nWhen comparing the behavior of `median_absolute_deviation` with ``np.std``,\nthe latter is affected when we change a single value of an array to have an\noutlier value while the MAD hardly changes:\n\n>>> from scipy import stats\n>>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)\n>>> x.std()\n0.9973906394005013\n>>> stats.median_absolute_deviation(x)\n1.2280762773108278\n>>> x[0] = 345.6\n>>> x.std()\n34.42304872314415\n>>> stats.median_absolute_deviation(x)\n1.2340335571164334\n\nAxis handling example:\n\n>>> x = np.array([[10, 7, 4], [3, 2, 1]])\n>>> x\narray([[10,  7,  4],\n       [ 3,  2,  1]])\n>>> stats.median_absolute_deviation(x)\narray([5.1891, 3.7065, 2.2239])\n>>> stats.median_absolute_deviation(x, axis=None)\n2.9652\n\"\"\"\n    if isinstance(scale, str):\n        if scale.lower() == 'raw':\n            warnings.warn(\n                \"use of scale='raw' is deprecated, use scale=1.0 instead\",\n                np.VisibleDeprecationWarning\n                )\n            scale = 1.0\n\n    if not isinstance(scale, str):\n        scale = 1 / scale\n\n    return median_abs_deviation(x, axis=axis, center=center, scale=scale,\n                                nan_policy=nan_policy)\n\n#####################################\n#         TRIMMING FUNCTIONS        #\n#####################################\n\n\nSigmaclipResult = namedtuple('SigmaclipResult', ('clipped', 'lower', 'upper'))\n\n\ndef sigmaclip(a, low=4., high=4.):\n    \"\"\"\n    Perform iterative sigma-clipping of array elements.\n\n    Starting from the full sample, all elements outside the critical range are\n    removed, i.e. all elements of the input array `c` that satisfy either of\n    the following conditions::\n\n        c < mean(c) - std(c)*low\n        c > mean(c) + std(c)*high\n\n    The iteration continues with the updated sample until no\n    elements are outside the (updated) range.\n\n    Parameters\n    ----------\n    a : array_like\n        Data array, will be raveled if not 1-D.\n    low : float, optional\n        Lower bound factor of sigma clipping. Default is 4.\n    high : float, optional\n        Upper bound factor of sigma clipping. Default is 4.\n\n    Returns\n    -------\n    clipped : ndarray\n        Input array with clipped elements removed.\n    lower : float\n        Lower threshold value use for clipping.\n    upper : float\n        Upper threshold value use for clipping.\n\n    Examples\n    --------\n    >>> from scipy.stats import sigmaclip\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n    ...                     np.linspace(0, 20, 5)))\n    >>> fact = 1.5\n    >>> c, low, upp = sigmaclip(a, fact, fact)\n    >>> c\n    array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n    >>> c.var(), c.std()\n    (0.00055555555555555165, 0.023570226039551501)\n    >>> low, c.mean() - fact*c.std(), c.min()\n    (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n    >>> upp, c.mean() + fact*c.std(), c.max()\n    (10.035355339059327, 10.035355339059327, 10.033333333333333)\n\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n    ...                     np.linspace(-100, -50, 3)))\n    >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n    >>> (c == np.linspace(9.5, 10.5, 11)).all()\n    True\n\n    \"\"\"\n    c = np.asarray(a).ravel()\n    delta = 1\n    while delta:\n        c_std = c.std()\n        c_mean = c.mean()\n        size = c.size\n        critlower = c_mean - c_std * low\n        critupper = c_mean + c_std * high\n        c = c[(c >= critlower) & (c <= critupper)]\n        delta = size - c.size\n\n    return SigmaclipResult(c, critlower, critupper)\n\n\ndef trimboth(a, proportiontocut, axis=0):\n    \"\"\"\n    Slice off a proportion of items from both ends of an array.\n\n    Slice off the passed proportion of items from both ends of the passed\n    array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n    rightmost 10% of scores). The trimmed values are the lowest and\n    highest ones.\n    Slice off less if proportion results in a non-integer slice index (i.e.\n    conservatively slices off `proportiontocut`).\n\n    Parameters\n    ----------\n    a : array_like\n        Data to trim.\n    proportiontocut : float\n        Proportion (in range 0-1) of total data set to trim of each end.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    out : ndarray\n        Trimmed version of array `a`. The order of the trimmed content\n        is undefined.\n\n    See Also\n    --------\n    trim_mean\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> a = np.arange(20)\n    >>> b = stats.trimboth(a, 0.1)\n    >>> b.shape\n    (16,)\n\n    \"\"\"\n    a = np.asarray(a)\n\n    if a.size == 0:\n        return a\n\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut\n    if (lowercut >= uppercut):\n        raise ValueError(\"Proportion too big.\")\n\n    atmp = np.partition(a, (lowercut, uppercut - 1), axis)\n\n    sl = [slice(None)] * atmp.ndim\n    sl[axis] = slice(lowercut, uppercut)\n    return atmp[tuple(sl)]\n\n\ndef trim1(a, proportiontocut, tail='right', axis=0):\n    \"\"\"\n    Slice off a proportion from ONE end of the passed array distribution.\n\n    If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n    10% of scores. The lowest or highest values are trimmed (depending on\n    the tail).\n    Slice off less if proportion results in a non-integer slice index\n    (i.e. conservatively slices off `proportiontocut` ).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction to cut off of 'left' or 'right' of distribution.\n    tail : {'left', 'right'}, optional\n        Defaults to 'right'.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    trim1 : ndarray\n        Trimmed version of array `a`. The order of the trimmed content is\n        undefined.\n\n    \"\"\"\n    a = np.asarray(a)\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n\n    nobs = a.shape[axis]\n\n    # avoid possible corner case\n    if proportiontocut >= 1:\n        return []\n\n    if tail.lower() == 'right':\n        lowercut = 0\n        uppercut = nobs - int(proportiontocut * nobs)\n\n    elif tail.lower() == 'left':\n        lowercut = int(proportiontocut * nobs)\n        uppercut = nobs\n\n    atmp = np.partition(a, (lowercut, uppercut - 1), axis)\n\n    return atmp[lowercut:uppercut]\n\n\ndef trim_mean(a, proportiontocut, axis=0):\n    \"\"\"\n    Return mean of array after trimming distribution from both tails.\n\n    If `proportiontocut` = 0.1, slices off 'leftmost' and 'rightmost' 10% of\n    scores. The input is sorted before slicing. Slices off less if proportion\n    results in a non-integer slice index (i.e., conservatively slices off\n    `proportiontocut` ).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction to cut off of both tails of the distribution.\n    axis : int or None, optional\n        Axis along which the trimmed means are computed. Default is 0.\n        If None, compute over the whole array `a`.\n\n    Returns\n    -------\n    trim_mean : ndarray\n        Mean of trimmed array.\n\n    See Also\n    --------\n    trimboth\n    tmean : Compute the trimmed mean ignoring values outside given `limits`.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = np.arange(20)\n    >>> stats.trim_mean(x, 0.1)\n    9.5\n    >>> x2 = x.reshape(5, 4)\n    >>> x2\n    array([[ 0,  1,  2,  3],\n           [ 4,  5,  6,  7],\n           [ 8,  9, 10, 11],\n           [12, 13, 14, 15],\n           [16, 17, 18, 19]])\n    >>> stats.trim_mean(x2, 0.25)\n    array([  8.,   9.,  10.,  11.])\n    >>> stats.trim_mean(x2, 0.25, axis=1)\n    array([  1.5,   5.5,   9.5,  13.5,  17.5])\n\n    \"\"\"\n    a = np.asarray(a)\n\n    if a.size == 0:\n        return np.nan\n\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n\n    nobs = a.shape[axis]\n    lowercut = int(proportiontocut * nobs)\n    uppercut = nobs - lowercut\n    if (lowercut > uppercut):\n        raise ValueError(\"Proportion too big.\")\n\n    atmp = np.partition(a, (lowercut, uppercut - 1), axis)\n\n    sl = [slice(None)] * atmp.ndim\n    sl[axis] = slice(lowercut, uppercut)\n    return np.mean(atmp[tuple(sl)], axis=axis)\n\n\nF_onewayResult = namedtuple('F_onewayResult', ('statistic', 'pvalue'))\n\n\nclass F_onewayConstantInputWarning(RuntimeWarning):\n    \"\"\"\n    Warning generated by `f_oneway` when an input is constant, e.g.\n    each of the samples provided is a constant array.\n    \"\"\"\n\n    def __init__(self, msg=None):\n        if msg is None:\n            msg = (\"Each of the input arrays is constant;\"\n                   \"the F statistic is not defined or infinite\")\n        self.args = (msg,)\n\n\nclass F_onewayBadInputSizesWarning(RuntimeWarning):\n    \"\"\"\n    Warning generated by `f_oneway` when an input has length 0,\n    or if all the inputs have length 1.\n    \"\"\"\n    pass\n\n\ndef _create_f_oneway_nan_result(shape, axis):\n    \"\"\"\n    This is a helper function for f_oneway for creating the return values\n    in certain degenerate conditions.  It creates return values that are\n    all nan with the appropriate shape for the given `shape` and `axis`.\n    \"\"\"\n    axis = np.core.multiarray.normalize_axis_index(axis, len(shape))\n    shp = shape[:axis] + shape[axis+1:]\n    if shp == ():\n        f = np.nan\n        prob = np.nan\n    else:\n        f = np.full(shp, fill_value=np.nan)\n        prob = f.copy()\n    return F_onewayResult(f, prob)\n\n\ndef _first(arr, axis):\n    \"\"\"\n    Return arr[..., 0:1, ...] where 0:1 is in the `axis` position.\n    \"\"\"\n    return np.take_along_axis(arr, np.array(0, ndmin=arr.ndim), axis)\n\n\ndef f_oneway(*args, axis=0):\n    \"\"\"\n    Perform one-way ANOVA.\n\n    The one-way ANOVA tests the null hypothesis that two or more groups have\n    the same population mean.  The test is applied to samples from two or\n    more groups, possibly with differing sizes.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group.  There must be at least\n        two arguments.  If the arrays are multidimensional, then all the\n        dimensions of the array must be the same except for `axis`.\n    axis : int, optional\n        Axis of the input arrays along which the test is applied.\n        Default is 0.\n\n    Returns\n    -------\n    statistic : float\n        The computed F statistic of the test.\n    pvalue : float\n        The associated p-value from the F distribution.\n\n    Warns\n    -----\n    F_onewayConstantInputWarning\n        Raised if each of the input arrays is constant array.\n        In this case the F statistic is either infinite or isn't defined,\n        so ``np.inf`` or ``np.nan`` is returned.\n\n    F_onewayBadInputSizesWarning\n        Raised if the length of any input array is 0, or if all the input\n        arrays have length 1.  ``np.nan`` is returned for the F statistic\n        and the p-value in these cases.\n\n    Notes\n    -----\n    The ANOVA test has important assumptions that must be satisfied in order\n    for the associated p-value to be valid.\n\n    1. The samples are independent.\n    2. Each sample is from a normally distributed population.\n    3. The population standard deviations of the groups are all equal.  This\n       property is known as homoscedasticity.\n\n    If these assumptions are not true for a given set of data, it may still\n    be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`)\n    although with some loss of power.\n\n    The length of each group must be at least one, and there must be at\n    least one group with length greater than one.  If these conditions\n    are not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\n    is returned.\n\n    If each group contains constant values, and there exist at least two\n    groups with different values, the function generates a warning and\n    returns (``np.inf``, 0).\n\n    If all values in all groups are the same, function generates a warning\n    and returns (``np.nan``, ``np.nan``).\n\n    The algorithm is from Heiman [2]_, pp.394-7.\n\n    References\n    ----------\n    .. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n           Chapter 14, 2014, http://vassarstats.net/textbook/\n\n    .. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n           integrated introduction for psychology\", Houghton, Mifflin and\n           Company, 2001.\n\n    .. [3] G.H. McDonald, \"Handbook of Biological Statistics\", One-way ANOVA.\n           http://www.biostathandbook.com/onewayanova.html\n\n    Examples\n    --------\n    >>> from scipy.stats import f_oneway\n\n    Here are some data [3]_ on a shell measurement (the length of the anterior\n    adductor muscle scar, standardized by dividing by length) in the mussel\n    Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\n    Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n    much larger data set used in McDonald et al. (1991).\n\n    >>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n    ...              0.0659, 0.0923, 0.0836]\n    >>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n    ...            0.0725]\n    >>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n    >>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n    ...            0.0689]\n    >>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n    >>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\n    F_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n\n    `f_oneway` accepts multidimensional input arrays.  When the inputs\n    are multidimensional and `axis` is not given, the test is performed\n    along the first axis of the input arrays.  For the following data, the\n    test is performed three times, once for each column.\n\n    >>> a = np.array([[9.87, 9.03, 6.81],\n    ...               [7.18, 8.35, 7.00],\n    ...               [8.39, 7.58, 7.68],\n    ...               [7.45, 6.33, 9.35],\n    ...               [6.41, 7.10, 9.33],\n    ...               [8.00, 8.24, 8.44]])\n    >>> b = np.array([[6.35, 7.30, 7.16],\n    ...               [6.65, 6.68, 7.63],\n    ...               [5.72, 7.73, 6.72],\n    ...               [7.01, 9.19, 7.41],\n    ...               [7.75, 7.87, 8.30],\n    ...               [6.90, 7.97, 6.97]])\n    >>> c = np.array([[3.31, 8.77, 1.01],\n    ...               [8.25, 3.24, 3.62],\n    ...               [6.32, 8.81, 5.19],\n    ...               [7.48, 8.83, 8.91],\n    ...               [8.59, 6.01, 6.07],\n    ...               [3.07, 9.72, 7.48]])\n    >>> F, p = f_oneway(a, b, c)\n    >>> F\n    array([1.75676344, 0.03701228, 3.76439349])\n    >>> p\n    array([0.20630784, 0.96375203, 0.04733157])\n\n    \"\"\"\n    if len(args) < 2:\n        raise TypeError(f'at least two inputs are required; got {len(args)}.')\n\n    args = [np.asarray(arg, dtype=float) for arg in args]\n\n    # ANOVA on N groups, each in its own array\n    num_groups = len(args)\n\n    # We haven't explicitly validated axis, but if it is bad, this call of\n    # np.concatenate will raise np.AxisError.  The call will raise ValueError\n    # if the dimensions of all the arrays, except the axis dimension, are not\n    # the same.\n    alldata = np.concatenate(args, axis=axis)\n    bign = alldata.shape[axis]\n\n    # Check this after forming alldata, so shape errors are detected\n    # and reported before checking for 0 length inputs.\n    if any(arg.shape[axis] == 0 for arg in args):\n        warnings.warn(F_onewayBadInputSizesWarning('at least one input '\n                                                   'has length 0'))\n        return _create_f_oneway_nan_result(alldata.shape, axis)\n\n    # Must have at least one group with length greater than 1.\n    if all(arg.shape[axis] == 1 for arg in args):\n        msg = ('all input arrays have length 1.  f_oneway requires that at '\n               'least one input has length greater than 1.')\n        warnings.warn(F_onewayBadInputSizesWarning(msg))\n        return _create_f_oneway_nan_result(alldata.shape, axis)\n\n    # Check if the values within each group are constant, and if the common\n    # value in at least one group is different from that in another group.\n    # Based on https://github.com/scipy/scipy/issues/11669\n\n    # If axis=0, say, and the groups have shape (n0, ...), (n1, ...), ...,\n    # then is_const is a boolean array with shape (num_groups, ...).\n    # It is True if the groups along the axis slice are each consant.\n    # In the typical case where each input array is 1-d, is_const is a\n    # 1-d array with length num_groups.\n    is_const = np.concatenate([(_first(a, axis) == a).all(axis=axis,\n                                                          keepdims=True)\n                               for a in args], axis=axis)\n\n    # all_const is a boolean array with shape (...) (see previous comment).\n    # It is True if the values within each group along the axis slice are\n    # the same (e.g. [[3, 3, 3], [5, 5, 5, 5], [4, 4, 4]]).\n    all_const = is_const.all(axis=axis)\n    if all_const.any():\n        warnings.warn(F_onewayConstantInputWarning())\n\n    # all_same_const is True if all the values in the groups along the axis=0\n    # slice are the same (e.g. [[3, 3, 3], [3, 3, 3, 3], [3, 3, 3]]).\n    all_same_const = (_first(alldata, axis) == alldata).all(axis=axis)\n\n    # Determine the mean of the data, and subtract that from all inputs to a\n    # variance (via sum_of_sq / sq_of_sum) calculation.  Variance is invariant\n    # to a shift in location, and centering all data around zero vastly\n    # improves numerical stability.\n    offset = alldata.mean(axis=axis, keepdims=True)\n    alldata -= offset\n\n    normalized_ss = _square_of_sums(alldata, axis=axis) / bign\n\n    sstot = _sum_of_squares(alldata, axis=axis) - normalized_ss\n\n    ssbn = 0\n    for a in args:\n        ssbn += _square_of_sums(a - offset, axis=axis) / a.shape[axis]\n\n    # Naming: variables ending in bn/b are for \"between treatments\", wn/w are\n    # for \"within treatments\"\n    ssbn -= normalized_ss\n    sswn = sstot - ssbn\n    dfbn = num_groups - 1\n    dfwn = bign - num_groups\n    msb = ssbn / dfbn\n    msw = sswn / dfwn\n    with np.errstate(divide='ignore', invalid='ignore'):\n        f = msb / msw\n\n    prob = special.fdtrc(dfbn, dfwn, f)   # equivalent to stats.f.sf\n\n    # Fix any f values that should be inf or nan because the corresponding\n    # inputs were constant.\n    if np.isscalar(f):\n        if all_same_const:\n            f = np.nan\n            prob = np.nan\n        elif all_const:\n            f = np.inf\n            prob = 0.0\n    else:\n        f[all_const] = np.inf\n        prob[all_const] = 0.0\n        f[all_same_const] = np.nan\n        prob[all_same_const] = np.nan\n\n    return F_onewayResult(f, prob)\n\n\nclass PearsonRConstantInputWarning(RuntimeWarning):\n    \"\"\"Warning generated by `pearsonr` when an input is constant.\"\"\"\n\n    def __init__(self, msg=None):\n        if msg is None:\n            msg = (\"An input array is constant; the correlation coefficent \"\n                   \"is not defined.\")\n        self.args = (msg,)\n\n\nclass PearsonRNearConstantInputWarning(RuntimeWarning):\n    \"\"\"Warning generated by `pearsonr` when an input is nearly constant.\"\"\"\n\n    def __init__(self, msg=None):\n        if msg is None:\n            msg = (\"An input array is nearly constant; the computed \"\n                   \"correlation coefficent may be inaccurate.\")\n        self.args = (msg,)\n\n\ndef pearsonr(x, y):\n    r\"\"\"\n    Pearson correlation coefficient and p-value for testing non-correlation.\n\n    The Pearson correlation coefficient [1]_ measures the linear relationship\n    between two datasets.  The calculation of the p-value relies on the\n    assumption that each dataset is normally distributed.  (See Kowalski [3]_\n    for a discussion of the effects of non-normality of the input on the\n    distribution of the correlation coefficient.)  Like other correlation\n    coefficients, this one varies between -1 and +1 with 0 implying no\n    correlation. Correlations of -1 or +1 imply an exact linear relationship.\n    Positive correlations imply that as x increases, so does y. Negative\n    correlations imply that as x increases, y decreases.\n\n    The p-value roughly indicates the probability of an uncorrelated system\n    producing datasets that have a Pearson correlation at least as extreme\n    as the one computed from these datasets.\n\n    Parameters\n    ----------\n    x : (N,) array_like\n        Input array.\n    y : (N,) array_like\n        Input array.\n\n    Returns\n    -------\n    r : float\n        Pearson's correlation coefficient.\n    p-value : float\n        Two-tailed p-value.\n\n    Warns\n    -----\n    PearsonRConstantInputWarning\n        Raised if an input is a constant array.  The correlation coefficient\n        is not defined in this case, so ``np.nan`` is returned.\n\n    PearsonRNearConstantInputWarning\n        Raised if an input is \"nearly\" constant.  The array ``x`` is considered\n        nearly constant if ``norm(x - mean(x)) < 1e-13 * abs(mean(x))``.\n        Numerical errors in the calculation ``x - mean(x)`` in this case might\n        result in an inaccurate calculation of r.\n\n    See Also\n    --------\n    spearmanr : Spearman rank-order correlation coefficient.\n    kendalltau : Kendall's tau, a correlation measure for ordinal data.\n\n    Notes\n    -----\n    The correlation coefficient is calculated as follows:\n\n    .. math::\n\n        r = \\frac{\\sum (x - m_x) (y - m_y)}\n                 {\\sqrt{\\sum (x - m_x)^2 \\sum (y - m_y)^2}}\n\n    where :math:`m_x` is the mean of the vector :math:`x` and :math:`m_y` is\n    the mean of the vector :math:`y`.\n\n    Under the assumption that :math:`x` and :math:`m_y` are drawn from\n    independent normal distributions (so the population correlation coefficient\n    is 0), the probability density function of the sample correlation\n    coefficient :math:`r` is ([1]_, [2]_):\n\n    .. math::\n\n        f(r) = \\frac{{(1-r^2)}^{n/2-2}}{\\mathrm{B}(\\frac{1}{2},\\frac{n}{2}-1)}\n\n    where n is the number of samples, and B is the beta function.  This\n    is sometimes referred to as the exact distribution of r.  This is\n    the distribution that is used in `pearsonr` to compute the p-value.\n    The distribution is a beta distribution on the interval [-1, 1],\n    with equal shape parameters a = b = n/2 - 1.  In terms of SciPy's\n    implementation of the beta distribution, the distribution of r is::\n\n        dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n\n    The p-value returned by `pearsonr` is a two-sided p-value.  For a\n    given sample with correlation coefficient r, the p-value is\n    the probability that abs(r') of a random sample x' and y' drawn from\n    the population with zero correlation would be greater than or equal\n    to abs(r).  In terms of the object ``dist`` shown above, the p-value\n    for a given r and length n can be computed as::\n\n        p = 2*dist.cdf(-abs(r))\n\n    When n is 2, the above continuous distribution is not well-defined.\n    One can interpret the limit of the beta distribution as the shape\n    parameters a and b approach a = b = 0 as a discrete distribution with\n    equal probability masses at r = 1 and r = -1.  More directly, one\n    can observe that, given the data x = [x1, x2] and y = [y1, y2], and\n    assuming x1 != x2 and y1 != y2, the only possible values for r are 1\n    and -1.  Because abs(r') for any sample x' and y' with length 2 will\n    be 1, the two-sided p-value for a sample of length 2 is always 1.\n\n    References\n    ----------\n    .. [1] \"Pearson correlation coefficient\", Wikipedia,\n           https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n    .. [2] Student, \"Probable error of a correlation coefficient\",\n           Biometrika, Volume 6, Issue 2-3, 1 September 1908, pp. 302-310.\n    .. [3] C. J. Kowalski, \"On the Effects of Non-Normality on the Distribution\n           of the Sample Product-Moment Correlation Coefficient\"\n           Journal of the Royal Statistical Society. Series C (Applied\n           Statistics), Vol. 21, No. 1 (1972), pp. 1-12.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> a = np.array([0, 0, 0, 1, 1, 1, 1])\n    >>> b = np.arange(7)\n    >>> stats.pearsonr(a, b)\n    (0.8660254037844386, 0.011724811003954649)\n\n    >>> stats.pearsonr([1, 2, 3, 4, 5], [10, 9, 2.5, 6, 4])\n    (-0.7426106572325057, 0.1505558088534455)\n\n    \"\"\"\n    n = len(x)\n    if n != len(y):\n        raise ValueError('x and y must have the same length.')\n\n    if n < 2:\n        raise ValueError('x and y must have length at least 2.')\n\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    # If an input is constant, the correlation coefficient is not defined.\n    if (x == x[0]).all() or (y == y[0]).all():\n        warnings.warn(PearsonRConstantInputWarning())\n        return np.nan, np.nan\n\n    # dtype is the data type for the calculations.  This expression ensures\n    # that the data type is at least 64 bit floating point.  It might have\n    # more precision if the input is, for example, np.longdouble.\n    dtype = type(1.0 + x[0] + y[0])\n\n    if n == 2:\n        return dtype(np.sign(x[1] - x[0])*np.sign(y[1] - y[0])), 1.0\n\n    xmean = x.mean(dtype=dtype)\n    ymean = y.mean(dtype=dtype)\n\n    # By using `astype(dtype)`, we ensure that the intermediate calculations\n    # use at least 64 bit floating point.\n    xm = x.astype(dtype) - xmean\n    ym = y.astype(dtype) - ymean\n\n    # Unlike np.linalg.norm or the expression sqrt((xm*xm).sum()),\n    # scipy.linalg.norm(xm) does not overflow if xm is, for example,\n    # [-5e210, 5e210, 3e200, -3e200]\n    normxm = linalg.norm(xm)\n    normym = linalg.norm(ym)\n\n    threshold = 1e-13\n    if normxm < threshold*abs(xmean) or normym < threshold*abs(ymean):\n        # If all the values in x (likewise y) are very close to the mean,\n        # the loss of precision that occurs in the subtraction xm = x - xmean\n        # might result in large errors in r.\n        warnings.warn(PearsonRNearConstantInputWarning())\n\n    r = np.dot(xm/normxm, ym/normym)\n\n    # Presumably, if abs(r) > 1, then it is only some small artifact of\n    # floating point arithmetic.\n    r = max(min(r, 1.0), -1.0)\n\n    # As explained in the docstring, the p-value can be computed as\n    #     p = 2*dist.cdf(-abs(r))\n    # where dist is the beta distribution on [-1, 1] with shape parameters\n    # a = b = n/2 - 1.  `special.btdtr` is the CDF for the beta distribution\n    # on [0, 1].  To use it, we make the transformation  x = (r + 1)/2; the\n    # shape parameters do not change.  Then -abs(r) used in `cdf(-abs(r))`\n    # becomes x = (-abs(r) + 1)/2 = 0.5*(1 - abs(r)).  (r is cast to float64\n    # to avoid a TypeError raised by btdtr when r is higher precision.)\n    ab = n/2 - 1\n    prob = 2*special.btdtr(ab, ab, 0.5*(1 - abs(np.float64(r))))\n\n    return r, prob\n\n\ndef fisher_exact(table, alternative='two-sided'):\n    \"\"\"\n    Perform a Fisher exact test on a 2x2 contingency table.\n\n    Parameters\n    ----------\n    table : array_like of ints\n        A 2x2 contingency table.  Elements should be non-negative integers.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided\n          * 'greater': one-sided\n\n    Returns\n    -------\n    oddsratio : float\n        This is prior odds ratio and not a posterior estimate.\n    p_value : float\n        P-value, the probability of obtaining a distribution at least as\n        extreme as the one that was actually observed, assuming that the\n        null hypothesis is true.\n\n    See Also\n    --------\n    chi2_contingency : Chi-square test of independence of variables in a\n        contingency table.\n\n    Notes\n    -----\n    The calculated odds ratio is different from the one R uses. This scipy\n    implementation returns the (more common) \"unconditional Maximum\n    Likelihood Estimate\", while R uses the \"conditional Maximum Likelihood\n    Estimate\".\n\n    For tables with large numbers, the (inexact) chi-square test implemented\n    in the function `chi2_contingency` can also be used.\n\n    Examples\n    --------\n    Say we spend a few days counting whales and sharks in the Atlantic and\n    Indian oceans. In the Atlantic ocean we find 8 whales and 1 shark, in the\n    Indian ocean 2 whales and 5 sharks. Then our contingency table is::\n\n                Atlantic  Indian\n        whales     8        2\n        sharks     1        5\n\n    We use this table to find the p-value:\n\n    >>> import scipy.stats as stats\n    >>> oddsratio, pvalue = stats.fisher_exact([[8, 2], [1, 5]])\n    >>> pvalue\n    0.0349...\n\n    The probability that we would observe this or an even more imbalanced ratio\n    by chance is about 3.5%.  A commonly used significance level is 5%--if we\n    adopt that, we can therefore conclude that our observed imbalance is\n    statistically significant; whales prefer the Atlantic while sharks prefer\n    the Indian ocean.\n\n    \"\"\"\n    hypergeom = distributions.hypergeom\n    c = np.asarray(table, dtype=np.int64)  # int32 is not enough for the algorithm\n    if not c.shape == (2, 2):\n        raise ValueError(\"The input `table` must be of shape (2, 2).\")\n\n    if np.any(c < 0):\n        raise ValueError(\"All values in `table` must be nonnegative.\")\n\n    if 0 in c.sum(axis=0) or 0 in c.sum(axis=1):\n        # If both values in a row or column are zero, the p-value is 1 and\n        # the odds ratio is NaN.\n        return np.nan, 1.0\n\n    if c[1, 0] > 0 and c[0, 1] > 0:\n        oddsratio = c[0, 0] * c[1, 1] / (c[1, 0] * c[0, 1])\n    else:\n        oddsratio = np.inf\n\n    n1 = c[0, 0] + c[0, 1]\n    n2 = c[1, 0] + c[1, 1]\n    n = c[0, 0] + c[1, 0]\n\n    def binary_search(n, n1, n2, side):\n        \"\"\"Binary search for where to begin halves in two-sided test.\"\"\"\n        if side == \"upper\":\n            minval = mode\n            maxval = n\n        else:\n            minval = 0\n            maxval = mode\n        guess = -1\n        while maxval - minval > 1:\n            if maxval == minval + 1 and guess == minval:\n                guess = maxval\n            else:\n                guess = (maxval + minval) // 2\n            pguess = hypergeom.pmf(guess, n1 + n2, n1, n)\n            if side == \"upper\":\n                ng = guess - 1\n            else:\n                ng = guess + 1\n            if pguess <= pexact < hypergeom.pmf(ng, n1 + n2, n1, n):\n                break\n            elif pguess < pexact:\n                maxval = guess\n            else:\n                minval = guess\n        if guess == -1:\n            guess = minval\n        if side == \"upper\":\n            while guess > 0 and hypergeom.pmf(guess, n1 + n2, n1, n) < pexact * epsilon:\n                guess -= 1\n            while hypergeom.pmf(guess, n1 + n2, n1, n) > pexact / epsilon:\n                guess += 1\n        else:\n            while hypergeom.pmf(guess, n1 + n2, n1, n) < pexact * epsilon:\n                guess += 1\n            while guess > 0 and hypergeom.pmf(guess, n1 + n2, n1, n) > pexact / epsilon:\n                guess -= 1\n        return guess\n\n    if alternative == 'less':\n        pvalue = hypergeom.cdf(c[0, 0], n1 + n2, n1, n)\n    elif alternative == 'greater':\n        # Same formula as the 'less' case, but with the second column.\n        pvalue = hypergeom.cdf(c[0, 1], n1 + n2, n1, c[0, 1] + c[1, 1])\n    elif alternative == 'two-sided':\n        mode = int((n + 1) * (n1 + 1) / (n1 + n2 + 2))\n        pexact = hypergeom.pmf(c[0, 0], n1 + n2, n1, n)\n        pmode = hypergeom.pmf(mode, n1 + n2, n1, n)\n\n        epsilon = 1 - 1e-4\n        if np.abs(pexact - pmode) / np.maximum(pexact, pmode) <= 1 - epsilon:\n            return oddsratio, 1.\n\n        elif c[0, 0] < mode:\n            plower = hypergeom.cdf(c[0, 0], n1 + n2, n1, n)\n            if hypergeom.pmf(n, n1 + n2, n1, n) > pexact / epsilon:\n                return oddsratio, plower\n\n            guess = binary_search(n, n1, n2, \"upper\")\n            pvalue = plower + hypergeom.sf(guess - 1, n1 + n2, n1, n)\n        else:\n            pupper = hypergeom.sf(c[0, 0] - 1, n1 + n2, n1, n)\n            if hypergeom.pmf(0, n1 + n2, n1, n) > pexact / epsilon:\n                return oddsratio, pupper\n\n            guess = binary_search(n, n1, n2, \"lower\")\n            pvalue = pupper + hypergeom.cdf(guess, n1 + n2, n1, n)\n    else:\n        msg = \"`alternative` should be one of {'two-sided', 'less', 'greater'}\"\n        raise ValueError(msg)\n\n    pvalue = min(pvalue, 1.0)\n\n    return oddsratio, pvalue\n\n\nclass SpearmanRConstantInputWarning(RuntimeWarning):\n    \"\"\"Warning generated by `spearmanr` when an input is constant.\"\"\"\n\n    def __init__(self, msg=None):\n        if msg is None:\n            msg = (\"An input array is constant; the correlation coefficent \"\n                   \"is not defined.\")\n        self.args = (msg,)\n\n\nSpearmanrResult = namedtuple('SpearmanrResult', ('correlation', 'pvalue'))\n\n\ndef spearmanr(a, b=None, axis=0, nan_policy='propagate'):\n    \"\"\"\n    Calculate a Spearman correlation coefficient with associated p-value.\n\n    The Spearman rank-order correlation coefficient is a nonparametric measure\n    of the monotonicity of the relationship between two datasets. Unlike the\n    Pearson correlation, the Spearman correlation does not assume that both\n    datasets are normally distributed. Like other correlation coefficients,\n    this one varies between -1 and +1 with 0 implying no correlation.\n    Correlations of -1 or +1 imply an exact monotonic relationship. Positive\n    correlations imply that as x increases, so does y. Negative correlations\n    imply that as x increases, y decreases.\n\n    The p-value roughly indicates the probability of an uncorrelated system\n    producing datasets that have a Spearman correlation at least as extreme\n    as the one computed from these datasets. The p-values are not entirely\n    reliable but are probably reasonable for datasets larger than 500 or so.\n\n    Parameters\n    ----------\n    a, b : 1D or 2D array_like, b is optional\n        One or two 1-D or 2-D arrays containing multiple variables and\n        observations. When these are 1-D, each represents a vector of\n        observations of a single variable. For the behavior in the 2-D case,\n        see under ``axis``, below.\n        Both arrays need to have the same length in the ``axis`` dimension.\n    axis : int or None, optional\n        If axis=0 (default), then each column represents a variable, with\n        observations in the rows. If axis=1, the relationship is transposed:\n        each row represents a variable, while the columns contain observations.\n        If axis=None, then both arrays will be raveled.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    correlation : float or ndarray (2-D square)\n        Spearman correlation matrix or correlation coefficient (if only 2\n        variables are given as parameters. Correlation matrix is square with\n        length equal to total number of variables (columns or rows) in ``a``\n        and ``b`` combined.\n    pvalue : float\n        The two-sided p-value for a hypothesis test whose null hypothesis is\n        that two sets of data are uncorrelated, has same dimension as rho.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n       Section  14.7\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> stats.spearmanr([1,2,3,4,5], [5,6,7,8,7])\n    (0.82078268166812329, 0.088587005313543798)\n    >>> np.random.seed(1234321)\n    >>> x2n = np.random.randn(100, 2)\n    >>> y2n = np.random.randn(100, 2)\n    >>> stats.spearmanr(x2n)\n    (0.059969996999699973, 0.55338590803773591)\n    >>> stats.spearmanr(x2n[:,0], x2n[:,1])\n    (0.059969996999699973, 0.55338590803773591)\n    >>> rho, pval = stats.spearmanr(x2n, y2n)\n    >>> rho\n    array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626],\n           [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653],\n           [ 0.18569457,  0.110003  ,  1.        ,  0.03488749],\n           [ 0.06258626,  0.02534653,  0.03488749,  1.        ]])\n    >>> pval\n    array([[ 0.        ,  0.55338591,  0.06435364,  0.53617935],\n           [ 0.55338591,  0.        ,  0.27592895,  0.80234077],\n           [ 0.06435364,  0.27592895,  0.        ,  0.73039992],\n           [ 0.53617935,  0.80234077,  0.73039992,  0.        ]])\n    >>> rho, pval = stats.spearmanr(x2n.T, y2n.T, axis=1)\n    >>> rho\n    array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626],\n           [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653],\n           [ 0.18569457,  0.110003  ,  1.        ,  0.03488749],\n           [ 0.06258626,  0.02534653,  0.03488749,  1.        ]])\n    >>> stats.spearmanr(x2n, y2n, axis=None)\n    (0.10816770419260482, 0.1273562188027364)\n    >>> stats.spearmanr(x2n.ravel(), y2n.ravel())\n    (0.10816770419260482, 0.1273562188027364)\n\n    >>> xint = np.random.randint(10, size=(100, 2))\n    >>> stats.spearmanr(xint)\n    (0.052760927029710199, 0.60213045837062351)\n\n    \"\"\"\n    if axis is not None and axis > 1:\n        raise ValueError(\"spearmanr only handles 1-D or 2-D arrays, supplied axis argument {}, please use only values 0, 1 or None for axis\".format(axis))\n\n    a, axisout = _chk_asarray(a, axis)\n    if a.ndim > 2:\n        raise ValueError(\"spearmanr only handles 1-D or 2-D arrays\")\n\n    if b is None:\n        if a.ndim < 2:\n            raise ValueError(\"`spearmanr` needs at least 2 variables to compare\")\n    else:\n        # Concatenate a and b, so that we now only have to handle the case\n        # of a 2-D `a`.\n        b, _ = _chk_asarray(b, axis)\n        if axisout == 0:\n            a = np.column_stack((a, b))\n        else:\n            a = np.row_stack((a, b))\n\n    n_vars = a.shape[1 - axisout]\n    n_obs = a.shape[axisout]\n    if n_obs <= 1:\n        # Handle empty arrays or single observations.\n        return SpearmanrResult(np.nan, np.nan)\n\n    if axisout == 0:\n        if (a[:, 0][0] == a[:, 0]).all() or (a[:, 1][0] == a[:, 1]).all():\n            # If an input is constant, the correlation coefficient is not defined.\n            warnings.warn(SpearmanRConstantInputWarning())\n            return SpearmanrResult(np.nan, np.nan)\n    else:  # case when axisout == 1 b/c a is 2 dim only\n        if (a[0, :][0] == a[0, :]).all() or (a[1, :][0] == a[1, :]).all():\n            # If an input is constant, the correlation coefficient is not defined.\n            warnings.warn(SpearmanRConstantInputWarning())\n            return SpearmanrResult(np.nan, np.nan)\n\n    a_contains_nan, nan_policy = _contains_nan(a, nan_policy)\n    variable_has_nan = np.zeros(n_vars, dtype=bool)\n    if a_contains_nan:\n        if nan_policy == 'omit':\n            return mstats_basic.spearmanr(a, axis=axis, nan_policy=nan_policy)\n        elif nan_policy == 'propagate':\n            if a.ndim == 1 or n_vars <= 2:\n                return SpearmanrResult(np.nan, np.nan)\n            else:\n                # Keep track of variables with NaNs, set the outputs to NaN\n                # only for those variables\n                variable_has_nan = np.isnan(a).any(axis=axisout)\n\n    a_ranked = np.apply_along_axis(rankdata, axisout, a)\n    rs = np.corrcoef(a_ranked, rowvar=axisout)\n    dof = n_obs - 2  # degrees of freedom\n\n    # rs can have elements equal to 1, so avoid zero division warnings\n    with np.errstate(divide='ignore'):\n        # clip the small negative values possibly caused by rounding\n        # errors before taking the square root\n        t = rs * np.sqrt((dof/((rs+1.0)*(1.0-rs))).clip(0))\n\n    prob = 2 * distributions.t.sf(np.abs(t), dof)\n\n    # For backwards compatibility, return scalars when comparing 2 columns\n    if rs.shape == (2, 2):\n        return SpearmanrResult(rs[1, 0], prob[1, 0])\n    else:\n        rs[variable_has_nan, :] = np.nan\n        rs[:, variable_has_nan] = np.nan\n        return SpearmanrResult(rs, prob)\n\n\nPointbiserialrResult = namedtuple('PointbiserialrResult',\n                                  ('correlation', 'pvalue'))\n\n\ndef pointbiserialr(x, y):\n    r\"\"\"\n    Calculate a point biserial correlation coefficient and its p-value.\n\n    The point biserial correlation is used to measure the relationship\n    between a binary variable, x, and a continuous variable, y. Like other\n    correlation coefficients, this one varies between -1 and +1 with 0\n    implying no correlation. Correlations of -1 or +1 imply a determinative\n    relationship.\n\n    This function uses a shortcut formula but produces the same result as\n    `pearsonr`.\n\n    Parameters\n    ----------\n    x : array_like of bools\n        Input array.\n    y : array_like\n        Input array.\n\n    Returns\n    -------\n    correlation : float\n        R value.\n    pvalue : float\n        Two-sided p-value.\n\n    Notes\n    -----\n    `pointbiserialr` uses a t-test with ``n-1`` degrees of freedom.\n    It is equivalent to `pearsonr`.\n\n    The value of the point-biserial correlation can be calculated from:\n\n    .. math::\n\n        r_{pb} = \\frac{\\overline{Y_{1}} -\n                 \\overline{Y_{0}}}{s_{y}}\\sqrt{\\frac{N_{1} N_{2}}{N (N - 1))}}\n\n    Where :math:`Y_{0}` and :math:`Y_{1}` are means of the metric\n    observations coded 0 and 1 respectively; :math:`N_{0}` and :math:`N_{1}`\n    are number of observations coded 0 and 1 respectively; :math:`N` is the\n    total number of observations and :math:`s_{y}` is the standard\n    deviation of all the metric observations.\n\n    A value of :math:`r_{pb}` that is significantly different from zero is\n    completely equivalent to a significant difference in means between the two\n    groups. Thus, an independent groups t Test with :math:`N-2` degrees of\n    freedom may be used to test whether :math:`r_{pb}` is nonzero. The\n    relation between the t-statistic for comparing two independent groups and\n    :math:`r_{pb}` is given by:\n\n    .. math::\n\n        t = \\sqrt{N - 2}\\frac{r_{pb}}{\\sqrt{1 - r^{2}_{pb}}}\n\n    References\n    ----------\n    .. [1] J. Lev, \"The Point Biserial Coefficient of Correlation\", Ann. Math.\n           Statist., Vol. 20, no.1, pp. 125-126, 1949.\n\n    .. [2] R.F. Tate, \"Correlation Between a Discrete and a Continuous\n           Variable. Point-Biserial Correlation.\", Ann. Math. Statist., Vol. 25,\n           np. 3, pp. 603-607, 1954.\n\n    .. [3] D. Kornbrot \"Point Biserial Correlation\", In Wiley StatsRef:\n           Statistics Reference Online (eds N. Balakrishnan, et al.), 2014.\n           :doi:`10.1002/9781118445112.stat06227`\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> a = np.array([0, 0, 0, 1, 1, 1, 1])\n    >>> b = np.arange(7)\n    >>> stats.pointbiserialr(a, b)\n    (0.8660254037844386, 0.011724811003954652)\n    >>> stats.pearsonr(a, b)\n    (0.86602540378443871, 0.011724811003954626)\n    >>> np.corrcoef(a, b)\n    array([[ 1.       ,  0.8660254],\n           [ 0.8660254,  1.       ]])\n\n    \"\"\"\n    rpb, prob = pearsonr(x, y)\n    return PointbiserialrResult(rpb, prob)\n\n\nKendalltauResult = namedtuple('KendalltauResult', ('correlation', 'pvalue'))\n\n\ndef kendalltau(x, y, initial_lexsort=None, nan_policy='propagate',\n               method='auto', variant='b'):\n    \"\"\"\n    Calculate Kendall's tau, a correlation measure for ordinal data.\n\n    Kendall's tau is a measure of the correspondence between two rankings.\n    Values close to 1 indicate strong agreement, and values close to -1\n    indicate strong disagreement. This implements two variants of Kendall's\n    tau: tau-b (the default) and tau-c (also known as Stuart's tau-c). These\n    differ only in how they are normalized to lie within the range -1 to 1;\n    the hypothesis tests (their p-values) are identical. Kendall's original\n    tau-a is not implemented separately because both tau-b and tau-c reduce\n    to tau-a in the absence of ties.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of rankings, of the same shape. If arrays are not 1-D, they\n        will be flattened to 1-D.\n    initial_lexsort : bool, optional\n        Unused (deprecated).\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    method : {'auto', 'asymptotic', 'exact'}, optional\n        Defines which method is used to calculate the p-value [5]_.\n        The following options are available (default is 'auto'):\n\n          * 'auto': selects the appropriate method based on a trade-off\n            between speed and accuracy\n          * 'asymptotic': uses a normal approximation valid for large samples\n          * 'exact': computes the exact p-value, but can only be used if no ties\n            are present. As the sample size increases, the 'exact' computation\n            time may grow and the result may lose some precision.\n\n    variant: {'b', 'c'}, optional\n        Defines which variant of Kendall's tau is returned. Default is 'b'.\n\n    Returns\n    -------\n    correlation : float\n       The tau statistic.\n    pvalue : float\n       The two-sided p-value for a hypothesis test whose null hypothesis is\n       an absence of association, tau = 0.\n\n    See Also\n    --------\n    spearmanr : Calculates a Spearman rank-order correlation coefficient.\n    theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n    weightedtau : Computes a weighted version of Kendall's tau.\n\n    Notes\n    -----\n    The definition of Kendall's tau that is used is [2]_::\n\n      tau_b = (P - Q) / sqrt((P + Q + T) * (P + Q + U))\n\n      tau_c = 2 (P - Q) / (n**2 * (m - 1) / m)\n\n    where P is the number of concordant pairs, Q the number of discordant\n    pairs, T the number of ties only in `x`, and U the number of ties only in\n    `y`.  If a tie occurs for the same pair in both `x` and `y`, it is not\n    added to either T or U. n is the total number of samples, and m is the\n    number of unique values in either `x` or `y`, whichever is smaller.\n\n    References\n    ----------\n    .. [1] Maurice G. Kendall, \"A New Measure of Rank Correlation\", Biometrika\n           Vol. 30, No. 1/2, pp. 81-93, 1938.\n    .. [2] Maurice G. Kendall, \"The treatment of ties in ranking problems\",\n           Biometrika Vol. 33, No. 3, pp. 239-251. 1945.\n    .. [3] Gottfried E. Noether, \"Elements of Nonparametric Statistics\", John\n           Wiley & Sons, 1967.\n    .. [4] Peter M. Fenwick, \"A new data structure for cumulative frequency\n           tables\", Software: Practice and Experience, Vol. 24, No. 3,\n           pp. 327-336, 1994.\n    .. [5] Maurice G. Kendall, \"Rank Correlation Methods\" (4th Edition),\n           Charles Griffin & Co., 1970.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x1 = [12, 2, 1, 12, 2]\n    >>> x2 = [1, 4, 7, 1, 0]\n    >>> tau, p_value = stats.kendalltau(x1, x2)\n    >>> tau\n    -0.47140452079103173\n    >>> p_value\n    0.2827454599327748\n\n    \"\"\"\n    x = np.asarray(x).ravel()\n    y = np.asarray(y).ravel()\n\n    if x.size != y.size:\n        raise ValueError(\"All inputs to `kendalltau` must be of the same \"\n                         f\"size, found x-size {x.size} and y-size {y.size}\")\n    elif not x.size or not y.size:\n        # Return NaN if arrays are empty\n        return KendalltauResult(np.nan, np.nan)\n\n    # check both x and y\n    cnx, npx = _contains_nan(x, nan_policy)\n    cny, npy = _contains_nan(y, nan_policy)\n    contains_nan = cnx or cny\n    if npx == 'omit' or npy == 'omit':\n        nan_policy = 'omit'\n\n    if contains_nan and nan_policy == 'propagate':\n        return KendalltauResult(np.nan, np.nan)\n\n    elif contains_nan and nan_policy == 'omit':\n        x = ma.masked_invalid(x)\n        y = ma.masked_invalid(y)\n        if variant == 'b':\n            return mstats_basic.kendalltau(x, y, method=method, use_ties=True)\n        else:\n            raise ValueError(\"Only variant 'b' is supported for masked arrays\")\n\n    if initial_lexsort is not None:  # deprecate to drop!\n        warnings.warn('\"initial_lexsort\" is gone!')\n\n    def count_rank_tie(ranks):\n        cnt = np.bincount(ranks).astype('int64', copy=False)\n        cnt = cnt[cnt > 1]\n        return ((cnt * (cnt - 1) // 2).sum(),\n                (cnt * (cnt - 1.) * (cnt - 2)).sum(),\n                (cnt * (cnt - 1.) * (2*cnt + 5)).sum())\n\n    size = x.size\n    perm = np.argsort(y)  # sort on y and convert y to dense ranks\n    x, y = x[perm], y[perm]\n    y = np.r_[True, y[1:] != y[:-1]].cumsum(dtype=np.intp)\n\n    # stable sort on x and convert x to dense ranks\n    perm = np.argsort(x, kind='mergesort')\n    x, y = x[perm], y[perm]\n    x = np.r_[True, x[1:] != x[:-1]].cumsum(dtype=np.intp)\n\n    dis = _kendall_dis(x, y)  # discordant pairs\n\n    obs = np.r_[True, (x[1:] != x[:-1]) | (y[1:] != y[:-1]), True]\n    cnt = np.diff(np.nonzero(obs)[0]).astype('int64', copy=False)\n\n    ntie = (cnt * (cnt - 1) // 2).sum()  # joint ties\n    xtie, x0, x1 = count_rank_tie(x)     # ties in x, stats\n    ytie, y0, y1 = count_rank_tie(y)     # ties in y, stats\n\n    tot = (size * (size - 1)) // 2\n\n    if xtie == tot or ytie == tot:\n        return KendalltauResult(np.nan, np.nan)\n\n    # Note that tot = con + dis + (xtie - ntie) + (ytie - ntie) + ntie\n    #               = con + dis + xtie + ytie - ntie\n    con_minus_dis = tot - xtie - ytie + ntie - 2 * dis\n    if variant == 'b':\n        tau = con_minus_dis / np.sqrt(tot - xtie) / np.sqrt(tot - ytie)\n    elif variant == 'c':\n        minclasses = min(len(set(x)), len(set(y)))\n        tau = 2*con_minus_dis / (size**2 * (minclasses-1)/minclasses)\n    else:\n        raise ValueError(f\"Unknown variant of the method chosen: {variant}. \"\n                         \"variant must be 'b' or 'c'.\")\n\n    # Limit range to fix computational errors\n    tau = min(1., max(-1., tau))\n\n    # The p-value calculation is the same for all variants since the p-value\n    # depends only on con_minus_dis.\n    if method == 'exact' and (xtie != 0 or ytie != 0):\n        raise ValueError(\"Ties found, exact method cannot be used.\")\n\n    if method == 'auto':\n        if (xtie == 0 and ytie == 0) and (size <= 33 or\n                                          min(dis, tot-dis) <= 1):\n            method = 'exact'\n        else:\n            method = 'asymptotic'\n\n    if xtie == 0 and ytie == 0 and method == 'exact':\n        pvalue = mstats_basic._kendall_p_exact(size, min(dis, tot-dis))\n\n\n    elif method == 'asymptotic':\n        # con_minus_dis is approx normally distributed with this variance [3]_\n        m = size * (size - 1.)\n        var = ((m * (2*size + 5) - x1 - y1) / 18 +\n               (2 * xtie * ytie) / m + x0 * y0 / (9 * m * (size - 2)))\n        pvalue = (special.erfc(np.abs(con_minus_dis) /\n                  np.sqrt(var) / np.sqrt(2)))\n    else:\n        raise ValueError(f\"Unknown method {method} specified.  Use 'auto', \"\n                         \"'exact' or 'asymptotic'.\")\n\n    return KendalltauResult(tau, pvalue)\n\n\nWeightedTauResult = namedtuple('WeightedTauResult', ('correlation', 'pvalue'))\n\n\ndef weightedtau(x, y, rank=True, weigher=None, additive=True):\n    r\"\"\"\n    Compute a weighted version of Kendall's :math:`\\tau`.\n\n    The weighted :math:`\\tau` is a weighted version of Kendall's\n    :math:`\\tau` in which exchanges of high weight are more influential than\n    exchanges of low weight. The default parameters compute the additive\n    hyperbolic version of the index, :math:`\\tau_\\mathrm h`, which has\n    been shown to provide the best balance between important and\n    unimportant elements [1]_.\n\n    The weighting is defined by means of a rank array, which assigns a\n    nonnegative rank to each element, and a weigher function, which\n    assigns a weight based from the rank to each element. The weight of an\n    exchange is then the sum or the product of the weights of the ranks of\n    the exchanged elements. The default parameters compute\n    :math:`\\tau_\\mathrm h`: an exchange between elements with rank\n    :math:`r` and :math:`s` (starting from zero) has weight\n    :math:`1/(r+1) + 1/(s+1)`.\n\n    Specifying a rank array is meaningful only if you have in mind an\n    external criterion of importance. If, as it usually happens, you do\n    not have in mind a specific rank, the weighted :math:`\\tau` is\n    defined by averaging the values obtained using the decreasing\n    lexicographical rank by (`x`, `y`) and by (`y`, `x`). This is the\n    behavior with default parameters.\n\n    Note that if you are computing the weighted :math:`\\tau` on arrays of\n    ranks, rather than of scores (i.e., a larger value implies a lower\n    rank) you must negate the ranks, so that elements of higher rank are\n    associated with a larger value.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of scores, of the same shape. If arrays are not 1-D, they will\n        be flattened to 1-D.\n    rank : array_like of ints or bool, optional\n        A nonnegative rank assigned to each element. If it is None, the\n        decreasing lexicographical rank by (`x`, `y`) will be used: elements of\n        higher rank will be those with larger `x`-values, using `y`-values to\n        break ties (in particular, swapping `x` and `y` will give a different\n        result). If it is False, the element indices will be used\n        directly as ranks. The default is True, in which case this\n        function returns the average of the values obtained using the\n        decreasing lexicographical rank by (`x`, `y`) and by (`y`, `x`).\n    weigher : callable, optional\n        The weigher function. Must map nonnegative integers (zero\n        representing the most important element) to a nonnegative weight.\n        The default, None, provides hyperbolic weighing, that is,\n        rank :math:`r` is mapped to weight :math:`1/(r+1)`.\n    additive : bool, optional\n        If True, the weight of an exchange is computed by adding the\n        weights of the ranks of the exchanged elements; otherwise, the weights\n        are multiplied. The default is True.\n\n    Returns\n    -------\n    correlation : float\n       The weighted :math:`\\tau` correlation index.\n    pvalue : float\n       Presently ``np.nan``, as the null statistics is unknown (even in the\n       additive hyperbolic case).\n\n    See Also\n    --------\n    kendalltau : Calculates Kendall's tau.\n    spearmanr : Calculates a Spearman rank-order correlation coefficient.\n    theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n\n    Notes\n    -----\n    This function uses an :math:`O(n \\log n)`, mergesort-based algorithm\n    [1]_ that is a weighted extension of Knight's algorithm for Kendall's\n    :math:`\\tau` [2]_. It can compute Shieh's weighted :math:`\\tau` [3]_\n    between rankings without ties (i.e., permutations) by setting\n    `additive` and `rank` to False, as the definition given in [1]_ is a\n    generalization of Shieh's.\n\n    NaNs are considered the smallest possible score.\n\n    .. versionadded:: 0.19.0\n\n    References\n    ----------\n    .. [1] Sebastiano Vigna, \"A weighted correlation index for rankings with\n           ties\", Proceedings of the 24th international conference on World\n           Wide Web, pp. 1166-1176, ACM, 2015.\n    .. [2] W.R. Knight, \"A Computer Method for Calculating Kendall's Tau with\n           Ungrouped Data\", Journal of the American Statistical Association,\n           Vol. 61, No. 314, Part 1, pp. 436-439, 1966.\n    .. [3] Grace S. Shieh. \"A weighted Kendall's tau statistic\", Statistics &\n           Probability Letters, Vol. 39, No. 1, pp. 17-24, 1998.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> tau, p_value = stats.weightedtau(x, y)\n    >>> tau\n    -0.56694968153682723\n    >>> p_value\n    nan\n    >>> tau, p_value = stats.weightedtau(x, y, additive=False)\n    >>> tau\n    -0.62205716951801038\n\n    NaNs are considered the smallest possible score:\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, np.nan]\n    >>> tau, _ = stats.weightedtau(x, y)\n    >>> tau\n    -0.56694968153682723\n\n    This is exactly Kendall's tau:\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> tau, _ = stats.weightedtau(x, y, weigher=lambda x: 1)\n    >>> tau\n    -0.47140452079103173\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> stats.weightedtau(x, y, rank=None)\n    WeightedTauResult(correlation=-0.4157652301037516, pvalue=nan)\n    >>> stats.weightedtau(y, x, rank=None)\n    WeightedTauResult(correlation=-0.7181341329699028, pvalue=nan)\n\n    \"\"\"\n    x = np.asarray(x).ravel()\n    y = np.asarray(y).ravel()\n\n    if x.size != y.size:\n        raise ValueError(\"All inputs to `weightedtau` must be of the same size, \"\n                         \"found x-size %s and y-size %s\" % (x.size, y.size))\n    if not x.size:\n        return WeightedTauResult(np.nan, np.nan)  # Return NaN if arrays are empty\n\n    # If there are NaNs we apply _toint64()\n    if np.isnan(np.sum(x)):\n        x = _toint64(x)\n    if np.isnan(np.sum(y)):\n        y = _toint64(y)\n\n    # Reduce to ranks unsupported types\n    if x.dtype != y.dtype:\n        if x.dtype != np.int64:\n            x = _toint64(x)\n        if y.dtype != np.int64:\n            y = _toint64(y)\n    else:\n        if x.dtype not in (np.int32, np.int64, np.float32, np.float64):\n            x = _toint64(x)\n            y = _toint64(y)\n\n    if rank is True:\n        return WeightedTauResult((\n            _weightedrankedtau(x, y, None, weigher, additive) +\n            _weightedrankedtau(y, x, None, weigher, additive)\n            ) / 2, np.nan)\n\n    if rank is False:\n        rank = np.arange(x.size, dtype=np.intp)\n    elif rank is not None:\n        rank = np.asarray(rank).ravel()\n        if rank.size != x.size:\n            raise ValueError(\"All inputs to `weightedtau` must be of the same size, \"\n                         \"found x-size %s and rank-size %s\" % (x.size, rank.size))\n\n    return WeightedTauResult(_weightedrankedtau(x, y, rank, weigher, additive), np.nan)\n\n\n# FROM MGCPY: https://github.com/neurodata/mgcpy\n\nclass _ParallelP(object):\n    \"\"\"\n    Helper function to calculate parallel p-value.\n    \"\"\"\n    def __init__(self, x, y, random_states):\n        self.x = x\n        self.y = y\n        self.random_states = random_states\n\n    def __call__(self, index):\n        order = self.random_states[index].permutation(self.y.shape[0])\n        permy = self.y[order][:, order]\n\n        # calculate permuted stats, store in null distribution\n        perm_stat = _mgc_stat(self.x, permy)[0]\n\n        return perm_stat\n\n\ndef _perm_test(x, y, stat, reps=1000, workers=-1, random_state=None):\n    r\"\"\"\n    Helper function that calculates the p-value. See below for uses.\n\n    Parameters\n    ----------\n    x, y : ndarray\n        `x` and `y` have shapes `(n, p)` and `(n, q)`.\n    stat : float\n        The sample test statistic.\n    reps : int, optional\n        The number of replications used to estimate the null when using the\n        permutation test. The default is 1000 replications.\n    workers : int or map-like callable, optional\n        If `workers` is an int the population is subdivided into `workers`\n        sections and evaluated in parallel (uses\n        `multiprocessing.Pool <multiprocessing>`). Supply `-1` to use all cores\n        available to the Process. Alternatively supply a map-like callable,\n        such as `multiprocessing.Pool.map` for evaluating the population in\n        parallel. This evaluation is carried out as `workers(func, iterable)`.\n        Requires that `func` be pickleable.\n    random_state : int or np.random.RandomState instance, optional\n        If already a RandomState instance, use it.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If None, use np.random.RandomState. Default is None.\n\n    Returns\n    -------\n    pvalue : float\n        The sample test p-value.\n    null_dist : list\n        The approximated null distribution.\n    \"\"\"\n    # generate seeds for each rep (change to new parallel random number\n    # capabilities in numpy >= 1.17+)\n    random_state = check_random_state(random_state)\n    random_states = [np.random.RandomState(rng_integers(random_state, 1 << 32,\n                     size=4, dtype=np.uint32)) for _ in range(reps)]\n\n    # parallelizes with specified workers over number of reps and set seeds\n    parallelp = _ParallelP(x=x, y=y, random_states=random_states)\n    with MapWrapper(workers) as mapwrapper:\n        null_dist = np.array(list(mapwrapper(parallelp, range(reps))))\n\n    # calculate p-value and significant permutation map through list\n    pvalue = (null_dist >= stat).sum() / reps\n\n    # correct for a p-value of 0. This is because, with bootstrapping\n    # permutations, a p-value of 0 is incorrect\n    if pvalue == 0:\n        pvalue = 1 / reps\n\n    return pvalue, null_dist\n\n\ndef _euclidean_dist(x):\n    return cdist(x, x)\n\n\nMGCResult = namedtuple('MGCResult', ('stat', 'pvalue', 'mgc_dict'))\n\n\ndef multiscale_graphcorr(x, y, compute_distance=_euclidean_dist, reps=1000,\n                         workers=1, is_twosamp=False, random_state=None):\n    r\"\"\"\n    Computes the Multiscale Graph Correlation (MGC) test statistic.\n\n    Specifically, for each point, MGC finds the :math:`k`-nearest neighbors for\n    one property (e.g. cloud density), and the :math:`l`-nearest neighbors for\n    the other property (e.g. grass wetness) [1]_. This pair :math:`(k, l)` is\n    called the \"scale\". A priori, however, it is not know which scales will be\n    most informative. So, MGC computes all distance pairs, and then efficiently\n    computes the distance correlations for all scales. The local correlations\n    illustrate which scales are relatively informative about the relationship.\n    The key, therefore, to successfully discover and decipher relationships\n    between disparate data modalities is to adaptively determine which scales\n    are the most informative, and the geometric implication for the most\n    informative scales. Doing so not only provides an estimate of whether the\n    modalities are related, but also provides insight into how the\n    determination was made. This is especially important in high-dimensional\n    data, where simple visualizations do not reveal relationships to the\n    unaided human eye. Characterizations of this implementation in particular\n    have been derived from and benchmarked within in [2]_.\n\n    Parameters\n    ----------\n    x, y : ndarray\n        If ``x`` and ``y`` have shapes ``(n, p)`` and ``(n, q)`` where `n` is\n        the number of samples and `p` and `q` are the number of dimensions,\n        then the MGC independence test will be run.  Alternatively, ``x`` and\n        ``y`` can have shapes ``(n, n)`` if they are distance or similarity\n        matrices, and ``compute_distance`` must be sent to ``None``. If ``x``\n        and ``y`` have shapes ``(n, p)`` and ``(m, p)``, an unpaired\n        two-sample MGC test will be run.\n    compute_distance : callable, optional\n        A function that computes the distance or similarity among the samples\n        within each data matrix. Set to ``None`` if ``x`` and ``y`` are\n        already distance matrices. The default uses the euclidean norm metric.\n        If you are calling a custom function, either create the distance\n        matrix before-hand or create a function of the form\n        ``compute_distance(x)`` where `x` is the data matrix for which\n        pairwise distances are calculated.\n    reps : int, optional\n        The number of replications used to estimate the null when using the\n        permutation test. The default is ``1000``.\n    workers : int or map-like callable, optional\n        If ``workers`` is an int the population is subdivided into ``workers``\n        sections and evaluated in parallel (uses ``multiprocessing.Pool\n        <multiprocessing>``). Supply ``-1`` to use all cores available to the\n        Process. Alternatively supply a map-like callable, such as\n        ``multiprocessing.Pool.map`` for evaluating the p-value in parallel.\n        This evaluation is carried out as ``workers(func, iterable)``.\n        Requires that `func` be pickleable. The default is ``1``.\n    is_twosamp : bool, optional\n        If `True`, a two sample test will be run. If ``x`` and ``y`` have\n        shapes ``(n, p)`` and ``(m, p)``, this optional will be overriden and\n        set to ``True``. Set to ``True`` if ``x`` and ``y`` both have shapes\n        ``(n, p)`` and a two sample test is desired. The default is ``False``.\n        Note that this will not run if inputs are distance matrices.\n    random_state : int or np.random.RandomState instance, optional\n        If already a RandomState instance, use it.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If None, use np.random.RandomState. Default is None.\n\n    Returns\n    -------\n    stat : float\n        The sample MGC test statistic within `[-1, 1]`.\n    pvalue : float\n        The p-value obtained via permutation.\n    mgc_dict : dict\n        Contains additional useful additional returns containing the following\n        keys:\n\n            - mgc_map : ndarray\n                A 2D representation of the latent geometry of the relationship.\n                of the relationship.\n            - opt_scale : (int, int)\n                The estimated optimal scale as a `(x, y)` pair.\n            - null_dist : list\n                The null distribution derived from the permuted matrices\n\n    See Also\n    --------\n    pearsonr : Pearson correlation coefficient and p-value for testing\n               non-correlation.\n    kendalltau : Calculates Kendall's tau.\n    spearmanr : Calculates a Spearman rank-order correlation coefficient.\n\n    Notes\n    -----\n    A description of the process of MGC and applications on neuroscience data\n    can be found in [1]_. It is performed using the following steps:\n\n    #. Two distance matrices :math:`D^X` and :math:`D^Y` are computed and\n       modified to be mean zero columnwise. This results in two\n       :math:`n \\times n` distance matrices :math:`A` and :math:`B` (the\n       centering and unbiased modification) [3]_.\n\n    #. For all values :math:`k` and :math:`l` from :math:`1, ..., n`,\n\n       * The :math:`k`-nearest neighbor and :math:`l`-nearest neighbor graphs\n         are calculated for each property. Here, :math:`G_k (i, j)` indicates\n         the :math:`k`-smallest values of the :math:`i`-th row of :math:`A`\n         and :math:`H_l (i, j)` indicates the :math:`l` smallested values of\n         the :math:`i`-th row of :math:`B`\n\n       * Let :math:`\\circ` denotes the entry-wise matrix product, then local\n         correlations are summed and normalized using the following statistic:\n\n    .. math::\n\n        c^{kl} = \\frac{\\sum_{ij} A G_k B H_l}\n                      {\\sqrt{\\sum_{ij} A^2 G_k \\times \\sum_{ij} B^2 H_l}}\n\n    #. The MGC test statistic is the smoothed optimal local correlation of\n       :math:`\\{ c^{kl} \\}`. Denote the smoothing operation as :math:`R(\\cdot)`\n       (which essentially set all isolated large correlations) as 0 and\n       connected large correlations the same as before, see [3]_.) MGC is,\n\n    .. math::\n\n        MGC_n (x, y) = \\max_{(k, l)} R \\left(c^{kl} \\left( x_n, y_n \\right)\n                                                    \\right)\n\n    The test statistic returns a value between :math:`(-1, 1)` since it is\n    normalized.\n\n    The p-value returned is calculated using a permutation test. This process\n    is completed by first randomly permuting :math:`y` to estimate the null\n    distribution and then calculating the probability of observing a test\n    statistic, under the null, at least as extreme as the observed test\n    statistic.\n\n    MGC requires at least 5 samples to run with reliable results. It can also\n    handle high-dimensional data sets.\n    In addition, by manipulating the input data matrices, the two-sample\n    testing problem can be reduced to the independence testing problem [4]_.\n    Given sample data :math:`U` and :math:`V` of sizes :math:`p \\times n`\n    :math:`p \\times m`, data matrix :math:`X` and :math:`Y` can be created as\n    follows:\n\n    .. math::\n\n        X = [U | V] \\in \\mathcal{R}^{p \\times (n + m)}\n        Y = [0_{1 \\times n} | 1_{1 \\times m}] \\in \\mathcal{R}^{(n + m)}\n\n    Then, the MGC statistic can be calculated as normal. This methodology can\n    be extended to similar tests such as distance correlation [4]_.\n\n    .. versionadded:: 1.4.0\n\n    References\n    ----------\n    .. [1] Vogelstein, J. T., Bridgeford, E. W., Wang, Q., Priebe, C. E.,\n           Maggioni, M., & Shen, C. (2019). Discovering and deciphering\n           relationships across disparate data modalities. ELife.\n    .. [2] Panda, S., Palaniappan, S., Xiong, J., Swaminathan, A.,\n           Ramachandran, S., Bridgeford, E. W., ... Vogelstein, J. T. (2019).\n           mgcpy: A Comprehensive High Dimensional Independence Testing Python\n           Package. :arXiv:`1907.02088`\n    .. [3] Shen, C., Priebe, C.E., & Vogelstein, J. T. (2019). From distance\n           correlation to multiscale graph correlation. Journal of the American\n           Statistical Association.\n    .. [4] Shen, C. & Vogelstein, J. T. (2018). The Exact Equivalence of\n           Distance and Kernel Methods for Hypothesis Testing.\n           :arXiv:`1806.05514`\n\n    Examples\n    --------\n    >>> from scipy.stats import multiscale_graphcorr\n    >>> x = np.arange(100)\n    >>> y = x\n    >>> stat, pvalue, _ = multiscale_graphcorr(x, y, workers=-1)\n    >>> '%.1f, %.3f' % (stat, pvalue)\n    '1.0, 0.001'\n\n    Alternatively,\n\n    >>> x = np.arange(100)\n    >>> y = x\n    >>> mgc = multiscale_graphcorr(x, y)\n    >>> '%.1f, %.3f' % (mgc.stat, mgc.pvalue)\n    '1.0, 0.001'\n\n    To run an unpaired two-sample test,\n\n    >>> x = np.arange(100)\n    >>> y = np.arange(79)\n    >>> mgc = multiscale_graphcorr(x, y, random_state=1)\n    >>> '%.3f, %.2f' % (mgc.stat, mgc.pvalue)\n    '0.033, 0.02'\n\n    or, if shape of the inputs are the same,\n\n    >>> x = np.arange(100)\n    >>> y = x\n    >>> mgc = multiscale_graphcorr(x, y, is_twosamp=True)\n    >>> '%.3f, %.1f' % (mgc.stat, mgc.pvalue)\n    '-0.008, 1.0'\n    \"\"\"\n    if not isinstance(x, np.ndarray) or not isinstance(y, np.ndarray):\n        raise ValueError(\"x and y must be ndarrays\")\n\n    # convert arrays of type (n,) to (n, 1)\n    if x.ndim == 1:\n        x = x[:, np.newaxis]\n    elif x.ndim != 2:\n        raise ValueError(\"Expected a 2-D array `x`, found shape \"\n                         \"{}\".format(x.shape))\n    if y.ndim == 1:\n        y = y[:, np.newaxis]\n    elif y.ndim != 2:\n        raise ValueError(\"Expected a 2-D array `y`, found shape \"\n                         \"{}\".format(y.shape))\n\n    nx, px = x.shape\n    ny, py = y.shape\n\n    # check for NaNs\n    _contains_nan(x, nan_policy='raise')\n    _contains_nan(y, nan_policy='raise')\n\n    # check for positive or negative infinity and raise error\n    if np.sum(np.isinf(x)) > 0 or np.sum(np.isinf(y)) > 0:\n        raise ValueError(\"Inputs contain infinities\")\n\n    if nx != ny:\n        if px == py:\n            # reshape x and y for two sample testing\n            is_twosamp = True\n        else:\n            raise ValueError(\"Shape mismatch, x and y must have shape [n, p] \"\n                             \"and [n, q] or have shape [n, p] and [m, p].\")\n\n    if nx < 5 or ny < 5:\n        raise ValueError(\"MGC requires at least 5 samples to give reasonable \"\n                         \"results.\")\n\n    # convert x and y to float\n    x = x.astype(np.float64)\n    y = y.astype(np.float64)\n\n    # check if compute_distance_matrix if a callable()\n    if not callable(compute_distance) and compute_distance is not None:\n        raise ValueError(\"Compute_distance must be a function.\")\n\n    # check if number of reps exists, integer, or > 0 (if under 1000 raises\n    # warning)\n    if not isinstance(reps, int) or reps < 0:\n        raise ValueError(\"Number of reps must be an integer greater than 0.\")\n    elif reps < 1000:\n        msg = (\"The number of replications is low (under 1000), and p-value \"\n               \"calculations may be unreliable. Use the p-value result, with \"\n               \"caution!\")\n        warnings.warn(msg, RuntimeWarning)\n\n    if is_twosamp:\n        if compute_distance is None:\n            raise ValueError(\"Cannot run if inputs are distance matrices\")\n        x, y = _two_sample_transform(x, y)\n\n    if compute_distance is not None:\n        # compute distance matrices for x and y\n        x = compute_distance(x)\n        y = compute_distance(y)\n\n    # calculate MGC stat\n    stat, stat_dict = _mgc_stat(x, y)\n    stat_mgc_map = stat_dict[\"stat_mgc_map\"]\n    opt_scale = stat_dict[\"opt_scale\"]\n\n    # calculate permutation MGC p-value\n    pvalue, null_dist = _perm_test(x, y, stat, reps=reps, workers=workers,\n                                   random_state=random_state)\n\n    # save all stats (other than stat/p-value) in dictionary\n    mgc_dict = {\"mgc_map\": stat_mgc_map,\n                \"opt_scale\": opt_scale,\n                \"null_dist\": null_dist}\n\n    return MGCResult(stat, pvalue, mgc_dict)\n\n\ndef _mgc_stat(distx, disty):\n    r\"\"\"\n    Helper function that calculates the MGC stat. See above for use.\n\n    Parameters\n    ----------\n    x, y : ndarray\n        `x` and `y` have shapes `(n, p)` and `(n, q)` or `(n, n)` and `(n, n)`\n        if distance matrices.\n\n    Returns\n    -------\n    stat : float\n        The sample MGC test statistic within `[-1, 1]`.\n    stat_dict : dict\n        Contains additional useful additional returns containing the following\n        keys:\n\n            - stat_mgc_map : ndarray\n                MGC-map of the statistics.\n            - opt_scale : (float, float)\n                The estimated optimal scale as a `(x, y)` pair.\n    \"\"\"\n    # calculate MGC map and optimal scale\n    stat_mgc_map = _local_correlations(distx, disty, global_corr='mgc')\n\n    n, m = stat_mgc_map.shape\n    if m == 1 or n == 1:\n        # the global scale at is the statistic calculated at maximial nearest\n        # neighbors. There is not enough local scale to search over, so\n        # default to global scale\n        stat = stat_mgc_map[m - 1][n - 1]\n        opt_scale = m * n\n    else:\n        samp_size = len(distx) - 1\n\n        # threshold to find connected region of significant local correlations\n        sig_connect = _threshold_mgc_map(stat_mgc_map, samp_size)\n\n        # maximum within the significant region\n        stat, opt_scale = _smooth_mgc_map(sig_connect, stat_mgc_map)\n\n    stat_dict = {\"stat_mgc_map\": stat_mgc_map,\n                 \"opt_scale\": opt_scale}\n\n    return stat, stat_dict\n\n\ndef _threshold_mgc_map(stat_mgc_map, samp_size):\n    r\"\"\"\n    Finds a connected region of significance in the MGC-map by thresholding.\n\n    Parameters\n    ----------\n    stat_mgc_map : ndarray\n        All local correlations within `[-1,1]`.\n    samp_size : int\n        The sample size of original data.\n\n    Returns\n    -------\n    sig_connect : ndarray\n        A binary matrix with 1's indicating the significant region.\n    \"\"\"\n    m, n = stat_mgc_map.shape\n\n    # 0.02 is simply an empirical threshold, this can be set to 0.01 or 0.05\n    # with varying levels of performance. Threshold is based on a beta\n    # approximation.\n    per_sig = 1 - (0.02 / samp_size)  # Percentile to consider as significant\n    threshold = samp_size * (samp_size - 3)/4 - 1/2  # Beta approximation\n    threshold = distributions.beta.ppf(per_sig, threshold, threshold) * 2 - 1\n\n    # the global scale at is the statistic calculated at maximial nearest\n    # neighbors. Threshold is the maximium on the global and local scales\n    threshold = max(threshold, stat_mgc_map[m - 1][n - 1])\n\n    # find the largest connected component of significant correlations\n    sig_connect = stat_mgc_map > threshold\n    if np.sum(sig_connect) > 0:\n        sig_connect, _ = measurements.label(sig_connect)\n        _, label_counts = np.unique(sig_connect, return_counts=True)\n\n        # skip the first element in label_counts, as it is count(zeros)\n        max_label = np.argmax(label_counts[1:]) + 1\n        sig_connect = sig_connect == max_label\n    else:\n        sig_connect = np.array([[False]])\n\n    return sig_connect\n\n\ndef _smooth_mgc_map(sig_connect, stat_mgc_map):\n    \"\"\"\n    Finds the smoothed maximal within the significant region R.\n    If area of R is too small it returns the last local correlation. Otherwise,\n    returns the maximum within significant_connected_region.\n\n    Parameters\n    ----------\n    sig_connect: ndarray\n        A binary matrix with 1's indicating the significant region.\n    stat_mgc_map: ndarray\n        All local correlations within `[-1, 1]`.\n\n    Returns\n    -------\n    stat : float\n        The sample MGC statistic within `[-1, 1]`.\n    opt_scale: (float, float)\n        The estimated optimal scale as an `(x, y)` pair.\n    \"\"\"\n\n    m, n = stat_mgc_map.shape\n\n    # the global scale at is the statistic calculated at maximial nearest\n    # neighbors. By default, statistic and optimal scale are global.\n    stat = stat_mgc_map[m - 1][n - 1]\n    opt_scale = [m, n]\n\n    if np.linalg.norm(sig_connect) != 0:\n        # proceed only when the connected region's area is sufficiently large\n        # 0.02 is simply an empirical threshold, this can be set to 0.01 or 0.05\n        # with varying levels of performance\n        if np.sum(sig_connect) >= np.ceil(0.02 * max(m, n)) * min(m, n):\n            max_corr = max(stat_mgc_map[sig_connect])\n\n            # find all scales within significant_connected_region that maximize\n            # the local correlation\n            max_corr_index = np.where((stat_mgc_map >= max_corr) & sig_connect)\n\n            if max_corr >= stat:\n                stat = max_corr\n\n                k, l = max_corr_index\n                one_d_indices = k * n + l  # 2D to 1D indexing\n                k = np.max(one_d_indices) // n\n                l = np.max(one_d_indices) % n\n                opt_scale = [k+1, l+1]  # adding 1s to match R indexing\n\n    return stat, opt_scale\n\n\ndef _two_sample_transform(u, v):\n    \"\"\"\n    Helper function that concatenates x and y for two sample MGC stat. See\n    above for use.\n\n    Parameters\n    ----------\n    u, v : ndarray\n        `u` and `v` have shapes `(n, p)` and `(m, p)`.\n\n    Returns\n    -------\n    x : ndarray\n        Concatenate `u` and `v` along the `axis = 0`. `x` thus has shape\n        `(2n, p)`.\n    y : ndarray\n        Label matrix for `x` where 0 refers to samples that comes from `u` and\n        1 refers to samples that come from `v`. `y` thus has shape `(2n, 1)`.\n    \"\"\"\n    nx = u.shape[0]\n    ny = v.shape[0]\n    x = np.concatenate([u, v], axis=0)\n    y = np.concatenate([np.zeros(nx), np.ones(ny)], axis=0).reshape(-1, 1)\n    return x, y\n\n\n#####################################\n#       INFERENTIAL STATISTICS      #\n#####################################\n\nTtest_1sampResult = namedtuple('Ttest_1sampResult', ('statistic', 'pvalue'))\n\n\ndef ttest_1samp(a, popmean, axis=0, nan_policy='propagate',\n                alternative=\"two-sided\"):\n    \"\"\"\n    Calculate the T-test for the mean of ONE group of scores.\n\n    This is a two-sided test for the null hypothesis that the expected value\n    (mean) of a sample of independent observations `a` is equal to the given\n    population mean, `popmean`.\n\n    Parameters\n    ----------\n    a : array_like\n        Sample observation.\n    popmean : float or array_like\n        Expected value in null hypothesis. If array_like, then it must have the\n        same shape as `a` excluding the axis dimension.\n    axis : int or None, optional\n        Axis along which to compute test; default is 0. If None, compute over\n        the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided\n          * 'greater': one-sided\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    statistic : float or array\n        t-statistic.\n    pvalue : float or array\n        Two-sided p-value.\n\n    Examples\n    --------\n    >>> from scipy import stats\n\n    >>> np.random.seed(7654567)  # fix seed to get the same result\n    >>> rvs = stats.norm.rvs(loc=5, scale=10, size=(50,2))\n\n    Test if mean of random sample is equal to true mean, and different mean.\n    We reject the null hypothesis in the second case and don't reject it in\n    the first case.\n\n    >>> stats.ttest_1samp(rvs,5.0)\n    (array([-0.68014479, -0.04323899]), array([ 0.49961383,  0.96568674]))\n    >>> stats.ttest_1samp(rvs,0.0)\n    (array([ 2.77025808,  4.11038784]), array([ 0.00789095,  0.00014999]))\n\n    Examples using axis and non-scalar dimension for population mean.\n\n    >>> result = stats.ttest_1samp(rvs, [5.0, 0.0])\n    >>> result.statistic\n    array([-0.68014479,  4.11038784]),\n    >>> result.pvalue\n    array([4.99613833e-01, 1.49986458e-04])\n\n    >>> result = stats.ttest_1samp(rvs.T, [5.0, 0.0], axis=1)\n    >>> result.statistic\n    array([-0.68014479,  4.11038784])\n    >>> result.pvalue\n    array([4.99613833e-01, 1.49986458e-04])\n\n    >>> result = stats.ttest_1samp(rvs, [[5.0], [0.0]])\n    >>> result.statistic\n    array([[-0.68014479, -0.04323899],\n           [ 2.77025808,  4.11038784]])\n    >>> result.pvalue\n    array([[4.99613833e-01, 9.65686743e-01],\n           [7.89094663e-03, 1.49986458e-04]])\n\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n\n    contains_nan, nan_policy = _contains_nan(a, nan_policy)\n\n    if contains_nan and nan_policy == 'omit':\n        if alternative != 'two-sided':\n            raise ValueError(\"nan-containing/masked inputs with \"\n                             \"nan_policy='omit' are currently not \"\n                             \"supported by one-sided alternatives.\")\n        a = ma.masked_invalid(a)\n        return mstats_basic.ttest_1samp(a, popmean, axis)\n\n    n = a.shape[axis]\n    df = n - 1\n\n    d = np.mean(a, axis) - popmean\n    v = np.var(a, axis, ddof=1)\n    denom = np.sqrt(v / n)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        t = np.divide(d, denom)\n    t, prob = _ttest_finish(df, t, alternative)\n\n    return Ttest_1sampResult(t, prob)\n\n\ndef _ttest_finish(df, t, alternative):\n    \"\"\"Common code between all 3 t-test functions.\"\"\"\n    if alternative == 'less':\n        prob = distributions.t.cdf(t, df)\n    elif alternative == 'greater':\n        prob = distributions.t.sf(t, df)\n    elif alternative == 'two-sided':\n        prob = 2 * distributions.t.sf(np.abs(t), df)\n    else:\n        raise ValueError(\"alternative must be \"\n                         \"'less', 'greater' or 'two-sided'\")\n\n    if t.ndim == 0:\n        t = t[()]\n\n    return t, prob\n\n\ndef _ttest_ind_from_stats(mean1, mean2, denom, df, alternative):\n\n    d = mean1 - mean2\n    with np.errstate(divide='ignore', invalid='ignore'):\n        t = np.divide(d, denom)\n    t, prob = _ttest_finish(df, t, alternative)\n\n    return (t, prob)\n\n\ndef _unequal_var_ttest_denom(v1, n1, v2, n2):\n    vn1 = v1 / n1\n    vn2 = v2 / n2\n    with np.errstate(divide='ignore', invalid='ignore'):\n        df = (vn1 + vn2)**2 / (vn1**2 / (n1 - 1) + vn2**2 / (n2 - 1))\n\n    # If df is undefined, variances are zero (assumes n1 > 0 & n2 > 0).\n    # Hence it doesn't matter what df is as long as it's not NaN.\n    df = np.where(np.isnan(df), 1, df)\n    denom = np.sqrt(vn1 + vn2)\n    return df, denom\n\n\ndef _equal_var_ttest_denom(v1, n1, v2, n2):\n    df = n1 + n2 - 2.0\n    svar = ((n1 - 1) * v1 + (n2 - 1) * v2) / df\n    denom = np.sqrt(svar * (1.0 / n1 + 1.0 / n2))\n    return df, denom\n\n\nTtest_indResult = namedtuple('Ttest_indResult', ('statistic', 'pvalue'))\n\n\ndef ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2,\n                         equal_var=True, alternative=\"two-sided\"):\n    r\"\"\"\n    T-test for means of two independent samples from descriptive statistics.\n\n    This is a two-sided test for the null hypothesis that two independent\n    samples have identical average (expected) values.\n\n    Parameters\n    ----------\n    mean1 : array_like\n        The mean(s) of sample 1.\n    std1 : array_like\n        The standard deviation(s) of sample 1.\n    nobs1 : array_like\n        The number(s) of observations of sample 1.\n    mean2 : array_like\n        The mean(s) of sample 2.\n    std2 : array_like\n        The standard deviations(s) of sample 2.\n    nobs2 : array_like\n        The number(s) of observations of sample 2.\n    equal_var : bool, optional\n        If True (default), perform a standard independent 2 sample test\n        that assumes equal population variances [1]_.\n        If False, perform Welch's t-test, which does not assume equal\n        population variance [2]_.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided\n          * 'greater': one-sided\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    statistic : float or array\n        The calculated t-statistics.\n    pvalue : float or array\n        The two-tailed p-value.\n\n    See Also\n    --------\n    scipy.stats.ttest_ind\n\n    Notes\n    -----\n    .. versionadded:: 0.16.0\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n\n    .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n\n    Examples\n    --------\n    Suppose we have the summary data for two samples, as follows::\n\n                         Sample   Sample\n                   Size   Mean   Variance\n        Sample 1    13    15.0     87.5\n        Sample 2    11    12.0     39.0\n\n    Apply the t-test to this data (with the assumption that the population\n    variances are equal):\n\n    >>> from scipy.stats import ttest_ind_from_stats\n    >>> ttest_ind_from_stats(mean1=15.0, std1=np.sqrt(87.5), nobs1=13,\n    ...                      mean2=12.0, std2=np.sqrt(39.0), nobs2=11)\n    Ttest_indResult(statistic=0.9051358093310269, pvalue=0.3751996797581487)\n\n    For comparison, here is the data from which those summary statistics\n    were taken.  With this data, we can compute the same result using\n    `scipy.stats.ttest_ind`:\n\n    >>> a = np.array([1, 3, 4, 6, 11, 13, 15, 19, 22, 24, 25, 26, 26])\n    >>> b = np.array([2, 4, 6, 9, 11, 13, 14, 15, 18, 19, 21])\n    >>> from scipy.stats import ttest_ind\n    >>> ttest_ind(a, b)\n    Ttest_indResult(statistic=0.905135809331027, pvalue=0.3751996797581486)\n\n    Suppose we instead have binary data and would like to apply a t-test to\n    compare the proportion of 1s in two independent groups::\n\n                          Number of    Sample     Sample\n                    Size    ones        Mean     Variance\n        Sample 1    150      30         0.2        0.16\n        Sample 2    200      45         0.225      0.174375\n\n    The sample mean :math:`\\hat{p}` is the proportion of ones in the sample\n    and the variance for a binary observation is estimated by\n    :math:`\\hat{p}(1-\\hat{p})`.\n\n    >>> ttest_ind_from_stats(mean1=0.2, std1=np.sqrt(0.16), nobs1=150,\n    ...                      mean2=0.225, std2=np.sqrt(0.17437), nobs2=200)\n    Ttest_indResult(statistic=-0.564327545549774, pvalue=0.5728947691244874)\n\n    For comparison, we could compute the t statistic and p-value using\n    arrays of 0s and 1s and `scipy.stat.ttest_ind`, as above.\n\n    >>> group1 = np.array([1]*30 + [0]*(150-30))\n    >>> group2 = np.array([1]*45 + [0]*(200-45))\n    >>> ttest_ind(group1, group2)\n    Ttest_indResult(statistic=-0.5627179589855622, pvalue=0.573989277115258)\n\n    \"\"\"\n    if equal_var:\n        df, denom = _equal_var_ttest_denom(std1**2, nobs1, std2**2, nobs2)\n    else:\n        df, denom = _unequal_var_ttest_denom(std1**2, nobs1,\n                                             std2**2, nobs2)\n\n    res = _ttest_ind_from_stats(mean1, mean2, denom, df, alternative)\n    return Ttest_indResult(*res)\n\n\ndef _ttest_nans(a, b, axis, namedtuple_type):\n    \"\"\"\n    Generate an array of `nan`, with shape determined by `a`, `b` and `axis`.\n\n    This function is used by ttest_ind and ttest_rel to create the return\n    value when one of the inputs has size 0.\n\n    The shapes of the arrays are determined by dropping `axis` from the\n    shapes of `a` and `b` and broadcasting what is left.\n\n    The return value is a named tuple of the type given in `namedtuple_type`.\n\n    Examples\n    --------\n    >>> a = np.zeros((9, 2))\n    >>> b = np.zeros((5, 1))\n    >>> _ttest_nans(a, b, 0, Ttest_indResult)\n    Ttest_indResult(statistic=array([nan, nan]), pvalue=array([nan, nan]))\n\n    >>> a = np.zeros((3, 0, 9))\n    >>> b = np.zeros((1, 10))\n    >>> stat, p = _ttest_nans(a, b, -1, Ttest_indResult)\n    >>> stat\n    array([], shape=(3, 0), dtype=float64)\n    >>> p\n    array([], shape=(3, 0), dtype=float64)\n\n    >>> a = np.zeros(10)\n    >>> b = np.zeros(7)\n    >>> _ttest_nans(a, b, 0, Ttest_indResult)\n    Ttest_indResult(statistic=nan, pvalue=nan)\n    \"\"\"\n    shp = _broadcast_shapes_with_dropped_axis(a, b, axis)\n    if len(shp) == 0:\n        t = np.nan\n        p = np.nan\n    else:\n        t = np.full(shp, fill_value=np.nan)\n        p = t.copy()\n    return namedtuple_type(t, p)\n\n\ndef ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate',\n              alternative=\"two-sided\"):\n    \"\"\"\n    Calculate the T-test for the means of *two independent* samples of scores.\n\n    This is a two-sided test for the null hypothesis that 2 independent samples\n    have identical average (expected) values. This test assumes that the\n    populations have identical variances by default.\n\n    Parameters\n    ----------\n    a, b : array_like\n        The arrays must have the same shape, except in the dimension\n        corresponding to `axis` (the first, by default).\n    axis : int or None, optional\n        Axis along which to compute test. If None, compute over the whole\n        arrays, `a`, and `b`.\n    equal_var : bool, optional\n        If True (default), perform a standard independent 2 sample test\n        that assumes equal population variances [1]_.\n        If False, perform Welch's t-test, which does not assume equal\n        population variance [2]_.\n\n        .. versionadded:: 0.11.0\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided\n          * 'greater': one-sided\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    statistic : float or array\n        The calculated t-statistic.\n    pvalue : float or array\n        The two-tailed p-value.\n\n    Notes\n    -----\n    We can use this test, if we observe two independent samples from\n    the same or different population, e.g. exam scores of boys and\n    girls or of two ethnic groups. The test measures whether the\n    average (expected) value differs significantly across samples. If\n    we observe a large p-value, for example larger than 0.05 or 0.1,\n    then we cannot reject the null hypothesis of identical average scores.\n    If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%,\n    then we reject the null hypothesis of equal averages.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n\n    .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> np.random.seed(12345678)\n\n    Test with sample with identical means:\n\n    >>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\n    >>> rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)\n    >>> stats.ttest_ind(rvs1,rvs2)\n    (0.26833823296239279, 0.78849443369564776)\n    >>> stats.ttest_ind(rvs1,rvs2, equal_var = False)\n    (0.26833823296239279, 0.78849452749500748)\n\n    `ttest_ind` underestimates p for unequal variances:\n\n    >>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)\n    >>> stats.ttest_ind(rvs1, rvs3)\n    (-0.46580283298287162, 0.64145827413436174)\n    >>> stats.ttest_ind(rvs1, rvs3, equal_var = False)\n    (-0.46580283298287162, 0.64149646246569292)\n\n    When n1 != n2, the equal variance t-statistic is no longer equal to the\n    unequal variance t-statistic:\n\n    >>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)\n    >>> stats.ttest_ind(rvs1, rvs4)\n    (-0.99882539442782481, 0.3182832709103896)\n    >>> stats.ttest_ind(rvs1, rvs4, equal_var = False)\n    (-0.69712570584654099, 0.48716927725402048)\n\n    T-test with different means, variance, and n:\n\n    >>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)\n    >>> stats.ttest_ind(rvs1, rvs5)\n    (-1.4679669854490653, 0.14263895620529152)\n    >>> stats.ttest_ind(rvs1, rvs5, equal_var = False)\n    (-0.94365973617132992, 0.34744170334794122)\n\n    \"\"\"\n    a, b, axis = _chk2_asarray(a, b, axis)\n\n    # check both a and b\n    cna, npa = _contains_nan(a, nan_policy)\n    cnb, npb = _contains_nan(b, nan_policy)\n    contains_nan = cna or cnb\n    if npa == 'omit' or npb == 'omit':\n        nan_policy = 'omit'\n\n    if contains_nan and nan_policy == 'omit':\n        if alternative != 'two-sided':\n            raise ValueError(\"nan-containing/masked inputs with \"\n                             \"nan_policy='omit' are currently not \"\n                             \"supported by one-sided alternatives.\")\n        a = ma.masked_invalid(a)\n        b = ma.masked_invalid(b)\n        return mstats_basic.ttest_ind(a, b, axis, equal_var)\n\n    if a.size == 0 or b.size == 0:\n        return _ttest_nans(a, b, axis, Ttest_indResult)\n\n    v1 = np.var(a, axis, ddof=1)\n    v2 = np.var(b, axis, ddof=1)\n    n1 = a.shape[axis]\n    n2 = b.shape[axis]\n\n    if equal_var:\n        df, denom = _equal_var_ttest_denom(v1, n1, v2, n2)\n    else:\n        df, denom = _unequal_var_ttest_denom(v1, n1, v2, n2)\n\n    res = _ttest_ind_from_stats(np.mean(a, axis), np.mean(b, axis), denom, df,\n                                alternative)\n\n    return Ttest_indResult(*res)\n\n\ndef _get_len(a, axis, msg):\n    try:\n        n = a.shape[axis]\n    except IndexError:\n        raise np.AxisError(axis, a.ndim, msg) from None\n    return n\n\n\nTtest_relResult = namedtuple('Ttest_relResult', ('statistic', 'pvalue'))\n\n\ndef ttest_rel(a, b, axis=0, nan_policy='propagate', alternative=\"two-sided\"):\n    \"\"\"\n    Calculate the t-test on TWO RELATED samples of scores, a and b.\n\n    This is a two-sided test for the null hypothesis that 2 related or\n    repeated samples have identical average (expected) values.\n\n    Parameters\n    ----------\n    a, b : array_like\n        The arrays must have the same shape.\n    axis : int or None, optional\n        Axis along which to compute test. If None, compute over the whole\n        arrays, `a`, and `b`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided\n          * 'greater': one-sided\n\n          .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    statistic : float or array\n        t-statistic.\n    pvalue : float or array\n        Two-sided p-value.\n\n    Notes\n    -----\n    Examples for use are scores of the same set of student in\n    different exams, or repeated sampling from the same units. The\n    test measures whether the average score differs significantly\n    across samples (e.g. exams). If we observe a large p-value, for\n    example greater than 0.05 or 0.1 then we cannot reject the null\n    hypothesis of identical average scores. If the p-value is smaller\n    than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n    hypothesis of equal averages. Small p-values are associated with\n    large t-statistics.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> np.random.seed(12345678) # fix random seed to get same numbers\n\n    >>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\n    >>> rvs2 = (stats.norm.rvs(loc=5,scale=10,size=500) +\n    ...         stats.norm.rvs(scale=0.2,size=500))\n    >>> stats.ttest_rel(rvs1,rvs2)\n    (0.24101764965300962, 0.80964043445811562)\n    >>> rvs3 = (stats.norm.rvs(loc=8,scale=10,size=500) +\n    ...         stats.norm.rvs(scale=0.2,size=500))\n    >>> stats.ttest_rel(rvs1,rvs3)\n    (-3.9995108708727933, 7.3082402191726459e-005)\n\n    \"\"\"\n    a, b, axis = _chk2_asarray(a, b, axis)\n\n    cna, npa = _contains_nan(a, nan_policy)\n    cnb, npb = _contains_nan(b, nan_policy)\n    contains_nan = cna or cnb\n    if npa == 'omit' or npb == 'omit':\n        nan_policy = 'omit'\n\n    if contains_nan and nan_policy == 'omit':\n        if alternative != 'two-sided':\n            raise ValueError(\"nan-containing/masked inputs with \"\n                             \"nan_policy='omit' are currently not \"\n                             \"supported by one-sided alternatives.\")\n        a = ma.masked_invalid(a)\n        b = ma.masked_invalid(b)\n        m = ma.mask_or(ma.getmask(a), ma.getmask(b))\n        aa = ma.array(a, mask=m, copy=True)\n        bb = ma.array(b, mask=m, copy=True)\n        return mstats_basic.ttest_rel(aa, bb, axis)\n\n    na = _get_len(a, axis, \"first argument\")\n    nb = _get_len(b, axis, \"second argument\")\n    if na != nb:\n        raise ValueError('unequal length arrays')\n\n    if na == 0:\n        return _ttest_nans(a, b, axis, Ttest_relResult)\n\n    n = a.shape[axis]\n    df = n - 1\n\n    d = (a - b).astype(np.float64)\n    v = np.var(d, axis, ddof=1)\n    dm = np.mean(d, axis)\n    denom = np.sqrt(v / n)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        t = np.divide(dm, denom)\n    t, prob = _ttest_finish(df, t, alternative)\n\n    return Ttest_relResult(t, prob)\n\n\n# Map from names to lambda_ values used in power_divergence().\n_power_div_lambda_names = {\n    \"pearson\": 1,\n    \"log-likelihood\": 0,\n    \"freeman-tukey\": -0.5,\n    \"mod-log-likelihood\": -1,\n    \"neyman\": -2,\n    \"cressie-read\": 2/3,\n}\n\n\ndef _count(a, axis=None):\n    \"\"\"\n    Count the number of non-masked elements of an array.\n\n    This function behaves like np.ma.count(), but is much faster\n    for ndarrays.\n    \"\"\"\n    if hasattr(a, 'count'):\n        num = a.count(axis=axis)\n        if isinstance(num, np.ndarray) and num.ndim == 0:\n            # In some cases, the `count` method returns a scalar array (e.g.\n            # np.array(3)), but we want a plain integer.\n            num = int(num)\n    else:\n        if axis is None:\n            num = a.size\n        else:\n            num = a.shape[axis]\n    return num\n\n\nPower_divergenceResult = namedtuple('Power_divergenceResult',\n                                    ('statistic', 'pvalue'))\n\n\ndef power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None):\n    \"\"\"\n    Cressie-Read power divergence statistic and goodness of fit test.\n\n    This function tests the null hypothesis that the categorical data\n    has the given frequencies, using the Cressie-Read power divergence\n    statistic.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    lambda_ : float or str, optional\n        The power in the Cressie-Read power divergence statistic.  The default\n        is 1.  For convenience, `lambda_` may be assigned one of the following\n        strings, in which case the corresponding numerical value is used::\n\n            String              Value   Description\n            \"pearson\"             1     Pearson's chi-squared statistic.\n                                        In this case, the function is\n                                        equivalent to `stats.chisquare`.\n            \"log-likelihood\"      0     Log-likelihood ratio. Also known as\n                                        the G-test [3]_.\n            \"freeman-tukey\"      -1/2   Freeman-Tukey statistic.\n            \"mod-log-likelihood\" -1     Modified log-likelihood ratio.\n            \"neyman\"             -2     Neyman's statistic.\n            \"cressie-read\"        2/3   The power recommended in [5]_.\n\n    Returns\n    -------\n    statistic : float or ndarray\n        The Cressie-Read power divergence test statistic.  The value is\n        a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n    pvalue : float or ndarray\n        The p-value of the test.  The value is a float if `ddof` and the\n        return value `stat` are scalars.\n\n    See Also\n    --------\n    chisquare\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5.\n\n    When `lambda_` is less than zero, the formula for the statistic involves\n    dividing by `f_obs`, so a warning or error may be generated if any value\n    in `f_obs` is 0.\n\n    Similarly, a warning or error may be generated if any value in `f_exp` is\n    zero when `lambda_` >= 0.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not a chisquare, in which case this\n    test is not appropriate.\n\n    This function handles masked arrays.  If an element of `f_obs` or `f_exp`\n    is masked, then data at that position is ignored, and does not count\n    towards the size of the data set.\n\n    .. versionadded:: 0.13.0\n\n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n    .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n    .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n           practice of statistics in biological research\", New York: Freeman\n           (1981)\n    .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n           pp. 440-464.\n\n    Examples\n    --------\n    (See `chisquare` for more examples.)\n\n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.  Here we\n    perform a G-test (i.e. use the log-likelihood ratio statistic):\n\n    >>> from scipy.stats import power_divergence\n    >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n    (2.006573162632538, 0.84823476779463769)\n\n    The expected frequencies can be given with the `f_exp` argument:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[16, 16, 16, 16, 16, 8],\n    ...                  lambda_='log-likelihood')\n    (3.3281031458963746, 0.6495419288047497)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> power_divergence(obs, lambda_=\"log-likelihood\")\n    (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> power_divergence(obs, axis=None)\n    (23.31034482758621, 0.015975692534127565)\n    >>> power_divergence(obs.ravel())\n    (23.31034482758621, 0.015975692534127565)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n    (2.0, 0.73575888234288467)\n\n    The calculation of the p-values is done by broadcasting the\n    test statistic with `ddof`.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we must use ``axis=1``:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n    ...                         [8, 20, 20, 16, 12, 12]],\n    ...                  axis=1)\n    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n\n    \"\"\"\n    # Convert the input argument `lambda_` to a numerical value.\n    if isinstance(lambda_, str):\n        if lambda_ not in _power_div_lambda_names:\n            names = repr(list(_power_div_lambda_names.keys()))[1:-1]\n            raise ValueError(\"invalid string for lambda_: {0!r}.  Valid strings \"\n                             \"are {1}\".format(lambda_, names))\n        lambda_ = _power_div_lambda_names[lambda_]\n    elif lambda_ is None:\n        lambda_ = 1\n\n    f_obs = np.asanyarray(f_obs)\n\n    if f_exp is not None:\n        f_exp = np.asanyarray(f_exp)\n    else:\n        # Ignore 'invalid' errors so the edge case of a data set with length 0\n        # is handled without spurious warnings.\n        with np.errstate(invalid='ignore'):\n            f_exp = f_obs.mean(axis=axis, keepdims=True)\n\n    # `terms` is the array of terms that are summed along `axis` to create\n    # the test statistic.  We use some specialized code for a few special\n    # cases of lambda_.\n    if lambda_ == 1:\n        # Pearson's chi-squared statistic\n        terms = (f_obs.astype(np.float64) - f_exp)**2 / f_exp\n    elif lambda_ == 0:\n        # Log-likelihood ratio (i.e. G-test)\n        terms = 2.0 * special.xlogy(f_obs, f_obs / f_exp)\n    elif lambda_ == -1:\n        # Modified log-likelihood ratio\n        terms = 2.0 * special.xlogy(f_exp, f_exp / f_obs)\n    else:\n        # General Cressie-Read power divergence.\n        terms = f_obs * ((f_obs / f_exp)**lambda_ - 1)\n        terms /= 0.5 * lambda_ * (lambda_ + 1)\n\n    stat = terms.sum(axis=axis)\n\n    num_obs = _count(terms, axis=axis)\n    ddof = asarray(ddof)\n    p = distributions.chi2.sf(stat, num_obs - 1 - ddof)\n\n    return Power_divergenceResult(stat, p)\n\n\ndef chisquare(f_obs, f_exp=None, ddof=0, axis=0):\n    \"\"\"\n    Calculate a one-way chi-square test.\n\n    The chi-square test tests the null hypothesis that the categorical data\n    has the given frequencies.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n\n    Returns\n    -------\n    chisq : float or ndarray\n        The chi-squared test statistic.  The value is a float if `axis` is\n        None or `f_obs` and `f_exp` are 1-D.\n    p : float or ndarray\n        The p-value of the test.  The value is a float if `ddof` and the\n        return value `chisq` are scalars.\n\n    See Also\n    --------\n    scipy.stats.power_divergence\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not chi-square, in which case this test\n    is not appropriate.\n\n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n\n    Examples\n    --------\n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.\n\n    >>> from scipy.stats import chisquare\n    >>> chisquare([16, 18, 16, 14, 12, 12])\n    (2.0, 0.84914503608460956)\n\n    With `f_exp` the expected frequencies can be given.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n    (3.5, 0.62338762774958223)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> chisquare(obs)\n    (array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> chisquare(obs, axis=None)\n    (23.31034482758621, 0.015975692534127565)\n    >>> chisquare(obs.ravel())\n    (23.31034482758621, 0.015975692534127565)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n    (2.0, 0.73575888234288467)\n\n    The calculation of the p-values is done by broadcasting the\n    chi-squared statistic with `ddof`.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we use ``axis=1``:\n\n    >>> chisquare([16, 18, 16, 14, 12, 12],\n    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n    ...           axis=1)\n    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n\n    \"\"\"\n    return power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis,\n                            lambda_=\"pearson\")\n\n\nKstestResult = namedtuple('KstestResult', ('statistic', 'pvalue'))\n\n\ndef _compute_dplus(cdfvals):\n    \"\"\"Computes D+ as used in the Kolmogorov-Smirnov test.\n\n    Parameters\n    ----------\n    cdfvals: array_like\n      Sorted array of CDF values between 0 and 1\n\n    Returns\n    -------\n      Maximum distance of the CDF values below Uniform(0, 1)\n\"\"\"\n    n = len(cdfvals)\n    return (np.arange(1.0, n + 1) / n - cdfvals).max()\n\n\ndef _compute_dminus(cdfvals):\n    \"\"\"Computes D- as used in the Kolmogorov-Smirnov test.\n\n    Parameters\n    ----------\n    cdfvals: array_like\n      Sorted array of CDF values between 0 and 1\n\n    Returns\n    -------\n      Maximum distance of the CDF values above Uniform(0, 1)\n    \"\"\"\n    n = len(cdfvals)\n    return (cdfvals - np.arange(0.0, n)/n).max()\n\n\ndef ks_1samp(x, cdf, args=(), alternative='two-sided', mode='auto'):\n    \"\"\"\n    Performs the Kolmogorov-Smirnov test for goodness of fit.\n\n    This performs a test of the distribution F(x) of an observed\n    random variable against a given distribution G(x). Under the null\n    hypothesis, the two distributions are identical, F(x)=G(x). The\n    alternative hypothesis can be either 'two-sided' (default), 'less'\n    or 'greater'. The KS test is only valid for continuous distributions.\n\n    Parameters\n    ----------\n    x : array_like\n        a 1-D array of observations of iid random variables.\n    cdf : callable\n        callable used to calculate the cdf.\n    args : tuple, sequence, optional\n        Distribution parameters, used with `cdf`.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided, see explanation in Notes\n          * 'greater': one-sided, see explanation in Notes\n    mode : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n          * 'auto' : selects one of the other options.\n          * 'exact' : uses the exact distribution of test statistic.\n          * 'approx' : approximates the two-sided probability with twice the one-sided probability\n          * 'asymp': uses asymptotic distribution of test statistic\n\n    Returns\n    -------\n    statistic : float\n        KS test statistic, either D, D+ or D- (depending on the value of 'alternative')\n    pvalue :  float\n        One-tailed or two-tailed p-value.\n\n    See Also\n    --------\n    ks_2samp, kstest\n\n    Notes\n    -----\n    In the one-sided test, the alternative is that the empirical\n    cumulative distribution function of the random variable is \"less\"\n    or \"greater\" than the cumulative distribution function G(x) of the\n    hypothesis, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n\n    >>> x = np.linspace(-15, 15, 9)\n    >>> stats.ks_1samp(x, stats.norm.cdf)\n    (0.44435602715924361, 0.038850142705171065)\n\n    >>> np.random.seed(987654321) # set random seed to get the same result\n    >>> stats.ks_1samp(stats.norm.rvs(size=100), stats.norm.cdf)\n    (0.058352892479417884, 0.8653960860778898)\n\n    *Test against one-sided alternative hypothesis*\n\n    Shift distribution to larger values, so that `` CDF(x) < norm.cdf(x)``:\n\n    >>> np.random.seed(987654321)\n    >>> x = stats.norm.rvs(loc=0.2, size=100)\n    >>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\n    (0.12464329735846891, 0.040989164077641749)\n\n    Reject equal distribution against alternative hypothesis: less\n\n    >>> stats.ks_1samp(x, stats.norm.cdf, alternative='greater')\n    (0.0072115233216311081, 0.98531158590396395)\n\n    Don't reject equal distribution against alternative hypothesis: greater\n\n    >>> stats.ks_1samp(x, stats.norm.cdf)\n    (0.12464329735846891, 0.08197335233541582)\n\n    Don't reject equal distribution against alternative hypothesis: two-sided\n\n    *Testing t distributed random variables against normal distribution*\n\n    With 100 degrees of freedom the t distribution looks close to the normal\n    distribution, and the K-S test does not reject the hypothesis that the\n    sample came from the normal distribution:\n\n    >>> np.random.seed(987654321)\n    >>> stats.ks_1samp(stats.t.rvs(100,size=100), stats.norm.cdf)\n    (0.072018929165471257, 0.6505883498379312)\n\n    With 3 degrees of freedom the t distribution looks sufficiently different\n    from the normal distribution, that we can reject the hypothesis that the\n    sample came from the normal distribution at the 10% level:\n\n    >>> np.random.seed(987654321)\n    >>> stats.ks_1samp(stats.t.rvs(3,size=100), stats.norm.cdf)\n    (0.131016895759829, 0.058826222555312224)\n\n    \"\"\"\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n       alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(\"Unexpected alternative %s\" % alternative)\n    if np.ma.is_masked(x):\n        x = x.compressed()\n\n    N = len(x)\n    x = np.sort(x)\n    cdfvals = cdf(x, *args)\n\n    if alternative == 'greater':\n        Dplus = _compute_dplus(cdfvals)\n        return KstestResult(Dplus, distributions.ksone.sf(Dplus, N))\n\n    if alternative == 'less':\n        Dminus = _compute_dminus(cdfvals)\n        return KstestResult(Dminus, distributions.ksone.sf(Dminus, N))\n\n    # alternative == 'two-sided':\n    Dplus = _compute_dplus(cdfvals)\n    Dminus = _compute_dminus(cdfvals)\n    D = np.max([Dplus, Dminus])\n    if mode == 'auto':  # Always select exact\n        mode = 'exact'\n    if mode == 'exact':\n        prob = distributions.kstwo.sf(D, N)\n    elif mode == 'asymp':\n        prob = distributions.kstwobign.sf(D * np.sqrt(N))\n    else:\n        # mode == 'approx'\n        prob = 2 * distributions.ksone.sf(D, N)\n    prob = np.clip(prob, 0, 1)\n    return KstestResult(D, prob)\n\n\nKs_2sampResult = KstestResult\n\n\ndef _compute_prob_inside_method(m, n, g, h):\n    \"\"\"\n    Count the proportion of paths that stay strictly inside two diagonal lines.\n\n    Parameters\n    ----------\n    m : integer\n        m > 0\n    n : integer\n        n > 0\n    g : integer\n        g is greatest common divisor of m and n\n    h : integer\n        0 <= h <= lcm(m,n)\n\n    Returns\n    -------\n    p : float\n        The proportion of paths that stay inside the two lines.\n\n\n    Count the integer lattice paths from (0, 0) to (m, n) which satisfy\n    |x/m - y/n| < h / lcm(m, n).\n    The paths make steps of size +1 in either positive x or positive y directions.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk.\n    Hodges, J.L. Jr.,\n    \"The Significance Probability of the Smirnov Two-Sample Test,\"\n    Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n\n    \"\"\"\n    # Probability is symmetrical in m, n.  Computation below uses m >= n.\n    if m < n:\n        m, n = n, m\n    mg = m // g\n    ng = n // g\n\n    # Count the integer lattice paths from (0, 0) to (m, n) which satisfy\n    # |nx/g - my/g| < h.\n    # Compute matrix A such that:\n    #  A(x, 0) = A(0, y) = 1\n    #  A(x, y) = A(x, y-1) + A(x-1, y), for x,y>=1, except that\n    #  A(x, y) = 0 if |x/m - y/n|>= h\n    # Probability is A(m, n)/binom(m+n, n)\n    # Optimizations exist for m==n, m==n*p.\n    # Only need to preserve a single column of A, and only a sliding window of it.\n    # minj keeps track of the slide.\n    minj, maxj = 0, min(int(np.ceil(h / mg)), n + 1)\n    curlen = maxj - minj\n    # Make a vector long enough to hold maximum window needed.\n    lenA = min(2 * maxj + 2, n + 1)\n    # This is an integer calculation, but the entries are essentially\n    # binomial coefficients, hence grow quickly.\n    # Scaling after each column is computed avoids dividing by a\n    # large binomial coefficent at the end, but is not sufficient to avoid\n    # the large dyanamic range which appears during the calculation.\n    # Instead we rescale based on the magnitude of the right most term in\n    # the column and keep track of an exponent separately and apply\n    # it at the end of the calculation.  Similarly when multiplying by\n    # the binomial coefficint\n    dtype = np.float64\n    A = np.zeros(lenA, dtype=dtype)\n    # Initialize the first column\n    A[minj:maxj] = 1\n    expnt = 0\n    for i in range(1, m + 1):\n        # Generate the next column.\n        # First calculate the sliding window\n        lastminj, lastlen = minj, curlen\n        minj = max(int(np.floor((ng * i - h) / mg)) + 1, 0)\n        minj = min(minj, n)\n        maxj = min(int(np.ceil((ng * i + h) / mg)), n + 1)\n        if maxj <= minj:\n            return 0\n        # Now fill in the values\n        A[0:maxj - minj] = np.cumsum(A[minj - lastminj:maxj - lastminj])\n        curlen = maxj - minj\n        if lastlen > curlen:\n            # Set some carried-over elements to 0\n            A[maxj - minj:maxj - minj + (lastlen - curlen)] = 0\n        # Rescale if the right most value is over 2**900\n        val = A[maxj - minj - 1]\n        _, valexpt = math.frexp(val)\n        if valexpt > 900:\n            # Scaling to bring down to about 2**800 appears\n            # sufficient for sizes under 10000.\n            valexpt -= 800\n            A = np.ldexp(A, -valexpt)\n            expnt += valexpt\n\n    val = A[maxj - minj - 1]\n    # Now divide by the binomial (m+n)!/m!/n!\n    for i in range(1, n + 1):\n        val = (val * i) / (m + i)\n        _, valexpt = math.frexp(val)\n        if valexpt < -128:\n            val = np.ldexp(val, -valexpt)\n            expnt += valexpt\n    # Finally scale if needed.\n    return np.ldexp(val, expnt)\n\n\ndef _compute_prob_outside_square(n, h):\n    \"\"\"\n    Compute the proportion of paths that pass outside the two diagonal lines.\n\n    Parameters\n    ----------\n    n : integer\n        n > 0\n    h : integer\n        0 <= h <= n\n\n    Returns\n    -------\n    p : float\n        The proportion of paths that pass outside the lines x-y = +/-h.\n\n    \"\"\"\n    # Compute Pr(D_{n,n} >= h/n)\n    # Prob = 2 * ( binom(2n, n-h) - binom(2n, n-2a) + binom(2n, n-3a) - ... )  / binom(2n, n)\n    # This formulation exhibits subtractive cancellation.\n    # Instead divide each term by binom(2n, n), then factor common terms\n    # and use a Horner-like algorithm\n    # P = 2 * A0 * (1 - A1*(1 - A2*(1 - A3*(1 - A4*(...)))))\n\n    P = 0.0\n    k = int(np.floor(n / h))\n    while k >= 0:\n        p1 = 1.0\n        # Each of the Ai terms has numerator and denominator with h simple terms.\n        for j in range(h):\n            p1 = (n - k * h - j) * p1 / (n + k * h + j + 1)\n        P = p1 * (1.0 - P)\n        k -= 1\n    return 2 * P\n\n\ndef _count_paths_outside_method(m, n, g, h):\n    \"\"\"\n    Count the number of paths that pass outside the specified diagonal.\n\n    Parameters\n    ----------\n    m : integer\n        m > 0\n    n : integer\n        n > 0\n    g : integer\n        g is greatest common divisor of m and n\n    h : integer\n        0 <= h <= lcm(m,n)\n\n    Returns\n    -------\n    p : float\n        The number of paths that go low.\n        The calculation may overflow - check for a finite answer.\n\n    Raises\n    ------\n    FloatingPointError: Raised if the intermediate computation goes outside\n    the range of a float.\n\n    Notes\n    -----\n    Count the integer lattice paths from (0, 0) to (m, n), which at some\n    point (x, y) along the path, satisfy:\n      m*y <= n*x - h*g\n    The paths make steps of size +1 in either positive x or positive y directions.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk.\n    Hodges, J.L. Jr.,\n    \"The Significance Probability of the Smirnov Two-Sample Test,\"\n    Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n\n    \"\"\"\n    # Compute #paths which stay lower than x/m-y/n = h/lcm(m,n)\n    # B(x, y) = #{paths from (0,0) to (x,y) without previously crossing the boundary}\n    #         = binom(x, y) - #{paths which already reached the boundary}\n    # Multiply by the number of path extensions going from (x, y) to (m, n)\n    # Sum.\n\n    # Probability is symmetrical in m, n.  Computation below assumes m >= n.\n    if m < n:\n        m, n = n, m\n    mg = m // g\n    ng = n // g\n\n    # Not every x needs to be considered.\n    # xj holds the list of x values to be checked.\n    # Wherever n*x/m + ng*h crosses an integer\n    lxj = n + (mg-h)//mg\n    xj = [(h + mg * j + ng-1)//ng for j in range(lxj)]\n    # B is an array just holding a few values of B(x,y), the ones needed.\n    # B[j] == B(x_j, j)\n    if lxj == 0:\n        return np.round(special.binom(m + n, n))\n    B = np.zeros(lxj)\n    B[0] = 1\n    # Compute the B(x, y) terms\n    # The binomial coefficient is an integer, but special.binom() may return a float.\n    # Round it to the nearest integer.\n    for j in range(1, lxj):\n        Bj = np.round(special.binom(xj[j] + j, j))\n        if not np.isfinite(Bj):\n            raise FloatingPointError()\n        for i in range(j):\n            bin = np.round(special.binom(xj[j] - xj[i] + j - i, j-i))\n            Bj -= bin * B[i]\n        B[j] = Bj\n        if not np.isfinite(Bj):\n            raise FloatingPointError()\n    # Compute the number of path extensions...\n    num_paths = 0\n    for j in range(lxj):\n        bin = np.round(special.binom((m-xj[j]) + (n - j), n-j))\n        term = B[j] * bin\n        if not np.isfinite(term):\n            raise FloatingPointError()\n        num_paths += term\n    return np.round(num_paths)\n\n\ndef _attempt_exact_2kssamp(n1, n2, g, d, alternative):\n    \"\"\"Attempts to compute the exact 2sample probability.\n\n    n1, n2 are the sample sizes\n    g is the gcd(n1, n2)\n    d is the computed max difference in ECDFs\n\n    Returns (success, d, probability)\n    \"\"\"\n    lcm = (n1 // g) * n2\n    h = int(np.round(d * lcm))\n    d = h * 1.0 / lcm\n    if h == 0:\n        return True, d, 1.0\n    saw_fp_error, prob = False, np.nan\n    try:\n        if alternative == 'two-sided':\n            if n1 == n2:\n                prob = _compute_prob_outside_square(n1, h)\n            else:\n                prob = 1 - _compute_prob_inside_method(n1, n2, g, h)\n        else:\n            if n1 == n2:\n                # prob = binom(2n, n-h) / binom(2n, n)\n                # Evaluating in that form incurs roundoff errors\n                # from special.binom. Instead calculate directly\n                jrange = np.arange(h)\n                prob = np.prod((n1 - jrange) / (n1 + jrange + 1.0))\n            else:\n                num_paths = _count_paths_outside_method(n1, n2, g, h)\n                bin = special.binom(n1 + n2, n1)\n                if not np.isfinite(bin) or not np.isfinite(num_paths) or num_paths > bin:\n                    saw_fp_error = True\n                else:\n                    prob = num_paths / bin\n\n    except FloatingPointError:\n        saw_fp_error = True\n\n    if saw_fp_error:\n        return False, d, np.nan\n    if not (0 <= prob <= 1):\n        return False, d, prob\n    return True, d, prob\n\n\ndef ks_2samp(data1, data2, alternative='two-sided', mode='auto'):\n    \"\"\"\n    Compute the Kolmogorov-Smirnov statistic on 2 samples.\n\n    This is a two-sided test for the null hypothesis that 2 independent samples\n    are drawn from the same continuous distribution.  The alternative hypothesis\n    can be either 'two-sided' (default), 'less' or 'greater'.\n\n    Parameters\n    ----------\n    data1, data2 : array_like, 1-Dimensional\n        Two arrays of sample observations assumed to be drawn from a continuous\n        distribution, sample sizes can be different.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided, see explanation in Notes\n          * 'greater': one-sided, see explanation in Notes\n    mode : {'auto', 'exact', 'asymp'}, optional\n        Defines the method used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n          * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n          * 'exact' : use exact distribution of test statistic\n          * 'asymp' : use asymptotic distribution of test statistic\n\n    Returns\n    -------\n    statistic : float\n        KS statistic.\n    pvalue : float\n        Two-tailed p-value.\n\n    See Also\n    --------\n    kstest, ks_1samp, epps_singleton_2samp, anderson_ksamp\n\n    Notes\n    -----\n    This tests whether 2 samples are drawn from the same distribution. Note\n    that, like in the case of the one-sample KS test, the distribution is\n    assumed to be continuous.\n\n    In the one-sided test, the alternative is that the empirical\n    cumulative distribution function F(x) of the data1 variable is \"less\"\n    or \"greater\" than the empirical cumulative distribution function G(x)\n    of the data2 variable, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.\n\n    If the KS statistic is small or the p-value is high, then we cannot\n    reject the hypothesis that the distributions of the two samples\n    are the same.\n\n    If the mode is 'auto', the computation is exact if the sample sizes are\n    less than 10000.  For larger sizes, the computation uses the\n    Kolmogorov-Smirnov distributions to compute an approximate value.\n\n    The 'two-sided' 'exact' computation computes the complementary probability\n    and then subtracts from 1.  As such, the minimum probability it can return\n    is about 1e-16.  While the algorithm itself is exact, numerical\n    errors may accumulate for large sample sizes.   It is most suited to\n    situations in which one of the sample sizes is only a few thousand.\n\n    We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n\n    References\n    ----------\n    .. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n           Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> np.random.seed(12345678)  #fix random seed to get the same result\n    >>> n1 = 200  # size of first sample\n    >>> n2 = 300  # size of second sample\n\n    For a different distribution, we can reject the null hypothesis since the\n    pvalue is below 1%:\n\n    >>> rvs1 = stats.norm.rvs(size=n1, loc=0., scale=1)\n    >>> rvs2 = stats.norm.rvs(size=n2, loc=0.5, scale=1.5)\n    >>> stats.ks_2samp(rvs1, rvs2)\n    (0.20833333333333334, 5.129279597781977e-05)\n\n    For a slightly different distribution, we cannot reject the null hypothesis\n    at a 10% or lower alpha since the p-value at 0.144 is higher than 10%\n\n    >>> rvs3 = stats.norm.rvs(size=n2, loc=0.01, scale=1.0)\n    >>> stats.ks_2samp(rvs1, rvs3)\n    (0.10333333333333333, 0.14691437867433876)\n\n    For an identical distribution, we cannot reject the null hypothesis since\n    the p-value is high, 41%:\n\n    >>> rvs4 = stats.norm.rvs(size=n2, loc=0.0, scale=1.0)\n    >>> stats.ks_2samp(rvs1, rvs4)\n    (0.07999999999999996, 0.41126949729859719)\n\n    \"\"\"\n    if mode not in ['auto', 'exact', 'asymp']:\n        raise ValueError(f'Invalid value for mode: {mode}')\n    alternative = {'t': 'two-sided', 'g': 'greater', 'l': 'less'}.get(\n       alternative.lower()[0], alternative)\n    if alternative not in ['two-sided', 'less', 'greater']:\n        raise ValueError(f'Invalid value for alternative: {alternative}')\n    MAX_AUTO_N = 10000  # 'auto' will attempt to be exact if n1,n2 <= MAX_AUTO_N\n    if np.ma.is_masked(data1):\n        data1 = data1.compressed()\n    if np.ma.is_masked(data2):\n        data2 = data2.compressed()\n    data1 = np.sort(data1)\n    data2 = np.sort(data2)\n    n1 = data1.shape[0]\n    n2 = data2.shape[0]\n    if min(n1, n2) == 0:\n        raise ValueError('Data passed to ks_2samp must not be empty')\n\n    data_all = np.concatenate([data1, data2])\n    # using searchsorted solves equal data problem\n    cdf1 = np.searchsorted(data1, data_all, side='right') / n1\n    cdf2 = np.searchsorted(data2, data_all, side='right') / n2\n    cddiffs = cdf1 - cdf2\n    minS = np.clip(-np.min(cddiffs), 0, 1)  # Ensure sign of minS is not negative.\n    maxS = np.max(cddiffs)\n    alt2Dvalue = {'less': minS, 'greater': maxS, 'two-sided': max(minS, maxS)}\n    d = alt2Dvalue[alternative]\n    g = gcd(n1, n2)\n    n1g = n1 // g\n    n2g = n2 // g\n    prob = -np.inf\n    original_mode = mode\n    if mode == 'auto':\n        mode = 'exact' if max(n1, n2) <= MAX_AUTO_N else 'asymp'\n    elif mode == 'exact':\n        # If lcm(n1, n2) is too big, switch from exact to asymp\n        if n1g >= np.iinfo(np.int_).max / n2g:\n            mode = 'asymp'\n            warnings.warn(\n                f\"Exact ks_2samp calculation not possible with samples sizes \"\n                f\"{n1} and {n2}. Switching to 'asymp'.\", RuntimeWarning)\n\n    if mode == 'exact':\n        success, d, prob = _attempt_exact_2kssamp(n1, n2, g, d, alternative)\n        if not success:\n            mode = 'asymp'\n            if original_mode == 'exact':\n                warnings.warn(f\"ks_2samp: Exact calculation unsuccessful. \"\n                              f\"Switching to mode={mode}.\", RuntimeWarning)\n\n    if mode == 'asymp':\n        # The product n1*n2 is large.  Use Smirnov's asymptoptic formula.\n        # Ensure float to avoid overflow in multiplication\n        # sorted because the one-sided formula is not symmetric in n1, n2\n        m, n = sorted([float(n1), float(n2)], reverse=True)\n        en = m * n / (m + n)\n        if alternative == 'two-sided':\n            prob = distributions.kstwo.sf(d, np.round(en))\n        else:\n            z = np.sqrt(en) * d\n            # Use Hodges' suggested approximation Eqn 5.3\n            # Requires m to be the larger of (n1, n2)\n            expt = -2 * z**2 - 2 * z * (m + 2*n)/np.sqrt(m*n*(m+n))/3.0\n            prob = np.exp(expt)\n\n    prob = np.clip(prob, 0, 1)\n    return KstestResult(d, prob)\n\n\ndef _parse_kstest_args(data1, data2, args, N):\n    # kstest allows many different variations of arguments.\n    # Pull out the parsing into a separate function\n    # (xvals, yvals, )  # 2sample\n    # (xvals, cdf function,..)\n    # (xvals, name of distribution, ...)\n    # (name of distribution, name of distribution, ...)\n\n    # Returns xvals, yvals, cdf\n    # where cdf is a cdf function, or None\n    # and yvals is either an array_like of values, or None\n    # and xvals is array_like.\n    rvsfunc, cdf = None, None\n    if isinstance(data1, str):\n        rvsfunc = getattr(distributions, data1).rvs\n    elif callable(data1):\n        rvsfunc = data1\n\n    if isinstance(data2, str):\n        cdf = getattr(distributions, data2).cdf\n        data2 = None\n    elif callable(data2):\n        cdf = data2\n        data2 = None\n\n    data1 = np.sort(rvsfunc(*args, size=N) if rvsfunc else data1)\n    return data1, data2, cdf\n\n\ndef kstest(rvs, cdf, args=(), N=20, alternative='two-sided', mode='auto'):\n    \"\"\"\n    Performs the (one sample or two samples) Kolmogorov-Smirnov test for goodness of fit.\n\n    The one-sample test performs a test of the distribution F(x) of an observed\n    random variable against a given distribution G(x). Under the null\n    hypothesis, the two distributions are identical, F(x)=G(x). The\n    alternative hypothesis can be either 'two-sided' (default), 'less'\n    or 'greater'. The KS test is only valid for continuous distributions.\n    The two-sample test tests whether the two independent samples are drawn\n    from the same continuous distribution.\n\n    Parameters\n    ----------\n    rvs : str, array_like, or callable\n        If an array, it should be a 1-D array of observations of random\n        variables.\n        If a callable, it should be a function to generate random variables;\n        it is required to have a keyword argument `size`.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used to generate random variables.\n    cdf : str, array_like or callable\n        If array_like, it should be a 1-D array of observations of random\n        variables, and the two-sample test is performed (and rvs must be array_like)\n        If a callable, that callable is used to calculate the cdf.\n        If a string, it should be the name of a distribution in `scipy.stats`,\n        which will be used as the cdf function.\n    args : tuple, sequence, optional\n        Distribution parameters, used if `rvs` or `cdf` are strings or callables.\n    N : int, optional\n        Sample size if `rvs` is string or callable.  Default is 20.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided, see explanation in Notes\n          * 'greater': one-sided, see explanation in Notes\n    mode : {'auto', 'exact', 'approx', 'asymp'}, optional\n        Defines the distribution used for calculating the p-value.\n        The following options are available (default is 'auto'):\n\n          * 'auto' : selects one of the other options.\n          * 'exact' : uses the exact distribution of test statistic.\n          * 'approx' : approximates the two-sided probability with twice the one-sided probability\n          * 'asymp': uses asymptotic distribution of test statistic\n\n    Returns\n    -------\n    statistic : float\n        KS test statistic, either D, D+ or D-.\n    pvalue :  float\n        One-tailed or two-tailed p-value.\n\n    See Also\n    --------\n    ks_2samp\n\n    Notes\n    -----\n    In the one-sided test, the alternative is that the empirical\n    cumulative distribution function of the random variable is \"less\"\n    or \"greater\" than the cumulative distribution function G(x) of the\n    hypothesis, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n\n    >>> x = np.linspace(-15, 15, 9)\n    >>> stats.kstest(x, 'norm')\n    (0.44435602715924361, 0.038850142705171065)\n\n    >>> np.random.seed(987654321) # set random seed to get the same result\n    >>> stats.kstest(stats.norm.rvs(size=100), stats.norm.cdf)\n    (0.058352892479417884, 0.8653960860778898)\n\n    The above lines are equivalent to:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.norm.rvs, 'norm', N=100)\n    (0.058352892479417884, 0.8653960860778898)\n\n    *Test against one-sided alternative hypothesis*\n\n    Shift distribution to larger values, so that ``CDF(x) < norm.cdf(x)``:\n\n    >>> np.random.seed(987654321)\n    >>> x = stats.norm.rvs(loc=0.2, size=100)\n    >>> stats.kstest(x, 'norm', alternative='less')\n    (0.12464329735846891, 0.040989164077641749)\n\n    Reject equal distribution against alternative hypothesis: less\n\n    >>> stats.kstest(x, 'norm', alternative='greater')\n    (0.0072115233216311081, 0.98531158590396395)\n\n    Don't reject equal distribution against alternative hypothesis: greater\n\n    >>> stats.kstest(x, 'norm')\n    (0.12464329735846891, 0.08197335233541582)\n\n    *Testing t distributed random variables against normal distribution*\n\n    With 100 degrees of freedom the t distribution looks close to the normal\n    distribution, and the K-S test does not reject the hypothesis that the\n    sample came from the normal distribution:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.t.rvs(100, size=100), 'norm')\n    (0.072018929165471257, 0.6505883498379312)\n\n    With 3 degrees of freedom the t distribution looks sufficiently different\n    from the normal distribution, that we can reject the hypothesis that the\n    sample came from the normal distribution at the 10% level:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.t.rvs(3, size=100), 'norm')\n    (0.131016895759829, 0.058826222555312224)\n\n    \"\"\"\n    # to not break compatibility with existing code\n    if alternative == 'two_sided':\n        alternative = 'two-sided'\n    if alternative not in ['two-sided', 'greater', 'less']:\n        raise ValueError(\"Unexpected alternative %s\" % alternative)\n    xvals, yvals, cdf = _parse_kstest_args(rvs, cdf, args, N)\n    if cdf:\n        return ks_1samp(xvals, cdf, args=args, alternative=alternative, mode=mode)\n    return ks_2samp(xvals, yvals, alternative=alternative, mode=mode)\n\n\ndef tiecorrect(rankvals):\n    \"\"\"\n    Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n\n    Parameters\n    ----------\n    rankvals : array_like\n        A 1-D sequence of ranks.  Typically this will be the array\n        returned by `~scipy.stats.rankdata`.\n\n    Returns\n    -------\n    factor : float\n        Correction factor for U or H.\n\n    See Also\n    --------\n    rankdata : Assign ranks to the data\n    mannwhitneyu : Mann-Whitney rank test\n    kruskal : Kruskal-Wallis H test\n\n    References\n    ----------\n    .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n           Sciences.  New York: McGraw-Hill.\n\n    Examples\n    --------\n    >>> from scipy.stats import tiecorrect, rankdata\n    >>> tiecorrect([1, 2.5, 2.5, 4])\n    0.9\n    >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n    >>> ranks\n    array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n    >>> tiecorrect(ranks)\n    0.9833333333333333\n\n    \"\"\"\n    arr = np.sort(rankvals)\n    idx = np.nonzero(np.r_[True, arr[1:] != arr[:-1], True])[0]\n    cnt = np.diff(idx).astype(np.float64)\n\n    size = np.float64(arr.size)\n    return 1.0 if size < 2 else 1.0 - (cnt**3 - cnt).sum() / (size**3 - size)\n\n\nMannwhitneyuResult = namedtuple('MannwhitneyuResult', ('statistic', 'pvalue'))\n\n\ndef mannwhitneyu(x, y, use_continuity=True, alternative=None):\n    \"\"\"\n    Compute the Mann-Whitney rank test on samples x and y.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Array of samples, should be one-dimensional.\n    use_continuity : bool, optional\n            Whether a continuity correction (1/2.) should be taken into\n            account. Default is True.\n    alternative : {None, 'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is None):\n\n          * None: computes p-value half the size of the 'two-sided' p-value and\n            a different U statistic. The default behavior is not the same as\n            using 'less' or 'greater'; it only exists for backward compatibility\n            and is deprecated.\n          * 'two-sided'\n          * 'less': one-sided\n          * 'greater': one-sided\n\n        Use of the None option is deprecated.\n\n    Returns\n    -------\n    statistic : float\n        The Mann-Whitney U statistic, equal to min(U for x, U for y) if\n        `alternative` is equal to None (deprecated; exists for backward\n        compatibility), and U for y otherwise.\n    pvalue : float\n        p-value assuming an asymptotic normal distribution. One-sided or\n        two-sided, depending on the choice of `alternative`.\n\n    Notes\n    -----\n    Use only when the number of observation in each sample is > 20 and\n    you have 2 independent samples of ranks. Mann-Whitney U is\n    significant if the u-obtained is LESS THAN or equal to the critical\n    value of U.\n\n    This test corrects for ties and by default uses a continuity correction.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Mann-Whitney_U_test\n\n    .. [2] H.B. Mann and D.R. Whitney, \"On a Test of Whether one of Two Random\n           Variables is Stochastically Larger than the Other,\" The Annals of\n           Mathematical Statistics, vol. 18, no. 1, pp. 50-60, 1947.\n\n    \"\"\"\n    if alternative is None:\n        warnings.warn(\"Calling `mannwhitneyu` without specifying \"\n                      \"`alternative` is deprecated.\", DeprecationWarning)\n\n    x = np.asarray(x)\n    y = np.asarray(y)\n    n1 = len(x)\n    n2 = len(y)\n    ranked = rankdata(np.concatenate((x, y)))\n    rankx = ranked[0:n1]  # get the x-ranks\n    u1 = n1*n2 + (n1*(n1+1))/2.0 - np.sum(rankx, axis=0)  # calc U for x\n    u2 = n1*n2 - u1  # remainder is U for y\n    T = tiecorrect(ranked)\n    if T == 0:\n        raise ValueError('All numbers are identical in mannwhitneyu')\n    sd = np.sqrt(T * n1 * n2 * (n1+n2+1) / 12.0)\n\n    meanrank = n1*n2/2.0 + 0.5 * use_continuity\n    if alternative is None or alternative == 'two-sided':\n        bigu = max(u1, u2)\n    elif alternative == 'less':\n        bigu = u1\n    elif alternative == 'greater':\n        bigu = u2\n    else:\n        raise ValueError(\"alternative should be None, 'less', 'greater' \"\n                         \"or 'two-sided'\")\n\n    z = (bigu - meanrank) / sd\n    if alternative is None:\n        # This behavior, equal to half the size of the two-sided\n        # p-value, is deprecated.\n        p = distributions.norm.sf(abs(z))\n    elif alternative == 'two-sided':\n        p = 2 * distributions.norm.sf(abs(z))\n    else:\n        p = distributions.norm.sf(z)\n\n    u = u2\n    # This behavior is deprecated.\n    if alternative is None:\n        u = min(u1, u2)\n    return MannwhitneyuResult(u, p)\n\n\nRanksumsResult = namedtuple('RanksumsResult', ('statistic', 'pvalue'))\n\n\ndef ranksums(x, y):\n    \"\"\"\n    Compute the Wilcoxon rank-sum statistic for two samples.\n\n    The Wilcoxon rank-sum test tests the null hypothesis that two sets\n    of measurements are drawn from the same distribution.  The alternative\n    hypothesis is that values in one sample are more likely to be\n    larger than the values in the other sample.\n\n    This test should be used to compare two samples from continuous\n    distributions.  It does not handle ties between measurements\n    in x and y.  For tie-handling and an optional continuity correction\n    see `scipy.stats.mannwhitneyu`.\n\n    Parameters\n    ----------\n    x,y : array_like\n        The data from the two samples.\n\n    Returns\n    -------\n    statistic : float\n        The test statistic under the large-sample approximation that the\n        rank sum statistic is normally distributed.\n    pvalue : float\n        The two-sided p-value of the test.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n\n    Examples\n    --------\n    We can test the hypothesis that two independent unequal-sized samples are\n    drawn from the same distribution with computing the Wilcoxon rank-sum\n    statistic.\n\n    >>> from scipy.stats import ranksums\n    >>> sample1 = np.random.uniform(-1, 1, 200)\n    >>> sample2 = np.random.uniform(-0.5, 1.5, 300) # a shifted distribution\n    >>> ranksums(sample1, sample2)\n    RanksumsResult(statistic=-7.887059, pvalue=3.09390448e-15)  # may vary\n\n    The p-value of less than ``0.05`` indicates that this test rejects the\n    hypothesis at the 5% significance level.\n\n    \"\"\"\n    x, y = map(np.asarray, (x, y))\n    n1 = len(x)\n    n2 = len(y)\n    alldata = np.concatenate((x, y))\n    ranked = rankdata(alldata)\n    x = ranked[:n1]\n    s = np.sum(x, axis=0)\n    expected = n1 * (n1+n2+1) / 2.0\n    z = (s - expected) / np.sqrt(n1*n2*(n1+n2+1)/12.0)\n    prob = 2 * distributions.norm.sf(abs(z))\n\n    return RanksumsResult(z, prob)\n\n\nKruskalResult = namedtuple('KruskalResult', ('statistic', 'pvalue'))\n\n\ndef kruskal(*args, nan_policy='propagate'):\n    \"\"\"\n    Compute the Kruskal-Wallis H-test for independent samples.\n\n    The Kruskal-Wallis H-test tests the null hypothesis that the population\n    median of all of the groups are equal.  It is a non-parametric version of\n    ANOVA.  The test works on 2 or more independent samples, which may have\n    different sizes.  Note that rejecting the null hypothesis does not\n    indicate which of the groups differs.  Post hoc comparisons between\n    groups are required to determine which groups are different.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n       Two or more arrays with the sample measurements can be given as\n       arguments.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    statistic : float\n       The Kruskal-Wallis H statistic, corrected for ties.\n    pvalue : float\n       The p-value for the test using the assumption that H has a chi\n       square distribution. The p-value returned is the survival function of\n       the chi square distribution evaluated at H.\n\n    See Also\n    --------\n    f_oneway : 1-way ANOVA.\n    mannwhitneyu : Mann-Whitney rank test on two samples.\n    friedmanchisquare : Friedman test for repeated measurements.\n\n    Notes\n    -----\n    Due to the assumption that H has a chi square distribution, the number\n    of samples in each group must not be too small.  A typical rule is\n    that each sample must have at least 5 measurements.\n\n    References\n    ----------\n    .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n       One-Criterion Variance Analysis\", Journal of the American Statistical\n       Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n    .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = [1, 3, 5, 7, 9]\n    >>> y = [2, 4, 6, 8, 10]\n    >>> stats.kruskal(x, y)\n    KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n\n    >>> x = [1, 1, 1]\n    >>> y = [2, 2, 2]\n    >>> z = [2, 2]\n    >>> stats.kruskal(x, y, z)\n    KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n\n    \"\"\"\n    args = list(map(np.asarray, args))\n    num_groups = len(args)\n    if num_groups < 2:\n        raise ValueError(\"Need at least two groups in stats.kruskal()\")\n\n    for arg in args:\n        if arg.size == 0:\n            return KruskalResult(np.nan, np.nan)\n    n = np.asarray(list(map(len, args)))\n\n    if nan_policy not in ('propagate', 'raise', 'omit'):\n        raise ValueError(\"nan_policy must be 'propagate', 'raise' or 'omit'\")\n\n    contains_nan = False\n    for arg in args:\n        cn = _contains_nan(arg, nan_policy)\n        if cn[0]:\n            contains_nan = True\n            break\n\n    if contains_nan and nan_policy == 'omit':\n        for a in args:\n            a = ma.masked_invalid(a)\n        return mstats_basic.kruskal(*args)\n\n    if contains_nan and nan_policy == 'propagate':\n        return KruskalResult(np.nan, np.nan)\n\n    alldata = np.concatenate(args)\n    ranked = rankdata(alldata)\n    ties = tiecorrect(ranked)\n    if ties == 0:\n        raise ValueError('All numbers are identical in kruskal')\n\n    # Compute sum^2/n for each group and sum\n    j = np.insert(np.cumsum(n), 0, 0)\n    ssbn = 0\n    for i in range(num_groups):\n        ssbn += _square_of_sums(ranked[j[i]:j[i+1]]) / n[i]\n\n    totaln = np.sum(n, dtype=float)\n    h = 12.0 / (totaln * (totaln + 1)) * ssbn - 3 * (totaln + 1)\n    df = num_groups - 1\n    h /= ties\n\n    return KruskalResult(h, distributions.chi2.sf(h, df))\n\n\nFriedmanchisquareResult = namedtuple('FriedmanchisquareResult',\n                                     ('statistic', 'pvalue'))\n\n\ndef friedmanchisquare(*args):\n    \"\"\"\n    Compute the Friedman test for repeated measurements.\n\n    The Friedman test tests the null hypothesis that repeated measurements of\n    the same individuals have the same distribution.  It is often used\n    to test for consistency among measurements obtained in different ways.\n    For example, if two measurement techniques are used on the same set of\n    individuals, the Friedman test can be used to determine if the two\n    measurement techniques are consistent.\n\n    Parameters\n    ----------\n    measurements1, measurements2, measurements3... : array_like\n        Arrays of measurements.  All of the arrays must have the same number\n        of elements.  At least 3 sets of measurements must be given.\n\n    Returns\n    -------\n    statistic : float\n        The test statistic, correcting for ties.\n    pvalue : float\n        The associated p-value assuming that the test statistic has a chi\n        squared distribution.\n\n    Notes\n    -----\n    Due to the assumption that the test statistic has a chi squared\n    distribution, the p-value is only reliable for n > 10 and more than\n    6 repeated measurements.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Friedman_test\n\n    \"\"\"\n    k = len(args)\n    if k < 3:\n        raise ValueError('At least 3 sets of measurements must be given for Friedman test, got {}.'.format(k))\n\n    n = len(args[0])\n    for i in range(1, k):\n        if len(args[i]) != n:\n            raise ValueError('Unequal N in friedmanchisquare.  Aborting.')\n\n    # Rank data\n    data = np.vstack(args).T\n    data = data.astype(float)\n    for i in range(len(data)):\n        data[i] = rankdata(data[i])\n\n    # Handle ties\n    ties = 0\n    for i in range(len(data)):\n        replist, repnum = find_repeats(array(data[i]))\n        for t in repnum:\n            ties += t * (t*t - 1)\n    c = 1 - ties / (k*(k*k - 1)*n)\n\n    ssbn = np.sum(data.sum(axis=0)**2)\n    chisq = (12.0 / (k*n*(k+1)) * ssbn - 3*n*(k+1)) / c\n\n    return FriedmanchisquareResult(chisq, distributions.chi2.sf(chisq, k - 1))\n\n\nBrunnerMunzelResult = namedtuple('BrunnerMunzelResult',\n                                 ('statistic', 'pvalue'))\n\n\ndef brunnermunzel(x, y, alternative=\"two-sided\", distribution=\"t\",\n                  nan_policy='propagate'):\n    \"\"\"\n    Compute the Brunner-Munzel test on samples x and y.\n\n    The Brunner-Munzel test is a nonparametric test of the null hypothesis that\n    when values are taken one by one from each group, the probabilities of\n    getting large values in both groups are equal.\n    Unlike the Wilcoxon-Mann-Whitney's U test, this does not require the\n    assumption of equivariance of two groups. Note that this does not assume\n    the distributions are same. This test works on two independent samples,\n    which may have different sizes.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Array of samples, should be one-dimensional.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided'\n          * 'less': one-sided\n          * 'greater': one-sided\n    distribution : {'t', 'normal'}, optional\n        Defines how to get the p-value.\n        The following options are available (default is 't'):\n\n          * 't': get the p-value by t-distribution\n          * 'normal': get the p-value by standard normal distribution.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    statistic : float\n        The Brunner-Munzer W statistic.\n    pvalue : float\n        p-value assuming an t distribution. One-sided or\n        two-sided, depending on the choice of `alternative` and `distribution`.\n\n    See Also\n    --------\n    mannwhitneyu : Mann-Whitney rank test on two samples.\n\n    Notes\n    -----\n    Brunner and Munzel recommended to estimate the p-value by t-distribution\n    when the size of data is 50 or less. If the size is lower than 10, it would\n    be better to use permuted Brunner Munzel test (see [2]_).\n\n    References\n    ----------\n    .. [1] Brunner, E. and Munzel, U. \"The nonparametric Benhrens-Fisher\n           problem: Asymptotic theory and a small-sample approximation\".\n           Biometrical Journal. Vol. 42(2000): 17-25.\n    .. [2] Neubert, K. and Brunner, E. \"A studentized permutation test for the\n           non-parametric Behrens-Fisher problem\". Computational Statistics and\n           Data Analysis. Vol. 51(2007): 5192-5204.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x1 = [1,2,1,1,1,1,1,1,1,1,2,4,1,1]\n    >>> x2 = [3,3,4,3,1,2,3,1,1,5,4]\n    >>> w, p_value = stats.brunnermunzel(x1, x2)\n    >>> w\n    3.1374674823029505\n    >>> p_value\n    0.0057862086661515377\n\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    # check both x and y\n    cnx, npx = _contains_nan(x, nan_policy)\n    cny, npy = _contains_nan(y, nan_policy)\n    contains_nan = cnx or cny\n    if npx == \"omit\" or npy == \"omit\":\n        nan_policy = \"omit\"\n\n    if contains_nan and nan_policy == \"propagate\":\n        return BrunnerMunzelResult(np.nan, np.nan)\n    elif contains_nan and nan_policy == \"omit\":\n        x = ma.masked_invalid(x)\n        y = ma.masked_invalid(y)\n        return mstats_basic.brunnermunzel(x, y, alternative, distribution)\n\n    nx = len(x)\n    ny = len(y)\n    if nx == 0 or ny == 0:\n        return BrunnerMunzelResult(np.nan, np.nan)\n    rankc = rankdata(np.concatenate((x, y)))\n    rankcx = rankc[0:nx]\n    rankcy = rankc[nx:nx+ny]\n    rankcx_mean = np.mean(rankcx)\n    rankcy_mean = np.mean(rankcy)\n    rankx = rankdata(x)\n    ranky = rankdata(y)\n    rankx_mean = np.mean(rankx)\n    ranky_mean = np.mean(ranky)\n\n    Sx = np.sum(np.power(rankcx - rankx - rankcx_mean + rankx_mean, 2.0))\n    Sx /= nx - 1\n    Sy = np.sum(np.power(rankcy - ranky - rankcy_mean + ranky_mean, 2.0))\n    Sy /= ny - 1\n\n    wbfn = nx * ny * (rankcy_mean - rankcx_mean)\n    wbfn /= (nx + ny) * np.sqrt(nx * Sx + ny * Sy)\n\n    if distribution == \"t\":\n        df_numer = np.power(nx * Sx + ny * Sy, 2.0)\n        df_denom = np.power(nx * Sx, 2.0) / (nx - 1)\n        df_denom += np.power(ny * Sy, 2.0) / (ny - 1)\n        df = df_numer / df_denom\n        p = distributions.t.cdf(wbfn, df)\n    elif distribution == \"normal\":\n        p = distributions.norm.cdf(wbfn)\n    else:\n        raise ValueError(\n            \"distribution should be 't' or 'normal'\")\n\n    if alternative == \"greater\":\n        pass\n    elif alternative == \"less\":\n        p = 1 - p\n    elif alternative == \"two-sided\":\n        p = 2 * np.min([p, 1-p])\n    else:\n        raise ValueError(\n            \"alternative should be 'less', 'greater' or 'two-sided'\")\n\n    return BrunnerMunzelResult(wbfn, p)\n\n\ndef combine_pvalues(pvalues, method='fisher', weights=None):\n    \"\"\"\n    Combine p-values from independent tests bearing upon the same hypothesis.\n\n    Parameters\n    ----------\n    pvalues : array_like, 1-D\n        Array of p-values assumed to come from independent tests.\n    method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}, optional\n        Name of method to use to combine p-values.\n        The following methods are available (default is 'fisher'):\n\n          * 'fisher': Fisher's method (Fisher's combined probability test), the\n            sum of the logarithm of the p-values\n          * 'pearson': Pearson's method (similar to Fisher's but uses sum of the\n            complement of the p-values inside the logarithms)\n          * 'tippett': Tippett's method (minimum of p-values)\n          * 'stouffer': Stouffer's Z-score method\n          * 'mudholkar_george': the difference of Fisher's and Pearson's methods\n            divided by 2\n    weights : array_like, 1-D, optional\n        Optional array of weights used only for Stouffer's Z-score method.\n\n    Returns\n    -------\n    statistic: float\n        The statistic calculated by the specified method.\n    pval: float\n        The combined p-value.\n\n    Notes\n    -----\n    Fisher's method (also known as Fisher's combined probability test) [1]_ uses\n    a chi-squared statistic to compute a combined p-value. The closely related\n    Stouffer's Z-score method [2]_ uses Z-scores rather than p-values. The\n    advantage of Stouffer's method is that it is straightforward to introduce\n    weights, which can make Stouffer's method more powerful than Fisher's\n    method when the p-values are from studies of different size [6]_ [7]_.\n    The Pearson's method uses :math:`log(1-p_i)` inside the sum whereas Fisher's\n    method uses :math:`log(p_i)` [4]_. For Fisher's and Pearson's method, the\n    sum of the logarithms is multiplied by -2 in the implementation. This\n    quantity has a chi-square distribution that determines the p-value. The\n    `mudholkar_george` method is the difference of the Fisher's and Pearson's\n    test statistics, each of which include the -2 factor [4]_. However, the\n    `mudholkar_george` method does not include these -2 factors. The test\n    statistic of `mudholkar_george` is the sum of logisitic random variables and\n    equation 3.6 in [3]_ is used to approximate the p-value based on Student's\n    t-distribution.\n\n    Fisher's method may be extended to combine p-values from dependent tests\n    [5]_. Extensions such as Brown's method and Kost's method are not currently\n    implemented.\n\n    .. versionadded:: 0.15.0\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Fisher%27s_method\n    .. [2] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n    .. [3] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n           random variables.\" Metrika 30.1 (1983): 1-13.\n    .. [4] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n           combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n    .. [5] Whitlock, M. C. \"Combining probability from independent tests: the\n           weighted Z-method is superior to Fisher's approach.\" Journal of\n           Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n    .. [6] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n           for combining probabilities in meta-analysis.\" Journal of\n           Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n    .. [7] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n\n    \"\"\"\n    pvalues = np.asarray(pvalues)\n    if pvalues.ndim != 1:\n        raise ValueError(\"pvalues is not 1-D\")\n\n    if method == 'fisher':\n        statistic = -2 * np.sum(np.log(pvalues))\n        pval = distributions.chi2.sf(statistic, 2 * len(pvalues))\n    elif method == 'pearson':\n        statistic = -2 * np.sum(np.log1p(-pvalues))\n        pval = distributions.chi2.sf(statistic, 2 * len(pvalues))\n    elif method == 'mudholkar_george':\n        normalizing_factor = np.sqrt(3/len(pvalues))/np.pi\n        statistic = -np.sum(np.log(pvalues)) + np.sum(np.log1p(-pvalues))\n        nu = 5 * len(pvalues) + 4\n        approx_factor = np.sqrt(nu / (nu - 2))\n        pval = distributions.t.sf(statistic * normalizing_factor * approx_factor, nu)\n    elif method == 'tippett':\n        statistic = np.min(pvalues)\n        pval = distributions.beta.sf(statistic, 1, len(pvalues))\n    elif method == 'stouffer':\n        if weights is None:\n            weights = np.ones_like(pvalues)\n        elif len(weights) != len(pvalues):\n            raise ValueError(\"pvalues and weights must be of the same size.\")\n\n        weights = np.asarray(weights)\n        if weights.ndim != 1:\n            raise ValueError(\"weights is not 1-D\")\n\n        Zi = distributions.norm.isf(pvalues)\n        statistic = np.dot(weights, Zi) / np.linalg.norm(weights)\n        pval = distributions.norm.sf(statistic)\n\n    else:\n        raise ValueError(\n            \"Invalid method '%s'. Options are 'fisher', 'pearson', \\\n            'mudholkar_george', 'tippett', 'or 'stouffer'\", method)\n\n    return (statistic, pval)\n\n\n#####################################\n#       STATISTICAL DISTANCES       #\n#####################################\n\ndef wasserstein_distance(u_values, v_values, u_weights=None, v_weights=None):\n    r\"\"\"\n    Compute the first Wasserstein distance between two 1D distributions.\n\n    This distance is also known as the earth mover's distance, since it can be\n    seen as the minimum amount of \"work\" required to transform :math:`u` into\n    :math:`v`, where \"work\" is measured as the amount of distribution weight\n    that must be moved, multiplied by the distance it has to be moved.\n\n    .. versionadded:: 1.0.0\n\n    Parameters\n    ----------\n    u_values, v_values : array_like\n        Values observed in the (empirical) distribution.\n    u_weights, v_weights : array_like, optional\n        Weight for each value. If unspecified, each value is assigned the same\n        weight.\n        `u_weights` (resp. `v_weights`) must have the same length as\n        `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n        must still be positive and finite so that the weights can be normalized\n        to sum to 1.\n\n    Returns\n    -------\n    distance : float\n        The computed distance between the distributions.\n\n    Notes\n    -----\n    The first Wasserstein distance between the distributions :math:`u` and\n    :math:`v` is:\n\n    .. math::\n\n        l_1 (u, v) = \\inf_{\\pi \\in \\Gamma (u, v)} \\int_{\\mathbb{R} \\times\n        \\mathbb{R}} |x-y| \\mathrm{d} \\pi (x, y)\n\n    where :math:`\\Gamma (u, v)` is the set of (probability) distributions on\n    :math:`\\mathbb{R} \\times \\mathbb{R}` whose marginals are :math:`u` and\n    :math:`v` on the first and second factors respectively.\n\n    If :math:`U` and :math:`V` are the respective CDFs of :math:`u` and\n    :math:`v`, this distance also equals to:\n\n    .. math::\n\n        l_1(u, v) = \\int_{-\\infty}^{+\\infty} |U-V|\n\n    See [2]_ for a proof of the equivalence of both definitions.\n\n    The input distributions can be empirical, therefore coming from samples\n    whose values are effectively inputs of the function, or they can be seen as\n    generalized functions, in which case they are weighted sums of Dirac delta\n    functions located at the specified values.\n\n    References\n    ----------\n    .. [1] \"Wasserstein metric\", https://en.wikipedia.org/wiki/Wasserstein_metric\n    .. [2] Ramdas, Garcia, Cuturi \"On Wasserstein Two Sample Testing and Related\n           Families of Nonparametric Tests\" (2015). :arXiv:`1509.02237`.\n\n    Examples\n    --------\n    >>> from scipy.stats import wasserstein_distance\n    >>> wasserstein_distance([0, 1, 3], [5, 6, 8])\n    5.0\n    >>> wasserstein_distance([0, 1], [0, 1], [3, 1], [2, 2])\n    0.25\n    >>> wasserstein_distance([3.4, 3.9, 7.5, 7.8], [4.5, 1.4],\n    ...                      [1.4, 0.9, 3.1, 7.2], [3.2, 3.5])\n    4.0781331438047861\n\n    \"\"\"\n    return _cdf_distance(1, u_values, v_values, u_weights, v_weights)\n\n\ndef energy_distance(u_values, v_values, u_weights=None, v_weights=None):\n    r\"\"\"\n    Compute the energy distance between two 1D distributions.\n\n    .. versionadded:: 1.0.0\n\n    Parameters\n    ----------\n    u_values, v_values : array_like\n        Values observed in the (empirical) distribution.\n    u_weights, v_weights : array_like, optional\n        Weight for each value. If unspecified, each value is assigned the same\n        weight.\n        `u_weights` (resp. `v_weights`) must have the same length as\n        `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n        must still be positive and finite so that the weights can be normalized\n        to sum to 1.\n\n    Returns\n    -------\n    distance : float\n        The computed distance between the distributions.\n\n    Notes\n    -----\n    The energy distance between two distributions :math:`u` and :math:`v`, whose\n    respective CDFs are :math:`U` and :math:`V`, equals to:\n\n    .. math::\n\n        D(u, v) = \\left( 2\\mathbb E|X - Y| - \\mathbb E|X - X'| -\n        \\mathbb E|Y - Y'| \\right)^{1/2}\n\n    where :math:`X` and :math:`X'` (resp. :math:`Y` and :math:`Y'`) are\n    independent random variables whose probability distribution is :math:`u`\n    (resp. :math:`v`).\n\n    As shown in [2]_, for one-dimensional real-valued variables, the energy\n    distance is linked to the non-distribution-free version of the Cramér-von\n    Mises distance:\n\n    .. math::\n\n        D(u, v) = \\sqrt{2} l_2(u, v) = \\left( 2 \\int_{-\\infty}^{+\\infty} (U-V)^2\n        \\right)^{1/2}\n\n    Note that the common Cramér-von Mises criterion uses the distribution-free\n    version of the distance. See [2]_ (section 2), for more details about both\n    versions of the distance.\n\n    The input distributions can be empirical, therefore coming from samples\n    whose values are effectively inputs of the function, or they can be seen as\n    generalized functions, in which case they are weighted sums of Dirac delta\n    functions located at the specified values.\n\n    References\n    ----------\n    .. [1] \"Energy distance\", https://en.wikipedia.org/wiki/Energy_distance\n    .. [2] Szekely \"E-statistics: The energy of statistical samples.\" Bowling\n           Green State University, Department of Mathematics and Statistics,\n           Technical Report 02-16 (2002).\n    .. [3] Rizzo, Szekely \"Energy distance.\" Wiley Interdisciplinary Reviews:\n           Computational Statistics, 8(1):27-38 (2015).\n    .. [4] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n           Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n           Gradients\" (2017). :arXiv:`1705.10743`.\n\n    Examples\n    --------\n    >>> from scipy.stats import energy_distance\n    >>> energy_distance([0], [2])\n    2.0000000000000004\n    >>> energy_distance([0, 8], [0, 8], [3, 1], [2, 2])\n    1.0000000000000002\n    >>> energy_distance([0.7, 7.4, 2.4, 6.8], [1.4, 8. ],\n    ...                 [2.1, 4.2, 7.4, 8. ], [7.6, 8.8])\n    0.88003340976158217\n\n    \"\"\"\n    return np.sqrt(2) * _cdf_distance(2, u_values, v_values,\n                                      u_weights, v_weights)\n\n\ndef _cdf_distance(p, u_values, v_values, u_weights=None, v_weights=None):\n    r\"\"\"\n    Compute, between two one-dimensional distributions :math:`u` and\n    :math:`v`, whose respective CDFs are :math:`U` and :math:`V`, the\n    statistical distance that is defined as:\n\n    .. math::\n\n        l_p(u, v) = \\left( \\int_{-\\infty}^{+\\infty} |U-V|^p \\right)^{1/p}\n\n    p is a positive parameter; p = 1 gives the Wasserstein distance, p = 2\n    gives the energy distance.\n\n    Parameters\n    ----------\n    u_values, v_values : array_like\n        Values observed in the (empirical) distribution.\n    u_weights, v_weights : array_like, optional\n        Weight for each value. If unspecified, each value is assigned the same\n        weight.\n        `u_weights` (resp. `v_weights`) must have the same length as\n        `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n        must still be positive and finite so that the weights can be normalized\n        to sum to 1.\n\n    Returns\n    -------\n    distance : float\n        The computed distance between the distributions.\n\n    Notes\n    -----\n    The input distributions can be empirical, therefore coming from samples\n    whose values are effectively inputs of the function, or they can be seen as\n    generalized functions, in which case they are weighted sums of Dirac delta\n    functions located at the specified values.\n\n    References\n    ----------\n    .. [1] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n           Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n           Gradients\" (2017). :arXiv:`1705.10743`.\n\n    \"\"\"\n    u_values, u_weights = _validate_distribution(u_values, u_weights)\n    v_values, v_weights = _validate_distribution(v_values, v_weights)\n\n    u_sorter = np.argsort(u_values)\n    v_sorter = np.argsort(v_values)\n\n    all_values = np.concatenate((u_values, v_values))\n    all_values.sort(kind='mergesort')\n\n    # Compute the differences between pairs of successive values of u and v.\n    deltas = np.diff(all_values)\n\n    # Get the respective positions of the values of u and v among the values of\n    # both distributions.\n    u_cdf_indices = u_values[u_sorter].searchsorted(all_values[:-1], 'right')\n    v_cdf_indices = v_values[v_sorter].searchsorted(all_values[:-1], 'right')\n\n    # Calculate the CDFs of u and v using their weights, if specified.\n    if u_weights is None:\n        u_cdf = u_cdf_indices / u_values.size\n    else:\n        u_sorted_cumweights = np.concatenate(([0],\n                                              np.cumsum(u_weights[u_sorter])))\n        u_cdf = u_sorted_cumweights[u_cdf_indices] / u_sorted_cumweights[-1]\n\n    if v_weights is None:\n        v_cdf = v_cdf_indices / v_values.size\n    else:\n        v_sorted_cumweights = np.concatenate(([0],\n                                              np.cumsum(v_weights[v_sorter])))\n        v_cdf = v_sorted_cumweights[v_cdf_indices] / v_sorted_cumweights[-1]\n\n    # Compute the value of the integral based on the CDFs.\n    # If p = 1 or p = 2, we avoid using np.power, which introduces an overhead\n    # of about 15%.\n    if p == 1:\n        return np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))\n    if p == 2:\n        return np.sqrt(np.sum(np.multiply(np.square(u_cdf - v_cdf), deltas)))\n    return np.power(np.sum(np.multiply(np.power(np.abs(u_cdf - v_cdf), p),\n                                       deltas)), 1/p)\n\n\ndef _validate_distribution(values, weights):\n    \"\"\"\n    Validate the values and weights from a distribution input of `cdf_distance`\n    and return them as ndarray objects.\n\n    Parameters\n    ----------\n    values : array_like\n        Values observed in the (empirical) distribution.\n    weights : array_like\n        Weight for each value.\n\n    Returns\n    -------\n    values : ndarray\n        Values as ndarray.\n    weights : ndarray\n        Weights as ndarray.\n\n    \"\"\"\n    # Validate the value array.\n    values = np.asarray(values, dtype=float)\n    if len(values) == 0:\n        raise ValueError(\"Distribution can't be empty.\")\n\n    # Validate the weight array, if specified.\n    if weights is not None:\n        weights = np.asarray(weights, dtype=float)\n        if len(weights) != len(values):\n            raise ValueError('Value and weight array-likes for the same '\n                             'empirical distribution must be of the same size.')\n        if np.any(weights < 0):\n            raise ValueError('All weights must be non-negative.')\n        if not 0 < np.sum(weights) < np.inf:\n            raise ValueError('Weight array-like sum must be positive and '\n                             'finite. Set as None for an equal distribution of '\n                             'weight.')\n\n        return values, weights\n\n    return values, None\n\n\n#####################################\n#         SUPPORT FUNCTIONS         #\n#####################################\n\nRepeatedResults = namedtuple('RepeatedResults', ('values', 'counts'))\n\n\ndef find_repeats(arr):\n    \"\"\"\n    Find repeats and repeat counts.\n\n    Parameters\n    ----------\n    arr : array_like\n        Input array. This is cast to float64.\n\n    Returns\n    -------\n    values : ndarray\n        The unique values from the (flattened) input that are repeated.\n\n    counts : ndarray\n        Number of times the corresponding 'value' is repeated.\n\n    Notes\n    -----\n    In numpy >= 1.9 `numpy.unique` provides similar functionality. The main\n    difference is that `find_repeats` only returns repeated values.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> stats.find_repeats([2, 1, 2, 3, 2, 2, 5])\n    RepeatedResults(values=array([2.]), counts=array([4]))\n\n    >>> stats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])\n    RepeatedResults(values=array([4.,  5.]), counts=array([2, 2]))\n\n    \"\"\"\n    # Note: always copies.\n    return RepeatedResults(*_find_repeats(np.array(arr, dtype=np.float64)))\n\n\ndef _sum_of_squares(a, axis=0):\n    \"\"\"\n    Square each element of the input array, and return the sum(s) of that.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int or None, optional\n        Axis along which to calculate. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    sum_of_squares : ndarray\n        The sum along the given axis for (a**2).\n\n    See Also\n    --------\n    _square_of_sums : The square(s) of the sum(s) (the opposite of\n    `_sum_of_squares`).\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n    return np.sum(a*a, axis)\n\n\ndef _square_of_sums(a, axis=0):\n    \"\"\"\n    Sum elements of the input array, and return the square(s) of that sum.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : int or None, optional\n        Axis along which to calculate. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    square_of_sums : float or ndarray\n        The square of the sum over `axis`.\n\n    See Also\n    --------\n    _sum_of_squares : The sum of squares (the opposite of `square_of_sums`).\n\n    \"\"\"\n    a, axis = _chk_asarray(a, axis)\n    s = np.sum(a, axis)\n    if not np.isscalar(s):\n        return s.astype(float) * s\n    else:\n        return float(s) * s\n\n\ndef rankdata(a, method='average', *, axis=None):\n    \"\"\"\n    Assign ranks to data, dealing with ties appropriately.\n\n    By default (``axis=None``), the data array is first flattened, and a flat\n    array of ranks is returned. Separately reshape the rank array to the\n    shape of the data array if desired (see Examples).\n\n    Ranks begin at 1.  The `method` argument controls how ranks are assigned\n    to equal values.  See [1]_ for further discussion of ranking methods.\n\n    Parameters\n    ----------\n    a : array_like\n        The array of values to be ranked.\n    method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n        The method used to assign ranks to tied elements.\n        The following methods are available (default is 'average'):\n\n          * 'average': The average of the ranks that would have been assigned to\n            all the tied values is assigned to each value.\n          * 'min': The minimum of the ranks that would have been assigned to all\n            the tied values is assigned to each value.  (This is also\n            referred to as \"competition\" ranking.)\n          * 'max': The maximum of the ranks that would have been assigned to all\n            the tied values is assigned to each value.\n          * 'dense': Like 'min', but the rank of the next highest element is\n            assigned the rank immediately after those assigned to the tied\n            elements.\n          * 'ordinal': All values are given a distinct rank, corresponding to\n            the order that the values occur in `a`.\n    axis : {None, int}, optional\n        Axis along which to perform the ranking. If ``None``, the data array\n        is first flattened.\n\n    Returns\n    -------\n    ranks : ndarray\n         An array of size equal to the size of `a`, containing rank\n         scores.\n\n    References\n    ----------\n    .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n\n    Examples\n    --------\n    >>> from scipy.stats import rankdata\n    >>> rankdata([0, 2, 3, 2])\n    array([ 1. ,  2.5,  4. ,  2.5])\n    >>> rankdata([0, 2, 3, 2], method='min')\n    array([ 1,  2,  4,  2])\n    >>> rankdata([0, 2, 3, 2], method='max')\n    array([ 1,  3,  4,  3])\n    >>> rankdata([0, 2, 3, 2], method='dense')\n    array([ 1,  2,  3,  2])\n    >>> rankdata([0, 2, 3, 2], method='ordinal')\n    array([ 1,  2,  4,  3])\n    >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n    array([[1. , 2.5],\n          [4. , 2.5]])\n    >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n    array([[1. , 2.5, 2.5],\n           [2. , 1. , 3. ]])\n    \"\"\"\n    if method not in ('average', 'min', 'max', 'dense', 'ordinal'):\n        raise ValueError('unknown method \"{0}\"'.format(method))\n\n    if axis is not None:\n        a = np.asarray(a)\n        if a.size == 0:\n            # The return values of `normalize_axis_index` are ignored.  The\n            # call validates `axis`, even though we won't use it.\n            # use scipy._lib._util._normalize_axis_index when available\n            np.core.multiarray.normalize_axis_index(axis, a.ndim)\n            dt = np.float64 if method == 'average' else np.int_\n            return np.empty(a.shape, dtype=dt)\n        return np.apply_along_axis(rankdata, axis, a, method)\n\n    arr = np.ravel(np.asarray(a))\n    algo = 'mergesort' if method == 'ordinal' else 'quicksort'\n    sorter = np.argsort(arr, kind=algo)\n\n    inv = np.empty(sorter.size, dtype=np.intp)\n    inv[sorter] = np.arange(sorter.size, dtype=np.intp)\n\n    if method == 'ordinal':\n        return inv + 1\n\n    arr = arr[sorter]\n    obs = np.r_[True, arr[1:] != arr[:-1]]\n    dense = obs.cumsum()[inv]\n\n    if method == 'dense':\n        return dense\n\n    # cumulative counts of each unique value\n    count = np.r_[np.nonzero(obs)[0], len(obs)]\n\n    if method == 'max':\n        return count[dense]\n\n    if method == 'min':\n        return count[dense - 1] + 1\n\n    # average method\n    return .5 * (count[dense] + count[dense - 1] + 1)\n",8197],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py":["\"\"\"\nFunctions for changing global ufunc configuration\n\nThis provides helpers which wrap `umath.geterrobj` and `umath.seterrobj`\n\"\"\"\nimport collections.abc\nimport contextlib\n\nfrom .overrides import set_module\nfrom .umath import (\n    UFUNC_BUFSIZE_DEFAULT,\n    ERR_IGNORE, ERR_WARN, ERR_RAISE, ERR_CALL, ERR_PRINT, ERR_LOG, ERR_DEFAULT,\n    SHIFT_DIVIDEBYZERO, SHIFT_OVERFLOW, SHIFT_UNDERFLOW, SHIFT_INVALID,\n)\nfrom . import umath\n\n__all__ = [\n    \"seterr\", \"geterr\", \"setbufsize\", \"getbufsize\", \"seterrcall\", \"geterrcall\",\n    \"errstate\",\n]\n\n_errdict = {\"ignore\": ERR_IGNORE,\n            \"warn\": ERR_WARN,\n            \"raise\": ERR_RAISE,\n            \"call\": ERR_CALL,\n            \"print\": ERR_PRINT,\n            \"log\": ERR_LOG}\n\n_errdict_rev = {value: key for key, value in _errdict.items()}\n\n\n@set_module('numpy')\ndef seterr(all=None, divide=None, over=None, under=None, invalid=None):\n    \"\"\"\n    Set how floating-point errors are handled.\n\n    Note that operations on integer scalar types (such as `int16`) are\n    handled like floating point, and are affected by these settings.\n\n    Parameters\n    ----------\n    all : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Set treatment for all types of floating-point errors at once:\n\n        - ignore: Take no action when the exception occurs.\n        - warn: Print a `RuntimeWarning` (via the Python `warnings` module).\n        - raise: Raise a `FloatingPointError`.\n        - call: Call a function specified using the `seterrcall` function.\n        - print: Print a warning directly to ``stdout``.\n        - log: Record error in a Log object specified by `seterrcall`.\n\n        The default is not to change the current behavior.\n    divide : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Treatment for division by zero.\n    over : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Treatment for floating-point overflow.\n    under : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Treatment for floating-point underflow.\n    invalid : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Treatment for invalid floating-point operation.\n\n    Returns\n    -------\n    old_settings : dict\n        Dictionary containing the old settings.\n\n    See also\n    --------\n    seterrcall : Set a callback function for the 'call' mode.\n    geterr, geterrcall, errstate\n\n    Notes\n    -----\n    The floating-point exceptions are defined in the IEEE 754 standard [1]_:\n\n    - Division by zero: infinite result obtained from finite numbers.\n    - Overflow: result too large to be expressed.\n    - Underflow: result so close to zero that some precision\n      was lost.\n    - Invalid operation: result is not an expressible number, typically\n      indicates that a NaN was produced.\n\n    .. [1] https://en.wikipedia.org/wiki/IEEE_754\n\n    Examples\n    --------\n    >>> old_settings = np.seterr(all='ignore')  #seterr to known value\n    >>> np.seterr(over='raise')\n    {'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n    >>> np.seterr(**old_settings)  # reset to default\n    {'divide': 'ignore', 'over': 'raise', 'under': 'ignore', 'invalid': 'ignore'}\n\n    >>> np.int16(32000) * np.int16(3)\n    30464\n    >>> old_settings = np.seterr(all='warn', over='raise')\n    >>> np.int16(32000) * np.int16(3)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n    FloatingPointError: overflow encountered in short_scalars\n\n    >>> from collections import OrderedDict\n    >>> old_settings = np.seterr(all='print')\n    >>> OrderedDict(np.geterr())\n    OrderedDict([('divide', 'print'), ('over', 'print'), ('under', 'print'), ('invalid', 'print')])\n    >>> np.int16(32000) * np.int16(3)\n    30464\n\n    \"\"\"\n\n    pyvals = umath.geterrobj()\n    old = geterr()\n\n    if divide is None:\n        divide = all or old['divide']\n    if over is None:\n        over = all or old['over']\n    if under is None:\n        under = all or old['under']\n    if invalid is None:\n        invalid = all or old['invalid']\n\n    maskvalue = ((_errdict[divide] << SHIFT_DIVIDEBYZERO) +\n                 (_errdict[over] << SHIFT_OVERFLOW) +\n                 (_errdict[under] << SHIFT_UNDERFLOW) +\n                 (_errdict[invalid] << SHIFT_INVALID))\n\n    pyvals[1] = maskvalue\n    umath.seterrobj(pyvals)\n    return old\n\n\n@set_module('numpy')\ndef geterr():\n    \"\"\"\n    Get the current way of handling floating-point errors.\n\n    Returns\n    -------\n    res : dict\n        A dictionary with keys \"divide\", \"over\", \"under\", and \"invalid\",\n        whose values are from the strings \"ignore\", \"print\", \"log\", \"warn\",\n        \"raise\", and \"call\". The keys represent possible floating-point\n        exceptions, and the values define how these exceptions are handled.\n\n    See Also\n    --------\n    geterrcall, seterr, seterrcall\n\n    Notes\n    -----\n    For complete documentation of the types of floating-point exceptions and\n    treatment options, see `seterr`.\n\n    Examples\n    --------\n    >>> from collections import OrderedDict\n    >>> sorted(np.geterr().items())\n    [('divide', 'warn'), ('invalid', 'warn'), ('over', 'warn'), ('under', 'ignore')]\n    >>> np.arange(3.) / np.arange(3.)\n    array([nan,  1.,  1.])\n\n    >>> oldsettings = np.seterr(all='warn', over='raise')\n    >>> OrderedDict(sorted(np.geterr().items()))\n    OrderedDict([('divide', 'warn'), ('invalid', 'warn'), ('over', 'raise'), ('under', 'warn')])\n    >>> np.arange(3.) / np.arange(3.)\n    array([nan,  1.,  1.])\n\n    \"\"\"\n    maskvalue = umath.geterrobj()[1]\n    mask = 7\n    res = {}\n    val = (maskvalue >> SHIFT_DIVIDEBYZERO) & mask\n    res['divide'] = _errdict_rev[val]\n    val = (maskvalue >> SHIFT_OVERFLOW) & mask\n    res['over'] = _errdict_rev[val]\n    val = (maskvalue >> SHIFT_UNDERFLOW) & mask\n    res['under'] = _errdict_rev[val]\n    val = (maskvalue >> SHIFT_INVALID) & mask\n    res['invalid'] = _errdict_rev[val]\n    return res\n\n\n@set_module('numpy')\ndef setbufsize(size):\n    \"\"\"\n    Set the size of the buffer used in ufuncs.\n\n    Parameters\n    ----------\n    size : int\n        Size of buffer.\n\n    \"\"\"\n    if size > 10e6:\n        raise ValueError(\"Buffer size, %s, is too big.\" % size)\n    if size < 5:\n        raise ValueError(\"Buffer size, %s, is too small.\" % size)\n    if size % 16 != 0:\n        raise ValueError(\"Buffer size, %s, is not a multiple of 16.\" % size)\n\n    pyvals = umath.geterrobj()\n    old = getbufsize()\n    pyvals[0] = size\n    umath.seterrobj(pyvals)\n    return old\n\n\n@set_module('numpy')\ndef getbufsize():\n    \"\"\"\n    Return the size of the buffer used in ufuncs.\n\n    Returns\n    -------\n    getbufsize : int\n        Size of ufunc buffer in bytes.\n\n    \"\"\"\n    return umath.geterrobj()[0]\n\n\n@set_module('numpy')\ndef seterrcall(func):\n    \"\"\"\n    Set the floating-point error callback function or log object.\n\n    There are two ways to capture floating-point error messages.  The first\n    is to set the error-handler to 'call', using `seterr`.  Then, set\n    the function to call using this function.\n\n    The second is to set the error-handler to 'log', using `seterr`.\n    Floating-point errors then trigger a call to the 'write' method of\n    the provided object.\n\n    Parameters\n    ----------\n    func : callable f(err, flag) or object with write method\n        Function to call upon floating-point errors ('call'-mode) or\n        object whose 'write' method is used to log such message ('log'-mode).\n\n        The call function takes two arguments. The first is a string describing\n        the type of error (such as \"divide by zero\", \"overflow\", \"underflow\",\n        or \"invalid value\"), and the second is the status flag.  The flag is a\n        byte, whose four least-significant bits indicate the type of error, one\n        of \"divide\", \"over\", \"under\", \"invalid\"::\n\n          [0 0 0 0 divide over under invalid]\n\n        In other words, ``flags = divide + 2*over + 4*under + 8*invalid``.\n\n        If an object is provided, its write method should take one argument,\n        a string.\n\n    Returns\n    -------\n    h : callable, log instance or None\n        The old error handler.\n\n    See Also\n    --------\n    seterr, geterr, geterrcall\n\n    Examples\n    --------\n    Callback upon error:\n\n    >>> def err_handler(type, flag):\n    ...     print(\"Floating point error (%s), with flag %s\" % (type, flag))\n    ...\n\n    >>> saved_handler = np.seterrcall(err_handler)\n    >>> save_err = np.seterr(all='call')\n    >>> from collections import OrderedDict\n\n    >>> np.array([1, 2, 3]) / 0.0\n    Floating point error (divide by zero), with flag 1\n    array([inf, inf, inf])\n\n    >>> np.seterrcall(saved_handler)\n    <function err_handler at 0x...>\n    >>> OrderedDict(sorted(np.seterr(**save_err).items()))\n    OrderedDict([('divide', 'call'), ('invalid', 'call'), ('over', 'call'), ('under', 'call')])\n\n    Log error message:\n\n    >>> class Log:\n    ...     def write(self, msg):\n    ...         print(\"LOG: %s\" % msg)\n    ...\n\n    >>> log = Log()\n    >>> saved_handler = np.seterrcall(log)\n    >>> save_err = np.seterr(all='log')\n\n    >>> np.array([1, 2, 3]) / 0.0\n    LOG: Warning: divide by zero encountered in true_divide\n    array([inf, inf, inf])\n\n    >>> np.seterrcall(saved_handler)\n    <numpy.core.numeric.Log object at 0x...>\n    >>> OrderedDict(sorted(np.seterr(**save_err).items()))\n    OrderedDict([('divide', 'log'), ('invalid', 'log'), ('over', 'log'), ('under', 'log')])\n\n    \"\"\"\n    if func is not None and not isinstance(func, collections.abc.Callable):\n        if (not hasattr(func, 'write') or\n                not isinstance(func.write, collections.abc.Callable)):\n            raise ValueError(\"Only callable can be used as callback\")\n    pyvals = umath.geterrobj()\n    old = geterrcall()\n    pyvals[2] = func\n    umath.seterrobj(pyvals)\n    return old\n\n\n@set_module('numpy')\ndef geterrcall():\n    \"\"\"\n    Return the current callback function used on floating-point errors.\n\n    When the error handling for a floating-point error (one of \"divide\",\n    \"over\", \"under\", or \"invalid\") is set to 'call' or 'log', the function\n    that is called or the log instance that is written to is returned by\n    `geterrcall`. This function or log instance has been set with\n    `seterrcall`.\n\n    Returns\n    -------\n    errobj : callable, log instance or None\n        The current error handler. If no handler was set through `seterrcall`,\n        ``None`` is returned.\n\n    See Also\n    --------\n    seterrcall, seterr, geterr\n\n    Notes\n    -----\n    For complete documentation of the types of floating-point exceptions and\n    treatment options, see `seterr`.\n\n    Examples\n    --------\n    >>> np.geterrcall()  # we did not yet set a handler, returns None\n\n    >>> oldsettings = np.seterr(all='call')\n    >>> def err_handler(type, flag):\n    ...     print(\"Floating point error (%s), with flag %s\" % (type, flag))\n    >>> oldhandler = np.seterrcall(err_handler)\n    >>> np.array([1, 2, 3]) / 0.0\n    Floating point error (divide by zero), with flag 1\n    array([inf, inf, inf])\n\n    >>> cur_handler = np.geterrcall()\n    >>> cur_handler is err_handler\n    True\n\n    \"\"\"\n    return umath.geterrobj()[2]\n\n\nclass _unspecified:\n    pass\n\n\n_Unspecified = _unspecified()\n\n\n@set_module('numpy')\nclass errstate(contextlib.ContextDecorator):\n    \"\"\"\n    errstate(**kwargs)\n\n    Context manager for floating-point error handling.\n\n    Using an instance of `errstate` as a context manager allows statements in\n    that context to execute with a known error handling behavior. Upon entering\n    the context the error handling is set with `seterr` and `seterrcall`, and\n    upon exiting it is reset to what it was before.\n\n    ..  versionchanged:: 1.17.0\n        `errstate` is also usable as a function decorator, saving\n        a level of indentation if an entire function is wrapped.\n        See :py:class:`contextlib.ContextDecorator` for more information.\n\n    Parameters\n    ----------\n    kwargs : {divide, over, under, invalid}\n        Keyword arguments. The valid keywords are the possible floating-point\n        exceptions. Each keyword should have a string value that defines the\n        treatment for the particular error. Possible values are\n        {'ignore', 'warn', 'raise', 'call', 'print', 'log'}.\n\n    See Also\n    --------\n    seterr, geterr, seterrcall, geterrcall\n\n    Notes\n    -----\n    For complete documentation of the types of floating-point exceptions and\n    treatment options, see `seterr`.\n\n    Examples\n    --------\n    >>> from collections import OrderedDict\n    >>> olderr = np.seterr(all='ignore')  # Set error handling to known state.\n\n    >>> np.arange(3) / 0.\n    array([nan, inf, inf])\n    >>> with np.errstate(divide='warn'):\n    ...     np.arange(3) / 0.\n    array([nan, inf, inf])\n\n    >>> np.sqrt(-1)\n    nan\n    >>> with np.errstate(invalid='raise'):\n    ...     np.sqrt(-1)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 2, in <module>\n    FloatingPointError: invalid value encountered in sqrt\n\n    Outside the context the error handling behavior has not changed:\n\n    >>> OrderedDict(sorted(np.geterr().items()))\n    OrderedDict([('divide', 'ignore'), ('invalid', 'ignore'), ('over', 'ignore'), ('under', 'ignore')])\n\n    \"\"\"\n\n    def __init__(self, *, call=_Unspecified, **kwargs):\n        self.call = call\n        self.kwargs = kwargs\n\n    def __enter__(self):\n        self.oldstate = seterr(**self.kwargs)\n        if self.call is not _Unspecified:\n            self.oldcall = seterrcall(self.call)\n\n    def __exit__(self, *exc_info):\n        seterr(**self.oldstate)\n        if self.call is not _Unspecified:\n            seterrcall(self.oldcall)\n\n\ndef _setdef():\n    defval = [UFUNC_BUFSIZE_DEFAULT, ERR_DEFAULT, None]\n    umath.seterrobj(defval)\n\n\n# set the default values\n_setdef()\n",450],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py":["\"\"\"\nUtilities that manipulate strides to achieve desirable effects.\n\nAn explanation of strides can be found in the \"ndarray.rst\" file in the\nNumPy reference guide.\n\n\"\"\"\nimport numpy as np\nfrom numpy.core.numeric import normalize_axis_tuple\nfrom numpy.core.overrides import array_function_dispatch, set_module\n\n__all__ = ['broadcast_to', 'broadcast_arrays', 'broadcast_shapes']\n\n\nclass DummyArray:\n    \"\"\"Dummy object that just exists to hang __array_interface__ dictionaries\n    and possibly keep alive a reference to a base array.\n    \"\"\"\n\n    def __init__(self, interface, base=None):\n        self.__array_interface__ = interface\n        self.base = base\n\n\ndef _maybe_view_as_subclass(original_array, new_array):\n    if type(original_array) is not type(new_array):\n        # if input was an ndarray subclass and subclasses were OK,\n        # then view the result as that subclass.\n        new_array = new_array.view(type=type(original_array))\n        # Since we have done something akin to a view from original_array, we\n        # should let the subclass finalize (if it has it implemented, i.e., is\n        # not None).\n        if new_array.__array_finalize__:\n            new_array.__array_finalize__(original_array)\n    return new_array\n\n\ndef as_strided(x, shape=None, strides=None, subok=False, writeable=True):\n    \"\"\"\n    Create a view into the array with the given shape and strides.\n\n    .. warning:: This function has to be used with extreme care, see notes.\n\n    Parameters\n    ----------\n    x : ndarray\n        Array to create a new.\n    shape : sequence of int, optional\n        The shape of the new array. Defaults to ``x.shape``.\n    strides : sequence of int, optional\n        The strides of the new array. Defaults to ``x.strides``.\n    subok : bool, optional\n        .. versionadded:: 1.10\n\n        If True, subclasses are preserved.\n    writeable : bool, optional\n        .. versionadded:: 1.12\n\n        If set to False, the returned array will always be readonly.\n        Otherwise it will be writable if the original array was. It\n        is advisable to set this to False if possible (see Notes).\n\n    Returns\n    -------\n    view : ndarray\n\n    See also\n    --------\n    broadcast_to : broadcast an array to a given shape.\n    reshape : reshape an array.\n    lib.stride_tricks.sliding_window_view :\n        userfriendly and safe function for the creation of sliding window views.\n\n    Notes\n    -----\n    ``as_strided`` creates a view into the array given the exact strides\n    and shape. This means it manipulates the internal data structure of\n    ndarray and, if done incorrectly, the array elements can point to\n    invalid memory and can corrupt results or crash your program.\n    It is advisable to always use the original ``x.strides`` when\n    calculating new strides to avoid reliance on a contiguous memory\n    layout.\n\n    Furthermore, arrays created with this function often contain self\n    overlapping memory, so that two elements are identical.\n    Vectorized write operations on such arrays will typically be\n    unpredictable. They may even give different results for small, large,\n    or transposed arrays.\n    Since writing to these arrays has to be tested and done with great\n    care, you may want to use ``writeable=False`` to avoid accidental write\n    operations.\n\n    For these reasons it is advisable to avoid ``as_strided`` when\n    possible.\n    \"\"\"\n    # first convert input to array, possibly keeping subclass\n    x = np.array(x, copy=False, subok=subok)\n    interface = dict(x.__array_interface__)\n    if shape is not None:\n        interface['shape'] = tuple(shape)\n    if strides is not None:\n        interface['strides'] = tuple(strides)\n\n    array = np.asarray(DummyArray(interface, base=x))\n    # The route via `__interface__` does not preserve structured\n    # dtypes. Since dtype should remain unchanged, we set it explicitly.\n    array.dtype = x.dtype\n\n    view = _maybe_view_as_subclass(x, array)\n\n    if view.flags.writeable and not writeable:\n        view.flags.writeable = False\n\n    return view\n\n\ndef _sliding_window_view_dispatcher(x, window_shape, axis=None, *,\n                                    subok=None, writeable=None):\n    return (x,)\n\n\n@array_function_dispatch(_sliding_window_view_dispatcher)\ndef sliding_window_view(x, window_shape, axis=None, *,\n                        subok=False, writeable=False):\n    \"\"\"\n    Create a sliding window view into the array with the given window shape.\n\n    Also known as rolling or moving window, the window slides across all\n    dimensions of the array and extracts subsets of the array at all window\n    positions.\n    \n    .. versionadded:: 1.20.0\n\n    Parameters\n    ----------\n    x : array_like\n        Array to create the sliding window view from.\n    window_shape : int or tuple of int\n        Size of window over each axis that takes part in the sliding window.\n        If `axis` is not present, must have same length as the number of input\n        array dimensions. Single integers `i` are treated as if they were the\n        tuple `(i,)`.\n    axis : int or tuple of int, optional\n        Axis or axes along which the sliding window is applied.\n        By default, the sliding window is applied to all axes and\n        `window_shape[i]` will refer to axis `i` of `x`.\n        If `axis` is given as a `tuple of int`, `window_shape[i]` will refer to\n        the axis `axis[i]` of `x`.\n        Single integers `i` are treated as if they were the tuple `(i,)`.\n    subok : bool, optional\n        If True, sub-classes will be passed-through, otherwise the returned\n        array will be forced to be a base-class array (default).\n    writeable : bool, optional\n        When true, allow writing to the returned view. The default is false,\n        as this should be used with caution: the returned view contains the\n        same memory location multiple times, so writing to one location will\n        cause others to change.\n\n    Returns\n    -------\n    view : ndarray\n        Sliding window view of the array. The sliding window dimensions are\n        inserted at the end, and the original dimensions are trimmed as\n        required by the size of the sliding window.\n        That is, ``view.shape = x_shape_trimmed + window_shape``, where\n        ``x_shape_trimmed`` is ``x.shape`` with every entry reduced by one less\n        than the corresponding window size.\n\n    See Also\n    --------\n    lib.stride_tricks.as_strided: A lower-level and less safe routine for\n        creating arbitrary views from custom shape and strides.\n    broadcast_to: broadcast an array to a given shape.\n\n    Notes\n    -----\n    For many applications using a sliding window view can be convenient, but\n    potentially very slow. Often specialized solutions exist, for example:\n\n    - `scipy.signal.fftconvolve`\n\n    - filtering functions in `scipy.ndimage`\n\n    - moving window functions provided by\n      `bottleneck <https://github.com/pydata/bottleneck>`_.\n\n    As a rough estimate, a sliding window approach with an input size of `N`\n    and a window size of `W` will scale as `O(N*W)` where frequently a special\n    algorithm can achieve `O(N)`. That means that the sliding window variant\n    for a window size of 100 can be a 100 times slower than a more specialized\n    version.\n\n    Nevertheless, for small window sizes, when no custom algorithm exists, or\n    as a prototyping and developing tool, this function can be a good solution.\n\n    Examples\n    --------\n    >>> x = np.arange(6)\n    >>> x.shape\n    (6,)\n    >>> v = sliding_window_view(x, 3)\n    >>> v.shape\n    (4, 3)\n    >>> v\n    array([[0, 1, 2],\n           [1, 2, 3],\n           [2, 3, 4],\n           [3, 4, 5]])\n\n    This also works in more dimensions, e.g.\n\n    >>> i, j = np.ogrid[:3, :4]\n    >>> x = 10*i + j\n    >>> x.shape\n    (3, 4)\n    >>> x\n    array([[ 0,  1,  2,  3],\n           [10, 11, 12, 13],\n           [20, 21, 22, 23]])\n    >>> shape = (2,2)\n    >>> v = sliding_window_view(x, shape)\n    >>> v.shape\n    (2, 3, 2, 2)\n    >>> v\n    array([[[[ 0,  1],\n             [10, 11]],\n            [[ 1,  2],\n             [11, 12]],\n            [[ 2,  3],\n             [12, 13]]],\n           [[[10, 11],\n             [20, 21]],\n            [[11, 12],\n             [21, 22]],\n            [[12, 13],\n             [22, 23]]]])\n\n    The axis can be specified explicitly:\n\n    >>> v = sliding_window_view(x, 3, 0)\n    >>> v.shape\n    (1, 4, 3)\n    >>> v\n    array([[[ 0, 10, 20],\n            [ 1, 11, 21],\n            [ 2, 12, 22],\n            [ 3, 13, 23]]])\n\n    The same axis can be used several times. In that case, every use reduces\n    the corresponding original dimension:\n\n    >>> v = sliding_window_view(x, (2, 3), (1, 1))\n    >>> v.shape\n    (3, 1, 2, 3)\n    >>> v\n    array([[[[ 0,  1,  2],\n             [ 1,  2,  3]]],\n           [[[10, 11, 12],\n             [11, 12, 13]]],\n           [[[20, 21, 22],\n             [21, 22, 23]]]])\n\n    Combining with stepped slicing (`::step`), this can be used to take sliding\n    views which skip elements:\n\n    >>> x = np.arange(7)\n    >>> sliding_window_view(x, 5)[:, ::2]\n    array([[0, 2, 4],\n           [1, 3, 5],\n           [2, 4, 6]])\n\n    or views which move by multiple elements\n\n    >>> x = np.arange(7)\n    >>> sliding_window_view(x, 3)[::2, :]\n    array([[0, 1, 2],\n           [2, 3, 4],\n           [4, 5, 6]])\n\n    A common application of `sliding_window_view` is the calculation of running\n    statistics. The simplest example is the\n    `moving average <https://en.wikipedia.org/wiki/Moving_average>`_:\n\n    >>> x = np.arange(6)\n    >>> x.shape\n    (6,)\n    >>> v = sliding_window_view(x, 3)\n    >>> v.shape\n    (4, 3)\n    >>> v\n    array([[0, 1, 2],\n           [1, 2, 3],\n           [2, 3, 4],\n           [3, 4, 5]])\n    >>> moving_average = v.mean(axis=-1)\n    >>> moving_average\n    array([1., 2., 3., 4.])\n\n    Note that a sliding window approach is often **not** optimal (see Notes).\n    \"\"\"\n    window_shape = (tuple(window_shape)\n                    if np.iterable(window_shape)\n                    else (window_shape,))\n    # first convert input to array, possibly keeping subclass\n    x = np.array(x, copy=False, subok=subok)\n\n    window_shape_array = np.array(window_shape)\n    if np.any(window_shape_array < 0):\n        raise ValueError('`window_shape` cannot contain negative values')\n\n    if axis is None:\n        axis = tuple(range(x.ndim))\n        if len(window_shape) != len(axis):\n            raise ValueError(f'Since axis is `None`, must provide '\n                             f'window_shape for all dimensions of `x`; '\n                             f'got {len(window_shape)} window_shape elements '\n                             f'and `x.ndim` is {x.ndim}.')\n    else:\n        axis = normalize_axis_tuple(axis, x.ndim, allow_duplicate=True)\n        if len(window_shape) != len(axis):\n            raise ValueError(f'Must provide matching length window_shape and '\n                             f'axis; got {len(window_shape)} window_shape '\n                             f'elements and {len(axis)} axes elements.')\n\n    out_strides = x.strides + tuple(x.strides[ax] for ax in axis)\n\n    # note: same axis can be windowed repeatedly\n    x_shape_trimmed = list(x.shape)\n    for ax, dim in zip(axis, window_shape):\n        if x_shape_trimmed[ax] < dim:\n            raise ValueError(\n                'window shape cannot be larger than input array shape')\n        x_shape_trimmed[ax] -= dim - 1\n    out_shape = tuple(x_shape_trimmed) + window_shape\n    return as_strided(x, strides=out_strides, shape=out_shape,\n                      subok=subok, writeable=writeable)\n\n\ndef _broadcast_to(array, shape, subok, readonly):\n    shape = tuple(shape) if np.iterable(shape) else (shape,)\n    array = np.array(array, copy=False, subok=subok)\n    if not shape and array.shape:\n        raise ValueError('cannot broadcast a non-scalar to a scalar array')\n    if any(size < 0 for size in shape):\n        raise ValueError('all elements of broadcast shape must be non-'\n                         'negative')\n    extras = []\n    it = np.nditer(\n        (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,\n        op_flags=['readonly'], itershape=shape, order='C')\n    with it:\n        # never really has writebackifcopy semantics\n        broadcast = it.itviews[0]\n    result = _maybe_view_as_subclass(array, broadcast)\n    # In a future version this will go away\n    if not readonly and array.flags._writeable_no_warn:\n        result.flags.writeable = True\n        result.flags._warn_on_write = True\n    return result\n\n\ndef _broadcast_to_dispatcher(array, shape, subok=None):\n    return (array,)\n\n\n@array_function_dispatch(_broadcast_to_dispatcher, module='numpy')\ndef broadcast_to(array, shape, subok=False):\n    \"\"\"Broadcast an array to a new shape.\n\n    Parameters\n    ----------\n    array : array_like\n        The array to broadcast.\n    shape : tuple\n        The shape of the desired array.\n    subok : bool, optional\n        If True, then sub-classes will be passed-through, otherwise\n        the returned array will be forced to be a base-class array (default).\n\n    Returns\n    -------\n    broadcast : array\n        A readonly view on the original array with the given shape. It is\n        typically not contiguous. Furthermore, more than one element of a\n        broadcasted array may refer to a single memory location.\n\n    Raises\n    ------\n    ValueError\n        If the array is not compatible with the new shape according to NumPy's\n        broadcasting rules.\n\n    See Also\n    --------\n    broadcast\n    broadcast_arrays\n    broadcast_shapes\n\n    Notes\n    -----\n    .. versionadded:: 1.10.0\n\n    Examples\n    --------\n    >>> x = np.array([1, 2, 3])\n    >>> np.broadcast_to(x, (3, 3))\n    array([[1, 2, 3],\n           [1, 2, 3],\n           [1, 2, 3]])\n    \"\"\"\n    return _broadcast_to(array, shape, subok=subok, readonly=True)\n\n\ndef _broadcast_shape(*args):\n    \"\"\"Returns the shape of the arrays that would result from broadcasting the\n    supplied arrays against each other.\n    \"\"\"\n    # use the old-iterator because np.nditer does not handle size 0 arrays\n    # consistently\n    b = np.broadcast(*args[:32])\n    # unfortunately, it cannot handle 32 or more arguments directly\n    for pos in range(32, len(args), 31):\n        # ironically, np.broadcast does not properly handle np.broadcast\n        # objects (it treats them as scalars)\n        # use broadcasting to avoid allocating the full array\n        b = broadcast_to(0, b.shape)\n        b = np.broadcast(b, *args[pos:(pos + 31)])\n    return b.shape\n\n\n@set_module('numpy')\ndef broadcast_shapes(*args):\n    \"\"\"\n    Broadcast the input shapes into a single shape.\n\n    :ref:`Learn more about broadcasting here <basics.broadcasting>`.\n\n    .. versionadded:: 1.20.0\n\n    Parameters\n    ----------\n    `*args` : tuples of ints, or ints\n        The shapes to be broadcast against each other.\n\n    Returns\n    -------\n    tuple\n        Broadcasted shape.\n\n    Raises\n    ------\n    ValueError\n        If the shapes are not compatible and cannot be broadcast according\n        to NumPy's broadcasting rules.\n\n    See Also\n    --------\n    broadcast\n    broadcast_arrays\n    broadcast_to\n\n    Examples\n    --------\n    >>> np.broadcast_shapes((1, 2), (3, 1), (3, 2))\n    (3, 2)\n\n    >>> np.broadcast_shapes((6, 7), (5, 6, 1), (7,), (5, 1, 7))\n    (5, 6, 7)\n    \"\"\"\n    arrays = [np.empty(x, dtype=[]) for x in args]\n    return _broadcast_shape(*arrays)\n\n\ndef _broadcast_arrays_dispatcher(*args, subok=None):\n    return args\n\n\n@array_function_dispatch(_broadcast_arrays_dispatcher, module='numpy')\ndef broadcast_arrays(*args, subok=False):\n    \"\"\"\n    Broadcast any number of arrays against each other.\n\n    Parameters\n    ----------\n    `*args` : array_likes\n        The arrays to broadcast.\n\n    subok : bool, optional\n        If True, then sub-classes will be passed-through, otherwise\n        the returned arrays will be forced to be a base-class array (default).\n\n    Returns\n    -------\n    broadcasted : list of arrays\n        These arrays are views on the original arrays.  They are typically\n        not contiguous.  Furthermore, more than one element of a\n        broadcasted array may refer to a single memory location. If you need\n        to write to the arrays, make copies first. While you can set the\n        ``writable`` flag True, writing to a single output value may end up\n        changing more than one location in the output array.\n\n        .. deprecated:: 1.17\n            The output is currently marked so that if written to, a deprecation\n            warning will be emitted. A future version will set the\n            ``writable`` flag False so writing to it will raise an error.\n\n    See Also\n    --------\n    broadcast\n    broadcast_to\n    broadcast_shapes\n\n    Examples\n    --------\n    >>> x = np.array([[1,2,3]])\n    >>> y = np.array([[4],[5]])\n    >>> np.broadcast_arrays(x, y)\n    [array([[1, 2, 3],\n           [1, 2, 3]]), array([[4, 4, 4],\n           [5, 5, 5]])]\n\n    Here is a useful idiom for getting contiguous copies instead of\n    non-contiguous views.\n\n    >>> [np.array(a) for a in np.broadcast_arrays(x, y)]\n    [array([[1, 2, 3],\n           [1, 2, 3]]), array([[4, 4, 4],\n           [5, 5, 5]])]\n\n    \"\"\"\n    # nditer is not used here to avoid the limit of 32 arrays.\n    # Otherwise, something like the following one-liner would suffice:\n    # return np.nditer(args, flags=['multi_index', 'zerosize_ok'],\n    #                  order='C').itviews\n\n    args = [np.array(_m, copy=False, subok=subok) for _m in args]\n\n    shape = _broadcast_shape(*args)\n\n    if all(array.shape == shape for array in args):\n        # Common case where nothing needs to be broadcasted.\n        return args\n\n    return [_broadcast_to(array, shape, subok=subok, readonly=False)\n            for array in args]\n",545],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py":["import functools\nimport sys\nimport math\nimport warnings\n\nimport numpy.core.numeric as _nx\nfrom numpy.core.numeric import (\n    asarray, ScalarType, array, alltrue, cumprod, arange, ndim\n    )\nfrom numpy.core.numerictypes import find_common_type, issubdtype\n\nimport numpy.matrixlib as matrixlib\nfrom .function_base import diff\nfrom numpy.core.multiarray import ravel_multi_index, unravel_index\nfrom numpy.core.overrides import set_module\nfrom numpy.core import overrides, linspace\nfrom numpy.lib.stride_tricks import as_strided\n\n\narray_function_dispatch = functools.partial(\n    overrides.array_function_dispatch, module='numpy')\n\n\n__all__ = [\n    'ravel_multi_index', 'unravel_index', 'mgrid', 'ogrid', 'r_', 'c_',\n    's_', 'index_exp', 'ix_', 'ndenumerate', 'ndindex', 'fill_diagonal',\n    'diag_indices', 'diag_indices_from'\n    ]\n\n\ndef _ix__dispatcher(*args):\n    return args\n\n\n@array_function_dispatch(_ix__dispatcher)\ndef ix_(*args):\n    \"\"\"\n    Construct an open mesh from multiple sequences.\n\n    This function takes N 1-D sequences and returns N outputs with N\n    dimensions each, such that the shape is 1 in all but one dimension\n    and the dimension with the non-unit shape value cycles through all\n    N dimensions.\n\n    Using `ix_` one can quickly construct index arrays that will index\n    the cross product. ``a[np.ix_([1,3],[2,5])]`` returns the array\n    ``[[a[1,2] a[1,5]], [a[3,2] a[3,5]]]``.\n\n    Parameters\n    ----------\n    args : 1-D sequences\n        Each sequence should be of integer or boolean type.\n        Boolean sequences will be interpreted as boolean masks for the\n        corresponding dimension (equivalent to passing in\n        ``np.nonzero(boolean_sequence)``).\n\n    Returns\n    -------\n    out : tuple of ndarrays\n        N arrays with N dimensions each, with N the number of input\n        sequences. Together these arrays form an open mesh.\n\n    See Also\n    --------\n    ogrid, mgrid, meshgrid\n\n    Examples\n    --------\n    >>> a = np.arange(10).reshape(2, 5)\n    >>> a\n    array([[0, 1, 2, 3, 4],\n           [5, 6, 7, 8, 9]])\n    >>> ixgrid = np.ix_([0, 1], [2, 4])\n    >>> ixgrid\n    (array([[0],\n           [1]]), array([[2, 4]]))\n    >>> ixgrid[0].shape, ixgrid[1].shape\n    ((2, 1), (1, 2))\n    >>> a[ixgrid]\n    array([[2, 4],\n           [7, 9]])\n\n    >>> ixgrid = np.ix_([True, True], [2, 4])\n    >>> a[ixgrid]\n    array([[2, 4],\n           [7, 9]])\n    >>> ixgrid = np.ix_([True, True], [False, False, True, False, True])\n    >>> a[ixgrid]\n    array([[2, 4],\n           [7, 9]])\n\n    \"\"\"\n    out = []\n    nd = len(args)\n    for k, new in enumerate(args):\n        if not isinstance(new, _nx.ndarray):\n            new = asarray(new)\n            if new.size == 0:\n                # Explicitly type empty arrays to avoid float default\n                new = new.astype(_nx.intp)\n        if new.ndim != 1:\n            raise ValueError(\"Cross index must be 1 dimensional\")\n        if issubdtype(new.dtype, _nx.bool_):\n            new, = new.nonzero()\n        new = new.reshape((1,)*k + (new.size,) + (1,)*(nd-k-1))\n        out.append(new)\n    return tuple(out)\n\nclass nd_grid:\n    \"\"\"\n    Construct a multi-dimensional \"meshgrid\".\n\n    ``grid = nd_grid()`` creates an instance which will return a mesh-grid\n    when indexed.  The dimension and number of the output arrays are equal\n    to the number of indexing dimensions.  If the step length is not a\n    complex number, then the stop is not inclusive.\n\n    However, if the step length is a **complex number** (e.g. 5j), then the\n    integer part of its magnitude is interpreted as specifying the\n    number of points to create between the start and stop values, where\n    the stop value **is inclusive**.\n\n    If instantiated with an argument of ``sparse=True``, the mesh-grid is\n    open (or not fleshed out) so that only one-dimension of each returned\n    argument is greater than 1.\n\n    Parameters\n    ----------\n    sparse : bool, optional\n        Whether the grid is sparse or not. Default is False.\n\n    Notes\n    -----\n    Two instances of `nd_grid` are made available in the NumPy namespace,\n    `mgrid` and `ogrid`, approximately defined as::\n\n        mgrid = nd_grid(sparse=False)\n        ogrid = nd_grid(sparse=True)\n\n    Users should use these pre-defined instances instead of using `nd_grid`\n    directly.\n    \"\"\"\n\n    def __init__(self, sparse=False):\n        self.sparse = sparse\n\n    def __getitem__(self, key):\n        try:\n            size = []\n            typ = int\n            for k in range(len(key)):\n                step = key[k].step\n                start = key[k].start\n                if start is None:\n                    start = 0\n                if step is None:\n                    step = 1\n                if isinstance(step, (_nx.complexfloating, complex)):\n                    size.append(int(abs(step)))\n                    typ = float\n                else:\n                    size.append(\n                        int(math.ceil((key[k].stop - start)/(step*1.0))))\n                if (isinstance(step, (_nx.floating, float)) or\n                        isinstance(start, (_nx.floating, float)) or\n                        isinstance(key[k].stop, (_nx.floating, float))):\n                    typ = float\n            if self.sparse:\n                nn = [_nx.arange(_x, dtype=_t)\n                        for _x, _t in zip(size, (typ,)*len(size))]\n            else:\n                nn = _nx.indices(size, typ)\n            for k in range(len(size)):\n                step = key[k].step\n                start = key[k].start\n                if start is None:\n                    start = 0\n                if step is None:\n                    step = 1\n                if isinstance(step, (_nx.complexfloating, complex)):\n                    step = int(abs(step))\n                    if step != 1:\n                        step = (key[k].stop - start)/float(step-1)\n                nn[k] = (nn[k]*step+start)\n            if self.sparse:\n                slobj = [_nx.newaxis]*len(size)\n                for k in range(len(size)):\n                    slobj[k] = slice(None, None)\n                    nn[k] = nn[k][tuple(slobj)]\n                    slobj[k] = _nx.newaxis\n            return nn\n        except (IndexError, TypeError):\n            step = key.step\n            stop = key.stop\n            start = key.start\n            if start is None:\n                start = 0\n            if isinstance(step, (_nx.complexfloating, complex)):\n                step = abs(step)\n                length = int(step)\n                if step != 1:\n                    step = (key.stop-start)/float(step-1)\n                stop = key.stop + step\n                return _nx.arange(0, length, 1, float)*step + start\n            else:\n                return _nx.arange(start, stop, step)\n\n\nclass MGridClass(nd_grid):\n    \"\"\"\n    `nd_grid` instance which returns a dense multi-dimensional \"meshgrid\".\n\n    An instance of `numpy.lib.index_tricks.nd_grid` which returns an dense\n    (or fleshed out) mesh-grid when indexed, so that each returned argument\n    has the same shape.  The dimensions and number of the output arrays are\n    equal to the number of indexing dimensions.  If the step length is not a\n    complex number, then the stop is not inclusive.\n\n    However, if the step length is a **complex number** (e.g. 5j), then\n    the integer part of its magnitude is interpreted as specifying the\n    number of points to create between the start and stop values, where\n    the stop value **is inclusive**.\n\n    Returns\n    -------\n    mesh-grid `ndarrays` all of the same dimensions\n\n    See Also\n    --------\n    numpy.lib.index_tricks.nd_grid : class of `ogrid` and `mgrid` objects\n    ogrid : like mgrid but returns open (not fleshed out) mesh grids\n    r_ : array concatenator\n\n    Examples\n    --------\n    >>> np.mgrid[0:5,0:5]\n    array([[[0, 0, 0, 0, 0],\n            [1, 1, 1, 1, 1],\n            [2, 2, 2, 2, 2],\n            [3, 3, 3, 3, 3],\n            [4, 4, 4, 4, 4]],\n           [[0, 1, 2, 3, 4],\n            [0, 1, 2, 3, 4],\n            [0, 1, 2, 3, 4],\n            [0, 1, 2, 3, 4],\n            [0, 1, 2, 3, 4]]])\n    >>> np.mgrid[-1:1:5j]\n    array([-1. , -0.5,  0. ,  0.5,  1. ])\n\n    \"\"\"\n    def __init__(self):\n        super(MGridClass, self).__init__(sparse=False)\n\nmgrid = MGridClass()\n\nclass OGridClass(nd_grid):\n    \"\"\"\n    `nd_grid` instance which returns an open multi-dimensional \"meshgrid\".\n\n    An instance of `numpy.lib.index_tricks.nd_grid` which returns an open\n    (i.e. not fleshed out) mesh-grid when indexed, so that only one dimension\n    of each returned array is greater than 1.  The dimension and number of the\n    output arrays are equal to the number of indexing dimensions.  If the step\n    length is not a complex number, then the stop is not inclusive.\n\n    However, if the step length is a **complex number** (e.g. 5j), then\n    the integer part of its magnitude is interpreted as specifying the\n    number of points to create between the start and stop values, where\n    the stop value **is inclusive**.\n\n    Returns\n    -------\n    mesh-grid\n        `ndarrays` with only one dimension not equal to 1\n\n    See Also\n    --------\n    np.lib.index_tricks.nd_grid : class of `ogrid` and `mgrid` objects\n    mgrid : like `ogrid` but returns dense (or fleshed out) mesh grids\n    r_ : array concatenator\n\n    Examples\n    --------\n    >>> from numpy import ogrid\n    >>> ogrid[-1:1:5j]\n    array([-1. , -0.5,  0. ,  0.5,  1. ])\n    >>> ogrid[0:5,0:5]\n    [array([[0],\n            [1],\n            [2],\n            [3],\n            [4]]), array([[0, 1, 2, 3, 4]])]\n\n    \"\"\"\n    def __init__(self):\n        super(OGridClass, self).__init__(sparse=True)\n\nogrid = OGridClass()\n\n\nclass AxisConcatenator:\n    \"\"\"\n    Translates slice objects to concatenation along an axis.\n\n    For detailed documentation on usage, see `r_`.\n    \"\"\"\n    # allow ma.mr_ to override this\n    concatenate = staticmethod(_nx.concatenate)\n    makemat = staticmethod(matrixlib.matrix)\n\n    def __init__(self, axis=0, matrix=False, ndmin=1, trans1d=-1):\n        self.axis = axis\n        self.matrix = matrix\n        self.trans1d = trans1d\n        self.ndmin = ndmin\n\n    def __getitem__(self, key):\n        # handle matrix builder syntax\n        if isinstance(key, str):\n            frame = sys._getframe().f_back\n            mymat = matrixlib.bmat(key, frame.f_globals, frame.f_locals)\n            return mymat\n\n        if not isinstance(key, tuple):\n            key = (key,)\n\n        # copy attributes, since they can be overridden in the first argument\n        trans1d = self.trans1d\n        ndmin = self.ndmin\n        matrix = self.matrix\n        axis = self.axis\n\n        objs = []\n        scalars = []\n        arraytypes = []\n        scalartypes = []\n\n        for k, item in enumerate(key):\n            scalar = False\n            if isinstance(item, slice):\n                step = item.step\n                start = item.start\n                stop = item.stop\n                if start is None:\n                    start = 0\n                if step is None:\n                    step = 1\n                if isinstance(step, (_nx.complexfloating, complex)):\n                    size = int(abs(step))\n                    newobj = linspace(start, stop, num=size)\n                else:\n                    newobj = _nx.arange(start, stop, step)\n                if ndmin > 1:\n                    newobj = array(newobj, copy=False, ndmin=ndmin)\n                    if trans1d != -1:\n                        newobj = newobj.swapaxes(-1, trans1d)\n            elif isinstance(item, str):\n                if k != 0:\n                    raise ValueError(\"special directives must be the \"\n                            \"first entry.\")\n                if item in ('r', 'c'):\n                    matrix = True\n                    col = (item == 'c')\n                    continue\n                if ',' in item:\n                    vec = item.split(',')\n                    try:\n                        axis, ndmin = [int(x) for x in vec[:2]]\n                        if len(vec) == 3:\n                            trans1d = int(vec[2])\n                        continue\n                    except Exception as e:\n                        raise ValueError(\n                            \"unknown special directive {!r}\".format(item)\n                        ) from e\n                try:\n                    axis = int(item)\n                    continue\n                except (ValueError, TypeError):\n                    raise ValueError(\"unknown special directive\")\n            elif type(item) in ScalarType:\n                newobj = array(item, ndmin=ndmin)\n                scalars.append(len(objs))\n                scalar = True\n                scalartypes.append(newobj.dtype)\n            else:\n                item_ndim = ndim(item)\n                newobj = array(item, copy=False, subok=True, ndmin=ndmin)\n                if trans1d != -1 and item_ndim < ndmin:\n                    k2 = ndmin - item_ndim\n                    k1 = trans1d\n                    if k1 < 0:\n                        k1 += k2 + 1\n                    defaxes = list(range(ndmin))\n                    axes = defaxes[:k1] + defaxes[k2:] + defaxes[k1:k2]\n                    newobj = newobj.transpose(axes)\n            objs.append(newobj)\n            if not scalar and isinstance(newobj, _nx.ndarray):\n                arraytypes.append(newobj.dtype)\n\n        # Ensure that scalars won't up-cast unless warranted\n        final_dtype = find_common_type(arraytypes, scalartypes)\n        if final_dtype is not None:\n            for k in scalars:\n                objs[k] = objs[k].astype(final_dtype)\n\n        res = self.concatenate(tuple(objs), axis=axis)\n\n        if matrix:\n            oldndim = res.ndim\n            res = self.makemat(res)\n            if oldndim == 1 and col:\n                res = res.T\n        return res\n\n    def __len__(self):\n        return 0\n\n# separate classes are used here instead of just making r_ = concatentor(0),\n# etc. because otherwise we couldn't get the doc string to come out right\n# in help(r_)\n\nclass RClass(AxisConcatenator):\n    \"\"\"\n    Translates slice objects to concatenation along the first axis.\n\n    This is a simple way to build up arrays quickly. There are two use cases.\n\n    1. If the index expression contains comma separated arrays, then stack\n       them along their first axis.\n    2. If the index expression contains slice notation or scalars then create\n       a 1-D array with a range indicated by the slice notation.\n\n    If slice notation is used, the syntax ``start:stop:step`` is equivalent\n    to ``np.arange(start, stop, step)`` inside of the brackets. However, if\n    ``step`` is an imaginary number (i.e. 100j) then its integer portion is\n    interpreted as a number-of-points desired and the start and stop are\n    inclusive. In other words ``start:stop:stepj`` is interpreted as\n    ``np.linspace(start, stop, step, endpoint=1)`` inside of the brackets.\n    After expansion of slice notation, all comma separated sequences are\n    concatenated together.\n\n    Optional character strings placed as the first element of the index\n    expression can be used to change the output. The strings 'r' or 'c' result\n    in matrix output. If the result is 1-D and 'r' is specified a 1 x N (row)\n    matrix is produced. If the result is 1-D and 'c' is specified, then a N x 1\n    (column) matrix is produced. If the result is 2-D then both provide the\n    same matrix result.\n\n    A string integer specifies which axis to stack multiple comma separated\n    arrays along. A string of two comma-separated integers allows indication\n    of the minimum number of dimensions to force each entry into as the\n    second integer (the axis to concatenate along is still the first integer).\n\n    A string with three comma-separated integers allows specification of the\n    axis to concatenate along, the minimum number of dimensions to force the\n    entries to, and which axis should contain the start of the arrays which\n    are less than the specified number of dimensions. In other words the third\n    integer allows you to specify where the 1's should be placed in the shape\n    of the arrays that have their shapes upgraded. By default, they are placed\n    in the front of the shape tuple. The third argument allows you to specify\n    where the start of the array should be instead. Thus, a third argument of\n    '0' would place the 1's at the end of the array shape. Negative integers\n    specify where in the new shape tuple the last dimension of upgraded arrays\n    should be placed, so the default is '-1'.\n\n    Parameters\n    ----------\n    Not a function, so takes no parameters\n\n\n    Returns\n    -------\n    A concatenated ndarray or matrix.\n\n    See Also\n    --------\n    concatenate : Join a sequence of arrays along an existing axis.\n    c_ : Translates slice objects to concatenation along the second axis.\n\n    Examples\n    --------\n    >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]\n    array([1, 2, 3, ..., 4, 5, 6])\n    >>> np.r_[-1:1:6j, [0]*3, 5, 6]\n    array([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])\n\n    String integers specify the axis to concatenate along or the minimum\n    number of dimensions to force entries into.\n\n    >>> a = np.array([[0, 1, 2], [3, 4, 5]])\n    >>> np.r_['-1', a, a] # concatenate along last axis\n    array([[0, 1, 2, 0, 1, 2],\n           [3, 4, 5, 3, 4, 5]])\n    >>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2\n    array([[1, 2, 3],\n           [4, 5, 6]])\n\n    >>> np.r_['0,2,0', [1,2,3], [4,5,6]]\n    array([[1],\n           [2],\n           [3],\n           [4],\n           [5],\n           [6]])\n    >>> np.r_['1,2,0', [1,2,3], [4,5,6]]\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n\n    Using 'r' or 'c' as a first string argument creates a matrix.\n\n    >>> np.r_['r',[1,2,3], [4,5,6]]\n    matrix([[1, 2, 3, 4, 5, 6]])\n\n    \"\"\"\n\n    def __init__(self):\n        AxisConcatenator.__init__(self, 0)\n\nr_ = RClass()\n\nclass CClass(AxisConcatenator):\n    \"\"\"\n    Translates slice objects to concatenation along the second axis.\n\n    This is short-hand for ``np.r_['-1,2,0', index expression]``, which is\n    useful because of its common occurrence. In particular, arrays will be\n    stacked along their last axis after being upgraded to at least 2-D with\n    1's post-pended to the shape (column vectors made out of 1-D arrays).\n    \n    See Also\n    --------\n    column_stack : Stack 1-D arrays as columns into a 2-D array.\n    r_ : For more detailed documentation.\n\n    Examples\n    --------\n    >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]\n    array([[1, 4],\n           [2, 5],\n           [3, 6]])\n    >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n    array([[1, 2, 3, ..., 4, 5, 6]])\n\n    \"\"\"\n\n    def __init__(self):\n        AxisConcatenator.__init__(self, -1, ndmin=2, trans1d=0)\n\n\nc_ = CClass()\n\n\n@set_module('numpy')\nclass ndenumerate:\n    \"\"\"\n    Multidimensional index iterator.\n\n    Return an iterator yielding pairs of array coordinates and values.\n\n    Parameters\n    ----------\n    arr : ndarray\n      Input array.\n\n    See Also\n    --------\n    ndindex, flatiter\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> for index, x in np.ndenumerate(a):\n    ...     print(index, x)\n    (0, 0) 1\n    (0, 1) 2\n    (1, 0) 3\n    (1, 1) 4\n\n    \"\"\"\n\n    def __init__(self, arr):\n        self.iter = asarray(arr).flat\n\n    def __next__(self):\n        \"\"\"\n        Standard iterator method, returns the index tuple and array value.\n\n        Returns\n        -------\n        coords : tuple of ints\n            The indices of the current iteration.\n        val : scalar\n            The array element of the current iteration.\n\n        \"\"\"\n        return self.iter.coords, next(self.iter)\n\n    def __iter__(self):\n        return self\n\n\n@set_module('numpy')\nclass ndindex:\n    \"\"\"\n    An N-dimensional iterator object to index arrays.\n\n    Given the shape of an array, an `ndindex` instance iterates over\n    the N-dimensional index of the array. At each iteration a tuple\n    of indices is returned, the last dimension is iterated over first.\n\n    Parameters\n    ----------\n    shape : ints, or a single tuple of ints\n        The size of each dimension of the array can be passed as \n        individual parameters or as the elements of a tuple.\n\n    See Also\n    --------\n    ndenumerate, flatiter\n\n    Examples\n    --------\n    # dimensions as individual arguments\n    >>> for index in np.ndindex(3, 2, 1):\n    ...     print(index)\n    (0, 0, 0)\n    (0, 1, 0)\n    (1, 0, 0)\n    (1, 1, 0)\n    (2, 0, 0)\n    (2, 1, 0)\n\n    # same dimensions - but in a tuple (3, 2, 1)\n    >>> for index in np.ndindex((3, 2, 1)):\n    ...     print(index)\n    (0, 0, 0)\n    (0, 1, 0)\n    (1, 0, 0)\n    (1, 1, 0)\n    (2, 0, 0)\n    (2, 1, 0)\n\n    \"\"\"\n\n    def __init__(self, *shape):\n        if len(shape) == 1 and isinstance(shape[0], tuple):\n            shape = shape[0]\n        x = as_strided(_nx.zeros(1), shape=shape,\n                       strides=_nx.zeros_like(shape))\n        self._it = _nx.nditer(x, flags=['multi_index', 'zerosize_ok'],\n                              order='C')\n\n    def __iter__(self):\n        return self\n\n    def ndincr(self):\n        \"\"\"\n        Increment the multi-dimensional index by one.\n\n        This method is for backward compatibility only: do not use.\n\n        .. deprecated:: 1.20.0\n            This method has been advised against since numpy 1.8.0, but only\n            started emitting DeprecationWarning as of this version.\n        \"\"\"\n        # NumPy 1.20.0, 2020-09-08\n        warnings.warn(\n            \"`ndindex.ndincr()` is deprecated, use `next(ndindex)` instead\",\n            DeprecationWarning, stacklevel=2)\n        next(self)\n\n    def __next__(self):\n        \"\"\"\n        Standard iterator method, updates the index and returns the index\n        tuple.\n\n        Returns\n        -------\n        val : tuple of ints\n            Returns a tuple containing the indices of the current\n            iteration.\n\n        \"\"\"\n        next(self._it)\n        return self._it.multi_index\n\n\n# You can do all this with slice() plus a few special objects,\n# but there's a lot to remember. This version is simpler because\n# it uses the standard array indexing syntax.\n#\n# Written by Konrad Hinsen <hinsen@cnrs-orleans.fr>\n# last revision: 1999-7-23\n#\n# Cosmetic changes by T. Oliphant 2001\n#\n#\n\nclass IndexExpression:\n    \"\"\"\n    A nicer way to build up index tuples for arrays.\n\n    .. note::\n       Use one of the two predefined instances `index_exp` or `s_`\n       rather than directly using `IndexExpression`.\n\n    For any index combination, including slicing and axis insertion,\n    ``a[indices]`` is the same as ``a[np.index_exp[indices]]`` for any\n    array `a`. However, ``np.index_exp[indices]`` can be used anywhere\n    in Python code and returns a tuple of slice objects that can be\n    used in the construction of complex index expressions.\n\n    Parameters\n    ----------\n    maketuple : bool\n        If True, always returns a tuple.\n\n    See Also\n    --------\n    index_exp : Predefined instance that always returns a tuple:\n       `index_exp = IndexExpression(maketuple=True)`.\n    s_ : Predefined instance without tuple conversion:\n       `s_ = IndexExpression(maketuple=False)`.\n\n    Notes\n    -----\n    You can do all this with `slice()` plus a few special objects,\n    but there's a lot to remember and this version is simpler because\n    it uses the standard array indexing syntax.\n\n    Examples\n    --------\n    >>> np.s_[2::2]\n    slice(2, None, 2)\n    >>> np.index_exp[2::2]\n    (slice(2, None, 2),)\n\n    >>> np.array([0, 1, 2, 3, 4])[np.s_[2::2]]\n    array([2, 4])\n\n    \"\"\"\n\n    def __init__(self, maketuple):\n        self.maketuple = maketuple\n\n    def __getitem__(self, item):\n        if self.maketuple and not isinstance(item, tuple):\n            return (item,)\n        else:\n            return item\n\nindex_exp = IndexExpression(maketuple=True)\ns_ = IndexExpression(maketuple=False)\n\n# End contribution from Konrad.\n\n\n# The following functions complement those in twodim_base, but are\n# applicable to N-dimensions.\n\n\ndef _fill_diagonal_dispatcher(a, val, wrap=None):\n    return (a,)\n\n\n@array_function_dispatch(_fill_diagonal_dispatcher)\ndef fill_diagonal(a, val, wrap=False):\n    \"\"\"Fill the main diagonal of the given array of any dimensionality.\n\n    For an array `a` with ``a.ndim >= 2``, the diagonal is the list of\n    locations with indices ``a[i, ..., i]`` all identical. This function\n    modifies the input array in-place, it does not return a value.\n\n    Parameters\n    ----------\n    a : array, at least 2-D.\n      Array whose diagonal is to be filled, it gets modified in-place.\n\n    val : scalar or array_like\n      Value(s) to write on the diagonal. If `val` is scalar, the value is\n      written along the diagonal. If array-like, the flattened `val` is\n      written along the diagonal, repeating if necessary to fill all\n      diagonal entries.\n\n    wrap : bool\n      For tall matrices in NumPy version up to 1.6.2, the\n      diagonal \"wrapped\" after N columns. You can have this behavior\n      with this option. This affects only tall matrices.\n\n    See also\n    --------\n    diag_indices, diag_indices_from\n\n    Notes\n    -----\n    .. versionadded:: 1.4.0\n\n    This functionality can be obtained via `diag_indices`, but internally\n    this version uses a much faster implementation that never constructs the\n    indices and uses simple slicing.\n\n    Examples\n    --------\n    >>> a = np.zeros((3, 3), int)\n    >>> np.fill_diagonal(a, 5)\n    >>> a\n    array([[5, 0, 0],\n           [0, 5, 0],\n           [0, 0, 5]])\n\n    The same function can operate on a 4-D array:\n\n    >>> a = np.zeros((3, 3, 3, 3), int)\n    >>> np.fill_diagonal(a, 4)\n\n    We only show a few blocks for clarity:\n\n    >>> a[0, 0]\n    array([[4, 0, 0],\n           [0, 0, 0],\n           [0, 0, 0]])\n    >>> a[1, 1]\n    array([[0, 0, 0],\n           [0, 4, 0],\n           [0, 0, 0]])\n    >>> a[2, 2]\n    array([[0, 0, 0],\n           [0, 0, 0],\n           [0, 0, 4]])\n\n    The wrap option affects only tall matrices:\n\n    >>> # tall matrices no wrap\n    >>> a = np.zeros((5, 3), int)\n    >>> np.fill_diagonal(a, 4)\n    >>> a\n    array([[4, 0, 0],\n           [0, 4, 0],\n           [0, 0, 4],\n           [0, 0, 0],\n           [0, 0, 0]])\n\n    >>> # tall matrices wrap\n    >>> a = np.zeros((5, 3), int)\n    >>> np.fill_diagonal(a, 4, wrap=True)\n    >>> a\n    array([[4, 0, 0],\n           [0, 4, 0],\n           [0, 0, 4],\n           [0, 0, 0],\n           [4, 0, 0]])\n\n    >>> # wide matrices\n    >>> a = np.zeros((3, 5), int)\n    >>> np.fill_diagonal(a, 4, wrap=True)\n    >>> a\n    array([[4, 0, 0, 0, 0],\n           [0, 4, 0, 0, 0],\n           [0, 0, 4, 0, 0]])\n\n    The anti-diagonal can be filled by reversing the order of elements\n    using either `numpy.flipud` or `numpy.fliplr`.\n\n    >>> a = np.zeros((3, 3), int);\n    >>> np.fill_diagonal(np.fliplr(a), [1,2,3])  # Horizontal flip\n    >>> a\n    array([[0, 0, 1],\n           [0, 2, 0],\n           [3, 0, 0]])\n    >>> np.fill_diagonal(np.flipud(a), [1,2,3])  # Vertical flip\n    >>> a\n    array([[0, 0, 3],\n           [0, 2, 0],\n           [1, 0, 0]])\n\n    Note that the order in which the diagonal is filled varies depending\n    on the flip function.\n    \"\"\"\n    if a.ndim < 2:\n        raise ValueError(\"array must be at least 2-d\")\n    end = None\n    if a.ndim == 2:\n        # Explicit, fast formula for the common case.  For 2-d arrays, we\n        # accept rectangular ones.\n        step = a.shape[1] + 1\n        #This is needed to don't have tall matrix have the diagonal wrap.\n        if not wrap:\n            end = a.shape[1] * a.shape[1]\n    else:\n        # For more than d=2, the strided formula is only valid for arrays with\n        # all dimensions equal, so we check first.\n        if not alltrue(diff(a.shape) == 0):\n            raise ValueError(\"All dimensions of input must be of equal length\")\n        step = 1 + (cumprod(a.shape[:-1])).sum()\n\n    # Write the value out into the diagonal.\n    a.flat[:end:step] = val\n\n\n@set_module('numpy')\ndef diag_indices(n, ndim=2):\n    \"\"\"\n    Return the indices to access the main diagonal of an array.\n\n    This returns a tuple of indices that can be used to access the main\n    diagonal of an array `a` with ``a.ndim >= 2`` dimensions and shape\n    (n, n, ..., n). For ``a.ndim = 2`` this is the usual diagonal, for\n    ``a.ndim > 2`` this is the set of indices to access ``a[i, i, ..., i]``\n    for ``i = [0..n-1]``.\n\n    Parameters\n    ----------\n    n : int\n      The size, along each dimension, of the arrays for which the returned\n      indices can be used.\n\n    ndim : int, optional\n      The number of dimensions.\n\n    See Also\n    --------\n    diag_indices_from\n\n    Notes\n    -----\n    .. versionadded:: 1.4.0\n\n    Examples\n    --------\n    Create a set of indices to access the diagonal of a (4, 4) array:\n\n    >>> di = np.diag_indices(4)\n    >>> di\n    (array([0, 1, 2, 3]), array([0, 1, 2, 3]))\n    >>> a = np.arange(16).reshape(4, 4)\n    >>> a\n    array([[ 0,  1,  2,  3],\n           [ 4,  5,  6,  7],\n           [ 8,  9, 10, 11],\n           [12, 13, 14, 15]])\n    >>> a[di] = 100\n    >>> a\n    array([[100,   1,   2,   3],\n           [  4, 100,   6,   7],\n           [  8,   9, 100,  11],\n           [ 12,  13,  14, 100]])\n\n    Now, we create indices to manipulate a 3-D array:\n\n    >>> d3 = np.diag_indices(2, 3)\n    >>> d3\n    (array([0, 1]), array([0, 1]), array([0, 1]))\n\n    And use it to set the diagonal of an array of zeros to 1:\n\n    >>> a = np.zeros((2, 2, 2), dtype=int)\n    >>> a[d3] = 1\n    >>> a\n    array([[[1, 0],\n            [0, 0]],\n           [[0, 0],\n            [0, 1]]])\n\n    \"\"\"\n    idx = arange(n)\n    return (idx,) * ndim\n\n\ndef _diag_indices_from(arr):\n    return (arr,)\n\n\n@array_function_dispatch(_diag_indices_from)\ndef diag_indices_from(arr):\n    \"\"\"\n    Return the indices to access the main diagonal of an n-dimensional array.\n\n    See `diag_indices` for full details.\n\n    Parameters\n    ----------\n    arr : array, at least 2-D\n\n    See Also\n    --------\n    diag_indices\n\n    Notes\n    -----\n    .. versionadded:: 1.4.0\n\n    \"\"\"\n\n    if not arr.ndim >= 2:\n        raise ValueError(\"input array must be at least 2-d\")\n    # For more than d=2, the strided formula is only valid for arrays with\n    # all dimensions equal, so we check first.\n    if not alltrue(diff(arr.shape) == 0):\n        raise ValueError(\"All dimensions of input must be of equal length\")\n\n    return diag_indices(arr.shape[0], arr.ndim)\n",1003],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py":["\"\"\"\nSet operations for arrays based on sorting.\n\nNotes\n-----\n\nFor floating point arrays, inaccurate results may appear due to usual round-off\nand floating point comparison issues.\n\nSpeed could be gained in some operations by an implementation of\n`numpy.sort`, that can provide directly the permutation vectors, thus avoiding\ncalls to `numpy.argsort`.\n\nOriginal author: Robert Cimrman\n\n\"\"\"\nimport functools\n\nimport numpy as np\nfrom numpy.core import overrides\n\n\narray_function_dispatch = functools.partial(\n    overrides.array_function_dispatch, module='numpy')\n\n\n__all__ = [\n    'ediff1d', 'intersect1d', 'setxor1d', 'union1d', 'setdiff1d', 'unique',\n    'in1d', 'isin'\n    ]\n\n\ndef _ediff1d_dispatcher(ary, to_end=None, to_begin=None):\n    return (ary, to_end, to_begin)\n\n\n@array_function_dispatch(_ediff1d_dispatcher)\ndef ediff1d(ary, to_end=None, to_begin=None):\n    \"\"\"\n    The differences between consecutive elements of an array.\n\n    Parameters\n    ----------\n    ary : array_like\n        If necessary, will be flattened before the differences are taken.\n    to_end : array_like, optional\n        Number(s) to append at the end of the returned differences.\n    to_begin : array_like, optional\n        Number(s) to prepend at the beginning of the returned differences.\n\n    Returns\n    -------\n    ediff1d : ndarray\n        The differences. Loosely, this is ``ary.flat[1:] - ary.flat[:-1]``.\n\n    See Also\n    --------\n    diff, gradient\n\n    Notes\n    -----\n    When applied to masked arrays, this function drops the mask information\n    if the `to_begin` and/or `to_end` parameters are used.\n\n    Examples\n    --------\n    >>> x = np.array([1, 2, 4, 7, 0])\n    >>> np.ediff1d(x)\n    array([ 1,  2,  3, -7])\n\n    >>> np.ediff1d(x, to_begin=-99, to_end=np.array([88, 99]))\n    array([-99,   1,   2, ...,  -7,  88,  99])\n\n    The returned array is always 1D.\n\n    >>> y = [[1, 2, 4], [1, 6, 24]]\n    >>> np.ediff1d(y)\n    array([ 1,  2, -3,  5, 18])\n\n    \"\"\"\n    # force a 1d array\n    ary = np.asanyarray(ary).ravel()\n\n    # enforce that the dtype of `ary` is used for the output\n    dtype_req = ary.dtype\n\n    # fast track default case\n    if to_begin is None and to_end is None:\n        return ary[1:] - ary[:-1]\n\n    if to_begin is None:\n        l_begin = 0\n    else:\n        to_begin = np.asanyarray(to_begin)\n        if not np.can_cast(to_begin, dtype_req, casting=\"same_kind\"):\n            raise TypeError(\"dtype of `to_begin` must be compatible \"\n                            \"with input `ary` under the `same_kind` rule.\")\n\n        to_begin = to_begin.ravel()\n        l_begin = len(to_begin)\n\n    if to_end is None:\n        l_end = 0\n    else:\n        to_end = np.asanyarray(to_end)\n        if not np.can_cast(to_end, dtype_req, casting=\"same_kind\"):\n            raise TypeError(\"dtype of `to_end` must be compatible \"\n                            \"with input `ary` under the `same_kind` rule.\")\n\n        to_end = to_end.ravel()\n        l_end = len(to_end)\n\n    # do the calculation in place and copy to_begin and to_end\n    l_diff = max(len(ary) - 1, 0)\n    result = np.empty(l_diff + l_begin + l_end, dtype=ary.dtype)\n    result = ary.__array_wrap__(result)\n    if l_begin > 0:\n        result[:l_begin] = to_begin\n    if l_end > 0:\n        result[l_begin + l_diff:] = to_end\n    np.subtract(ary[1:], ary[:-1], result[l_begin:l_begin + l_diff])\n    return result\n\n\ndef _unpack_tuple(x):\n    \"\"\" Unpacks one-element tuples for use as return values \"\"\"\n    if len(x) == 1:\n        return x[0]\n    else:\n        return x\n\n\ndef _unique_dispatcher(ar, return_index=None, return_inverse=None,\n                       return_counts=None, axis=None):\n    return (ar,)\n\n\n@array_function_dispatch(_unique_dispatcher)\ndef unique(ar, return_index=False, return_inverse=False,\n           return_counts=False, axis=None):\n    \"\"\"\n    Find the unique elements of an array.\n\n    Returns the sorted unique elements of an array. There are three optional\n    outputs in addition to the unique elements:\n\n    * the indices of the input array that give the unique values\n    * the indices of the unique array that reconstruct the input array\n    * the number of times each unique value comes up in the input array\n\n    Parameters\n    ----------\n    ar : array_like\n        Input array. Unless `axis` is specified, this will be flattened if it\n        is not already 1-D.\n    return_index : bool, optional\n        If True, also return the indices of `ar` (along the specified axis,\n        if provided, or in the flattened array) that result in the unique array.\n    return_inverse : bool, optional\n        If True, also return the indices of the unique array (for the specified\n        axis, if provided) that can be used to reconstruct `ar`.\n    return_counts : bool, optional\n        If True, also return the number of times each unique item appears\n        in `ar`.\n\n        .. versionadded:: 1.9.0\n\n    axis : int or None, optional\n        The axis to operate on. If None, `ar` will be flattened. If an integer,\n        the subarrays indexed by the given axis will be flattened and treated\n        as the elements of a 1-D array with the dimension of the given axis,\n        see the notes for more details.  Object arrays or structured arrays\n        that contain objects are not supported if the `axis` kwarg is used. The\n        default is None.\n\n        .. versionadded:: 1.13.0\n\n    Returns\n    -------\n    unique : ndarray\n        The sorted unique values.\n    unique_indices : ndarray, optional\n        The indices of the first occurrences of the unique values in the\n        original array. Only provided if `return_index` is True.\n    unique_inverse : ndarray, optional\n        The indices to reconstruct the original array from the\n        unique array. Only provided if `return_inverse` is True.\n    unique_counts : ndarray, optional\n        The number of times each of the unique values comes up in the\n        original array. Only provided if `return_counts` is True.\n\n        .. versionadded:: 1.9.0\n\n    See Also\n    --------\n    numpy.lib.arraysetops : Module with a number of other functions for\n                            performing set operations on arrays.\n    repeat : Repeat elements of an array.\n\n    Notes\n    -----\n    When an axis is specified the subarrays indexed by the axis are sorted.\n    This is done by making the specified axis the first dimension of the array\n    (move the axis to the first dimension to keep the order of the other axes)\n    and then flattening the subarrays in C order. The flattened subarrays are\n    then viewed as a structured type with each element given a label, with the\n    effect that we end up with a 1-D array of structured types that can be\n    treated in the same way as any other 1-D array. The result is that the\n    flattened subarrays are sorted in lexicographic order starting with the\n    first element.\n\n    Examples\n    --------\n    >>> np.unique([1, 1, 2, 2, 3, 3])\n    array([1, 2, 3])\n    >>> a = np.array([[1, 1], [2, 3]])\n    >>> np.unique(a)\n    array([1, 2, 3])\n\n    Return the unique rows of a 2D array\n\n    >>> a = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 4]])\n    >>> np.unique(a, axis=0)\n    array([[1, 0, 0], [2, 3, 4]])\n\n    Return the indices of the original array that give the unique values:\n\n    >>> a = np.array(['a', 'b', 'b', 'c', 'a'])\n    >>> u, indices = np.unique(a, return_index=True)\n    >>> u\n    array(['a', 'b', 'c'], dtype='<U1')\n    >>> indices\n    array([0, 1, 3])\n    >>> a[indices]\n    array(['a', 'b', 'c'], dtype='<U1')\n\n    Reconstruct the input array from the unique values and inverse:\n\n    >>> a = np.array([1, 2, 6, 4, 2, 3, 2])\n    >>> u, indices = np.unique(a, return_inverse=True)\n    >>> u\n    array([1, 2, 3, 4, 6])\n    >>> indices\n    array([0, 1, 4, 3, 1, 2, 1])\n    >>> u[indices]\n    array([1, 2, 6, 4, 2, 3, 2])\n\n    Reconstruct the input values from the unique values and counts:\n\n    >>> a = np.array([1, 2, 6, 4, 2, 3, 2])\n    >>> values, counts = np.unique(a, return_counts=True)\n    >>> values\n    array([1, 2, 3, 4, 6])\n    >>> counts\n    array([1, 3, 1, 1, 1])\n    >>> np.repeat(values, counts)\n    array([1, 2, 2, 2, 3, 4, 6])    # original order not preserved\n\n    \"\"\"\n    ar = np.asanyarray(ar)\n    if axis is None:\n        ret = _unique1d(ar, return_index, return_inverse, return_counts)\n        return _unpack_tuple(ret)\n\n    # axis was specified and not None\n    try:\n        ar = np.moveaxis(ar, axis, 0)\n    except np.AxisError:\n        # this removes the \"axis1\" or \"axis2\" prefix from the error message\n        raise np.AxisError(axis, ar.ndim) from None\n\n    # Must reshape to a contiguous 2D array for this to work...\n    orig_shape, orig_dtype = ar.shape, ar.dtype\n    ar = ar.reshape(orig_shape[0], np.prod(orig_shape[1:], dtype=np.intp))\n    ar = np.ascontiguousarray(ar)\n    dtype = [('f{i}'.format(i=i), ar.dtype) for i in range(ar.shape[1])]\n\n    # At this point, `ar` has shape `(n, m)`, and `dtype` is a structured\n    # data type with `m` fields where each field has the data type of `ar`.\n    # In the following, we create the array `consolidated`, which has\n    # shape `(n,)` with data type `dtype`.\n    try:\n        if ar.shape[1] > 0:\n            consolidated = ar.view(dtype)\n        else:\n            # If ar.shape[1] == 0, then dtype will be `np.dtype([])`, which is\n            # a data type with itemsize 0, and the call `ar.view(dtype)` will\n            # fail.  Instead, we'll use `np.empty` to explicitly create the\n            # array with shape `(len(ar),)`.  Because `dtype` in this case has\n            # itemsize 0, the total size of the result is still 0 bytes.\n            consolidated = np.empty(len(ar), dtype=dtype)\n    except TypeError as e:\n        # There's no good way to do this for object arrays, etc...\n        msg = 'The axis argument to unique is not supported for dtype {dt}'\n        raise TypeError(msg.format(dt=ar.dtype)) from e\n\n    def reshape_uniq(uniq):\n        n = len(uniq)\n        uniq = uniq.view(orig_dtype)\n        uniq = uniq.reshape(n, *orig_shape[1:])\n        uniq = np.moveaxis(uniq, 0, axis)\n        return uniq\n\n    output = _unique1d(consolidated, return_index,\n                       return_inverse, return_counts)\n    output = (reshape_uniq(output[0]),) + output[1:]\n    return _unpack_tuple(output)\n\n\ndef _unique1d(ar, return_index=False, return_inverse=False,\n              return_counts=False):\n    \"\"\"\n    Find the unique elements of an array, ignoring shape.\n    \"\"\"\n    ar = np.asanyarray(ar).flatten()\n\n    optional_indices = return_index or return_inverse\n\n    if optional_indices:\n        perm = ar.argsort(kind='mergesort' if return_index else 'quicksort')\n        aux = ar[perm]\n    else:\n        ar.sort()\n        aux = ar\n    mask = np.empty(aux.shape, dtype=np.bool_)\n    mask[:1] = True\n    mask[1:] = aux[1:] != aux[:-1]\n\n    ret = (aux[mask],)\n    if return_index:\n        ret += (perm[mask],)\n    if return_inverse:\n        imask = np.cumsum(mask) - 1\n        inv_idx = np.empty(mask.shape, dtype=np.intp)\n        inv_idx[perm] = imask\n        ret += (inv_idx,)\n    if return_counts:\n        idx = np.concatenate(np.nonzero(mask) + ([mask.size],))\n        ret += (np.diff(idx),)\n    return ret\n\n\ndef _intersect1d_dispatcher(\n        ar1, ar2, assume_unique=None, return_indices=None):\n    return (ar1, ar2)\n\n\n@array_function_dispatch(_intersect1d_dispatcher)\ndef intersect1d(ar1, ar2, assume_unique=False, return_indices=False):\n    \"\"\"\n    Find the intersection of two arrays.\n\n    Return the sorted, unique values that are in both of the input arrays.\n\n    Parameters\n    ----------\n    ar1, ar2 : array_like\n        Input arrays. Will be flattened if not already 1D.\n    assume_unique : bool\n        If True, the input arrays are both assumed to be unique, which\n        can speed up the calculation.  If True but ``ar1`` or ``ar2`` are not\n        unique, incorrect results and out-of-bounds indices could result.\n        Default is False.\n    return_indices : bool\n        If True, the indices which correspond to the intersection of the two\n        arrays are returned. The first instance of a value is used if there are\n        multiple. Default is False.\n\n        .. versionadded:: 1.15.0\n\n    Returns\n    -------\n    intersect1d : ndarray\n        Sorted 1D array of common and unique elements.\n    comm1 : ndarray\n        The indices of the first occurrences of the common values in `ar1`.\n        Only provided if `return_indices` is True.\n    comm2 : ndarray\n        The indices of the first occurrences of the common values in `ar2`.\n        Only provided if `return_indices` is True.\n\n\n    See Also\n    --------\n    numpy.lib.arraysetops : Module with a number of other functions for\n                            performing set operations on arrays.\n\n    Examples\n    --------\n    >>> np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1])\n    array([1, 3])\n\n    To intersect more than two arrays, use functools.reduce:\n\n    >>> from functools import reduce\n    >>> reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2]))\n    array([3])\n\n    To return the indices of the values common to the input arrays\n    along with the intersected values:\n\n    >>> x = np.array([1, 1, 2, 3, 4])\n    >>> y = np.array([2, 1, 4, 6])\n    >>> xy, x_ind, y_ind = np.intersect1d(x, y, return_indices=True)\n    >>> x_ind, y_ind\n    (array([0, 2, 4]), array([1, 0, 2]))\n    >>> xy, x[x_ind], y[y_ind]\n    (array([1, 2, 4]), array([1, 2, 4]), array([1, 2, 4]))\n\n    \"\"\"\n    ar1 = np.asanyarray(ar1)\n    ar2 = np.asanyarray(ar2)\n\n    if not assume_unique:\n        if return_indices:\n            ar1, ind1 = unique(ar1, return_index=True)\n            ar2, ind2 = unique(ar2, return_index=True)\n        else:\n            ar1 = unique(ar1)\n            ar2 = unique(ar2)\n    else:\n        ar1 = ar1.ravel()\n        ar2 = ar2.ravel()\n\n    aux = np.concatenate((ar1, ar2))\n    if return_indices:\n        aux_sort_indices = np.argsort(aux, kind='mergesort')\n        aux = aux[aux_sort_indices]\n    else:\n        aux.sort()\n\n    mask = aux[1:] == aux[:-1]\n    int1d = aux[:-1][mask]\n\n    if return_indices:\n        ar1_indices = aux_sort_indices[:-1][mask]\n        ar2_indices = aux_sort_indices[1:][mask] - ar1.size\n        if not assume_unique:\n            ar1_indices = ind1[ar1_indices]\n            ar2_indices = ind2[ar2_indices]\n\n        return int1d, ar1_indices, ar2_indices\n    else:\n        return int1d\n\n\ndef _setxor1d_dispatcher(ar1, ar2, assume_unique=None):\n    return (ar1, ar2)\n\n\n@array_function_dispatch(_setxor1d_dispatcher)\ndef setxor1d(ar1, ar2, assume_unique=False):\n    \"\"\"\n    Find the set exclusive-or of two arrays.\n\n    Return the sorted, unique values that are in only one (not both) of the\n    input arrays.\n\n    Parameters\n    ----------\n    ar1, ar2 : array_like\n        Input arrays.\n    assume_unique : bool\n        If True, the input arrays are both assumed to be unique, which\n        can speed up the calculation.  Default is False.\n\n    Returns\n    -------\n    setxor1d : ndarray\n        Sorted 1D array of unique values that are in only one of the input\n        arrays.\n\n    Examples\n    --------\n    >>> a = np.array([1, 2, 3, 2, 4])\n    >>> b = np.array([2, 3, 5, 7, 5])\n    >>> np.setxor1d(a,b)\n    array([1, 4, 5, 7])\n\n    \"\"\"\n    if not assume_unique:\n        ar1 = unique(ar1)\n        ar2 = unique(ar2)\n\n    aux = np.concatenate((ar1, ar2))\n    if aux.size == 0:\n        return aux\n\n    aux.sort()\n    flag = np.concatenate(([True], aux[1:] != aux[:-1], [True]))\n    return aux[flag[1:] & flag[:-1]]\n\n\ndef _in1d_dispatcher(ar1, ar2, assume_unique=None, invert=None):\n    return (ar1, ar2)\n\n\n@array_function_dispatch(_in1d_dispatcher)\ndef in1d(ar1, ar2, assume_unique=False, invert=False):\n    \"\"\"\n    Test whether each element of a 1-D array is also present in a second array.\n\n    Returns a boolean array the same length as `ar1` that is True\n    where an element of `ar1` is in `ar2` and False otherwise.\n\n    We recommend using :func:`isin` instead of `in1d` for new code.\n\n    Parameters\n    ----------\n    ar1 : (M,) array_like\n        Input array.\n    ar2 : array_like\n        The values against which to test each value of `ar1`.\n    assume_unique : bool, optional\n        If True, the input arrays are both assumed to be unique, which\n        can speed up the calculation.  Default is False.\n    invert : bool, optional\n        If True, the values in the returned array are inverted (that is,\n        False where an element of `ar1` is in `ar2` and True otherwise).\n        Default is False. ``np.in1d(a, b, invert=True)`` is equivalent\n        to (but is faster than) ``np.invert(in1d(a, b))``.\n\n        .. versionadded:: 1.8.0\n\n    Returns\n    -------\n    in1d : (M,) ndarray, bool\n        The values `ar1[in1d]` are in `ar2`.\n\n    See Also\n    --------\n    isin                  : Version of this function that preserves the\n                            shape of ar1.\n    numpy.lib.arraysetops : Module with a number of other functions for\n                            performing set operations on arrays.\n\n    Notes\n    -----\n    `in1d` can be considered as an element-wise function version of the\n    python keyword `in`, for 1-D sequences. ``in1d(a, b)`` is roughly\n    equivalent to ``np.array([item in b for item in a])``.\n    However, this idea fails if `ar2` is a set, or similar (non-sequence)\n    container:  As ``ar2`` is converted to an array, in those cases\n    ``asarray(ar2)`` is an object array rather than the expected array of\n    contained values.\n\n    .. versionadded:: 1.4.0\n\n    Examples\n    --------\n    >>> test = np.array([0, 1, 2, 5, 0])\n    >>> states = [0, 2]\n    >>> mask = np.in1d(test, states)\n    >>> mask\n    array([ True, False,  True, False,  True])\n    >>> test[mask]\n    array([0, 2, 0])\n    >>> mask = np.in1d(test, states, invert=True)\n    >>> mask\n    array([False,  True, False,  True, False])\n    >>> test[mask]\n    array([1, 5])\n    \"\"\"\n    # Ravel both arrays, behavior for the first array could be different\n    ar1 = np.asarray(ar1).ravel()\n    ar2 = np.asarray(ar2).ravel()\n\n    # Check if one of the arrays may contain arbitrary objects\n    contains_object = ar1.dtype.hasobject or ar2.dtype.hasobject\n\n    # This code is run when\n    # a) the first condition is true, making the code significantly faster\n    # b) the second condition is true (i.e. `ar1` or `ar2` may contain\n    #    arbitrary objects), since then sorting is not guaranteed to work\n    if len(ar2) < 10 * len(ar1) ** 0.145 or contains_object:\n        if invert:\n            mask = np.ones(len(ar1), dtype=bool)\n            for a in ar2:\n                mask &= (ar1 != a)\n        else:\n            mask = np.zeros(len(ar1), dtype=bool)\n            for a in ar2:\n                mask |= (ar1 == a)\n        return mask\n\n    # Otherwise use sorting\n    if not assume_unique:\n        ar1, rev_idx = np.unique(ar1, return_inverse=True)\n        ar2 = np.unique(ar2)\n\n    ar = np.concatenate((ar1, ar2))\n    # We need this to be a stable sort, so always use 'mergesort'\n    # here. The values from the first array should always come before\n    # the values from the second array.\n    order = ar.argsort(kind='mergesort')\n    sar = ar[order]\n    if invert:\n        bool_ar = (sar[1:] != sar[:-1])\n    else:\n        bool_ar = (sar[1:] == sar[:-1])\n    flag = np.concatenate((bool_ar, [invert]))\n    ret = np.empty(ar.shape, dtype=bool)\n    ret[order] = flag\n\n    if assume_unique:\n        return ret[:len(ar1)]\n    else:\n        return ret[rev_idx]\n\n\ndef _isin_dispatcher(element, test_elements, assume_unique=None, invert=None):\n    return (element, test_elements)\n\n\n@array_function_dispatch(_isin_dispatcher)\ndef isin(element, test_elements, assume_unique=False, invert=False):\n    \"\"\"\n    Calculates `element in test_elements`, broadcasting over `element` only.\n    Returns a boolean array of the same shape as `element` that is True\n    where an element of `element` is in `test_elements` and False otherwise.\n\n    Parameters\n    ----------\n    element : array_like\n        Input array.\n    test_elements : array_like\n        The values against which to test each value of `element`.\n        This argument is flattened if it is an array or array_like.\n        See notes for behavior with non-array-like parameters.\n    assume_unique : bool, optional\n        If True, the input arrays are both assumed to be unique, which\n        can speed up the calculation.  Default is False.\n    invert : bool, optional\n        If True, the values in the returned array are inverted, as if\n        calculating `element not in test_elements`. Default is False.\n        ``np.isin(a, b, invert=True)`` is equivalent to (but faster\n        than) ``np.invert(np.isin(a, b))``.\n\n    Returns\n    -------\n    isin : ndarray, bool\n        Has the same shape as `element`. The values `element[isin]`\n        are in `test_elements`.\n\n    See Also\n    --------\n    in1d                  : Flattened version of this function.\n    numpy.lib.arraysetops : Module with a number of other functions for\n                            performing set operations on arrays.\n\n    Notes\n    -----\n\n    `isin` is an element-wise function version of the python keyword `in`.\n    ``isin(a, b)`` is roughly equivalent to\n    ``np.array([item in b for item in a])`` if `a` and `b` are 1-D sequences.\n\n    `element` and `test_elements` are converted to arrays if they are not\n    already. If `test_elements` is a set (or other non-sequence collection)\n    it will be converted to an object array with one element, rather than an\n    array of the values contained in `test_elements`. This is a consequence\n    of the `array` constructor's way of handling non-sequence collections.\n    Converting the set to a list usually gives the desired behavior.\n\n    .. versionadded:: 1.13.0\n\n    Examples\n    --------\n    >>> element = 2*np.arange(4).reshape((2, 2))\n    >>> element\n    array([[0, 2],\n           [4, 6]])\n    >>> test_elements = [1, 2, 4, 8]\n    >>> mask = np.isin(element, test_elements)\n    >>> mask\n    array([[False,  True],\n           [ True, False]])\n    >>> element[mask]\n    array([2, 4])\n\n    The indices of the matched values can be obtained with `nonzero`:\n\n    >>> np.nonzero(mask)\n    (array([0, 1]), array([1, 0]))\n\n    The test can also be inverted:\n\n    >>> mask = np.isin(element, test_elements, invert=True)\n    >>> mask\n    array([[ True, False],\n           [False,  True]])\n    >>> element[mask]\n    array([0, 6])\n\n    Because of how `array` handles sets, the following does not\n    work as expected:\n\n    >>> test_set = {1, 2, 4, 8}\n    >>> np.isin(element, test_set)\n    array([[False, False],\n           [False, False]])\n\n    Casting the set to a list gives the expected result:\n\n    >>> np.isin(element, list(test_set))\n    array([[False,  True],\n           [ True, False]])\n    \"\"\"\n    element = np.asarray(element)\n    return in1d(element, test_elements, assume_unique=assume_unique,\n                invert=invert).reshape(element.shape)\n\n\ndef _union1d_dispatcher(ar1, ar2):\n    return (ar1, ar2)\n\n\n@array_function_dispatch(_union1d_dispatcher)\ndef union1d(ar1, ar2):\n    \"\"\"\n    Find the union of two arrays.\n\n    Return the unique, sorted array of values that are in either of the two\n    input arrays.\n\n    Parameters\n    ----------\n    ar1, ar2 : array_like\n        Input arrays. They are flattened if they are not already 1D.\n\n    Returns\n    -------\n    union1d : ndarray\n        Unique, sorted union of the input arrays.\n\n    See Also\n    --------\n    numpy.lib.arraysetops : Module with a number of other functions for\n                            performing set operations on arrays.\n\n    Examples\n    --------\n    >>> np.union1d([-1, 0, 1], [-2, 0, 2])\n    array([-2, -1,  0,  1,  2])\n\n    To find the union of more than two arrays, use functools.reduce:\n\n    >>> from functools import reduce\n    >>> reduce(np.union1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2]))\n    array([1, 2, 3, 4, 6])\n    \"\"\"\n    return unique(np.concatenate((ar1, ar2), axis=None))\n\n\ndef _setdiff1d_dispatcher(ar1, ar2, assume_unique=None):\n    return (ar1, ar2)\n\n\n@array_function_dispatch(_setdiff1d_dispatcher)\ndef setdiff1d(ar1, ar2, assume_unique=False):\n    \"\"\"\n    Find the set difference of two arrays.\n\n    Return the unique values in `ar1` that are not in `ar2`.\n\n    Parameters\n    ----------\n    ar1 : array_like\n        Input array.\n    ar2 : array_like\n        Input comparison array.\n    assume_unique : bool\n        If True, the input arrays are both assumed to be unique, which\n        can speed up the calculation.  Default is False.\n\n    Returns\n    -------\n    setdiff1d : ndarray\n        1D array of values in `ar1` that are not in `ar2`. The result\n        is sorted when `assume_unique=False`, but otherwise only sorted\n        if the input is sorted.\n\n    See Also\n    --------\n    numpy.lib.arraysetops : Module with a number of other functions for\n                            performing set operations on arrays.\n\n    Examples\n    --------\n    >>> a = np.array([1, 2, 3, 2, 4, 1])\n    >>> b = np.array([3, 4, 5, 6])\n    >>> np.setdiff1d(a, b)\n    array([1, 2])\n\n    \"\"\"\n    if assume_unique:\n        ar1 = np.asarray(ar1).ravel()\n    else:\n        ar1 = unique(ar1)\n        ar2 = unique(ar2)\n    return ar1[in1d(ar1, ar2, assume_unique=True, invert=True)]\n",801],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/function_base.py":["import collections.abc\nimport functools\nimport re\nimport sys\nimport warnings\n\nimport numpy as np\nimport numpy.core.numeric as _nx\nfrom numpy.core import transpose\nfrom numpy.core.numeric import (\n    ones, zeros, arange, concatenate, array, asarray, asanyarray, empty,\n    ndarray, around, floor, ceil, take, dot, where, intp,\n    integer, isscalar, absolute\n    )\nfrom numpy.core.umath import (\n    pi, add, arctan2, frompyfunc, cos, less_equal, sqrt, sin,\n    mod, exp, not_equal, subtract\n    )\nfrom numpy.core.fromnumeric import (\n    ravel, nonzero, partition, mean, any, sum\n    )\nfrom numpy.core.numerictypes import typecodes\nfrom numpy.core.overrides import set_module\nfrom numpy.core import overrides\nfrom numpy.core.function_base import add_newdoc\nfrom numpy.lib.twodim_base import diag\nfrom numpy.core.multiarray import (\n    _insert, add_docstring, bincount, normalize_axis_index, _monotonicity,\n    interp as compiled_interp, interp_complex as compiled_interp_complex\n    )\nfrom numpy.core.umath import _add_newdoc_ufunc as add_newdoc_ufunc\n\nimport builtins\n\n# needed in this module for compatibility\nfrom numpy.lib.histograms import histogram, histogramdd\n\n\narray_function_dispatch = functools.partial(\n    overrides.array_function_dispatch, module='numpy')\n\n\n__all__ = [\n    'select', 'piecewise', 'trim_zeros', 'copy', 'iterable', 'percentile',\n    'diff', 'gradient', 'angle', 'unwrap', 'sort_complex', 'disp', 'flip',\n    'rot90', 'extract', 'place', 'vectorize', 'asarray_chkfinite', 'average',\n    'bincount', 'digitize', 'cov', 'corrcoef',\n    'msort', 'median', 'sinc', 'hamming', 'hanning', 'bartlett',\n    'blackman', 'kaiser', 'trapz', 'i0', 'add_newdoc', 'add_docstring',\n    'meshgrid', 'delete', 'insert', 'append', 'interp', 'add_newdoc_ufunc',\n    'quantile'\n    ]\n\n\ndef _rot90_dispatcher(m, k=None, axes=None):\n    return (m,)\n\n\n@array_function_dispatch(_rot90_dispatcher)\ndef rot90(m, k=1, axes=(0, 1)):\n    \"\"\"\n    Rotate an array by 90 degrees in the plane specified by axes.\n\n    Rotation direction is from the first towards the second axis.\n\n    Parameters\n    ----------\n    m : array_like\n        Array of two or more dimensions.\n    k : integer\n        Number of times the array is rotated by 90 degrees.\n    axes: (2,) array_like\n        The array is rotated in the plane defined by the axes.\n        Axes must be different.\n\n        .. versionadded:: 1.12.0\n\n    Returns\n    -------\n    y : ndarray\n        A rotated view of `m`.\n\n    See Also\n    --------\n    flip : Reverse the order of elements in an array along the given axis.\n    fliplr : Flip an array horizontally.\n    flipud : Flip an array vertically.\n\n    Notes\n    -----\n    rot90(m, k=1, axes=(1,0)) is the reverse of rot90(m, k=1, axes=(0,1))\n    rot90(m, k=1, axes=(1,0)) is equivalent to rot90(m, k=-1, axes=(0,1))\n\n    Examples\n    --------\n    >>> m = np.array([[1,2],[3,4]], int)\n    >>> m\n    array([[1, 2],\n           [3, 4]])\n    >>> np.rot90(m)\n    array([[2, 4],\n           [1, 3]])\n    >>> np.rot90(m, 2)\n    array([[4, 3],\n           [2, 1]])\n    >>> m = np.arange(8).reshape((2,2,2))\n    >>> np.rot90(m, 1, (1,2))\n    array([[[1, 3],\n            [0, 2]],\n           [[5, 7],\n            [4, 6]]])\n\n    \"\"\"\n    axes = tuple(axes)\n    if len(axes) != 2:\n        raise ValueError(\"len(axes) must be 2.\")\n\n    m = asanyarray(m)\n\n    if axes[0] == axes[1] or absolute(axes[0] - axes[1]) == m.ndim:\n        raise ValueError(\"Axes must be different.\")\n\n    if (axes[0] >= m.ndim or axes[0] < -m.ndim\n        or axes[1] >= m.ndim or axes[1] < -m.ndim):\n        raise ValueError(\"Axes={} out of range for array of ndim={}.\"\n            .format(axes, m.ndim))\n\n    k %= 4\n\n    if k == 0:\n        return m[:]\n    if k == 2:\n        return flip(flip(m, axes[0]), axes[1])\n\n    axes_list = arange(0, m.ndim)\n    (axes_list[axes[0]], axes_list[axes[1]]) = (axes_list[axes[1]],\n                                                axes_list[axes[0]])\n\n    if k == 1:\n        return transpose(flip(m, axes[1]), axes_list)\n    else:\n        # k == 3\n        return flip(transpose(m, axes_list), axes[1])\n\n\ndef _flip_dispatcher(m, axis=None):\n    return (m,)\n\n\n@array_function_dispatch(_flip_dispatcher)\ndef flip(m, axis=None):\n    \"\"\"\n    Reverse the order of elements in an array along the given axis.\n\n    The shape of the array is preserved, but the elements are reordered.\n\n    .. versionadded:: 1.12.0\n\n    Parameters\n    ----------\n    m : array_like\n        Input array.\n    axis : None or int or tuple of ints, optional\n         Axis or axes along which to flip over. The default,\n         axis=None, will flip over all of the axes of the input array.\n         If axis is negative it counts from the last to the first axis.\n\n         If axis is a tuple of ints, flipping is performed on all of the axes\n         specified in the tuple.\n\n         .. versionchanged:: 1.15.0\n            None and tuples of axes are supported\n\n    Returns\n    -------\n    out : array_like\n        A view of `m` with the entries of axis reversed.  Since a view is\n        returned, this operation is done in constant time.\n\n    See Also\n    --------\n    flipud : Flip an array vertically (axis=0).\n    fliplr : Flip an array horizontally (axis=1).\n\n    Notes\n    -----\n    flip(m, 0) is equivalent to flipud(m).\n\n    flip(m, 1) is equivalent to fliplr(m).\n\n    flip(m, n) corresponds to ``m[...,::-1,...]`` with ``::-1`` at position n.\n\n    flip(m) corresponds to ``m[::-1,::-1,...,::-1]`` with ``::-1`` at all\n    positions.\n\n    flip(m, (0, 1)) corresponds to ``m[::-1,::-1,...]`` with ``::-1`` at\n    position 0 and position 1.\n\n    Examples\n    --------\n    >>> A = np.arange(8).reshape((2,2,2))\n    >>> A\n    array([[[0, 1],\n            [2, 3]],\n           [[4, 5],\n            [6, 7]]])\n    >>> np.flip(A, 0)\n    array([[[4, 5],\n            [6, 7]],\n           [[0, 1],\n            [2, 3]]])\n    >>> np.flip(A, 1)\n    array([[[2, 3],\n            [0, 1]],\n           [[6, 7],\n            [4, 5]]])\n    >>> np.flip(A)\n    array([[[7, 6],\n            [5, 4]],\n           [[3, 2],\n            [1, 0]]])\n    >>> np.flip(A, (0, 2))\n    array([[[5, 4],\n            [7, 6]],\n           [[1, 0],\n            [3, 2]]])\n    >>> A = np.random.randn(3,4,5)\n    >>> np.all(np.flip(A,2) == A[:,:,::-1,...])\n    True\n    \"\"\"\n    if not hasattr(m, 'ndim'):\n        m = asarray(m)\n    if axis is None:\n        indexer = (np.s_[::-1],) * m.ndim\n    else:\n        axis = _nx.normalize_axis_tuple(axis, m.ndim)\n        indexer = [np.s_[:]] * m.ndim\n        for ax in axis:\n            indexer[ax] = np.s_[::-1]\n        indexer = tuple(indexer)\n    return m[indexer]\n\n\n@set_module('numpy')\ndef iterable(y):\n    \"\"\"\n    Check whether or not an object can be iterated over.\n\n    Parameters\n    ----------\n    y : object\n      Input object.\n\n    Returns\n    -------\n    b : bool\n      Return ``True`` if the object has an iterator method or is a\n      sequence and ``False`` otherwise.\n\n\n    Examples\n    --------\n    >>> np.iterable([1, 2, 3])\n    True\n    >>> np.iterable(2)\n    False\n\n    \"\"\"\n    try:\n        iter(y)\n    except TypeError:\n        return False\n    return True\n\n\ndef _average_dispatcher(a, axis=None, weights=None, returned=None):\n    return (a, weights)\n\n\n@array_function_dispatch(_average_dispatcher)\ndef average(a, axis=None, weights=None, returned=False):\n    \"\"\"\n    Compute the weighted average along the specified axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing data to be averaged. If `a` is not an array, a\n        conversion is attempted.\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which to average `a`.  The default,\n        axis=None, will average over all of the elements of the input array.\n        If axis is negative it counts from the last to the first axis.\n\n        .. versionadded:: 1.7.0\n\n        If axis is a tuple of ints, averaging is performed on all of the axes\n        specified in the tuple instead of a single axis or all the axes as\n        before.\n    weights : array_like, optional\n        An array of weights associated with the values in `a`. Each value in\n        `a` contributes to the average according to its associated weight.\n        The weights array can either be 1-D (in which case its length must be\n        the size of `a` along the given axis) or of the same shape as `a`.\n        If `weights=None`, then all data in `a` are assumed to have a\n        weight equal to one.  The 1-D calculation is::\n\n            avg = sum(a * weights) / sum(weights)\n\n        The only constraint on `weights` is that `sum(weights)` must not be 0.\n    returned : bool, optional\n        Default is `False`. If `True`, the tuple (`average`, `sum_of_weights`)\n        is returned, otherwise only the average is returned.\n        If `weights=None`, `sum_of_weights` is equivalent to the number of\n        elements over which the average is taken.\n\n    Returns\n    -------\n    retval, [sum_of_weights] : array_type or double\n        Return the average along the specified axis. When `returned` is `True`,\n        return a tuple with the average as the first element and the sum\n        of the weights as the second element. `sum_of_weights` is of the\n        same type as `retval`. The result dtype follows a genereal pattern.\n        If `weights` is None, the result dtype will be that of `a` , or ``float64``\n        if `a` is integral. Otherwise, if `weights` is not None and `a` is non-\n        integral, the result type will be the type of lowest precision capable of\n        representing values of both `a` and `weights`. If `a` happens to be\n        integral, the previous rules still applies but the result dtype will\n        at least be ``float64``.\n\n    Raises\n    ------\n    ZeroDivisionError\n        When all weights along axis are zero. See `numpy.ma.average` for a\n        version robust to this type of error.\n    TypeError\n        When the length of 1D `weights` is not the same as the shape of `a`\n        along axis.\n\n    See Also\n    --------\n    mean\n\n    ma.average : average for masked arrays -- useful if your data contains\n                 \"missing\" values\n    numpy.result_type : Returns the type that results from applying the\n                        numpy type promotion rules to the arguments.\n\n    Examples\n    --------\n    >>> data = np.arange(1, 5)\n    >>> data\n    array([1, 2, 3, 4])\n    >>> np.average(data)\n    2.5\n    >>> np.average(np.arange(1, 11), weights=np.arange(10, 0, -1))\n    4.0\n\n    >>> data = np.arange(6).reshape((3,2))\n    >>> data\n    array([[0, 1],\n           [2, 3],\n           [4, 5]])\n    >>> np.average(data, axis=1, weights=[1./4, 3./4])\n    array([0.75, 2.75, 4.75])\n    >>> np.average(data, weights=[1./4, 3./4])\n    Traceback (most recent call last):\n        ...\n    TypeError: Axis must be specified when shapes of a and weights differ.\n\n    >>> a = np.ones(5, dtype=np.float128)\n    >>> w = np.ones(5, dtype=np.complex64)\n    >>> avg = np.average(a, weights=w)\n    >>> print(avg.dtype)\n    complex256\n    \"\"\"\n    a = np.asanyarray(a)\n\n    if weights is None:\n        avg = a.mean(axis)\n        scl = avg.dtype.type(a.size/avg.size)\n    else:\n        wgt = np.asanyarray(weights)\n\n        if issubclass(a.dtype.type, (np.integer, np.bool_)):\n            result_dtype = np.result_type(a.dtype, wgt.dtype, 'f8')\n        else:\n            result_dtype = np.result_type(a.dtype, wgt.dtype)\n\n        # Sanity checks\n        if a.shape != wgt.shape:\n            if axis is None:\n                raise TypeError(\n                    \"Axis must be specified when shapes of a and weights \"\n                    \"differ.\")\n            if wgt.ndim != 1:\n                raise TypeError(\n                    \"1D weights expected when shapes of a and weights differ.\")\n            if wgt.shape[0] != a.shape[axis]:\n                raise ValueError(\n                    \"Length of weights not compatible with specified axis.\")\n\n            # setup wgt to broadcast along axis\n            wgt = np.broadcast_to(wgt, (a.ndim-1)*(1,) + wgt.shape)\n            wgt = wgt.swapaxes(-1, axis)\n\n        scl = wgt.sum(axis=axis, dtype=result_dtype)\n        if np.any(scl == 0.0):\n            raise ZeroDivisionError(\n                \"Weights sum to zero, can't be normalized\")\n\n        avg = np.multiply(a, wgt, dtype=result_dtype).sum(axis)/scl\n\n    if returned:\n        if scl.shape != avg.shape:\n            scl = np.broadcast_to(scl, avg.shape).copy()\n        return avg, scl\n    else:\n        return avg\n\n\n@set_module('numpy')\ndef asarray_chkfinite(a, dtype=None, order=None):\n    \"\"\"Convert the input to an array, checking for NaNs or Infs.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data, in any form that can be converted to an array.  This\n        includes lists, lists of tuples, tuples, tuples of tuples, tuples\n        of lists and ndarrays.  Success requires no NaNs or Infs.\n    dtype : data-type, optional\n        By default, the data-type is inferred from the input data.\n    order : {'C', 'F', 'A', 'K'}, optional\n        Memory layout.  'A' and 'K' depend on the order of input array a.\n        'C' row-major (C-style),\n        'F' column-major (Fortran-style) memory representation.\n        'A' (any) means 'F' if `a` is Fortran contiguous, 'C' otherwise\n        'K' (keep) preserve input order\n        Defaults to 'C'.\n\n    Returns\n    -------\n    out : ndarray\n        Array interpretation of `a`.  No copy is performed if the input\n        is already an ndarray.  If `a` is a subclass of ndarray, a base\n        class ndarray is returned.\n\n    Raises\n    ------\n    ValueError\n        Raises ValueError if `a` contains NaN (Not a Number) or Inf (Infinity).\n\n    See Also\n    --------\n    asarray : Create and array.\n    asanyarray : Similar function which passes through subclasses.\n    ascontiguousarray : Convert input to a contiguous array.\n    asfarray : Convert input to a floating point ndarray.\n    asfortranarray : Convert input to an ndarray with column-major\n                     memory order.\n    fromiter : Create an array from an iterator.\n    fromfunction : Construct an array by executing a function on grid\n                   positions.\n\n    Examples\n    --------\n    Convert a list into an array.  If all elements are finite\n    ``asarray_chkfinite`` is identical to ``asarray``.\n\n    >>> a = [1, 2]\n    >>> np.asarray_chkfinite(a, dtype=float)\n    array([1., 2.])\n\n    Raises ValueError if array_like contains Nans or Infs.\n\n    >>> a = [1, 2, np.inf]\n    >>> try:\n    ...     np.asarray_chkfinite(a)\n    ... except ValueError:\n    ...     print('ValueError')\n    ...\n    ValueError\n\n    \"\"\"\n    a = asarray(a, dtype=dtype, order=order)\n    if a.dtype.char in typecodes['AllFloat'] and not np.isfinite(a).all():\n        raise ValueError(\n            \"array must not contain infs or NaNs\")\n    return a\n\n\ndef _piecewise_dispatcher(x, condlist, funclist, *args, **kw):\n    yield x\n    # support the undocumented behavior of allowing scalars\n    if np.iterable(condlist):\n        yield from condlist\n\n\n@array_function_dispatch(_piecewise_dispatcher)\ndef piecewise(x, condlist, funclist, *args, **kw):\n    \"\"\"\n    Evaluate a piecewise-defined function.\n\n    Given a set of conditions and corresponding functions, evaluate each\n    function on the input data wherever its condition is true.\n\n    Parameters\n    ----------\n    x : ndarray or scalar\n        The input domain.\n    condlist : list of bool arrays or bool scalars\n        Each boolean array corresponds to a function in `funclist`.  Wherever\n        `condlist[i]` is True, `funclist[i](x)` is used as the output value.\n\n        Each boolean array in `condlist` selects a piece of `x`,\n        and should therefore be of the same shape as `x`.\n\n        The length of `condlist` must correspond to that of `funclist`.\n        If one extra function is given, i.e. if\n        ``len(funclist) == len(condlist) + 1``, then that extra function\n        is the default value, used wherever all conditions are false.\n    funclist : list of callables, f(x,*args,**kw), or scalars\n        Each function is evaluated over `x` wherever its corresponding\n        condition is True.  It should take a 1d array as input and give an 1d\n        array or a scalar value as output.  If, instead of a callable,\n        a scalar is provided then a constant function (``lambda x: scalar``) is\n        assumed.\n    args : tuple, optional\n        Any further arguments given to `piecewise` are passed to the functions\n        upon execution, i.e., if called ``piecewise(..., ..., 1, 'a')``, then\n        each function is called as ``f(x, 1, 'a')``.\n    kw : dict, optional\n        Keyword arguments used in calling `piecewise` are passed to the\n        functions upon execution, i.e., if called\n        ``piecewise(..., ..., alpha=1)``, then each function is called as\n        ``f(x, alpha=1)``.\n\n    Returns\n    -------\n    out : ndarray\n        The output is the same shape and type as x and is found by\n        calling the functions in `funclist` on the appropriate portions of `x`,\n        as defined by the boolean arrays in `condlist`.  Portions not covered\n        by any condition have a default value of 0.\n\n\n    See Also\n    --------\n    choose, select, where\n\n    Notes\n    -----\n    This is similar to choose or select, except that functions are\n    evaluated on elements of `x` that satisfy the corresponding condition from\n    `condlist`.\n\n    The result is::\n\n            |--\n            |funclist[0](x[condlist[0]])\n      out = |funclist[1](x[condlist[1]])\n            |...\n            |funclist[n2](x[condlist[n2]])\n            |--\n\n    Examples\n    --------\n    Define the sigma function, which is -1 for ``x < 0`` and +1 for ``x >= 0``.\n\n    >>> x = np.linspace(-2.5, 2.5, 6)\n    >>> np.piecewise(x, [x < 0, x >= 0], [-1, 1])\n    array([-1., -1., -1.,  1.,  1.,  1.])\n\n    Define the absolute value, which is ``-x`` for ``x <0`` and ``x`` for\n    ``x >= 0``.\n\n    >>> np.piecewise(x, [x < 0, x >= 0], [lambda x: -x, lambda x: x])\n    array([2.5,  1.5,  0.5,  0.5,  1.5,  2.5])\n\n    Apply the same function to a scalar value.\n\n    >>> y = -2\n    >>> np.piecewise(y, [y < 0, y >= 0], [lambda x: -x, lambda x: x])\n    array(2)\n\n    \"\"\"\n    x = asanyarray(x)\n    n2 = len(funclist)\n\n    # undocumented: single condition is promoted to a list of one condition\n    if isscalar(condlist) or (\n            not isinstance(condlist[0], (list, ndarray)) and x.ndim != 0):\n        condlist = [condlist]\n\n    condlist = array(condlist, dtype=bool)\n    n = len(condlist)\n\n    if n == n2 - 1:  # compute the \"otherwise\" condition.\n        condelse = ~np.any(condlist, axis=0, keepdims=True)\n        condlist = np.concatenate([condlist, condelse], axis=0)\n        n += 1\n    elif n != n2:\n        raise ValueError(\n            \"with {} condition(s), either {} or {} functions are expected\"\n            .format(n, n, n+1)\n        )\n\n    y = zeros(x.shape, x.dtype)\n    for cond, func in zip(condlist, funclist):\n        if not isinstance(func, collections.abc.Callable):\n            y[cond] = func\n        else:\n            vals = x[cond]\n            if vals.size > 0:\n                y[cond] = func(vals, *args, **kw)\n\n    return y\n\n\ndef _select_dispatcher(condlist, choicelist, default=None):\n    yield from condlist\n    yield from choicelist\n\n\n@array_function_dispatch(_select_dispatcher)\ndef select(condlist, choicelist, default=0):\n    \"\"\"\n    Return an array drawn from elements in choicelist, depending on conditions.\n\n    Parameters\n    ----------\n    condlist : list of bool ndarrays\n        The list of conditions which determine from which array in `choicelist`\n        the output elements are taken. When multiple conditions are satisfied,\n        the first one encountered in `condlist` is used.\n    choicelist : list of ndarrays\n        The list of arrays from which the output elements are taken. It has\n        to be of the same length as `condlist`.\n    default : scalar, optional\n        The element inserted in `output` when all conditions evaluate to False.\n\n    Returns\n    -------\n    output : ndarray\n        The output at position m is the m-th element of the array in\n        `choicelist` where the m-th element of the corresponding array in\n        `condlist` is True.\n\n    See Also\n    --------\n    where : Return elements from one of two arrays depending on condition.\n    take, choose, compress, diag, diagonal\n\n    Examples\n    --------\n    >>> x = np.arange(10)\n    >>> condlist = [x<3, x>5]\n    >>> choicelist = [x, x**2]\n    >>> np.select(condlist, choicelist)\n    array([ 0,  1,  2, ..., 49, 64, 81])\n\n    \"\"\"\n    # Check the size of condlist and choicelist are the same, or abort.\n    if len(condlist) != len(choicelist):\n        raise ValueError(\n            'list of cases must be same length as list of conditions')\n\n    # Now that the dtype is known, handle the deprecated select([], []) case\n    if len(condlist) == 0:\n        raise ValueError(\"select with an empty condition list is not possible\")\n\n    choicelist = [np.asarray(choice) for choice in choicelist]\n    choicelist.append(np.asarray(default))\n\n    # need to get the result type before broadcasting for correct scalar\n    # behaviour\n    dtype = np.result_type(*choicelist)\n\n    # Convert conditions to arrays and broadcast conditions and choices\n    # as the shape is needed for the result. Doing it separately optimizes\n    # for example when all choices are scalars.\n    condlist = np.broadcast_arrays(*condlist)\n    choicelist = np.broadcast_arrays(*choicelist)\n\n    # If cond array is not an ndarray in boolean format or scalar bool, abort.\n    for i, cond in enumerate(condlist):\n        if cond.dtype.type is not np.bool_:\n            raise TypeError(\n                'invalid entry {} in condlist: should be boolean ndarray'.format(i))\n\n    if choicelist[0].ndim == 0:\n        # This may be common, so avoid the call.\n        result_shape = condlist[0].shape\n    else:\n        result_shape = np.broadcast_arrays(condlist[0], choicelist[0])[0].shape\n\n    result = np.full(result_shape, choicelist[-1], dtype)\n\n    # Use np.copyto to burn each choicelist array onto result, using the\n    # corresponding condlist as a boolean mask. This is done in reverse\n    # order since the first choice should take precedence.\n    choicelist = choicelist[-2::-1]\n    condlist = condlist[::-1]\n    for choice, cond in zip(choicelist, condlist):\n        np.copyto(result, choice, where=cond)\n\n    return result\n\n\ndef _copy_dispatcher(a, order=None, subok=None):\n    return (a,)\n\n\n@array_function_dispatch(_copy_dispatcher)\ndef copy(a, order='K', subok=False):\n    \"\"\"\n    Return an array copy of the given object.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    order : {'C', 'F', 'A', 'K'}, optional\n        Controls the memory layout of the copy. 'C' means C-order,\n        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n        'C' otherwise. 'K' means match the layout of `a` as closely\n        as possible. (Note that this function and :meth:`ndarray.copy` are very\n        similar, but have different default values for their order=\n        arguments.)\n    subok : bool, optional\n        If True, then sub-classes will be passed-through, otherwise the\n        returned array will be forced to be a base-class array (defaults to False).\n\n        .. versionadded:: 1.19.0\n\n    Returns\n    -------\n    arr : ndarray\n        Array interpretation of `a`.\n\n    See Also\n    --------\n    ndarray.copy : Preferred method for creating an array copy\n\n    Notes\n    -----\n    This is equivalent to:\n\n    >>> np.array(a, copy=True)  #doctest: +SKIP\n\n    Examples\n    --------\n    Create an array x, with a reference y and a copy z:\n\n    >>> x = np.array([1, 2, 3])\n    >>> y = x\n    >>> z = np.copy(x)\n\n    Note that, when we modify x, y changes, but not z:\n\n    >>> x[0] = 10\n    >>> x[0] == y[0]\n    True\n    >>> x[0] == z[0]\n    False\n\n    Note that np.copy is a shallow copy and will not copy object\n    elements within arrays. This is mainly important for arrays\n    containing Python objects. The new array will contain the\n    same object which may lead to surprises if that object can\n    be modified (is mutable):\n\n    >>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n    >>> b = np.copy(a)\n    >>> b[2][0] = 10\n    >>> a\n    array([1, 'm', list([10, 3, 4])], dtype=object)\n\n    To ensure all elements within an ``object`` array are copied,\n    use `copy.deepcopy`:\n\n    >>> import copy\n    >>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n    >>> c = copy.deepcopy(a)\n    >>> c[2][0] = 10\n    >>> c\n    array([1, 'm', list([10, 3, 4])], dtype=object)\n    >>> a\n    array([1, 'm', list([2, 3, 4])], dtype=object)\n\n    \"\"\"\n    return array(a, order=order, subok=subok, copy=True)\n\n# Basic operations\n\n\ndef _gradient_dispatcher(f, *varargs, axis=None, edge_order=None):\n    yield f\n    yield from varargs\n\n\n@array_function_dispatch(_gradient_dispatcher)\ndef gradient(f, *varargs, axis=None, edge_order=1):\n    \"\"\"\n    Return the gradient of an N-dimensional array.\n\n    The gradient is computed using second order accurate central differences\n    in the interior points and either first or second order accurate one-sides\n    (forward or backwards) differences at the boundaries.\n    The returned gradient hence has the same shape as the input array.\n\n    Parameters\n    ----------\n    f : array_like\n        An N-dimensional array containing samples of a scalar function.\n    varargs : list of scalar or array, optional\n        Spacing between f values. Default unitary spacing for all dimensions.\n        Spacing can be specified using:\n\n        1. single scalar to specify a sample distance for all dimensions.\n        2. N scalars to specify a constant sample distance for each dimension.\n           i.e. `dx`, `dy`, `dz`, ...\n        3. N arrays to specify the coordinates of the values along each\n           dimension of F. The length of the array must match the size of\n           the corresponding dimension\n        4. Any combination of N scalars/arrays with the meaning of 2. and 3.\n\n        If `axis` is given, the number of varargs must equal the number of axes.\n        Default: 1.\n\n    edge_order : {1, 2}, optional\n        Gradient is calculated using N-th order accurate differences\n        at the boundaries. Default: 1.\n\n        .. versionadded:: 1.9.1\n\n    axis : None or int or tuple of ints, optional\n        Gradient is calculated only along the given axis or axes\n        The default (axis = None) is to calculate the gradient for all the axes\n        of the input array. axis may be negative, in which case it counts from\n        the last to the first axis.\n\n        .. versionadded:: 1.11.0\n\n    Returns\n    -------\n    gradient : ndarray or list of ndarray\n        A set of ndarrays (or a single ndarray if there is only one dimension)\n        corresponding to the derivatives of f with respect to each dimension.\n        Each derivative has the same shape as f.\n\n    Examples\n    --------\n    >>> f = np.array([1, 2, 4, 7, 11, 16], dtype=float)\n    >>> np.gradient(f)\n    array([1. , 1.5, 2.5, 3.5, 4.5, 5. ])\n    >>> np.gradient(f, 2)\n    array([0.5 ,  0.75,  1.25,  1.75,  2.25,  2.5 ])\n\n    Spacing can be also specified with an array that represents the coordinates\n    of the values F along the dimensions.\n    For instance a uniform spacing:\n\n    >>> x = np.arange(f.size)\n    >>> np.gradient(f, x)\n    array([1. ,  1.5,  2.5,  3.5,  4.5,  5. ])\n\n    Or a non uniform one:\n\n    >>> x = np.array([0., 1., 1.5, 3.5, 4., 6.], dtype=float)\n    >>> np.gradient(f, x)\n    array([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])\n\n    For two dimensional arrays, the return will be two arrays ordered by\n    axis. In this example the first array stands for the gradient in\n    rows and the second one in columns direction:\n\n    >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float))\n    [array([[ 2.,  2., -1.],\n           [ 2.,  2., -1.]]), array([[1. , 2.5, 4. ],\n           [1. , 1. , 1. ]])]\n\n    In this example the spacing is also specified:\n    uniform for axis=0 and non uniform for axis=1\n\n    >>> dx = 2.\n    >>> y = [1., 1.5, 3.5]\n    >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float), dx, y)\n    [array([[ 1. ,  1. , -0.5],\n           [ 1. ,  1. , -0.5]]), array([[2. , 2. , 2. ],\n           [2. , 1.7, 0.5]])]\n\n    It is possible to specify how boundaries are treated using `edge_order`\n\n    >>> x = np.array([0, 1, 2, 3, 4])\n    >>> f = x**2\n    >>> np.gradient(f, edge_order=1)\n    array([1.,  2.,  4.,  6.,  7.])\n    >>> np.gradient(f, edge_order=2)\n    array([0., 2., 4., 6., 8.])\n\n    The `axis` keyword can be used to specify a subset of axes of which the\n    gradient is calculated\n\n    >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float), axis=0)\n    array([[ 2.,  2., -1.],\n           [ 2.,  2., -1.]])\n\n    Notes\n    -----\n    Assuming that :math:`f\\\\in C^{3}` (i.e., :math:`f` has at least 3 continuous\n    derivatives) and let :math:`h_{*}` be a non-homogeneous stepsize, we\n    minimize the \"consistency error\" :math:`\\\\eta_{i}` between the true gradient\n    and its estimate from a linear combination of the neighboring grid-points:\n\n    .. math::\n\n        \\\\eta_{i} = f_{i}^{\\\\left(1\\\\right)} -\n                    \\\\left[ \\\\alpha f\\\\left(x_{i}\\\\right) +\n                            \\\\beta f\\\\left(x_{i} + h_{d}\\\\right) +\n                            \\\\gamma f\\\\left(x_{i}-h_{s}\\\\right)\n                    \\\\right]\n\n    By substituting :math:`f(x_{i} + h_{d})` and :math:`f(x_{i} - h_{s})`\n    with their Taylor series expansion, this translates into solving\n    the following the linear system:\n\n    .. math::\n\n        \\\\left\\\\{\n            \\\\begin{array}{r}\n                \\\\alpha+\\\\beta+\\\\gamma=0 \\\\\\\\\n                \\\\beta h_{d}-\\\\gamma h_{s}=1 \\\\\\\\\n                \\\\beta h_{d}^{2}+\\\\gamma h_{s}^{2}=0\n            \\\\end{array}\n        \\\\right.\n\n    The resulting approximation of :math:`f_{i}^{(1)}` is the following:\n\n    .. math::\n\n        \\\\hat f_{i}^{(1)} =\n            \\\\frac{\n                h_{s}^{2}f\\\\left(x_{i} + h_{d}\\\\right)\n                + \\\\left(h_{d}^{2} - h_{s}^{2}\\\\right)f\\\\left(x_{i}\\\\right)\n                - h_{d}^{2}f\\\\left(x_{i}-h_{s}\\\\right)}\n                { h_{s}h_{d}\\\\left(h_{d} + h_{s}\\\\right)}\n            + \\\\mathcal{O}\\\\left(\\\\frac{h_{d}h_{s}^{2}\n                                + h_{s}h_{d}^{2}}{h_{d}\n                                + h_{s}}\\\\right)\n\n    It is worth noting that if :math:`h_{s}=h_{d}`\n    (i.e., data are evenly spaced)\n    we find the standard second order approximation:\n\n    .. math::\n\n        \\\\hat f_{i}^{(1)}=\n            \\\\frac{f\\\\left(x_{i+1}\\\\right) - f\\\\left(x_{i-1}\\\\right)}{2h}\n            + \\\\mathcal{O}\\\\left(h^{2}\\\\right)\n\n    With a similar procedure the forward/backward approximations used for\n    boundaries can be derived.\n\n    References\n    ----------\n    .. [1]  Quarteroni A., Sacco R., Saleri F. (2007) Numerical Mathematics\n            (Texts in Applied Mathematics). New York: Springer.\n    .. [2]  Durran D. R. (1999) Numerical Methods for Wave Equations\n            in Geophysical Fluid Dynamics. New York: Springer.\n    .. [3]  Fornberg B. (1988) Generation of Finite Difference Formulas on\n            Arbitrarily Spaced Grids,\n            Mathematics of Computation 51, no. 184 : 699-706.\n            `PDF <http://www.ams.org/journals/mcom/1988-51-184/\n            S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf>`_.\n    \"\"\"\n    f = np.asanyarray(f)\n    N = f.ndim  # number of dimensions\n\n    if axis is None:\n        axes = tuple(range(N))\n    else:\n        axes = _nx.normalize_axis_tuple(axis, N)\n\n    len_axes = len(axes)\n    n = len(varargs)\n    if n == 0:\n        # no spacing argument - use 1 in all axes\n        dx = [1.0] * len_axes\n    elif n == 1 and np.ndim(varargs[0]) == 0:\n        # single scalar for all axes\n        dx = varargs * len_axes\n    elif n == len_axes:\n        # scalar or 1d array for each axis\n        dx = list(varargs)\n        for i, distances in enumerate(dx):\n            distances = np.asanyarray(distances)\n            if distances.ndim == 0:\n                continue\n            elif distances.ndim != 1:\n                raise ValueError(\"distances must be either scalars or 1d\")\n            if len(distances) != f.shape[axes[i]]:\n                raise ValueError(\"when 1d, distances must match \"\n                                 \"the length of the corresponding dimension\")\n            if np.issubdtype(distances.dtype, np.integer):\n                # Convert numpy integer types to float64 to avoid modular\n                # arithmetic in np.diff(distances).\n                distances = distances.astype(np.float64)\n            diffx = np.diff(distances)\n            # if distances are constant reduce to the scalar case\n            # since it brings a consistent speedup\n            if (diffx == diffx[0]).all():\n                diffx = diffx[0]\n            dx[i] = diffx\n    else:\n        raise TypeError(\"invalid number of arguments\")\n\n    if edge_order > 2:\n        raise ValueError(\"'edge_order' greater than 2 not supported\")\n\n    # use central differences on interior and one-sided differences on the\n    # endpoints. This preserves second order-accuracy over the full domain.\n\n    outvals = []\n\n    # create slice objects --- initially all are [:, :, ..., :]\n    slice1 = [slice(None)]*N\n    slice2 = [slice(None)]*N\n    slice3 = [slice(None)]*N\n    slice4 = [slice(None)]*N\n\n    otype = f.dtype\n    if otype.type is np.datetime64:\n        # the timedelta dtype with the same unit information\n        otype = np.dtype(otype.name.replace('datetime', 'timedelta'))\n        # view as timedelta to allow addition\n        f = f.view(otype)\n    elif otype.type is np.timedelta64:\n        pass\n    elif np.issubdtype(otype, np.inexact):\n        pass\n    else:\n        # All other types convert to floating point.\n        # First check if f is a numpy integer type; if so, convert f to float64\n        # to avoid modular arithmetic when computing the changes in f.\n        if np.issubdtype(otype, np.integer):\n            f = f.astype(np.float64)\n        otype = np.float64\n\n    for axis, ax_dx in zip(axes, dx):\n        if f.shape[axis] < edge_order + 1:\n            raise ValueError(\n                \"Shape of array too small to calculate a numerical gradient, \"\n                \"at least (edge_order + 1) elements are required.\")\n        # result allocation\n        out = np.empty_like(f, dtype=otype)\n\n        # spacing for the current axis\n        uniform_spacing = np.ndim(ax_dx) == 0\n\n        # Numerical differentiation: 2nd order interior\n        slice1[axis] = slice(1, -1)\n        slice2[axis] = slice(None, -2)\n        slice3[axis] = slice(1, -1)\n        slice4[axis] = slice(2, None)\n\n        if uniform_spacing:\n            out[tuple(slice1)] = (f[tuple(slice4)] - f[tuple(slice2)]) / (2. * ax_dx)\n        else:\n            dx1 = ax_dx[0:-1]\n            dx2 = ax_dx[1:]\n            a = -(dx2)/(dx1 * (dx1 + dx2))\n            b = (dx2 - dx1) / (dx1 * dx2)\n            c = dx1 / (dx2 * (dx1 + dx2))\n            # fix the shape for broadcasting\n            shape = np.ones(N, dtype=int)\n            shape[axis] = -1\n            a.shape = b.shape = c.shape = shape\n            # 1D equivalent -- out[1:-1] = a * f[:-2] + b * f[1:-1] + c * f[2:]\n            out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]\n\n        # Numerical differentiation: 1st order edges\n        if edge_order == 1:\n            slice1[axis] = 0\n            slice2[axis] = 1\n            slice3[axis] = 0\n            dx_0 = ax_dx if uniform_spacing else ax_dx[0]\n            # 1D equivalent -- out[0] = (f[1] - f[0]) / (x[1] - x[0])\n            out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0\n\n            slice1[axis] = -1\n            slice2[axis] = -1\n            slice3[axis] = -2\n            dx_n = ax_dx if uniform_spacing else ax_dx[-1]\n            # 1D equivalent -- out[-1] = (f[-1] - f[-2]) / (x[-1] - x[-2])\n            out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n\n        # Numerical differentiation: 2nd order edges\n        else:\n            slice1[axis] = 0\n            slice2[axis] = 0\n            slice3[axis] = 1\n            slice4[axis] = 2\n            if uniform_spacing:\n                a = -1.5 / ax_dx\n                b = 2. / ax_dx\n                c = -0.5 / ax_dx\n            else:\n                dx1 = ax_dx[0]\n                dx2 = ax_dx[1]\n                a = -(2. * dx1 + dx2)/(dx1 * (dx1 + dx2))\n                b = (dx1 + dx2) / (dx1 * dx2)\n                c = - dx1 / (dx2 * (dx1 + dx2))\n            # 1D equivalent -- out[0] = a * f[0] + b * f[1] + c * f[2]\n            out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]\n\n            slice1[axis] = -1\n            slice2[axis] = -3\n            slice3[axis] = -2\n            slice4[axis] = -1\n            if uniform_spacing:\n                a = 0.5 / ax_dx\n                b = -2. / ax_dx\n                c = 1.5 / ax_dx\n            else:\n                dx1 = ax_dx[-2]\n                dx2 = ax_dx[-1]\n                a = (dx2) / (dx1 * (dx1 + dx2))\n                b = - (dx2 + dx1) / (dx1 * dx2)\n                c = (2. * dx2 + dx1) / (dx2 * (dx1 + dx2))\n            # 1D equivalent -- out[-1] = a * f[-3] + b * f[-2] + c * f[-1]\n            out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]\n\n        outvals.append(out)\n\n        # reset the slice object in this dimension to \":\"\n        slice1[axis] = slice(None)\n        slice2[axis] = slice(None)\n        slice3[axis] = slice(None)\n        slice4[axis] = slice(None)\n\n    if len_axes == 1:\n        return outvals[0]\n    else:\n        return outvals\n\n\ndef _diff_dispatcher(a, n=None, axis=None, prepend=None, append=None):\n    return (a, prepend, append)\n\n\n@array_function_dispatch(_diff_dispatcher)\ndef diff(a, n=1, axis=-1, prepend=np._NoValue, append=np._NoValue):\n    \"\"\"\n    Calculate the n-th discrete difference along the given axis.\n\n    The first difference is given by ``out[i] = a[i+1] - a[i]`` along\n    the given axis, higher differences are calculated by using `diff`\n    recursively.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array\n    n : int, optional\n        The number of times values are differenced. If zero, the input\n        is returned as-is.\n    axis : int, optional\n        The axis along which the difference is taken, default is the\n        last axis.\n    prepend, append : array_like, optional\n        Values to prepend or append to `a` along axis prior to\n        performing the difference.  Scalar values are expanded to\n        arrays with length 1 in the direction of axis and the shape\n        of the input array in along all other axes.  Otherwise the\n        dimension and shape must match `a` except along axis.\n\n        .. versionadded:: 1.16.0\n\n    Returns\n    -------\n    diff : ndarray\n        The n-th differences. The shape of the output is the same as `a`\n        except along `axis` where the dimension is smaller by `n`. The\n        type of the output is the same as the type of the difference\n        between any two elements of `a`. This is the same as the type of\n        `a` in most cases. A notable exception is `datetime64`, which\n        results in a `timedelta64` output array.\n\n    See Also\n    --------\n    gradient, ediff1d, cumsum\n\n    Notes\n    -----\n    Type is preserved for boolean arrays, so the result will contain\n    `False` when consecutive elements are the same and `True` when they\n    differ.\n\n    For unsigned integer arrays, the results will also be unsigned. This\n    should not be surprising, as the result is consistent with\n    calculating the difference directly:\n\n    >>> u8_arr = np.array([1, 0], dtype=np.uint8)\n    >>> np.diff(u8_arr)\n    array([255], dtype=uint8)\n    >>> u8_arr[1,...] - u8_arr[0,...]\n    255\n\n    If this is not desirable, then the array should be cast to a larger\n    integer type first:\n\n    >>> i16_arr = u8_arr.astype(np.int16)\n    >>> np.diff(i16_arr)\n    array([-1], dtype=int16)\n\n    Examples\n    --------\n    >>> x = np.array([1, 2, 4, 7, 0])\n    >>> np.diff(x)\n    array([ 1,  2,  3, -7])\n    >>> np.diff(x, n=2)\n    array([  1,   1, -10])\n\n    >>> x = np.array([[1, 3, 6, 10], [0, 5, 6, 8]])\n    >>> np.diff(x)\n    array([[2, 3, 4],\n           [5, 1, 2]])\n    >>> np.diff(x, axis=0)\n    array([[-1,  2,  0, -2]])\n\n    >>> x = np.arange('1066-10-13', '1066-10-16', dtype=np.datetime64)\n    >>> np.diff(x)\n    array([1, 1], dtype='timedelta64[D]')\n\n    \"\"\"\n    if n == 0:\n        return a\n    if n < 0:\n        raise ValueError(\n            \"order must be non-negative but got \" + repr(n))\n\n    a = asanyarray(a)\n    nd = a.ndim\n    if nd == 0:\n        raise ValueError(\"diff requires input that is at least one dimensional\")\n    axis = normalize_axis_index(axis, nd)\n\n    combined = []\n    if prepend is not np._NoValue:\n        prepend = np.asanyarray(prepend)\n        if prepend.ndim == 0:\n            shape = list(a.shape)\n            shape[axis] = 1\n            prepend = np.broadcast_to(prepend, tuple(shape))\n        combined.append(prepend)\n\n    combined.append(a)\n\n    if append is not np._NoValue:\n        append = np.asanyarray(append)\n        if append.ndim == 0:\n            shape = list(a.shape)\n            shape[axis] = 1\n            append = np.broadcast_to(append, tuple(shape))\n        combined.append(append)\n\n    if len(combined) > 1:\n        a = np.concatenate(combined, axis)\n\n    slice1 = [slice(None)] * nd\n    slice2 = [slice(None)] * nd\n    slice1[axis] = slice(1, None)\n    slice2[axis] = slice(None, -1)\n    slice1 = tuple(slice1)\n    slice2 = tuple(slice2)\n\n    op = not_equal if a.dtype == np.bool_ else subtract\n    for _ in range(n):\n        a = op(a[slice1], a[slice2])\n\n    return a\n\n\ndef _interp_dispatcher(x, xp, fp, left=None, right=None, period=None):\n    return (x, xp, fp)\n\n\n@array_function_dispatch(_interp_dispatcher)\ndef interp(x, xp, fp, left=None, right=None, period=None):\n    \"\"\"\n    One-dimensional linear interpolation.\n\n    Returns the one-dimensional piecewise linear interpolant to a function\n    with given discrete data points (`xp`, `fp`), evaluated at `x`.\n\n    Parameters\n    ----------\n    x : array_like\n        The x-coordinates at which to evaluate the interpolated values.\n\n    xp : 1-D sequence of floats\n        The x-coordinates of the data points, must be increasing if argument\n        `period` is not specified. Otherwise, `xp` is internally sorted after\n        normalizing the periodic boundaries with ``xp = xp % period``.\n\n    fp : 1-D sequence of float or complex\n        The y-coordinates of the data points, same length as `xp`.\n\n    left : optional float or complex corresponding to fp\n        Value to return for `x < xp[0]`, default is `fp[0]`.\n\n    right : optional float or complex corresponding to fp\n        Value to return for `x > xp[-1]`, default is `fp[-1]`.\n\n    period : None or float, optional\n        A period for the x-coordinates. This parameter allows the proper\n        interpolation of angular x-coordinates. Parameters `left` and `right`\n        are ignored if `period` is specified.\n\n        .. versionadded:: 1.10.0\n\n    Returns\n    -------\n    y : float or complex (corresponding to fp) or ndarray\n        The interpolated values, same shape as `x`.\n\n    Raises\n    ------\n    ValueError\n        If `xp` and `fp` have different length\n        If `xp` or `fp` are not 1-D sequences\n        If `period == 0`\n\n    See Also\n    --------\n    scipy.interpolate\n\n    Notes\n    -----\n    The x-coordinate sequence is expected to be increasing, but this is not\n    explicitly enforced.  However, if the sequence `xp` is non-increasing,\n    interpolation results are meaningless.\n\n    Note that, since NaN is unsortable, `xp` also cannot contain NaNs.\n\n    A simple check for `xp` being strictly increasing is::\n\n        np.all(np.diff(xp) > 0)\n\n    Examples\n    --------\n    >>> xp = [1, 2, 3]\n    >>> fp = [3, 2, 0]\n    >>> np.interp(2.5, xp, fp)\n    1.0\n    >>> np.interp([0, 1, 1.5, 2.72, 3.14], xp, fp)\n    array([3.  , 3.  , 2.5 , 0.56, 0.  ])\n    >>> UNDEF = -99.0\n    >>> np.interp(3.14, xp, fp, right=UNDEF)\n    -99.0\n\n    Plot an interpolant to the sine function:\n\n    >>> x = np.linspace(0, 2*np.pi, 10)\n    >>> y = np.sin(x)\n    >>> xvals = np.linspace(0, 2*np.pi, 50)\n    >>> yinterp = np.interp(xvals, x, y)\n    >>> import matplotlib.pyplot as plt\n    >>> plt.plot(x, y, 'o')\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.plot(xvals, yinterp, '-x')\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.show()\n\n    Interpolation with periodic x-coordinates:\n\n    >>> x = [-180, -170, -185, 185, -10, -5, 0, 365]\n    >>> xp = [190, -190, 350, -350]\n    >>> fp = [5, 10, 3, 4]\n    >>> np.interp(x, xp, fp, period=360)\n    array([7.5 , 5.  , 8.75, 6.25, 3.  , 3.25, 3.5 , 3.75])\n\n    Complex interpolation:\n\n    >>> x = [1.5, 4.0]\n    >>> xp = [2,3,5]\n    >>> fp = [1.0j, 0, 2+3j]\n    >>> np.interp(x, xp, fp)\n    array([0.+1.j , 1.+1.5j])\n\n    \"\"\"\n\n    fp = np.asarray(fp)\n\n    if np.iscomplexobj(fp):\n        interp_func = compiled_interp_complex\n        input_dtype = np.complex128\n    else:\n        interp_func = compiled_interp\n        input_dtype = np.float64\n\n    if period is not None:\n        if period == 0:\n            raise ValueError(\"period must be a non-zero value\")\n        period = abs(period)\n        left = None\n        right = None\n\n        x = np.asarray(x, dtype=np.float64)\n        xp = np.asarray(xp, dtype=np.float64)\n        fp = np.asarray(fp, dtype=input_dtype)\n\n        if xp.ndim != 1 or fp.ndim != 1:\n            raise ValueError(\"Data points must be 1-D sequences\")\n        if xp.shape[0] != fp.shape[0]:\n            raise ValueError(\"fp and xp are not of the same length\")\n        # normalizing periodic boundaries\n        x = x % period\n        xp = xp % period\n        asort_xp = np.argsort(xp)\n        xp = xp[asort_xp]\n        fp = fp[asort_xp]\n        xp = np.concatenate((xp[-1:]-period, xp, xp[0:1]+period))\n        fp = np.concatenate((fp[-1:], fp, fp[0:1]))\n\n    return interp_func(x, xp, fp, left, right)\n\n\ndef _angle_dispatcher(z, deg=None):\n    return (z,)\n\n\n@array_function_dispatch(_angle_dispatcher)\ndef angle(z, deg=False):\n    \"\"\"\n    Return the angle of the complex argument.\n\n    Parameters\n    ----------\n    z : array_like\n        A complex number or sequence of complex numbers.\n    deg : bool, optional\n        Return angle in degrees if True, radians if False (default).\n\n    Returns\n    -------\n    angle : ndarray or scalar\n        The counterclockwise angle from the positive real axis on the complex\n        plane in the range ``(-pi, pi]``, with dtype as numpy.float64.\n\n        .. versionchanged:: 1.16.0\n            This function works on subclasses of ndarray like `ma.array`.\n\n    See Also\n    --------\n    arctan2\n    absolute\n\n    Notes\n    -----\n    Although the angle of the complex number 0 is undefined, ``numpy.angle(0)``\n    returns the value 0.\n\n    Examples\n    --------\n    >>> np.angle([1.0, 1.0j, 1+1j])               # in radians\n    array([ 0.        ,  1.57079633,  0.78539816]) # may vary\n    >>> np.angle(1+1j, deg=True)                  # in degrees\n    45.0\n\n    \"\"\"\n    z = asanyarray(z)\n    if issubclass(z.dtype.type, _nx.complexfloating):\n        zimag = z.imag\n        zreal = z.real\n    else:\n        zimag = 0\n        zreal = z\n\n    a = arctan2(zimag, zreal)\n    if deg:\n        a *= 180/pi\n    return a\n\n\ndef _unwrap_dispatcher(p, discont=None, axis=None):\n    return (p,)\n\n\n@array_function_dispatch(_unwrap_dispatcher)\ndef unwrap(p, discont=pi, axis=-1):\n    \"\"\"\n    Unwrap by changing deltas between values to 2*pi complement.\n\n    Unwrap radian phase `p` by changing absolute jumps greater than\n    `discont` to their 2*pi complement along the given axis.\n\n    Parameters\n    ----------\n    p : array_like\n        Input array.\n    discont : float, optional\n        Maximum discontinuity between values, default is ``pi``.\n    axis : int, optional\n        Axis along which unwrap will operate, default is the last axis.\n\n    Returns\n    -------\n    out : ndarray\n        Output array.\n\n    See Also\n    --------\n    rad2deg, deg2rad\n\n    Notes\n    -----\n    If the discontinuity in `p` is smaller than ``pi``, but larger than\n    `discont`, no unwrapping is done because taking the 2*pi complement\n    would only make the discontinuity larger.\n\n    Examples\n    --------\n    >>> phase = np.linspace(0, np.pi, num=5)\n    >>> phase[3:] += np.pi\n    >>> phase\n    array([ 0.        ,  0.78539816,  1.57079633,  5.49778714,  6.28318531]) # may vary\n    >>> np.unwrap(phase)\n    array([ 0.        ,  0.78539816,  1.57079633, -0.78539816,  0.        ]) # may vary\n\n    \"\"\"\n    p = asarray(p)\n    nd = p.ndim\n    dd = diff(p, axis=axis)\n    slice1 = [slice(None, None)]*nd     # full slices\n    slice1[axis] = slice(1, None)\n    slice1 = tuple(slice1)\n    ddmod = mod(dd + pi, 2*pi) - pi\n    _nx.copyto(ddmod, pi, where=(ddmod == -pi) & (dd > 0))\n    ph_correct = ddmod - dd\n    _nx.copyto(ph_correct, 0, where=abs(dd) < discont)\n    up = array(p, copy=True, dtype='d')\n    up[slice1] = p[slice1] + ph_correct.cumsum(axis)\n    return up\n\n\ndef _sort_complex(a):\n    return (a,)\n\n\n@array_function_dispatch(_sort_complex)\ndef sort_complex(a):\n    \"\"\"\n    Sort a complex array using the real part first, then the imaginary part.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array\n\n    Returns\n    -------\n    out : complex ndarray\n        Always returns a sorted complex array.\n\n    Examples\n    --------\n    >>> np.sort_complex([5, 3, 6, 2, 1])\n    array([1.+0.j, 2.+0.j, 3.+0.j, 5.+0.j, 6.+0.j])\n\n    >>> np.sort_complex([1 + 2j, 2 - 1j, 3 - 2j, 3 - 3j, 3 + 5j])\n    array([1.+2.j,  2.-1.j,  3.-3.j,  3.-2.j,  3.+5.j])\n\n    \"\"\"\n    b = array(a, copy=True)\n    b.sort()\n    if not issubclass(b.dtype.type, _nx.complexfloating):\n        if b.dtype.char in 'bhBH':\n            return b.astype('F')\n        elif b.dtype.char == 'g':\n            return b.astype('G')\n        else:\n            return b.astype('D')\n    else:\n        return b\n\n\ndef _trim_zeros(filt, trim=None):\n    return (filt,)\n\n\n@array_function_dispatch(_trim_zeros)\ndef trim_zeros(filt, trim='fb'):\n    \"\"\"\n    Trim the leading and/or trailing zeros from a 1-D array or sequence.\n\n    Parameters\n    ----------\n    filt : 1-D array or sequence\n        Input array.\n    trim : str, optional\n        A string with 'f' representing trim from front and 'b' to trim from\n        back. Default is 'fb', trim zeros from both front and back of the\n        array.\n\n    Returns\n    -------\n    trimmed : 1-D array or sequence\n        The result of trimming the input. The input data type is preserved.\n\n    Examples\n    --------\n    >>> a = np.array((0, 0, 0, 1, 2, 3, 0, 2, 1, 0))\n    >>> np.trim_zeros(a)\n    array([1, 2, 3, 0, 2, 1])\n\n    >>> np.trim_zeros(a, 'b')\n    array([0, 0, 0, ..., 0, 2, 1])\n\n    The input data type is preserved, list/tuple in means list/tuple out.\n\n    >>> np.trim_zeros([0, 1, 2, 0])\n    [1, 2]\n\n    \"\"\"\n\n    first = 0\n    trim = trim.upper()\n    if 'F' in trim:\n        for i in filt:\n            if i != 0.:\n                break\n            else:\n                first = first + 1\n    last = len(filt)\n    if 'B' in trim:\n        for i in filt[::-1]:\n            if i != 0.:\n                break\n            else:\n                last = last - 1\n    return filt[first:last]\n\n\ndef _extract_dispatcher(condition, arr):\n    return (condition, arr)\n\n\n@array_function_dispatch(_extract_dispatcher)\ndef extract(condition, arr):\n    \"\"\"\n    Return the elements of an array that satisfy some condition.\n\n    This is equivalent to ``np.compress(ravel(condition), ravel(arr))``.  If\n    `condition` is boolean ``np.extract`` is equivalent to ``arr[condition]``.\n\n    Note that `place` does the exact opposite of `extract`.\n\n    Parameters\n    ----------\n    condition : array_like\n        An array whose nonzero or True entries indicate the elements of `arr`\n        to extract.\n    arr : array_like\n        Input array of the same size as `condition`.\n\n    Returns\n    -------\n    extract : ndarray\n        Rank 1 array of values from `arr` where `condition` is True.\n\n    See Also\n    --------\n    take, put, copyto, compress, place\n\n    Examples\n    --------\n    >>> arr = np.arange(12).reshape((3, 4))\n    >>> arr\n    array([[ 0,  1,  2,  3],\n           [ 4,  5,  6,  7],\n           [ 8,  9, 10, 11]])\n    >>> condition = np.mod(arr, 3)==0\n    >>> condition\n    array([[ True, False, False,  True],\n           [False, False,  True, False],\n           [False,  True, False, False]])\n    >>> np.extract(condition, arr)\n    array([0, 3, 6, 9])\n\n\n    If `condition` is boolean:\n\n    >>> arr[condition]\n    array([0, 3, 6, 9])\n\n    \"\"\"\n    return _nx.take(ravel(arr), nonzero(ravel(condition))[0])\n\n\ndef _place_dispatcher(arr, mask, vals):\n    return (arr, mask, vals)\n\n\n@array_function_dispatch(_place_dispatcher)\ndef place(arr, mask, vals):\n    \"\"\"\n    Change elements of an array based on conditional and input values.\n\n    Similar to ``np.copyto(arr, vals, where=mask)``, the difference is that\n    `place` uses the first N elements of `vals`, where N is the number of\n    True values in `mask`, while `copyto` uses the elements where `mask`\n    is True.\n\n    Note that `extract` does the exact opposite of `place`.\n\n    Parameters\n    ----------\n    arr : ndarray\n        Array to put data into.\n    mask : array_like\n        Boolean mask array. Must have the same size as `a`.\n    vals : 1-D sequence\n        Values to put into `a`. Only the first N elements are used, where\n        N is the number of True values in `mask`. If `vals` is smaller\n        than N, it will be repeated, and if elements of `a` are to be masked,\n        this sequence must be non-empty.\n\n    See Also\n    --------\n    copyto, put, take, extract\n\n    Examples\n    --------\n    >>> arr = np.arange(6).reshape(2, 3)\n    >>> np.place(arr, arr>2, [44, 55])\n    >>> arr\n    array([[ 0,  1,  2],\n           [44, 55, 44]])\n\n    \"\"\"\n    if not isinstance(arr, np.ndarray):\n        raise TypeError(\"argument 1 must be numpy.ndarray, \"\n                        \"not {name}\".format(name=type(arr).__name__))\n\n    return _insert(arr, mask, vals)\n\n\ndef disp(mesg, device=None, linefeed=True):\n    \"\"\"\n    Display a message on a device.\n\n    Parameters\n    ----------\n    mesg : str\n        Message to display.\n    device : object\n        Device to write message. If None, defaults to ``sys.stdout`` which is\n        very similar to ``print``. `device` needs to have ``write()`` and\n        ``flush()`` methods.\n    linefeed : bool, optional\n        Option whether to print a line feed or not. Defaults to True.\n\n    Raises\n    ------\n    AttributeError\n        If `device` does not have a ``write()`` or ``flush()`` method.\n\n    Examples\n    --------\n    Besides ``sys.stdout``, a file-like object can also be used as it has\n    both required methods:\n\n    >>> from io import StringIO\n    >>> buf = StringIO()\n    >>> np.disp(u'\"Display\" in a file', device=buf)\n    >>> buf.getvalue()\n    '\"Display\" in a file\\\\n'\n\n    \"\"\"\n    if device is None:\n        device = sys.stdout\n    if linefeed:\n        device.write('%s\\n' % mesg)\n    else:\n        device.write('%s' % mesg)\n    device.flush()\n    return\n\n\n# See https://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html\n_DIMENSION_NAME = r'\\w+'\n_CORE_DIMENSION_LIST = '(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)\n_ARGUMENT = r'\\({}\\)'.format(_CORE_DIMENSION_LIST)\n_ARGUMENT_LIST = '{0:}(?:,{0:})*'.format(_ARGUMENT)\n_SIGNATURE = '^{0:}->{0:}$'.format(_ARGUMENT_LIST)\n\n\ndef _parse_gufunc_signature(signature):\n    \"\"\"\n    Parse string signatures for a generalized universal function.\n\n    Arguments\n    ---------\n    signature : string\n        Generalized universal function signature, e.g., ``(m,n),(n,p)->(m,p)``\n        for ``np.matmul``.\n\n    Returns\n    -------\n    Tuple of input and output core dimensions parsed from the signature, each\n    of the form List[Tuple[str, ...]].\n    \"\"\"\n    if not re.match(_SIGNATURE, signature):\n        raise ValueError(\n            'not a valid gufunc signature: {}'.format(signature))\n    return tuple([tuple(re.findall(_DIMENSION_NAME, arg))\n                  for arg in re.findall(_ARGUMENT, arg_list)]\n                 for arg_list in signature.split('->'))\n\n\ndef _update_dim_sizes(dim_sizes, arg, core_dims):\n    \"\"\"\n    Incrementally check and update core dimension sizes for a single argument.\n\n    Arguments\n    ---------\n    dim_sizes : Dict[str, int]\n        Sizes of existing core dimensions. Will be updated in-place.\n    arg : ndarray\n        Argument to examine.\n    core_dims : Tuple[str, ...]\n        Core dimensions for this argument.\n    \"\"\"\n    if not core_dims:\n        return\n\n    num_core_dims = len(core_dims)\n    if arg.ndim < num_core_dims:\n        raise ValueError(\n            '%d-dimensional argument does not have enough '\n            'dimensions for all core dimensions %r'\n            % (arg.ndim, core_dims))\n\n    core_shape = arg.shape[-num_core_dims:]\n    for dim, size in zip(core_dims, core_shape):\n        if dim in dim_sizes:\n            if size != dim_sizes[dim]:\n                raise ValueError(\n                    'inconsistent size for core dimension %r: %r vs %r'\n                    % (dim, size, dim_sizes[dim]))\n        else:\n            dim_sizes[dim] = size\n\n\ndef _parse_input_dimensions(args, input_core_dims):\n    \"\"\"\n    Parse broadcast and core dimensions for vectorize with a signature.\n\n    Arguments\n    ---------\n    args : Tuple[ndarray, ...]\n        Tuple of input arguments to examine.\n    input_core_dims : List[Tuple[str, ...]]\n        List of core dimensions corresponding to each input.\n\n    Returns\n    -------\n    broadcast_shape : Tuple[int, ...]\n        Common shape to broadcast all non-core dimensions to.\n    dim_sizes : Dict[str, int]\n        Common sizes for named core dimensions.\n    \"\"\"\n    broadcast_args = []\n    dim_sizes = {}\n    for arg, core_dims in zip(args, input_core_dims):\n        _update_dim_sizes(dim_sizes, arg, core_dims)\n        ndim = arg.ndim - len(core_dims)\n        dummy_array = np.lib.stride_tricks.as_strided(0, arg.shape[:ndim])\n        broadcast_args.append(dummy_array)\n    broadcast_shape = np.lib.stride_tricks._broadcast_shape(*broadcast_args)\n    return broadcast_shape, dim_sizes\n\n\ndef _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims):\n    \"\"\"Helper for calculating broadcast shapes with core dimensions.\"\"\"\n    return [broadcast_shape + tuple(dim_sizes[dim] for dim in core_dims)\n            for core_dims in list_of_core_dims]\n\n\ndef _create_arrays(broadcast_shape, dim_sizes, list_of_core_dims, dtypes):\n    \"\"\"Helper for creating output arrays in vectorize.\"\"\"\n    shapes = _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims)\n    arrays = tuple(np.empty(shape, dtype=dtype)\n                   for shape, dtype in zip(shapes, dtypes))\n    return arrays\n\n\n@set_module('numpy')\nclass vectorize:\n    \"\"\"\n    vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False,\n              signature=None)\n\n    Generalized function class.\n\n    Define a vectorized function which takes a nested sequence of objects or\n    numpy arrays as inputs and returns a single numpy array or a tuple of numpy\n    arrays. The vectorized function evaluates `pyfunc` over successive tuples\n    of the input arrays like the python map function, except it uses the\n    broadcasting rules of numpy.\n\n    The data type of the output of `vectorized` is determined by calling\n    the function with the first element of the input.  This can be avoided\n    by specifying the `otypes` argument.\n\n    Parameters\n    ----------\n    pyfunc : callable\n        A python function or method.\n    otypes : str or list of dtypes, optional\n        The output data type. It must be specified as either a string of\n        typecode characters or a list of data type specifiers. There should\n        be one data type specifier for each output.\n    doc : str, optional\n        The docstring for the function. If None, the docstring will be the\n        ``pyfunc.__doc__``.\n    excluded : set, optional\n        Set of strings or integers representing the positional or keyword\n        arguments for which the function will not be vectorized.  These will be\n        passed directly to `pyfunc` unmodified.\n\n        .. versionadded:: 1.7.0\n\n    cache : bool, optional\n        If `True`, then cache the first function call that determines the number\n        of outputs if `otypes` is not provided.\n\n        .. versionadded:: 1.7.0\n\n    signature : string, optional\n        Generalized universal function signature, e.g., ``(m,n),(n)->(m)`` for\n        vectorized matrix-vector multiplication. If provided, ``pyfunc`` will\n        be called with (and expected to return) arrays with shapes given by the\n        size of corresponding core dimensions. By default, ``pyfunc`` is\n        assumed to take scalars as input and output.\n\n        .. versionadded:: 1.12.0\n\n    Returns\n    -------\n    vectorized : callable\n        Vectorized function.\n\n    See Also\n    --------\n    frompyfunc : Takes an arbitrary Python function and returns a ufunc\n\n    Notes\n    -----\n    The `vectorize` function is provided primarily for convenience, not for\n    performance. The implementation is essentially a for loop.\n\n    If `otypes` is not specified, then a call to the function with the\n    first argument will be used to determine the number of outputs.  The\n    results of this call will be cached if `cache` is `True` to prevent\n    calling the function twice.  However, to implement the cache, the\n    original function must be wrapped which will slow down subsequent\n    calls, so only do this if your function is expensive.\n\n    The new keyword argument interface and `excluded` argument support\n    further degrades performance.\n\n    References\n    ----------\n    .. [1] :doc:`/reference/c-api/generalized-ufuncs`\n\n    Examples\n    --------\n    >>> def myfunc(a, b):\n    ...     \"Return a-b if a>b, otherwise return a+b\"\n    ...     if a > b:\n    ...         return a - b\n    ...     else:\n    ...         return a + b\n\n    >>> vfunc = np.vectorize(myfunc)\n    >>> vfunc([1, 2, 3, 4], 2)\n    array([3, 4, 1, 2])\n\n    The docstring is taken from the input function to `vectorize` unless it\n    is specified:\n\n    >>> vfunc.__doc__\n    'Return a-b if a>b, otherwise return a+b'\n    >>> vfunc = np.vectorize(myfunc, doc='Vectorized `myfunc`')\n    >>> vfunc.__doc__\n    'Vectorized `myfunc`'\n\n    The output type is determined by evaluating the first element of the input,\n    unless it is specified:\n\n    >>> out = vfunc([1, 2, 3, 4], 2)\n    >>> type(out[0])\n    <class 'numpy.int64'>\n    >>> vfunc = np.vectorize(myfunc, otypes=[float])\n    >>> out = vfunc([1, 2, 3, 4], 2)\n    >>> type(out[0])\n    <class 'numpy.float64'>\n\n    The `excluded` argument can be used to prevent vectorizing over certain\n    arguments.  This can be useful for array-like arguments of a fixed length\n    such as the coefficients for a polynomial as in `polyval`:\n\n    >>> def mypolyval(p, x):\n    ...     _p = list(p)\n    ...     res = _p.pop(0)\n    ...     while _p:\n    ...         res = res*x + _p.pop(0)\n    ...     return res\n    >>> vpolyval = np.vectorize(mypolyval, excluded=['p'])\n    >>> vpolyval(p=[1, 2, 3], x=[0, 1])\n    array([3, 6])\n\n    Positional arguments may also be excluded by specifying their position:\n\n    >>> vpolyval.excluded.add(0)\n    >>> vpolyval([1, 2, 3], x=[0, 1])\n    array([3, 6])\n\n    The `signature` argument allows for vectorizing functions that act on\n    non-scalar arrays of fixed length. For example, you can use it for a\n    vectorized calculation of Pearson correlation coefficient and its p-value:\n\n    >>> import scipy.stats\n    >>> pearsonr = np.vectorize(scipy.stats.pearsonr,\n    ...                 signature='(n),(n)->(),()')\n    >>> pearsonr([[0, 1, 2, 3]], [[1, 2, 3, 4], [4, 3, 2, 1]])\n    (array([ 1., -1.]), array([ 0.,  0.]))\n\n    Or for a vectorized convolution:\n\n    >>> convolve = np.vectorize(np.convolve, signature='(n),(m)->(k)')\n    >>> convolve(np.eye(4), [1, 2, 1])\n    array([[1., 2., 1., 0., 0., 0.],\n           [0., 1., 2., 1., 0., 0.],\n           [0., 0., 1., 2., 1., 0.],\n           [0., 0., 0., 1., 2., 1.]])\n\n    \"\"\"\n    def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,\n                 cache=False, signature=None):\n        self.pyfunc = pyfunc\n        self.cache = cache\n        self.signature = signature\n        self._ufunc = {}    # Caching to improve default performance\n\n        if doc is None:\n            self.__doc__ = pyfunc.__doc__\n        else:\n            self.__doc__ = doc\n\n        if isinstance(otypes, str):\n            for char in otypes:\n                if char not in typecodes['All']:\n                    raise ValueError(\"Invalid otype specified: %s\" % (char,))\n        elif iterable(otypes):\n            otypes = ''.join([_nx.dtype(x).char for x in otypes])\n        elif otypes is not None:\n            raise ValueError(\"Invalid otype specification\")\n        self.otypes = otypes\n\n        # Excluded variable support\n        if excluded is None:\n            excluded = set()\n        self.excluded = set(excluded)\n\n        if signature is not None:\n            self._in_and_out_core_dims = _parse_gufunc_signature(signature)\n        else:\n            self._in_and_out_core_dims = None\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Return arrays with the results of `pyfunc` broadcast (vectorized) over\n        `args` and `kwargs` not in `excluded`.\n        \"\"\"\n        excluded = self.excluded\n        if not kwargs and not excluded:\n            func = self.pyfunc\n            vargs = args\n        else:\n            # The wrapper accepts only positional arguments: we use `names` and\n            # `inds` to mutate `the_args` and `kwargs` to pass to the original\n            # function.\n            nargs = len(args)\n\n            names = [_n for _n in kwargs if _n not in excluded]\n            inds = [_i for _i in range(nargs) if _i not in excluded]\n            the_args = list(args)\n\n            def func(*vargs):\n                for _n, _i in enumerate(inds):\n                    the_args[_i] = vargs[_n]\n                kwargs.update(zip(names, vargs[len(inds):]))\n                return self.pyfunc(*the_args, **kwargs)\n\n            vargs = [args[_i] for _i in inds]\n            vargs.extend([kwargs[_n] for _n in names])\n\n        return self._vectorize_call(func=func, args=vargs)\n\n    def _get_ufunc_and_otypes(self, func, args):\n        \"\"\"Return (ufunc, otypes).\"\"\"\n        # frompyfunc will fail if args is empty\n        if not args:\n            raise ValueError('args can not be empty')\n\n        if self.otypes is not None:\n            otypes = self.otypes\n\n            # self._ufunc is a dictionary whose keys are the number of\n            # arguments (i.e. len(args)) and whose values are ufuncs created\n            # by frompyfunc. len(args) can be different for different calls if\n            # self.pyfunc has parameters with default values.  We only use the\n            # cache when func is self.pyfunc, which occurs when the call uses\n            # only positional arguments and no arguments are excluded.\n\n            nin = len(args)\n            nout = len(self.otypes)\n            if func is not self.pyfunc or nin not in self._ufunc:\n                ufunc = frompyfunc(func, nin, nout)\n            else:\n                ufunc = None  # We'll get it from self._ufunc\n            if func is self.pyfunc:\n                ufunc = self._ufunc.setdefault(nin, ufunc)\n        else:\n            # Get number of outputs and output types by calling the function on\n            # the first entries of args.  We also cache the result to prevent\n            # the subsequent call when the ufunc is evaluated.\n            # Assumes that ufunc first evaluates the 0th elements in the input\n            # arrays (the input values are not checked to ensure this)\n            args = [asarray(arg) for arg in args]\n            if builtins.any(arg.size == 0 for arg in args):\n                raise ValueError('cannot call `vectorize` on size 0 inputs '\n                                 'unless `otypes` is set')\n\n            inputs = [arg.flat[0] for arg in args]\n            outputs = func(*inputs)\n\n            # Performance note: profiling indicates that -- for simple\n            # functions at least -- this wrapping can almost double the\n            # execution time.\n            # Hence we make it optional.\n            if self.cache:\n                _cache = [outputs]\n\n                def _func(*vargs):\n                    if _cache:\n                        return _cache.pop()\n                    else:\n                        return func(*vargs)\n            else:\n                _func = func\n\n            if isinstance(outputs, tuple):\n                nout = len(outputs)\n            else:\n                nout = 1\n                outputs = (outputs,)\n\n            otypes = ''.join([asarray(outputs[_k]).dtype.char\n                              for _k in range(nout)])\n\n            # Performance note: profiling indicates that creating the ufunc is\n            # not a significant cost compared with wrapping so it seems not\n            # worth trying to cache this.\n            ufunc = frompyfunc(_func, len(args), nout)\n\n        return ufunc, otypes\n\n    def _vectorize_call(self, func, args):\n        \"\"\"Vectorized call to `func` over positional `args`.\"\"\"\n        if self.signature is not None:\n            res = self._vectorize_call_with_signature(func, args)\n        elif not args:\n            res = func()\n        else:\n            ufunc, otypes = self._get_ufunc_and_otypes(func=func, args=args)\n\n            # Convert args to object arrays first\n            inputs = [array(a, copy=False, subok=True, dtype=object)\n                      for a in args]\n\n            outputs = ufunc(*inputs)\n\n            if ufunc.nout == 1:\n                res = array(outputs, copy=False, subok=True, dtype=otypes[0])\n            else:\n                res = tuple([array(x, copy=False, subok=True, dtype=t)\n                             for x, t in zip(outputs, otypes)])\n        return res\n\n    def _vectorize_call_with_signature(self, func, args):\n        \"\"\"Vectorized call over positional arguments with a signature.\"\"\"\n        input_core_dims, output_core_dims = self._in_and_out_core_dims\n\n        if len(args) != len(input_core_dims):\n            raise TypeError('wrong number of positional arguments: '\n                            'expected %r, got %r'\n                            % (len(input_core_dims), len(args)))\n        args = tuple(asanyarray(arg) for arg in args)\n\n        broadcast_shape, dim_sizes = _parse_input_dimensions(\n            args, input_core_dims)\n        input_shapes = _calculate_shapes(broadcast_shape, dim_sizes,\n                                         input_core_dims)\n        args = [np.broadcast_to(arg, shape, subok=True)\n                for arg, shape in zip(args, input_shapes)]\n\n        outputs = None\n        otypes = self.otypes\n        nout = len(output_core_dims)\n\n        for index in np.ndindex(*broadcast_shape):\n            results = func(*(arg[index] for arg in args))\n\n            n_results = len(results) if isinstance(results, tuple) else 1\n\n            if nout != n_results:\n                raise ValueError(\n                    'wrong number of outputs from pyfunc: expected %r, got %r'\n                    % (nout, n_results))\n\n            if nout == 1:\n                results = (results,)\n\n            if outputs is None:\n                for result, core_dims in zip(results, output_core_dims):\n                    _update_dim_sizes(dim_sizes, result, core_dims)\n\n                if otypes is None:\n                    otypes = [asarray(result).dtype for result in results]\n\n                outputs = _create_arrays(broadcast_shape, dim_sizes,\n                                         output_core_dims, otypes)\n\n            for output, result in zip(outputs, results):\n                output[index] = result\n\n        if outputs is None:\n            # did not call the function even once\n            if otypes is None:\n                raise ValueError('cannot call `vectorize` on size 0 inputs '\n                                 'unless `otypes` is set')\n            if builtins.any(dim not in dim_sizes\n                            for dims in output_core_dims\n                            for dim in dims):\n                raise ValueError('cannot call `vectorize` with a signature '\n                                 'including new output dimensions on size 0 '\n                                 'inputs')\n            outputs = _create_arrays(broadcast_shape, dim_sizes,\n                                     output_core_dims, otypes)\n\n        return outputs[0] if nout == 1 else outputs\n\n\ndef _cov_dispatcher(m, y=None, rowvar=None, bias=None, ddof=None,\n                    fweights=None, aweights=None, *, dtype=None):\n    return (m, y, fweights, aweights)\n\n\n@array_function_dispatch(_cov_dispatcher)\ndef cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None,\n        aweights=None, *, dtype=None):\n    \"\"\"\n    Estimate a covariance matrix, given data and weights.\n\n    Covariance indicates the level to which two variables vary together.\n    If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`,\n    then the covariance matrix element :math:`C_{ij}` is the covariance of\n    :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance\n    of :math:`x_i`.\n\n    See the notes for an outline of the algorithm.\n\n    Parameters\n    ----------\n    m : array_like\n        A 1-D or 2-D array containing multiple variables and observations.\n        Each row of `m` represents a variable, and each column a single\n        observation of all those variables. Also see `rowvar` below.\n    y : array_like, optional\n        An additional set of variables and observations. `y` has the same form\n        as that of `m`.\n    rowvar : bool, optional\n        If `rowvar` is True (default), then each row represents a\n        variable, with observations in the columns. Otherwise, the relationship\n        is transposed: each column represents a variable, while the rows\n        contain observations.\n    bias : bool, optional\n        Default normalization (False) is by ``(N - 1)``, where ``N`` is the\n        number of observations given (unbiased estimate). If `bias` is True,\n        then normalization is by ``N``. These values can be overridden by using\n        the keyword ``ddof`` in numpy versions >= 1.5.\n    ddof : int, optional\n        If not ``None`` the default value implied by `bias` is overridden.\n        Note that ``ddof=1`` will return the unbiased estimate, even if both\n        `fweights` and `aweights` are specified, and ``ddof=0`` will return\n        the simple average. See the notes for the details. The default value\n        is ``None``.\n\n        .. versionadded:: 1.5\n    fweights : array_like, int, optional\n        1-D array of integer frequency weights; the number of times each\n        observation vector should be repeated.\n\n        .. versionadded:: 1.10\n    aweights : array_like, optional\n        1-D array of observation vector weights. These relative weights are\n        typically large for observations considered \"important\" and smaller for\n        observations considered less \"important\". If ``ddof=0`` the array of\n        weights can be used to assign probabilities to observation vectors.\n\n        .. versionadded:: 1.10\n    dtype : data-type, optional\n        Data-type of the result. By default, the return data-type will have\n        at least `numpy.float64` precision.\n\n        .. versionadded:: 1.20\n\n    Returns\n    -------\n    out : ndarray\n        The covariance matrix of the variables.\n\n    See Also\n    --------\n    corrcoef : Normalized covariance matrix\n\n    Notes\n    -----\n    Assume that the observations are in the columns of the observation\n    array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The\n    steps to compute the weighted covariance are as follows::\n\n        >>> m = np.arange(10, dtype=np.float64)\n        >>> f = np.arange(10) * 2\n        >>> a = np.arange(10) ** 2.\n        >>> ddof = 1\n        >>> w = f * a\n        >>> v1 = np.sum(w)\n        >>> v2 = np.sum(w * a)\n        >>> m -= np.sum(m * w, axis=None, keepdims=True) / v1\n        >>> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)\n\n    Note that when ``a == 1``, the normalization factor\n    ``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)``\n    as it should.\n\n    Examples\n    --------\n    Consider two variables, :math:`x_0` and :math:`x_1`, which\n    correlate perfectly, but in opposite directions:\n\n    >>> x = np.array([[0, 2], [1, 1], [2, 0]]).T\n    >>> x\n    array([[0, 1, 2],\n           [2, 1, 0]])\n\n    Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance\n    matrix shows this clearly:\n\n    >>> np.cov(x)\n    array([[ 1., -1.],\n           [-1.,  1.]])\n\n    Note that element :math:`C_{0,1}`, which shows the correlation between\n    :math:`x_0` and :math:`x_1`, is negative.\n\n    Further, note how `x` and `y` are combined:\n\n    >>> x = [-2.1, -1,  4.3]\n    >>> y = [3,  1.1,  0.12]\n    >>> X = np.stack((x, y), axis=0)\n    >>> np.cov(X)\n    array([[11.71      , -4.286     ], # may vary\n           [-4.286     ,  2.144133]])\n    >>> np.cov(x, y)\n    array([[11.71      , -4.286     ], # may vary\n           [-4.286     ,  2.144133]])\n    >>> np.cov(x)\n    array(11.71)\n\n    \"\"\"\n    # Check inputs\n    if ddof is not None and ddof != int(ddof):\n        raise ValueError(\n            \"ddof must be integer\")\n\n    # Handles complex arrays too\n    m = np.asarray(m)\n    if m.ndim > 2:\n        raise ValueError(\"m has more than 2 dimensions\")\n\n    if y is not None:\n        y = np.asarray(y)\n        if y.ndim > 2:\n            raise ValueError(\"y has more than 2 dimensions\")\n\n    if dtype is None:\n        if y is None:\n            dtype = np.result_type(m, np.float64)\n        else:\n            dtype = np.result_type(m, y, np.float64)\n\n    X = array(m, ndmin=2, dtype=dtype)\n    if not rowvar and X.shape[0] != 1:\n        X = X.T\n    if X.shape[0] == 0:\n        return np.array([]).reshape(0, 0)\n    if y is not None:\n        y = array(y, copy=False, ndmin=2, dtype=dtype)\n        if not rowvar and y.shape[0] != 1:\n            y = y.T\n        X = np.concatenate((X, y), axis=0)\n\n    if ddof is None:\n        if bias == 0:\n            ddof = 1\n        else:\n            ddof = 0\n\n    # Get the product of frequencies and weights\n    w = None\n    if fweights is not None:\n        fweights = np.asarray(fweights, dtype=float)\n        if not np.all(fweights == np.around(fweights)):\n            raise TypeError(\n                \"fweights must be integer\")\n        if fweights.ndim > 1:\n            raise RuntimeError(\n                \"cannot handle multidimensional fweights\")\n        if fweights.shape[0] != X.shape[1]:\n            raise RuntimeError(\n                \"incompatible numbers of samples and fweights\")\n        if any(fweights < 0):\n            raise ValueError(\n                \"fweights cannot be negative\")\n        w = fweights\n    if aweights is not None:\n        aweights = np.asarray(aweights, dtype=float)\n        if aweights.ndim > 1:\n            raise RuntimeError(\n                \"cannot handle multidimensional aweights\")\n        if aweights.shape[0] != X.shape[1]:\n            raise RuntimeError(\n                \"incompatible numbers of samples and aweights\")\n        if any(aweights < 0):\n            raise ValueError(\n                \"aweights cannot be negative\")\n        if w is None:\n            w = aweights\n        else:\n            w *= aweights\n\n    avg, w_sum = average(X, axis=1, weights=w, returned=True)\n    w_sum = w_sum[0]\n\n    # Determine the normalization\n    if w is None:\n        fact = X.shape[1] - ddof\n    elif ddof == 0:\n        fact = w_sum\n    elif aweights is None:\n        fact = w_sum - ddof\n    else:\n        fact = w_sum - ddof*sum(w*aweights)/w_sum\n\n    if fact <= 0:\n        warnings.warn(\"Degrees of freedom <= 0 for slice\",\n                      RuntimeWarning, stacklevel=3)\n        fact = 0.0\n\n    X -= avg[:, None]\n    if w is None:\n        X_T = X.T\n    else:\n        X_T = (X*w).T\n    c = dot(X, X_T.conj())\n    c *= np.true_divide(1, fact)\n    return c.squeeze()\n\n\ndef _corrcoef_dispatcher(x, y=None, rowvar=None, bias=None, ddof=None, *,\n                         dtype=None):\n    return (x, y)\n\n\n@array_function_dispatch(_corrcoef_dispatcher)\ndef corrcoef(x, y=None, rowvar=True, bias=np._NoValue, ddof=np._NoValue, *,\n             dtype=None):\n    \"\"\"\n    Return Pearson product-moment correlation coefficients.\n\n    Please refer to the documentation for `cov` for more detail.  The\n    relationship between the correlation coefficient matrix, `R`, and the\n    covariance matrix, `C`, is\n\n    .. math:: R_{ij} = \\\\frac{ C_{ij} } { \\\\sqrt{ C_{ii} * C_{jj} } }\n\n    The values of `R` are between -1 and 1, inclusive.\n\n    Parameters\n    ----------\n    x : array_like\n        A 1-D or 2-D array containing multiple variables and observations.\n        Each row of `x` represents a variable, and each column a single\n        observation of all those variables. Also see `rowvar` below.\n    y : array_like, optional\n        An additional set of variables and observations. `y` has the same\n        shape as `x`.\n    rowvar : bool, optional\n        If `rowvar` is True (default), then each row represents a\n        variable, with observations in the columns. Otherwise, the relationship\n        is transposed: each column represents a variable, while the rows\n        contain observations.\n    bias : _NoValue, optional\n        Has no effect, do not use.\n\n        .. deprecated:: 1.10.0\n    ddof : _NoValue, optional\n        Has no effect, do not use.\n\n        .. deprecated:: 1.10.0\n    dtype : data-type, optional\n        Data-type of the result. By default, the return data-type will have\n        at least `numpy.float64` precision.\n\n        .. versionadded:: 1.20\n\n    Returns\n    -------\n    R : ndarray\n        The correlation coefficient matrix of the variables.\n\n    See Also\n    --------\n    cov : Covariance matrix\n\n    Notes\n    -----\n    Due to floating point rounding the resulting array may not be Hermitian,\n    the diagonal elements may not be 1, and the elements may not satisfy the\n    inequality abs(a) <= 1. The real and imaginary parts are clipped to the\n    interval [-1,  1] in an attempt to improve on that situation but is not\n    much help in the complex case.\n\n    This function accepts but discards arguments `bias` and `ddof`.  This is\n    for backwards compatibility with previous versions of this function.  These\n    arguments had no effect on the return values of the function and can be\n    safely ignored in this and previous versions of numpy.\n\n    Examples\n    --------\n    In this example we generate two random arrays, ``xarr`` and ``yarr``, and\n    compute the row-wise and column-wise Pearson correlation coefficients,\n    ``R``. Since ``rowvar`` is  true by  default, we first find the row-wise\n    Pearson correlation coefficients between the variables of ``xarr``.\n\n    >>> import numpy as np\n    >>> rng = np.random.default_rng(seed=42)\n    >>> xarr = rng.random((3, 3))\n    >>> xarr\n    array([[0.77395605, 0.43887844, 0.85859792],\n           [0.69736803, 0.09417735, 0.97562235],\n           [0.7611397 , 0.78606431, 0.12811363]])\n    >>> R1 = np.corrcoef(xarr)\n    >>> R1\n    array([[ 1.        ,  0.99256089, -0.68080986],\n           [ 0.99256089,  1.        , -0.76492172],\n           [-0.68080986, -0.76492172,  1.        ]])\n\n    If we add another set of variables and observations ``yarr``, we can\n    compute the row-wise Pearson correlation coefficients between the\n    variables in ``xarr`` and ``yarr``.\n\n    >>> yarr = rng.random((3, 3))\n    >>> yarr\n    array([[0.45038594, 0.37079802, 0.92676499],\n           [0.64386512, 0.82276161, 0.4434142 ],\n           [0.22723872, 0.55458479, 0.06381726]])\n    >>> R2 = np.corrcoef(xarr, yarr)\n    >>> R2\n    array([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  ,\n            -0.99004057],\n           [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098,\n            -0.99981569],\n           [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355,\n             0.77714685],\n           [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855,\n            -0.83571711],\n           [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        ,\n             0.97517215],\n           [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215,\n             1.        ]])\n\n    Finally if we use the option ``rowvar=False``, the columns are now\n    being treated as the variables and we will find the column-wise Pearson\n    correlation coefficients between variables in ``xarr`` and ``yarr``.\n\n    >>> R3 = np.corrcoef(xarr, yarr, rowvar=False)\n    >>> R3\n    array([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 ,\n             0.22423734],\n           [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587,\n            -0.44069024],\n           [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648,\n             0.75137473],\n           [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469,\n             0.47536961],\n           [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        ,\n            -0.46666491],\n           [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491,\n             1.        ]])\n\n    \"\"\"\n    if bias is not np._NoValue or ddof is not np._NoValue:\n        # 2015-03-15, 1.10\n        warnings.warn('bias and ddof have no effect and are deprecated',\n                      DeprecationWarning, stacklevel=3)\n    c = cov(x, y, rowvar, dtype=dtype)\n    try:\n        d = diag(c)\n    except ValueError:\n        # scalar covariance\n        # nan if incorrect value (nan, inf, 0), 1 otherwise\n        return c / c\n    stddev = sqrt(d.real)\n    c /= stddev[:, None]\n    c /= stddev[None, :]\n\n    # Clip real and imaginary parts to [-1, 1].  This does not guarantee\n    # abs(a[i,j]) <= 1 for complex arrays, but is the best we can do without\n    # excessive work.\n    np.clip(c.real, -1, 1, out=c.real)\n    if np.iscomplexobj(c):\n        np.clip(c.imag, -1, 1, out=c.imag)\n\n    return c\n\n\n@set_module('numpy')\ndef blackman(M):\n    \"\"\"\n    Return the Blackman window.\n\n    The Blackman window is a taper formed by using the first three\n    terms of a summation of cosines. It was designed to have close to the\n    minimal leakage possible.  It is close to optimal, only slightly worse\n    than a Kaiser window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n\n    Returns\n    -------\n    out : ndarray\n        The window, with the maximum value normalized to one (the value one\n        appears only if the number of samples is odd).\n\n    See Also\n    --------\n    bartlett, hamming, hanning, kaiser\n\n    Notes\n    -----\n    The Blackman window is defined as\n\n    .. math::  w(n) = 0.42 - 0.5 \\\\cos(2\\\\pi n/M) + 0.08 \\\\cos(4\\\\pi n/M)\n\n    Most references to the Blackman window come from the signal processing\n    literature, where it is used as one of many windowing functions for\n    smoothing values.  It is also known as an apodization (which means\n    \"removing the foot\", i.e. smoothing discontinuities at the beginning\n    and end of the sampled signal) or tapering function. It is known as a\n    \"near optimal\" tapering function, almost as good (by some measures)\n    as the kaiser window.\n\n    References\n    ----------\n    Blackman, R.B. and Tukey, J.W., (1958) The measurement of power spectra,\n    Dover Publications, New York.\n\n    Oppenheim, A.V., and R.W. Schafer. Discrete-Time Signal Processing.\n    Upper Saddle River, NJ: Prentice-Hall, 1999, pp. 468-471.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> np.blackman(12)\n    array([-1.38777878e-17,   3.26064346e-02,   1.59903635e-01, # may vary\n            4.14397981e-01,   7.36045180e-01,   9.67046769e-01,\n            9.67046769e-01,   7.36045180e-01,   4.14397981e-01,\n            1.59903635e-01,   3.26064346e-02,  -1.38777878e-17])\n\n    Plot the window and the frequency response:\n\n    >>> from numpy.fft import fft, fftshift\n    >>> window = np.blackman(51)\n    >>> plt.plot(window)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Blackman window\")\n    Text(0.5, 1.0, 'Blackman window')\n    >>> plt.ylabel(\"Amplitude\")\n    Text(0, 0.5, 'Amplitude')\n    >>> plt.xlabel(\"Sample\")\n    Text(0.5, 0, 'Sample')\n    >>> plt.show()\n\n    >>> plt.figure()\n    <Figure size 640x480 with 0 Axes>\n    >>> A = fft(window, 2048) / 25.5\n    >>> mag = np.abs(fftshift(A))\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> with np.errstate(divide='ignore', invalid='ignore'):\n    ...     response = 20 * np.log10(mag)\n    ...\n    >>> response = np.clip(response, -100, 100)\n    >>> plt.plot(freq, response)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Frequency response of Blackman window\")\n    Text(0.5, 1.0, 'Frequency response of Blackman window')\n    >>> plt.ylabel(\"Magnitude [dB]\")\n    Text(0, 0.5, 'Magnitude [dB]')\n    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n    >>> _ = plt.axis('tight')\n    >>> plt.show()\n\n    \"\"\"\n    if M < 1:\n        return array([])\n    if M == 1:\n        return ones(1, float)\n    n = arange(1-M, M, 2)\n    return 0.42 + 0.5*cos(pi*n/(M-1)) + 0.08*cos(2.0*pi*n/(M-1))\n\n\n@set_module('numpy')\ndef bartlett(M):\n    \"\"\"\n    Return the Bartlett window.\n\n    The Bartlett window is very similar to a triangular window, except\n    that the end points are at zero.  It is often used in signal\n    processing for tapering a signal, without generating too much\n    ripple in the frequency domain.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an\n        empty array is returned.\n\n    Returns\n    -------\n    out : array\n        The triangular window, with the maximum value normalized to one\n        (the value one appears only if the number of samples is odd), with\n        the first and last samples equal to zero.\n\n    See Also\n    --------\n    blackman, hamming, hanning, kaiser\n\n    Notes\n    -----\n    The Bartlett window is defined as\n\n    .. math:: w(n) = \\\\frac{2}{M-1} \\\\left(\n              \\\\frac{M-1}{2} - \\\\left|n - \\\\frac{M-1}{2}\\\\right|\n              \\\\right)\n\n    Most references to the Bartlett window come from the signal\n    processing literature, where it is used as one of many windowing\n    functions for smoothing values.  Note that convolution with this\n    window produces linear interpolation.  It is also known as an\n    apodization (which means\"removing the foot\", i.e. smoothing\n    discontinuities at the beginning and end of the sampled signal) or\n    tapering function. The fourier transform of the Bartlett is the product\n    of two sinc functions.\n    Note the excellent discussion in Kanasewich.\n\n    References\n    ----------\n    .. [1] M.S. Bartlett, \"Periodogram Analysis and Continuous Spectra\",\n           Biometrika 37, 1-16, 1950.\n    .. [2] E.R. Kanasewich, \"Time Sequence Analysis in Geophysics\",\n           The University of Alberta Press, 1975, pp. 109-110.\n    .. [3] A.V. Oppenheim and R.W. Schafer, \"Discrete-Time Signal\n           Processing\", Prentice-Hall, 1999, pp. 468-471.\n    .. [4] Wikipedia, \"Window function\",\n           https://en.wikipedia.org/wiki/Window_function\n    .. [5] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,\n           \"Numerical Recipes\", Cambridge University Press, 1986, page 429.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> np.bartlett(12)\n    array([ 0.        ,  0.18181818,  0.36363636,  0.54545455,  0.72727273, # may vary\n            0.90909091,  0.90909091,  0.72727273,  0.54545455,  0.36363636,\n            0.18181818,  0.        ])\n\n    Plot the window and its frequency response (requires SciPy and matplotlib):\n\n    >>> from numpy.fft import fft, fftshift\n    >>> window = np.bartlett(51)\n    >>> plt.plot(window)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Bartlett window\")\n    Text(0.5, 1.0, 'Bartlett window')\n    >>> plt.ylabel(\"Amplitude\")\n    Text(0, 0.5, 'Amplitude')\n    >>> plt.xlabel(\"Sample\")\n    Text(0.5, 0, 'Sample')\n    >>> plt.show()\n\n    >>> plt.figure()\n    <Figure size 640x480 with 0 Axes>\n    >>> A = fft(window, 2048) / 25.5\n    >>> mag = np.abs(fftshift(A))\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> with np.errstate(divide='ignore', invalid='ignore'):\n    ...     response = 20 * np.log10(mag)\n    ...\n    >>> response = np.clip(response, -100, 100)\n    >>> plt.plot(freq, response)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Frequency response of Bartlett window\")\n    Text(0.5, 1.0, 'Frequency response of Bartlett window')\n    >>> plt.ylabel(\"Magnitude [dB]\")\n    Text(0, 0.5, 'Magnitude [dB]')\n    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n    >>> _ = plt.axis('tight')\n    >>> plt.show()\n\n    \"\"\"\n    if M < 1:\n        return array([])\n    if M == 1:\n        return ones(1, float)\n    n = arange(1-M, M, 2)\n    return where(less_equal(n, 0), 1 + n/(M-1), 1 - n/(M-1))\n\n\n@set_module('numpy')\ndef hanning(M):\n    \"\"\"\n    Return the Hanning window.\n\n    The Hanning window is a taper formed by using a weighted cosine.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an\n        empty array is returned.\n\n    Returns\n    -------\n    out : ndarray, shape(M,)\n        The window, with the maximum value normalized to one (the value\n        one appears only if `M` is odd).\n\n    See Also\n    --------\n    bartlett, blackman, hamming, kaiser\n\n    Notes\n    -----\n    The Hanning window is defined as\n\n    .. math::  w(n) = 0.5 - 0.5cos\\\\left(\\\\frac{2\\\\pi{n}}{M-1}\\\\right)\n               \\\\qquad 0 \\\\leq n \\\\leq M-1\n\n    The Hanning was named for Julius von Hann, an Austrian meteorologist.\n    It is also known as the Cosine Bell. Some authors prefer that it be\n    called a Hann window, to help avoid confusion with the very similar\n    Hamming window.\n\n    Most references to the Hanning window come from the signal processing\n    literature, where it is used as one of many windowing functions for\n    smoothing values.  It is also known as an apodization (which means\n    \"removing the foot\", i.e. smoothing discontinuities at the beginning\n    and end of the sampled signal) or tapering function.\n\n    References\n    ----------\n    .. [1] Blackman, R.B. and Tukey, J.W., (1958) The measurement of power\n           spectra, Dover Publications, New York.\n    .. [2] E.R. Kanasewich, \"Time Sequence Analysis in Geophysics\",\n           The University of Alberta Press, 1975, pp. 106-108.\n    .. [3] Wikipedia, \"Window function\",\n           https://en.wikipedia.org/wiki/Window_function\n    .. [4] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,\n           \"Numerical Recipes\", Cambridge University Press, 1986, page 425.\n\n    Examples\n    --------\n    >>> np.hanning(12)\n    array([0.        , 0.07937323, 0.29229249, 0.57115742, 0.82743037,\n           0.97974649, 0.97974649, 0.82743037, 0.57115742, 0.29229249,\n           0.07937323, 0.        ])\n\n    Plot the window and its frequency response:\n\n    >>> import matplotlib.pyplot as plt\n    >>> from numpy.fft import fft, fftshift\n    >>> window = np.hanning(51)\n    >>> plt.plot(window)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Hann window\")\n    Text(0.5, 1.0, 'Hann window')\n    >>> plt.ylabel(\"Amplitude\")\n    Text(0, 0.5, 'Amplitude')\n    >>> plt.xlabel(\"Sample\")\n    Text(0.5, 0, 'Sample')\n    >>> plt.show()\n\n    >>> plt.figure()\n    <Figure size 640x480 with 0 Axes>\n    >>> A = fft(window, 2048) / 25.5\n    >>> mag = np.abs(fftshift(A))\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> with np.errstate(divide='ignore', invalid='ignore'):\n    ...     response = 20 * np.log10(mag)\n    ...\n    >>> response = np.clip(response, -100, 100)\n    >>> plt.plot(freq, response)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Frequency response of the Hann window\")\n    Text(0.5, 1.0, 'Frequency response of the Hann window')\n    >>> plt.ylabel(\"Magnitude [dB]\")\n    Text(0, 0.5, 'Magnitude [dB]')\n    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n    >>> plt.axis('tight')\n    ...\n    >>> plt.show()\n\n    \"\"\"\n    if M < 1:\n        return array([])\n    if M == 1:\n        return ones(1, float)\n    n = arange(1-M, M, 2)\n    return 0.5 + 0.5*cos(pi*n/(M-1))\n\n\n@set_module('numpy')\ndef hamming(M):\n    \"\"\"\n    Return the Hamming window.\n\n    The Hamming window is a taper formed by using a weighted cosine.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an\n        empty array is returned.\n\n    Returns\n    -------\n    out : ndarray\n        The window, with the maximum value normalized to one (the value\n        one appears only if the number of samples is odd).\n\n    See Also\n    --------\n    bartlett, blackman, hanning, kaiser\n\n    Notes\n    -----\n    The Hamming window is defined as\n\n    .. math::  w(n) = 0.54 - 0.46cos\\\\left(\\\\frac{2\\\\pi{n}}{M-1}\\\\right)\n               \\\\qquad 0 \\\\leq n \\\\leq M-1\n\n    The Hamming was named for R. W. Hamming, an associate of J. W. Tukey\n    and is described in Blackman and Tukey. It was recommended for\n    smoothing the truncated autocovariance function in the time domain.\n    Most references to the Hamming window come from the signal processing\n    literature, where it is used as one of many windowing functions for\n    smoothing values.  It is also known as an apodization (which means\n    \"removing the foot\", i.e. smoothing discontinuities at the beginning\n    and end of the sampled signal) or tapering function.\n\n    References\n    ----------\n    .. [1] Blackman, R.B. and Tukey, J.W., (1958) The measurement of power\n           spectra, Dover Publications, New York.\n    .. [2] E.R. Kanasewich, \"Time Sequence Analysis in Geophysics\", The\n           University of Alberta Press, 1975, pp. 109-110.\n    .. [3] Wikipedia, \"Window function\",\n           https://en.wikipedia.org/wiki/Window_function\n    .. [4] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,\n           \"Numerical Recipes\", Cambridge University Press, 1986, page 425.\n\n    Examples\n    --------\n    >>> np.hamming(12)\n    array([ 0.08      ,  0.15302337,  0.34890909,  0.60546483,  0.84123594, # may vary\n            0.98136677,  0.98136677,  0.84123594,  0.60546483,  0.34890909,\n            0.15302337,  0.08      ])\n\n    Plot the window and the frequency response:\n\n    >>> import matplotlib.pyplot as plt\n    >>> from numpy.fft import fft, fftshift\n    >>> window = np.hamming(51)\n    >>> plt.plot(window)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Hamming window\")\n    Text(0.5, 1.0, 'Hamming window')\n    >>> plt.ylabel(\"Amplitude\")\n    Text(0, 0.5, 'Amplitude')\n    >>> plt.xlabel(\"Sample\")\n    Text(0.5, 0, 'Sample')\n    >>> plt.show()\n\n    >>> plt.figure()\n    <Figure size 640x480 with 0 Axes>\n    >>> A = fft(window, 2048) / 25.5\n    >>> mag = np.abs(fftshift(A))\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(mag)\n    >>> response = np.clip(response, -100, 100)\n    >>> plt.plot(freq, response)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Frequency response of Hamming window\")\n    Text(0.5, 1.0, 'Frequency response of Hamming window')\n    >>> plt.ylabel(\"Magnitude [dB]\")\n    Text(0, 0.5, 'Magnitude [dB]')\n    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n    >>> plt.axis('tight')\n    ...\n    >>> plt.show()\n\n    \"\"\"\n    if M < 1:\n        return array([])\n    if M == 1:\n        return ones(1, float)\n    n = arange(1-M, M, 2)\n    return 0.54 + 0.46*cos(pi*n/(M-1))\n\n\n## Code from cephes for i0\n\n_i0A = [\n    -4.41534164647933937950E-18,\n    3.33079451882223809783E-17,\n    -2.43127984654795469359E-16,\n    1.71539128555513303061E-15,\n    -1.16853328779934516808E-14,\n    7.67618549860493561688E-14,\n    -4.85644678311192946090E-13,\n    2.95505266312963983461E-12,\n    -1.72682629144155570723E-11,\n    9.67580903537323691224E-11,\n    -5.18979560163526290666E-10,\n    2.65982372468238665035E-9,\n    -1.30002500998624804212E-8,\n    6.04699502254191894932E-8,\n    -2.67079385394061173391E-7,\n    1.11738753912010371815E-6,\n    -4.41673835845875056359E-6,\n    1.64484480707288970893E-5,\n    -5.75419501008210370398E-5,\n    1.88502885095841655729E-4,\n    -5.76375574538582365885E-4,\n    1.63947561694133579842E-3,\n    -4.32430999505057594430E-3,\n    1.05464603945949983183E-2,\n    -2.37374148058994688156E-2,\n    4.93052842396707084878E-2,\n    -9.49010970480476444210E-2,\n    1.71620901522208775349E-1,\n    -3.04682672343198398683E-1,\n    6.76795274409476084995E-1\n    ]\n\n_i0B = [\n    -7.23318048787475395456E-18,\n    -4.83050448594418207126E-18,\n    4.46562142029675999901E-17,\n    3.46122286769746109310E-17,\n    -2.82762398051658348494E-16,\n    -3.42548561967721913462E-16,\n    1.77256013305652638360E-15,\n    3.81168066935262242075E-15,\n    -9.55484669882830764870E-15,\n    -4.15056934728722208663E-14,\n    1.54008621752140982691E-14,\n    3.85277838274214270114E-13,\n    7.18012445138366623367E-13,\n    -1.79417853150680611778E-12,\n    -1.32158118404477131188E-11,\n    -3.14991652796324136454E-11,\n    1.18891471078464383424E-11,\n    4.94060238822496958910E-10,\n    3.39623202570838634515E-9,\n    2.26666899049817806459E-8,\n    2.04891858946906374183E-7,\n    2.89137052083475648297E-6,\n    6.88975834691682398426E-5,\n    3.36911647825569408990E-3,\n    8.04490411014108831608E-1\n    ]\n\n\ndef _chbevl(x, vals):\n    b0 = vals[0]\n    b1 = 0.0\n\n    for i in range(1, len(vals)):\n        b2 = b1\n        b1 = b0\n        b0 = x*b1 - b2 + vals[i]\n\n    return 0.5*(b0 - b2)\n\n\ndef _i0_1(x):\n    return exp(x) * _chbevl(x/2.0-2, _i0A)\n\n\ndef _i0_2(x):\n    return exp(x) * _chbevl(32.0/x - 2.0, _i0B) / sqrt(x)\n\n\ndef _i0_dispatcher(x):\n    return (x,)\n\n\n@array_function_dispatch(_i0_dispatcher)\ndef i0(x):\n    \"\"\"\n    Modified Bessel function of the first kind, order 0.\n\n    Usually denoted :math:`I_0`.\n\n    Parameters\n    ----------\n    x : array_like of float\n        Argument of the Bessel function.\n\n    Returns\n    -------\n    out : ndarray, shape = x.shape, dtype = float\n        The modified Bessel function evaluated at each of the elements of `x`.\n\n    See Also\n    --------\n    scipy.special.i0, scipy.special.iv, scipy.special.ive\n\n    Notes\n    -----\n    The scipy implementation is recommended over this function: it is a\n    proper ufunc written in C, and more than an order of magnitude faster.\n\n    We use the algorithm published by Clenshaw [1]_ and referenced by\n    Abramowitz and Stegun [2]_, for which the function domain is\n    partitioned into the two intervals [0,8] and (8,inf), and Chebyshev\n    polynomial expansions are employed in each interval. Relative error on\n    the domain [0,30] using IEEE arithmetic is documented [3]_ as having a\n    peak of 5.8e-16 with an rms of 1.4e-16 (n = 30000).\n\n    References\n    ----------\n    .. [1] C. W. Clenshaw, \"Chebyshev series for mathematical functions\", in\n           *National Physical Laboratory Mathematical Tables*, vol. 5, London:\n           Her Majesty's Stationery Office, 1962.\n    .. [2] M. Abramowitz and I. A. Stegun, *Handbook of Mathematical\n           Functions*, 10th printing, New York: Dover, 1964, pp. 379.\n           http://www.math.sfu.ca/~cbm/aands/page_379.htm\n    .. [3] https://metacpan.org/pod/distribution/Math-Cephes/lib/Math/Cephes.pod#i0:-Modified-Bessel-function-of-order-zero\n\n    Examples\n    --------\n    >>> np.i0(0.)\n    array(1.0)\n    >>> np.i0([0, 1, 2, 3])\n    array([1.        , 1.26606588, 2.2795853 , 4.88079259])\n\n    \"\"\"\n    x = np.asanyarray(x)\n    if x.dtype.kind == 'c':\n        raise TypeError(\"i0 not supported for complex values\")\n    if x.dtype.kind != 'f':\n        x = x.astype(float)\n    x = np.abs(x)\n    return piecewise(x, [x <= 8.0], [_i0_1, _i0_2])\n\n## End of cephes code for i0\n\n\n@set_module('numpy')\ndef kaiser(M, beta):\n    \"\"\"\n    Return the Kaiser window.\n\n    The Kaiser window is a taper formed by using a Bessel function.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an\n        empty array is returned.\n    beta : float\n        Shape parameter for window.\n\n    Returns\n    -------\n    out : array\n        The window, with the maximum value normalized to one (the value\n        one appears only if the number of samples is odd).\n\n    See Also\n    --------\n    bartlett, blackman, hamming, hanning\n\n    Notes\n    -----\n    The Kaiser window is defined as\n\n    .. math::  w(n) = I_0\\\\left( \\\\beta \\\\sqrt{1-\\\\frac{4n^2}{(M-1)^2}}\n               \\\\right)/I_0(\\\\beta)\n\n    with\n\n    .. math:: \\\\quad -\\\\frac{M-1}{2} \\\\leq n \\\\leq \\\\frac{M-1}{2},\n\n    where :math:`I_0` is the modified zeroth-order Bessel function.\n\n    The Kaiser was named for Jim Kaiser, who discovered a simple\n    approximation to the DPSS window based on Bessel functions.  The Kaiser\n    window is a very good approximation to the Digital Prolate Spheroidal\n    Sequence, or Slepian window, which is the transform which maximizes the\n    energy in the main lobe of the window relative to total energy.\n\n    The Kaiser can approximate many other windows by varying the beta\n    parameter.\n\n    ====  =======================\n    beta  Window shape\n    ====  =======================\n    0     Rectangular\n    5     Similar to a Hamming\n    6     Similar to a Hanning\n    8.6   Similar to a Blackman\n    ====  =======================\n\n    A beta value of 14 is probably a good starting point. Note that as beta\n    gets large, the window narrows, and so the number of samples needs to be\n    large enough to sample the increasingly narrow spike, otherwise NaNs will\n    get returned.\n\n    Most references to the Kaiser window come from the signal processing\n    literature, where it is used as one of many windowing functions for\n    smoothing values.  It is also known as an apodization (which means\n    \"removing the foot\", i.e. smoothing discontinuities at the beginning\n    and end of the sampled signal) or tapering function.\n\n    References\n    ----------\n    .. [1] J. F. Kaiser, \"Digital Filters\" - Ch 7 in \"Systems analysis by\n           digital computer\", Editors: F.F. Kuo and J.F. Kaiser, p 218-285.\n           John Wiley and Sons, New York, (1966).\n    .. [2] E.R. Kanasewich, \"Time Sequence Analysis in Geophysics\", The\n           University of Alberta Press, 1975, pp. 177-178.\n    .. [3] Wikipedia, \"Window function\",\n           https://en.wikipedia.org/wiki/Window_function\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> np.kaiser(12, 14)\n     array([7.72686684e-06, 3.46009194e-03, 4.65200189e-02, # may vary\n            2.29737120e-01, 5.99885316e-01, 9.45674898e-01,\n            9.45674898e-01, 5.99885316e-01, 2.29737120e-01,\n            4.65200189e-02, 3.46009194e-03, 7.72686684e-06])\n\n\n    Plot the window and the frequency response:\n\n    >>> from numpy.fft import fft, fftshift\n    >>> window = np.kaiser(51, 14)\n    >>> plt.plot(window)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Kaiser window\")\n    Text(0.5, 1.0, 'Kaiser window')\n    >>> plt.ylabel(\"Amplitude\")\n    Text(0, 0.5, 'Amplitude')\n    >>> plt.xlabel(\"Sample\")\n    Text(0.5, 0, 'Sample')\n    >>> plt.show()\n\n    >>> plt.figure()\n    <Figure size 640x480 with 0 Axes>\n    >>> A = fft(window, 2048) / 25.5\n    >>> mag = np.abs(fftshift(A))\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(mag)\n    >>> response = np.clip(response, -100, 100)\n    >>> plt.plot(freq, response)\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Frequency response of Kaiser window\")\n    Text(0.5, 1.0, 'Frequency response of Kaiser window')\n    >>> plt.ylabel(\"Magnitude [dB]\")\n    Text(0, 0.5, 'Magnitude [dB]')\n    >>> plt.xlabel(\"Normalized frequency [cycles per sample]\")\n    Text(0.5, 0, 'Normalized frequency [cycles per sample]')\n    >>> plt.axis('tight')\n    (-0.5, 0.5, -100.0, ...) # may vary\n    >>> plt.show()\n\n    \"\"\"\n    if M == 1:\n        return np.array([1.])\n    n = arange(0, M)\n    alpha = (M-1)/2.0\n    return i0(beta * sqrt(1-((n-alpha)/alpha)**2.0))/i0(float(beta))\n\n\ndef _sinc_dispatcher(x):\n    return (x,)\n\n\n@array_function_dispatch(_sinc_dispatcher)\ndef sinc(x):\n    r\"\"\"\n    Return the normalized sinc function.\n\n    The sinc function is :math:`\\sin(\\pi x)/(\\pi x)`.\n\n    .. note::\n\n        Note the normalization factor of ``pi`` used in the definition.\n        This is the most commonly used definition in signal processing.\n        Use ``sinc(x / np.pi)`` to obtain the unnormalized sinc function\n        :math:`\\sin(x)/(x)` that is more common in mathematics.\n\n    Parameters\n    ----------\n    x : ndarray\n        Array (possibly multi-dimensional) of values for which to to\n        calculate ``sinc(x)``.\n\n    Returns\n    -------\n    out : ndarray\n        ``sinc(x)``, which has the same shape as the input.\n\n    Notes\n    -----\n    ``sinc(0)`` is the limit value 1.\n\n    The name sinc is short for \"sine cardinal\" or \"sinus cardinalis\".\n\n    The sinc function is used in various signal processing applications,\n    including in anti-aliasing, in the construction of a Lanczos resampling\n    filter, and in interpolation.\n\n    For bandlimited interpolation of discrete-time signals, the ideal\n    interpolation kernel is proportional to the sinc function.\n\n    References\n    ----------\n    .. [1] Weisstein, Eric W. \"Sinc Function.\" From MathWorld--A Wolfram Web\n           Resource. http://mathworld.wolfram.com/SincFunction.html\n    .. [2] Wikipedia, \"Sinc function\",\n           https://en.wikipedia.org/wiki/Sinc_function\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.linspace(-4, 4, 41)\n    >>> np.sinc(x)\n     array([-3.89804309e-17,  -4.92362781e-02,  -8.40918587e-02, # may vary\n            -8.90384387e-02,  -5.84680802e-02,   3.89804309e-17,\n            6.68206631e-02,   1.16434881e-01,   1.26137788e-01,\n            8.50444803e-02,  -3.89804309e-17,  -1.03943254e-01,\n            -1.89206682e-01,  -2.16236208e-01,  -1.55914881e-01,\n            3.89804309e-17,   2.33872321e-01,   5.04551152e-01,\n            7.56826729e-01,   9.35489284e-01,   1.00000000e+00,\n            9.35489284e-01,   7.56826729e-01,   5.04551152e-01,\n            2.33872321e-01,   3.89804309e-17,  -1.55914881e-01,\n           -2.16236208e-01,  -1.89206682e-01,  -1.03943254e-01,\n           -3.89804309e-17,   8.50444803e-02,   1.26137788e-01,\n            1.16434881e-01,   6.68206631e-02,   3.89804309e-17,\n            -5.84680802e-02,  -8.90384387e-02,  -8.40918587e-02,\n            -4.92362781e-02,  -3.89804309e-17])\n\n    >>> plt.plot(x, np.sinc(x))\n    [<matplotlib.lines.Line2D object at 0x...>]\n    >>> plt.title(\"Sinc Function\")\n    Text(0.5, 1.0, 'Sinc Function')\n    >>> plt.ylabel(\"Amplitude\")\n    Text(0, 0.5, 'Amplitude')\n    >>> plt.xlabel(\"X\")\n    Text(0.5, 0, 'X')\n    >>> plt.show()\n\n    \"\"\"\n    x = np.asanyarray(x)\n    y = pi * where(x == 0, 1.0e-20, x)\n    return sin(y)/y\n\n\ndef _msort_dispatcher(a):\n    return (a,)\n\n\n@array_function_dispatch(_msort_dispatcher)\ndef msort(a):\n    \"\"\"\n    Return a copy of an array sorted along the first axis.\n\n    Parameters\n    ----------\n    a : array_like\n        Array to be sorted.\n\n    Returns\n    -------\n    sorted_array : ndarray\n        Array of the same type and shape as `a`.\n\n    See Also\n    --------\n    sort\n\n    Notes\n    -----\n    ``np.msort(a)`` is equivalent to  ``np.sort(a, axis=0)``.\n\n    \"\"\"\n    b = array(a, subok=True, copy=True)\n    b.sort(0)\n    return b\n\n\ndef _ureduce(a, func, **kwargs):\n    \"\"\"\n    Internal Function.\n    Call `func` with `a` as first argument swapping the axes to use extended\n    axis on functions that don't support it natively.\n\n    Returns result and a.shape with axis dims set to 1.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    func : callable\n        Reduction function capable of receiving a single axis argument.\n        It is called with `a` as first argument followed by `kwargs`.\n    kwargs : keyword arguments\n        additional keyword arguments to pass to `func`.\n\n    Returns\n    -------\n    result : tuple\n        Result of func(a, **kwargs) and a.shape with axis dims set to 1\n        which can be used to reshape the result to the same shape a ufunc with\n        keepdims=True would produce.\n\n    \"\"\"\n    a = np.asanyarray(a)\n    axis = kwargs.get('axis', None)\n    if axis is not None:\n        keepdim = list(a.shape)\n        nd = a.ndim\n        axis = _nx.normalize_axis_tuple(axis, nd)\n\n        for ax in axis:\n            keepdim[ax] = 1\n\n        if len(axis) == 1:\n            kwargs['axis'] = axis[0]\n        else:\n            keep = set(range(nd)) - set(axis)\n            nkeep = len(keep)\n            # swap axis that should not be reduced to front\n            for i, s in enumerate(sorted(keep)):\n                a = a.swapaxes(i, s)\n            # merge reduced axis\n            a = a.reshape(a.shape[:nkeep] + (-1,))\n            kwargs['axis'] = -1\n        keepdim = tuple(keepdim)\n    else:\n        keepdim = (1,) * a.ndim\n\n    r = func(a, **kwargs)\n    return r, keepdim\n\n\ndef _median_dispatcher(\n        a, axis=None, out=None, overwrite_input=None, keepdims=None):\n    return (a, out)\n\n\n@array_function_dispatch(_median_dispatcher)\ndef median(a, axis=None, out=None, overwrite_input=False, keepdims=False):\n    \"\"\"\n    Compute the median along the specified axis.\n\n    Returns the median of the array elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : {int, sequence of int, None}, optional\n        Axis or axes along which the medians are computed. The default\n        is to compute the median along a flattened version of the array.\n        A sequence of axes is supported since version 1.9.0.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output,\n        but the type (of the output) will be cast if necessary.\n    overwrite_input : bool, optional\n       If True, then allow use of memory of input array `a` for\n       calculations. The input array will be modified by the call to\n       `median`. This will save memory when you do not need to preserve\n       the contents of the input array. Treat the input as undefined,\n       but it will probably be fully or partially sorted. Default is\n       False. If `overwrite_input` is ``True`` and `a` is not already an\n       `ndarray`, an error will be raised.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original `arr`.\n\n        .. versionadded:: 1.9.0\n\n    Returns\n    -------\n    median : ndarray\n        A new array holding the result. If the input contains integers\n        or floats smaller than ``float64``, then the output data-type is\n        ``np.float64``.  Otherwise, the data-type of the output is the\n        same as that of the input. If `out` is specified, that array is\n        returned instead.\n\n    See Also\n    --------\n    mean, percentile\n\n    Notes\n    -----\n    Given a vector ``V`` of length ``N``, the median of ``V`` is the\n    middle value of a sorted copy of ``V``, ``V_sorted`` - i\n    e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\n    two middle values of ``V_sorted`` when ``N`` is even.\n\n    Examples\n    --------\n    >>> a = np.array([[10, 7, 4], [3, 2, 1]])\n    >>> a\n    array([[10,  7,  4],\n           [ 3,  2,  1]])\n    >>> np.median(a)\n    3.5\n    >>> np.median(a, axis=0)\n    array([6.5, 4.5, 2.5])\n    >>> np.median(a, axis=1)\n    array([7.,  2.])\n    >>> m = np.median(a, axis=0)\n    >>> out = np.zeros_like(m)\n    >>> np.median(a, axis=0, out=m)\n    array([6.5,  4.5,  2.5])\n    >>> m\n    array([6.5,  4.5,  2.5])\n    >>> b = a.copy()\n    >>> np.median(b, axis=1, overwrite_input=True)\n    array([7.,  2.])\n    >>> assert not np.all(a==b)\n    >>> b = a.copy()\n    >>> np.median(b, axis=None, overwrite_input=True)\n    3.5\n    >>> assert not np.all(a==b)\n\n    \"\"\"\n    r, k = _ureduce(a, func=_median, axis=axis, out=out,\n                    overwrite_input=overwrite_input)\n    if keepdims:\n        return r.reshape(k)\n    else:\n        return r\n\n\ndef _median(a, axis=None, out=None, overwrite_input=False):\n    # can't be reasonably be implemented in terms of percentile as we have to\n    # call mean to not break astropy\n    a = np.asanyarray(a)\n\n    # Set the partition indexes\n    if axis is None:\n        sz = a.size\n    else:\n        sz = a.shape[axis]\n    if sz % 2 == 0:\n        szh = sz // 2\n        kth = [szh - 1, szh]\n    else:\n        kth = [(sz - 1) // 2]\n    # Check if the array contains any nan's\n    if np.issubdtype(a.dtype, np.inexact):\n        kth.append(-1)\n\n    if overwrite_input:\n        if axis is None:\n            part = a.ravel()\n            part.partition(kth)\n        else:\n            a.partition(kth, axis=axis)\n            part = a\n    else:\n        part = partition(a, kth, axis=axis)\n\n    if part.shape == ():\n        # make 0-D arrays work\n        return part.item()\n    if axis is None:\n        axis = 0\n\n    indexer = [slice(None)] * part.ndim\n    index = part.shape[axis] // 2\n    if part.shape[axis] % 2 == 1:\n        # index with slice to allow mean (below) to work\n        indexer[axis] = slice(index, index+1)\n    else:\n        indexer[axis] = slice(index-1, index+1)\n    indexer = tuple(indexer)\n\n    # Check if the array contains any nan's\n    if np.issubdtype(a.dtype, np.inexact) and sz > 0:\n        # warn and return nans like mean would\n        rout = mean(part[indexer], axis=axis, out=out)\n        return np.lib.utils._median_nancheck(part, rout, axis, out)\n    else:\n        # if there are no nans\n        # Use mean in odd and even case to coerce data type\n        # and check, use out array.\n        return mean(part[indexer], axis=axis, out=out)\n\n\ndef _percentile_dispatcher(a, q, axis=None, out=None, overwrite_input=None,\n                           interpolation=None, keepdims=None):\n    return (a, q, out)\n\n\n@array_function_dispatch(_percentile_dispatcher)\ndef percentile(a, q, axis=None, out=None,\n               overwrite_input=False, interpolation='linear', keepdims=False):\n    \"\"\"\n    Compute the q-th percentile of the data along the specified axis.\n\n    Returns the q-th percentile(s) of the array elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    q : array_like of float\n        Percentile or sequence of percentiles to compute, which must be between\n        0 and 100 inclusive.\n    axis : {int, tuple of int, None}, optional\n        Axis or axes along which the percentiles are computed. The\n        default is to compute the percentile(s) along a flattened\n        version of the array.\n\n        .. versionchanged:: 1.9.0\n            A tuple of axes is supported\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output,\n        but the type (of the output) will be cast if necessary.\n    overwrite_input : bool, optional\n        If True, then allow the input array `a` to be modified by intermediate\n        calculations, to save memory. In this case, the contents of the input\n        `a` after this function completes is undefined.\n\n    interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n        This optional parameter specifies the interpolation method to\n        use when the desired percentile lies between two data points\n        ``i < j``:\n\n        * 'linear': ``i + (j - i) * fraction``, where ``fraction``\n          is the fractional part of the index surrounded by ``i``\n          and ``j``.\n        * 'lower': ``i``.\n        * 'higher': ``j``.\n        * 'nearest': ``i`` or ``j``, whichever is nearest.\n        * 'midpoint': ``(i + j) / 2``.\n\n        .. versionadded:: 1.9.0\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the\n        result will broadcast correctly against the original array `a`.\n\n        .. versionadded:: 1.9.0\n\n    Returns\n    -------\n    percentile : scalar or ndarray\n        If `q` is a single percentile and `axis=None`, then the result\n        is a scalar. If multiple percentiles are given, first axis of\n        the result corresponds to the percentiles. The other axes are\n        the axes that remain after the reduction of `a`. If the input\n        contains integers or floats smaller than ``float64``, the output\n        data-type is ``float64``. Otherwise, the output data-type is the\n        same as that of the input. If `out` is specified, that array is\n        returned instead.\n\n    See Also\n    --------\n    mean\n    median : equivalent to ``percentile(..., 50)``\n    nanpercentile\n    quantile : equivalent to percentile, except with q in the range [0, 1].\n\n    Notes\n    -----\n    Given a vector ``V`` of length ``N``, the q-th percentile of\n    ``V`` is the value ``q/100`` of the way from the minimum to the\n    maximum in a sorted copy of ``V``. The values and distances of\n    the two nearest neighbors as well as the `interpolation` parameter\n    will determine the percentile if the normalized ranking does not\n    match the location of ``q`` exactly. This function is the same as\n    the median if ``q=50``, the same as the minimum if ``q=0`` and the\n    same as the maximum if ``q=100``.\n\n    Examples\n    --------\n    >>> a = np.array([[10, 7, 4], [3, 2, 1]])\n    >>> a\n    array([[10,  7,  4],\n           [ 3,  2,  1]])\n    >>> np.percentile(a, 50)\n    3.5\n    >>> np.percentile(a, 50, axis=0)\n    array([6.5, 4.5, 2.5])\n    >>> np.percentile(a, 50, axis=1)\n    array([7.,  2.])\n    >>> np.percentile(a, 50, axis=1, keepdims=True)\n    array([[7.],\n           [2.]])\n\n    >>> m = np.percentile(a, 50, axis=0)\n    >>> out = np.zeros_like(m)\n    >>> np.percentile(a, 50, axis=0, out=out)\n    array([6.5, 4.5, 2.5])\n    >>> m\n    array([6.5, 4.5, 2.5])\n\n    >>> b = a.copy()\n    >>> np.percentile(b, 50, axis=1, overwrite_input=True)\n    array([7.,  2.])\n    >>> assert not np.all(a == b)\n\n    The different types of interpolation can be visualized graphically:\n\n    .. plot::\n\n        import matplotlib.pyplot as plt\n\n        a = np.arange(4)\n        p = np.linspace(0, 100, 6001)\n        ax = plt.gca()\n        lines = [\n            ('linear', None),\n            ('higher', '--'),\n            ('lower', '--'),\n            ('nearest', '-.'),\n            ('midpoint', '-.'),\n        ]\n        for interpolation, style in lines:\n            ax.plot(\n                p, np.percentile(a, p, interpolation=interpolation),\n                label=interpolation, linestyle=style)\n        ax.set(\n            title='Interpolation methods for list: ' + str(a),\n            xlabel='Percentile',\n            ylabel='List item returned',\n            yticks=a)\n        ax.legend()\n        plt.show()\n\n    \"\"\"\n    q = np.true_divide(q, 100)\n    q = asanyarray(q)  # undo any decay that the ufunc performed (see gh-13105)\n    if not _quantile_is_valid(q):\n        raise ValueError(\"Percentiles must be in the range [0, 100]\")\n    return _quantile_unchecked(\n        a, q, axis, out, overwrite_input, interpolation, keepdims)\n\n\ndef _quantile_dispatcher(a, q, axis=None, out=None, overwrite_input=None,\n                         interpolation=None, keepdims=None):\n    return (a, q, out)\n\n\n@array_function_dispatch(_quantile_dispatcher)\ndef quantile(a, q, axis=None, out=None,\n             overwrite_input=False, interpolation='linear', keepdims=False):\n    \"\"\"\n    Compute the q-th quantile of the data along the specified axis.\n\n    .. versionadded:: 1.15.0\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    q : array_like of float\n        Quantile or sequence of quantiles to compute, which must be between\n        0 and 1 inclusive.\n    axis : {int, tuple of int, None}, optional\n        Axis or axes along which the quantiles are computed. The\n        default is to compute the quantile(s) along a flattened\n        version of the array.\n    out : ndarray, optional\n        Alternative output array in which to place the result. It must\n        have the same shape and buffer length as the expected output,\n        but the type (of the output) will be cast if necessary.\n    overwrite_input : bool, optional\n        If True, then allow the input array `a` to be modified by intermediate\n        calculations, to save memory. In this case, the contents of the input\n        `a` after this function completes is undefined.\n    interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n        This optional parameter specifies the interpolation method to\n        use when the desired quantile lies between two data points\n        ``i < j``:\n\n            * linear: ``i + (j - i) * fraction``, where ``fraction``\n              is the fractional part of the index surrounded by ``i``\n              and ``j``.\n            * lower: ``i``.\n            * higher: ``j``.\n            * nearest: ``i`` or ``j``, whichever is nearest.\n            * midpoint: ``(i + j) / 2``.\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the\n        result will broadcast correctly against the original array `a`.\n\n    Returns\n    -------\n    quantile : scalar or ndarray\n        If `q` is a single quantile and `axis=None`, then the result\n        is a scalar. If multiple quantiles are given, first axis of\n        the result corresponds to the quantiles. The other axes are\n        the axes that remain after the reduction of `a`. If the input\n        contains integers or floats smaller than ``float64``, the output\n        data-type is ``float64``. Otherwise, the output data-type is the\n        same as that of the input. If `out` is specified, that array is\n        returned instead.\n\n    See Also\n    --------\n    mean\n    percentile : equivalent to quantile, but with q in the range [0, 100].\n    median : equivalent to ``quantile(..., 0.5)``\n    nanquantile\n\n    Notes\n    -----\n    Given a vector ``V`` of length ``N``, the q-th quantile of\n    ``V`` is the value ``q`` of the way from the minimum to the\n    maximum in a sorted copy of ``V``. The values and distances of\n    the two nearest neighbors as well as the `interpolation` parameter\n    will determine the quantile if the normalized ranking does not\n    match the location of ``q`` exactly. This function is the same as\n    the median if ``q=0.5``, the same as the minimum if ``q=0.0`` and the\n    same as the maximum if ``q=1.0``.\n\n    Examples\n    --------\n    >>> a = np.array([[10, 7, 4], [3, 2, 1]])\n    >>> a\n    array([[10,  7,  4],\n           [ 3,  2,  1]])\n    >>> np.quantile(a, 0.5)\n    3.5\n    >>> np.quantile(a, 0.5, axis=0)\n    array([6.5, 4.5, 2.5])\n    >>> np.quantile(a, 0.5, axis=1)\n    array([7.,  2.])\n    >>> np.quantile(a, 0.5, axis=1, keepdims=True)\n    array([[7.],\n           [2.]])\n    >>> m = np.quantile(a, 0.5, axis=0)\n    >>> out = np.zeros_like(m)\n    >>> np.quantile(a, 0.5, axis=0, out=out)\n    array([6.5, 4.5, 2.5])\n    >>> m\n    array([6.5, 4.5, 2.5])\n    >>> b = a.copy()\n    >>> np.quantile(b, 0.5, axis=1, overwrite_input=True)\n    array([7.,  2.])\n    >>> assert not np.all(a == b)\n    \"\"\"\n    q = np.asanyarray(q)\n    if not _quantile_is_valid(q):\n        raise ValueError(\"Quantiles must be in the range [0, 1]\")\n    return _quantile_unchecked(\n        a, q, axis, out, overwrite_input, interpolation, keepdims)\n\n\ndef _quantile_unchecked(a, q, axis=None, out=None, overwrite_input=False,\n                        interpolation='linear', keepdims=False):\n    \"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\n    r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,\n                    overwrite_input=overwrite_input,\n                    interpolation=interpolation)\n    if keepdims:\n        return r.reshape(q.shape + k)\n    else:\n        return r\n\n\ndef _quantile_is_valid(q):\n    # avoid expensive reductions, relevant for arrays with < O(1000) elements\n    if q.ndim == 1 and q.size < 10:\n        for i in range(q.size):\n            if q[i] < 0.0 or q[i] > 1.0:\n                return False\n    else:\n        # faster than any()\n        if np.count_nonzero(q < 0.0) or np.count_nonzero(q > 1.0):\n            return False\n    return True\n\n\ndef _lerp(a, b, t, out=None):\n    \"\"\" Linearly interpolate from a to b by a factor of t \"\"\"\n    diff_b_a = subtract(b, a)\n    # asanyarray is a stop-gap until gh-13105\n    lerp_interpolation = asanyarray(add(a, diff_b_a*t, out=out))\n    subtract(b, diff_b_a * (1 - t), out=lerp_interpolation, where=t>=0.5)\n    if lerp_interpolation.ndim == 0 and out is None:\n        lerp_interpolation = lerp_interpolation[()]  # unpack 0d arrays\n    return lerp_interpolation\n\n\ndef _quantile_ureduce_func(a, q, axis=None, out=None, overwrite_input=False,\n                           interpolation='linear', keepdims=False):\n    a = asarray(a)\n\n    # ufuncs cause 0d array results to decay to scalars (see gh-13105), which\n    # makes them problematic for __setitem__ and attribute access. As a\n    # workaround, we call this on the result of every ufunc on a possibly-0d\n    # array.\n    not_scalar = np.asanyarray\n\n    # prepare a for partitioning\n    if overwrite_input:\n        if axis is None:\n            ap = a.ravel()\n        else:\n            ap = a\n    else:\n        if axis is None:\n            ap = a.flatten()\n        else:\n            ap = a.copy()\n\n    if axis is None:\n        axis = 0\n\n    if q.ndim > 2:\n        # The code below works fine for nd, but it might not have useful\n        # semantics. For now, keep the supported dimensions the same as it was\n        # before.\n        raise ValueError(\"q must be a scalar or 1d\")\n\n    Nx = ap.shape[axis]\n    indices = not_scalar(q * (Nx - 1))\n    # round fractional indices according to interpolation method\n    if interpolation == 'lower':\n        indices = floor(indices).astype(intp)\n    elif interpolation == 'higher':\n        indices = ceil(indices).astype(intp)\n    elif interpolation == 'midpoint':\n        indices = 0.5 * (floor(indices) + ceil(indices))\n    elif interpolation == 'nearest':\n        indices = around(indices).astype(intp)\n    elif interpolation == 'linear':\n        pass  # keep index as fraction and interpolate\n    else:\n        raise ValueError(\n            \"interpolation can only be 'linear', 'lower' 'higher', \"\n            \"'midpoint', or 'nearest'\")\n\n    # The dimensions of `q` are prepended to the output shape, so we need the\n    # axis being sampled from `ap` to be first.\n    ap = np.moveaxis(ap, axis, 0)\n    del axis\n\n    if np.issubdtype(indices.dtype, np.integer):\n        # take the points along axis\n\n        if np.issubdtype(a.dtype, np.inexact):\n            # may contain nan, which would sort to the end\n            ap.partition(concatenate((indices.ravel(), [-1])), axis=0)\n            n = np.isnan(ap[-1])\n        else:\n            # cannot contain nan\n            ap.partition(indices.ravel(), axis=0)\n            n = np.array(False, dtype=bool)\n\n        r = take(ap, indices, axis=0, out=out)\n\n    else:\n        # weight the points above and below the indices\n\n        indices_below = not_scalar(floor(indices)).astype(intp)\n        indices_above = not_scalar(indices_below + 1)\n        indices_above[indices_above > Nx - 1] = Nx - 1\n\n        if np.issubdtype(a.dtype, np.inexact):\n            # may contain nan, which would sort to the end\n            ap.partition(concatenate((\n                indices_below.ravel(), indices_above.ravel(), [-1]\n            )), axis=0)\n            n = np.isnan(ap[-1])\n        else:\n            # cannot contain nan\n            ap.partition(concatenate((\n                indices_below.ravel(), indices_above.ravel()\n            )), axis=0)\n            n = np.array(False, dtype=bool)\n\n        weights_shape = indices.shape + (1,) * (ap.ndim - 1)\n        weights_above = not_scalar(indices - indices_below).reshape(weights_shape)\n\n        x_below = take(ap, indices_below, axis=0)\n        x_above = take(ap, indices_above, axis=0)\n\n        r = _lerp(x_below, x_above, weights_above, out=out)\n\n    # if any slice contained a nan, then all results on that slice are also nan\n    if np.any(n):\n        if r.ndim == 0 and out is None:\n            # can't write to a scalar\n            r = a.dtype.type(np.nan)\n        else:\n            r[..., n] = a.dtype.type(np.nan)\n\n    return r\n\n\ndef _trapz_dispatcher(y, x=None, dx=None, axis=None):\n    return (y, x)\n\n\n@array_function_dispatch(_trapz_dispatcher)\ndef trapz(y, x=None, dx=1.0, axis=-1):\n    \"\"\"\n    Integrate along the given axis using the composite trapezoidal rule.\n\n    Integrate `y` (`x`) along given axis.\n\n    Parameters\n    ----------\n    y : array_like\n        Input array to integrate.\n    x : array_like, optional\n        The sample points corresponding to the `y` values. If `x` is None,\n        the sample points are assumed to be evenly spaced `dx` apart. The\n        default is None.\n    dx : scalar, optional\n        The spacing between sample points when `x` is None. The default is 1.\n    axis : int, optional\n        The axis along which to integrate.\n\n    Returns\n    -------\n    trapz : float\n        Definite integral as approximated by trapezoidal rule.\n\n    See Also\n    --------\n    sum, cumsum\n\n    Notes\n    -----\n    Image [2]_ illustrates trapezoidal rule -- y-axis locations of points\n    will be taken from `y` array, by default x-axis distances between\n    points will be 1.0, alternatively they can be provided with `x` array\n    or with `dx` scalar.  Return value will be equal to combined area under\n    the red lines.\n\n\n    References\n    ----------\n    .. [1] Wikipedia page: https://en.wikipedia.org/wiki/Trapezoidal_rule\n\n    .. [2] Illustration image:\n           https://en.wikipedia.org/wiki/File:Composite_trapezoidal_rule_illustration.png\n\n    Examples\n    --------\n    >>> np.trapz([1,2,3])\n    4.0\n    >>> np.trapz([1,2,3], x=[4,6,8])\n    8.0\n    >>> np.trapz([1,2,3], dx=2)\n    8.0\n    >>> a = np.arange(6).reshape(2, 3)\n    >>> a\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    >>> np.trapz(a, axis=0)\n    array([1.5, 2.5, 3.5])\n    >>> np.trapz(a, axis=1)\n    array([2.,  8.])\n\n    \"\"\"\n    y = asanyarray(y)\n    if x is None:\n        d = dx\n    else:\n        x = asanyarray(x)\n        if x.ndim == 1:\n            d = diff(x)\n            # reshape to correct shape\n            shape = [1]*y.ndim\n            shape[axis] = d.shape[0]\n            d = d.reshape(shape)\n        else:\n            d = diff(x, axis=axis)\n    nd = y.ndim\n    slice1 = [slice(None)]*nd\n    slice2 = [slice(None)]*nd\n    slice1[axis] = slice(1, None)\n    slice2[axis] = slice(None, -1)\n    try:\n        ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)\n    except ValueError:\n        # Operations didn't work, cast to ndarray\n        d = np.asarray(d)\n        y = np.asarray(y)\n        ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)\n    return ret\n\n\ndef _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):\n    return xi\n\n\n# Based on scitools meshgrid\n@array_function_dispatch(_meshgrid_dispatcher)\ndef meshgrid(*xi, copy=True, sparse=False, indexing='xy'):\n    \"\"\"\n    Return coordinate matrices from coordinate vectors.\n\n    Make N-D coordinate arrays for vectorized evaluations of\n    N-D scalar/vector fields over N-D grids, given\n    one-dimensional coordinate arrays x1, x2,..., xn.\n\n    .. versionchanged:: 1.9\n       1-D and 0-D cases are allowed.\n\n    Parameters\n    ----------\n    x1, x2,..., xn : array_like\n        1-D arrays representing the coordinates of a grid.\n    indexing : {'xy', 'ij'}, optional\n        Cartesian ('xy', default) or matrix ('ij') indexing of output.\n        See Notes for more details.\n\n        .. versionadded:: 1.7.0\n    sparse : bool, optional\n        If True a sparse grid is returned in order to conserve memory.\n        Default is False.\n\n        .. versionadded:: 1.7.0\n    copy : bool, optional\n        If False, a view into the original arrays are returned in order to\n        conserve memory.  Default is True.  Please note that\n        ``sparse=False, copy=False`` will likely return non-contiguous\n        arrays.  Furthermore, more than one element of a broadcast array\n        may refer to a single memory location.  If you need to write to the\n        arrays, make copies first.\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    X1, X2,..., XN : ndarray\n        For vectors `x1`, `x2`,..., 'xn' with lengths ``Ni=len(xi)`` ,\n        return ``(N1, N2, N3,...Nn)`` shaped arrays if indexing='ij'\n        or ``(N2, N1, N3,...Nn)`` shaped arrays if indexing='xy'\n        with the elements of `xi` repeated to fill the matrix along\n        the first dimension for `x1`, the second for `x2` and so on.\n\n    Notes\n    -----\n    This function supports both indexing conventions through the indexing\n    keyword argument.  Giving the string 'ij' returns a meshgrid with\n    matrix indexing, while 'xy' returns a meshgrid with Cartesian indexing.\n    In the 2-D case with inputs of length M and N, the outputs are of shape\n    (N, M) for 'xy' indexing and (M, N) for 'ij' indexing.  In the 3-D case\n    with inputs of length M, N and P, outputs are of shape (N, M, P) for\n    'xy' indexing and (M, N, P) for 'ij' indexing.  The difference is\n    illustrated by the following code snippet::\n\n        xv, yv = np.meshgrid(x, y, sparse=False, indexing='ij')\n        for i in range(nx):\n            for j in range(ny):\n                # treat xv[i,j], yv[i,j]\n\n        xv, yv = np.meshgrid(x, y, sparse=False, indexing='xy')\n        for i in range(nx):\n            for j in range(ny):\n                # treat xv[j,i], yv[j,i]\n\n    In the 1-D and 0-D case, the indexing and sparse keywords have no effect.\n\n    See Also\n    --------\n    mgrid : Construct a multi-dimensional \"meshgrid\" using indexing notation.\n    ogrid : Construct an open multi-dimensional \"meshgrid\" using indexing \n            notation.\n\n    Examples\n    --------\n    >>> nx, ny = (3, 2)\n    >>> x = np.linspace(0, 1, nx)\n    >>> y = np.linspace(0, 1, ny)\n    >>> xv, yv = np.meshgrid(x, y)\n    >>> xv\n    array([[0. , 0.5, 1. ],\n           [0. , 0.5, 1. ]])\n    >>> yv\n    array([[0.,  0.,  0.],\n           [1.,  1.,  1.]])\n    >>> xv, yv = np.meshgrid(x, y, sparse=True)  # make sparse output arrays\n    >>> xv\n    array([[0. ,  0.5,  1. ]])\n    >>> yv\n    array([[0.],\n           [1.]])\n\n    `meshgrid` is very useful to evaluate functions on a grid.\n\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.arange(-5, 5, 0.1)\n    >>> y = np.arange(-5, 5, 0.1)\n    >>> xx, yy = np.meshgrid(x, y, sparse=True)\n    >>> z = np.sin(xx**2 + yy**2) / (xx**2 + yy**2)\n    >>> h = plt.contourf(x,y,z)\n    >>> plt.show()\n\n    \"\"\"\n    ndim = len(xi)\n\n    if indexing not in ['xy', 'ij']:\n        raise ValueError(\n            \"Valid values for `indexing` are 'xy' and 'ij'.\")\n\n    s0 = (1,) * ndim\n    output = [np.asanyarray(x).reshape(s0[:i] + (-1,) + s0[i + 1:])\n              for i, x in enumerate(xi)]\n\n    if indexing == 'xy' and ndim > 1:\n        # switch first and second axis\n        output[0].shape = (1, -1) + s0[2:]\n        output[1].shape = (-1, 1) + s0[2:]\n\n    if not sparse:\n        # Return the full N-D matrix (not only the 1-D vector)\n        output = np.broadcast_arrays(*output, subok=True)\n\n    if copy:\n        output = [x.copy() for x in output]\n\n    return output\n\n\ndef _delete_dispatcher(arr, obj, axis=None):\n    return (arr, obj)\n\n\n@array_function_dispatch(_delete_dispatcher)\ndef delete(arr, obj, axis=None):\n    \"\"\"\n    Return a new array with sub-arrays along an axis deleted. For a one\n    dimensional array, this returns those entries not returned by\n    `arr[obj]`.\n\n    Parameters\n    ----------\n    arr : array_like\n        Input array.\n    obj : slice, int or array of ints\n        Indicate indices of sub-arrays to remove along the specified axis.\n\n        .. versionchanged:: 1.19.0\n            Boolean indices are now treated as a mask of elements to remove,\n            rather than being cast to the integers 0 and 1.\n\n    axis : int, optional\n        The axis along which to delete the subarray defined by `obj`.\n        If `axis` is None, `obj` is applied to the flattened array.\n\n    Returns\n    -------\n    out : ndarray\n        A copy of `arr` with the elements specified by `obj` removed. Note\n        that `delete` does not occur in-place. If `axis` is None, `out` is\n        a flattened array.\n\n    See Also\n    --------\n    insert : Insert elements into an array.\n    append : Append elements at the end of an array.\n\n    Notes\n    -----\n    Often it is preferable to use a boolean mask. For example:\n\n    >>> arr = np.arange(12) + 1\n    >>> mask = np.ones(len(arr), dtype=bool)\n    >>> mask[[0,2,4]] = False\n    >>> result = arr[mask,...]\n\n    Is equivalent to `np.delete(arr, [0,2,4], axis=0)`, but allows further\n    use of `mask`.\n\n    Examples\n    --------\n    >>> arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n    >>> arr\n    array([[ 1,  2,  3,  4],\n           [ 5,  6,  7,  8],\n           [ 9, 10, 11, 12]])\n    >>> np.delete(arr, 1, 0)\n    array([[ 1,  2,  3,  4],\n           [ 9, 10, 11, 12]])\n\n    >>> np.delete(arr, np.s_[::2], 1)\n    array([[ 2,  4],\n           [ 6,  8],\n           [10, 12]])\n    >>> np.delete(arr, [1,3,5], None)\n    array([ 1,  3,  5,  7,  8,  9, 10, 11, 12])\n\n    \"\"\"\n    wrap = None\n    if type(arr) is not ndarray:\n        try:\n            wrap = arr.__array_wrap__\n        except AttributeError:\n            pass\n\n    arr = asarray(arr)\n    ndim = arr.ndim\n    arrorder = 'F' if arr.flags.fnc else 'C'\n    if axis is None:\n        if ndim != 1:\n            arr = arr.ravel()\n        # needed for np.matrix, which is still not 1d after being ravelled\n        ndim = arr.ndim\n        axis = ndim - 1\n    else:\n        axis = normalize_axis_index(axis, ndim)\n\n    slobj = [slice(None)]*ndim\n    N = arr.shape[axis]\n    newshape = list(arr.shape)\n\n    if isinstance(obj, slice):\n        start, stop, step = obj.indices(N)\n        xr = range(start, stop, step)\n        numtodel = len(xr)\n\n        if numtodel <= 0:\n            if wrap:\n                return wrap(arr.copy(order=arrorder))\n            else:\n                return arr.copy(order=arrorder)\n\n        # Invert if step is negative:\n        if step < 0:\n            step = -step\n            start = xr[-1]\n            stop = xr[0] + 1\n\n        newshape[axis] -= numtodel\n        new = empty(newshape, arr.dtype, arrorder)\n        # copy initial chunk\n        if start == 0:\n            pass\n        else:\n            slobj[axis] = slice(None, start)\n            new[tuple(slobj)] = arr[tuple(slobj)]\n        # copy end chunk\n        if stop == N:\n            pass\n        else:\n            slobj[axis] = slice(stop-numtodel, None)\n            slobj2 = [slice(None)]*ndim\n            slobj2[axis] = slice(stop, None)\n            new[tuple(slobj)] = arr[tuple(slobj2)]\n        # copy middle pieces\n        if step == 1:\n            pass\n        else:  # use array indexing.\n            keep = ones(stop-start, dtype=bool)\n            keep[:stop-start:step] = False\n            slobj[axis] = slice(start, stop-numtodel)\n            slobj2 = [slice(None)]*ndim\n            slobj2[axis] = slice(start, stop)\n            arr = arr[tuple(slobj2)]\n            slobj2[axis] = keep\n            new[tuple(slobj)] = arr[tuple(slobj2)]\n        if wrap:\n            return wrap(new)\n        else:\n            return new\n\n    if isinstance(obj, (int, integer)) and not isinstance(obj, bool):\n        # optimization for a single value\n        if (obj < -N or obj >= N):\n            raise IndexError(\n                \"index %i is out of bounds for axis %i with \"\n                \"size %i\" % (obj, axis, N))\n        if (obj < 0):\n            obj += N\n        newshape[axis] -= 1\n        new = empty(newshape, arr.dtype, arrorder)\n        slobj[axis] = slice(None, obj)\n        new[tuple(slobj)] = arr[tuple(slobj)]\n        slobj[axis] = slice(obj, None)\n        slobj2 = [slice(None)]*ndim\n        slobj2[axis] = slice(obj+1, None)\n        new[tuple(slobj)] = arr[tuple(slobj2)]\n    else:\n        _obj = obj\n        obj = np.asarray(obj)\n        if obj.size == 0 and not isinstance(_obj, np.ndarray):\n            obj = obj.astype(intp)\n\n        if obj.dtype == bool:\n            if obj.shape != (N,):\n                raise ValueError('boolean array argument obj to delete '\n                                 'must be one dimensional and match the axis '\n                                 'length of {}'.format(N))\n\n            # optimization, the other branch is slower\n            keep = ~obj\n        else:\n            keep = ones(N, dtype=bool)\n            keep[obj,] = False\n\n        slobj[axis] = keep\n        new = arr[tuple(slobj)]\n\n    if wrap:\n        return wrap(new)\n    else:\n        return new\n\n\ndef _insert_dispatcher(arr, obj, values, axis=None):\n    return (arr, obj, values)\n\n\n@array_function_dispatch(_insert_dispatcher)\ndef insert(arr, obj, values, axis=None):\n    \"\"\"\n    Insert values along the given axis before the given indices.\n\n    Parameters\n    ----------\n    arr : array_like\n        Input array.\n    obj : int, slice or sequence of ints\n        Object that defines the index or indices before which `values` is\n        inserted.\n\n        .. versionadded:: 1.8.0\n\n        Support for multiple insertions when `obj` is a single scalar or a\n        sequence with one element (similar to calling insert multiple\n        times).\n    values : array_like\n        Values to insert into `arr`. If the type of `values` is different\n        from that of `arr`, `values` is converted to the type of `arr`.\n        `values` should be shaped so that ``arr[...,obj,...] = values``\n        is legal.\n    axis : int, optional\n        Axis along which to insert `values`.  If `axis` is None then `arr`\n        is flattened first.\n\n    Returns\n    -------\n    out : ndarray\n        A copy of `arr` with `values` inserted.  Note that `insert`\n        does not occur in-place: a new array is returned. If\n        `axis` is None, `out` is a flattened array.\n\n    See Also\n    --------\n    append : Append elements at the end of an array.\n    concatenate : Join a sequence of arrays along an existing axis.\n    delete : Delete elements from an array.\n\n    Notes\n    -----\n    Note that for higher dimensional inserts `obj=0` behaves very different\n    from `obj=[0]` just like `arr[:,0,:] = values` is different from\n    `arr[:,[0],:] = values`.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 1], [2, 2], [3, 3]])\n    >>> a\n    array([[1, 1],\n           [2, 2],\n           [3, 3]])\n    >>> np.insert(a, 1, 5)\n    array([1, 5, 1, ..., 2, 3, 3])\n    >>> np.insert(a, 1, 5, axis=1)\n    array([[1, 5, 1],\n           [2, 5, 2],\n           [3, 5, 3]])\n\n    Difference between sequence and scalars:\n\n    >>> np.insert(a, [1], [[1],[2],[3]], axis=1)\n    array([[1, 1, 1],\n           [2, 2, 2],\n           [3, 3, 3]])\n    >>> np.array_equal(np.insert(a, 1, [1, 2, 3], axis=1),\n    ...                np.insert(a, [1], [[1],[2],[3]], axis=1))\n    True\n\n    >>> b = a.flatten()\n    >>> b\n    array([1, 1, 2, 2, 3, 3])\n    >>> np.insert(b, [2, 2], [5, 6])\n    array([1, 1, 5, ..., 2, 3, 3])\n\n    >>> np.insert(b, slice(2, 4), [5, 6])\n    array([1, 1, 5, ..., 2, 3, 3])\n\n    >>> np.insert(b, [2, 2], [7.13, False]) # type casting\n    array([1, 1, 7, ..., 2, 3, 3])\n\n    >>> x = np.arange(8).reshape(2, 4)\n    >>> idx = (1, 3)\n    >>> np.insert(x, idx, 999, axis=1)\n    array([[  0, 999,   1,   2, 999,   3],\n           [  4, 999,   5,   6, 999,   7]])\n\n    \"\"\"\n    wrap = None\n    if type(arr) is not ndarray:\n        try:\n            wrap = arr.__array_wrap__\n        except AttributeError:\n            pass\n\n    arr = asarray(arr)\n    ndim = arr.ndim\n    arrorder = 'F' if arr.flags.fnc else 'C'\n    if axis is None:\n        if ndim != 1:\n            arr = arr.ravel()\n        # needed for np.matrix, which is still not 1d after being ravelled\n        ndim = arr.ndim\n        axis = ndim - 1\n    else:\n        axis = normalize_axis_index(axis, ndim)\n    slobj = [slice(None)]*ndim\n    N = arr.shape[axis]\n    newshape = list(arr.shape)\n\n    if isinstance(obj, slice):\n        # turn it into a range object\n        indices = arange(*obj.indices(N), dtype=intp)\n    else:\n        # need to copy obj, because indices will be changed in-place\n        indices = np.array(obj)\n        if indices.dtype == bool:\n            # See also delete\n            # 2012-10-11, NumPy 1.8\n            warnings.warn(\n                \"in the future insert will treat boolean arrays and \"\n                \"array-likes as a boolean index instead of casting it to \"\n                \"integer\", FutureWarning, stacklevel=3)\n            indices = indices.astype(intp)\n            # Code after warning period:\n            #if obj.ndim != 1:\n            #    raise ValueError('boolean array argument obj to insert '\n            #                     'must be one dimensional')\n            #indices = np.flatnonzero(obj)\n        elif indices.ndim > 1:\n            raise ValueError(\n                \"index array argument obj to insert must be one dimensional \"\n                \"or scalar\")\n    if indices.size == 1:\n        index = indices.item()\n        if index < -N or index > N:\n            raise IndexError(\n                \"index %i is out of bounds for axis %i with \"\n                \"size %i\" % (obj, axis, N))\n        if (index < 0):\n            index += N\n\n        # There are some object array corner cases here, but we cannot avoid\n        # that:\n        values = array(values, copy=False, ndmin=arr.ndim, dtype=arr.dtype)\n        if indices.ndim == 0:\n            # broadcasting is very different here, since a[:,0,:] = ... behaves\n            # very different from a[:,[0],:] = ...! This changes values so that\n            # it works likes the second case. (here a[:,0:1,:])\n            values = np.moveaxis(values, 0, axis)\n        numnew = values.shape[axis]\n        newshape[axis] += numnew\n        new = empty(newshape, arr.dtype, arrorder)\n        slobj[axis] = slice(None, index)\n        new[tuple(slobj)] = arr[tuple(slobj)]\n        slobj[axis] = slice(index, index+numnew)\n        new[tuple(slobj)] = values\n        slobj[axis] = slice(index+numnew, None)\n        slobj2 = [slice(None)] * ndim\n        slobj2[axis] = slice(index, None)\n        new[tuple(slobj)] = arr[tuple(slobj2)]\n        if wrap:\n            return wrap(new)\n        return new\n    elif indices.size == 0 and not isinstance(obj, np.ndarray):\n        # Can safely cast the empty list to intp\n        indices = indices.astype(intp)\n\n    indices[indices < 0] += N\n\n    numnew = len(indices)\n    order = indices.argsort(kind='mergesort')   # stable sort\n    indices[order] += np.arange(numnew)\n\n    newshape[axis] += numnew\n    old_mask = ones(newshape[axis], dtype=bool)\n    old_mask[indices] = False\n\n    new = empty(newshape, arr.dtype, arrorder)\n    slobj2 = [slice(None)]*ndim\n    slobj[axis] = indices\n    slobj2[axis] = old_mask\n    new[tuple(slobj)] = values\n    new[tuple(slobj2)] = arr\n\n    if wrap:\n        return wrap(new)\n    return new\n\n\ndef _append_dispatcher(arr, values, axis=None):\n    return (arr, values)\n\n\n@array_function_dispatch(_append_dispatcher)\ndef append(arr, values, axis=None):\n    \"\"\"\n    Append values to the end of an array.\n\n    Parameters\n    ----------\n    arr : array_like\n        Values are appended to a copy of this array.\n    values : array_like\n        These values are appended to a copy of `arr`.  It must be of the\n        correct shape (the same shape as `arr`, excluding `axis`).  If\n        `axis` is not specified, `values` can be any shape and will be\n        flattened before use.\n    axis : int, optional\n        The axis along which `values` are appended.  If `axis` is not\n        given, both `arr` and `values` are flattened before use.\n\n    Returns\n    -------\n    append : ndarray\n        A copy of `arr` with `values` appended to `axis`.  Note that\n        `append` does not occur in-place: a new array is allocated and\n        filled.  If `axis` is None, `out` is a flattened array.\n\n    See Also\n    --------\n    insert : Insert elements into an array.\n    delete : Delete elements from an array.\n\n    Examples\n    --------\n    >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\n    array([1, 2, 3, ..., 7, 8, 9])\n\n    When `axis` is specified, `values` must have the correct shape.\n\n    >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\n    array([[1, 2, 3],\n           [4, 5, 6],\n           [7, 8, 9]])\n    >>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\n    Traceback (most recent call last):\n        ...\n    ValueError: all the input arrays must have same number of dimensions, but\n    the array at index 0 has 2 dimension(s) and the array at index 1 has 1\n    dimension(s)\n\n    \"\"\"\n    arr = asanyarray(arr)\n    if axis is None:\n        if arr.ndim != 1:\n            arr = arr.ravel()\n        values = ravel(values)\n        axis = arr.ndim-1\n    return concatenate((arr, values), axis=axis)\n\n\ndef _digitize_dispatcher(x, bins, right=None):\n    return (x, bins)\n\n\n@array_function_dispatch(_digitize_dispatcher)\ndef digitize(x, bins, right=False):\n    \"\"\"\n    Return the indices of the bins to which each value in input array belongs.\n\n    =========  =============  ============================\n    `right`    order of bins  returned index `i` satisfies\n    =========  =============  ============================\n    ``False``  increasing     ``bins[i-1] <= x < bins[i]``\n    ``True``   increasing     ``bins[i-1] < x <= bins[i]``\n    ``False``  decreasing     ``bins[i-1] > x >= bins[i]``\n    ``True``   decreasing     ``bins[i-1] >= x > bins[i]``\n    =========  =============  ============================\n\n    If values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is\n    returned as appropriate.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array to be binned. Prior to NumPy 1.10.0, this array had to\n        be 1-dimensional, but can now have any shape.\n    bins : array_like\n        Array of bins. It has to be 1-dimensional and monotonic.\n    right : bool, optional\n        Indicating whether the intervals include the right or the left bin\n        edge. Default behavior is (right==False) indicating that the interval\n        does not include the right edge. The left bin end is open in this\n        case, i.e., bins[i-1] <= x < bins[i] is the default behavior for\n        monotonically increasing bins.\n\n    Returns\n    -------\n    indices : ndarray of ints\n        Output array of indices, of same shape as `x`.\n\n    Raises\n    ------\n    ValueError\n        If `bins` is not monotonic.\n    TypeError\n        If the type of the input is complex.\n\n    See Also\n    --------\n    bincount, histogram, unique, searchsorted\n\n    Notes\n    -----\n    If values in `x` are such that they fall outside the bin range,\n    attempting to index `bins` with the indices that `digitize` returns\n    will result in an IndexError.\n\n    .. versionadded:: 1.10.0\n\n    `np.digitize` is  implemented in terms of `np.searchsorted`. This means\n    that a binary search is used to bin the values, which scales much better\n    for larger number of bins than the previous linear search. It also removes\n    the requirement for the input array to be 1-dimensional.\n\n    For monotonically _increasing_ `bins`, the following are equivalent::\n\n        np.digitize(x, bins, right=True)\n        np.searchsorted(bins, x, side='left')\n\n    Note that as the order of the arguments are reversed, the side must be too.\n    The `searchsorted` call is marginally faster, as it does not do any\n    monotonicity checks. Perhaps more importantly, it supports all dtypes.\n\n    Examples\n    --------\n    >>> x = np.array([0.2, 6.4, 3.0, 1.6])\n    >>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])\n    >>> inds = np.digitize(x, bins)\n    >>> inds\n    array([1, 4, 3, 2])\n    >>> for n in range(x.size):\n    ...   print(bins[inds[n]-1], \"<=\", x[n], \"<\", bins[inds[n]])\n    ...\n    0.0 <= 0.2 < 1.0\n    4.0 <= 6.4 < 10.0\n    2.5 <= 3.0 < 4.0\n    1.0 <= 1.6 < 2.5\n\n    >>> x = np.array([1.2, 10.0, 12.4, 15.5, 20.])\n    >>> bins = np.array([0, 5, 10, 15, 20])\n    >>> np.digitize(x,bins,right=True)\n    array([1, 2, 3, 4, 4])\n    >>> np.digitize(x,bins,right=False)\n    array([1, 3, 3, 4, 5])\n    \"\"\"\n    x = _nx.asarray(x)\n    bins = _nx.asarray(bins)\n\n    # here for compatibility, searchsorted below is happy to take this\n    if np.issubdtype(x.dtype, _nx.complexfloating):\n        raise TypeError(\"x may not be complex\")\n\n    mono = _monotonicity(bins)\n    if mono == 0:\n        raise ValueError(\"bins must be monotonically increasing or decreasing\")\n\n    # this is backwards because the arguments below are swapped\n    side = 'left' if right else 'right'\n    if mono == -1:\n        # reverse the bins, and invert the results\n        return len(bins) - _nx.searchsorted(bins[::-1], x, side=side)\n    else:\n        return _nx.searchsorted(bins, x, side=side)\n",4860],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_methods.py":["\"\"\"\nArray methods which are called by both the C-code for the method\nand the Python code for the NumPy-namespace function\n\n\"\"\"\nimport warnings\n\nfrom numpy.core import multiarray as mu\nfrom numpy.core import umath as um\nfrom numpy.core._asarray import asanyarray\nfrom numpy.core import numerictypes as nt\nfrom numpy.core import _exceptions\nfrom numpy._globals import _NoValue\nfrom numpy.compat import pickle, os_fspath, contextlib_nullcontext\n\n# save those O(100) nanoseconds!\numr_maximum = um.maximum.reduce\numr_minimum = um.minimum.reduce\numr_sum = um.add.reduce\numr_prod = um.multiply.reduce\numr_any = um.logical_or.reduce\numr_all = um.logical_and.reduce\n\n# Complex types to -> (2,)float view for fast-path computation in _var()\n_complex_to_float = {\n    nt.dtype(nt.csingle) : nt.dtype(nt.single),\n    nt.dtype(nt.cdouble) : nt.dtype(nt.double),\n}\n# Special case for windows: ensure double takes precedence\nif nt.dtype(nt.longdouble) != nt.dtype(nt.double):\n    _complex_to_float.update({\n        nt.dtype(nt.clongdouble) : nt.dtype(nt.longdouble),\n    })\n\n# avoid keyword arguments to speed up parsing, saves about 15%-20% for very\n# small reductions\ndef _amax(a, axis=None, out=None, keepdims=False,\n          initial=_NoValue, where=True):\n    return umr_maximum(a, axis, None, out, keepdims, initial, where)\n\ndef _amin(a, axis=None, out=None, keepdims=False,\n          initial=_NoValue, where=True):\n    return umr_minimum(a, axis, None, out, keepdims, initial, where)\n\ndef _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n         initial=_NoValue, where=True):\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\ndef _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n          initial=_NoValue, where=True):\n    return umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\ndef _any(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n    # Parsing keyword arguments is currently fairly slow, so avoid it for now\n    if where is True:\n        return umr_any(a, axis, dtype, out, keepdims)\n    return umr_any(a, axis, dtype, out, keepdims, where=where)\n\ndef _all(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n    # Parsing keyword arguments is currently fairly slow, so avoid it for now\n    if where is True:\n        return umr_all(a, axis, dtype, out, keepdims)\n    return umr_all(a, axis, dtype, out, keepdims, where=where)\n\ndef _count_reduce_items(arr, axis, keepdims=False, where=True):\n    # fast-path for the default case\n    if where is True:\n        # no boolean mask given, calculate items according to axis\n        if axis is None:\n            axis = tuple(range(arr.ndim))\n        elif not isinstance(axis, tuple):\n            axis = (axis,)\n        items = nt.intp(1)\n        for ax in axis:\n            items *= arr.shape[mu.normalize_axis_index(ax, arr.ndim)]\n    else:\n        # TODO: Optimize case when `where` is broadcast along a non-reduction\n        # axis and full sum is more excessive than needed.\n\n        # guarded to protect circular imports\n        from numpy.lib.stride_tricks import broadcast_to\n        # count True values in (potentially broadcasted) boolean mask\n        items = umr_sum(broadcast_to(where, arr.shape), axis, nt.intp, None,\n                        keepdims)\n    return items\n\n# Numpy 1.17.0, 2019-02-24\n# Various clip behavior deprecations, marked with _clip_dep as a prefix.\n\ndef _clip_dep_is_scalar_nan(a):\n    # guarded to protect circular imports\n    from numpy.core.fromnumeric import ndim\n    if ndim(a) != 0:\n        return False\n    try:\n        return um.isnan(a)\n    except TypeError:\n        return False\n\ndef _clip_dep_is_byte_swapped(a):\n    if isinstance(a, mu.ndarray):\n        return not a.dtype.isnative\n    return False\n\ndef _clip_dep_invoke_with_casting(ufunc, *args, out=None, casting=None, **kwargs):\n    # normal path\n    if casting is not None:\n        return ufunc(*args, out=out, casting=casting, **kwargs)\n\n    # try to deal with broken casting rules\n    try:\n        return ufunc(*args, out=out, **kwargs)\n    except _exceptions._UFuncOutputCastingError as e:\n        # Numpy 1.17.0, 2019-02-24\n        warnings.warn(\n            \"Converting the output of clip from {!r} to {!r} is deprecated. \"\n            \"Pass `casting=\\\"unsafe\\\"` explicitly to silence this warning, or \"\n            \"correct the type of the variables.\".format(e.from_, e.to),\n            DeprecationWarning,\n            stacklevel=2\n        )\n        return ufunc(*args, out=out, casting=\"unsafe\", **kwargs)\n\ndef _clip(a, min=None, max=None, out=None, *, casting=None, **kwargs):\n    if min is None and max is None:\n        raise ValueError(\"One of max or min must be given\")\n\n    # Numpy 1.17.0, 2019-02-24\n    # This deprecation probably incurs a substantial slowdown for small arrays,\n    # it will be good to get rid of it.\n    if not _clip_dep_is_byte_swapped(a) and not _clip_dep_is_byte_swapped(out):\n        using_deprecated_nan = False\n        if _clip_dep_is_scalar_nan(min):\n            min = -float('inf')\n            using_deprecated_nan = True\n        if _clip_dep_is_scalar_nan(max):\n            max = float('inf')\n            using_deprecated_nan = True\n        if using_deprecated_nan:\n            warnings.warn(\n                \"Passing `np.nan` to mean no clipping in np.clip has always \"\n                \"been unreliable, and is now deprecated. \"\n                \"In future, this will always return nan, like it already does \"\n                \"when min or max are arrays that contain nan. \"\n                \"To skip a bound, pass either None or an np.inf of an \"\n                \"appropriate sign.\",\n                DeprecationWarning,\n                stacklevel=2\n            )\n\n    if min is None:\n        return _clip_dep_invoke_with_casting(\n            um.minimum, a, max, out=out, casting=casting, **kwargs)\n    elif max is None:\n        return _clip_dep_invoke_with_casting(\n            um.maximum, a, min, out=out, casting=casting, **kwargs)\n    else:\n        return _clip_dep_invoke_with_casting(\n            um.clip, a, min, max, out=out, casting=casting, **kwargs)\n\ndef _mean(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):\n    arr = asanyarray(a)\n\n    is_float16_result = False\n\n    rcount = _count_reduce_items(arr, axis, keepdims=keepdims, where=where)\n    if rcount == 0 if where is True else umr_any(rcount == 0, axis=None):\n        warnings.warn(\"Mean of empty slice.\", RuntimeWarning, stacklevel=2)\n\n    # Cast bool, unsigned int, and int to float64 by default\n    if dtype is None:\n        if issubclass(arr.dtype.type, (nt.integer, nt.bool_)):\n            dtype = mu.dtype('f8')\n        elif issubclass(arr.dtype.type, nt.float16):\n            dtype = mu.dtype('f4')\n            is_float16_result = True\n\n    ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n    if isinstance(ret, mu.ndarray):\n        ret = um.true_divide(\n                ret, rcount, out=ret, casting='unsafe', subok=False)\n        if is_float16_result and out is None:\n            ret = arr.dtype.type(ret)\n    elif hasattr(ret, 'dtype'):\n        if is_float16_result:\n            ret = arr.dtype.type(ret / rcount)\n        else:\n            ret = ret.dtype.type(ret / rcount)\n    else:\n        ret = ret / rcount\n\n    return ret\n\ndef _var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n         where=True):\n    arr = asanyarray(a)\n\n    rcount = _count_reduce_items(arr, axis, keepdims=keepdims, where=where)\n    # Make this warning show up on top.\n    if ddof >= rcount if where is True else umr_any(ddof >= rcount, axis=None):\n        warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning,\n                      stacklevel=2)\n\n    # Cast bool, unsigned int, and int to float64 by default\n    if dtype is None and issubclass(arr.dtype.type, (nt.integer, nt.bool_)):\n        dtype = mu.dtype('f8')\n\n    # Compute the mean.\n    # Note that if dtype is not of inexact type then arraymean will\n    # not be either.\n    arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n    # The shape of rcount has to match arrmean to not change the shape of out\n    # in broadcasting. Otherwise, it cannot be stored back to arrmean.\n    if rcount.ndim == 0:\n        # fast-path for default case when where is True\n        div = rcount\n    else:\n        # matching rcount to arrmean when where is specified as array\n        div = rcount.reshape(arrmean.shape)\n    if isinstance(arrmean, mu.ndarray):\n        arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n                                 subok=False)\n    else:\n        arrmean = arrmean.dtype.type(arrmean / rcount)\n\n    # Compute sum of squared deviations from mean\n    # Note that x may not be inexact and that we need it to be an array,\n    # not a scalar.\n    x = asanyarray(arr - arrmean)\n\n    if issubclass(arr.dtype.type, (nt.floating, nt.integer)):\n        x = um.multiply(x, x, out=x)\n    # Fast-paths for built-in complex types\n    elif x.dtype in _complex_to_float:\n        xv = x.view(dtype=(_complex_to_float[x.dtype], (2,)))\n        um.multiply(xv, xv, out=xv)\n        x = um.add(xv[..., 0], xv[..., 1], out=x.real).real\n    # Most general case; includes handling object arrays containing imaginary\n    # numbers and complex types with non-native byteorder\n    else:\n        x = um.multiply(x, um.conjugate(x), out=x).real\n\n    ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n\n    # Compute degrees of freedom and make sure it is not negative.\n    rcount = um.maximum(rcount - ddof, 0)\n\n    # divide by degrees of freedom\n    if isinstance(ret, mu.ndarray):\n        ret = um.true_divide(\n                ret, rcount, out=ret, casting='unsafe', subok=False)\n    elif hasattr(ret, 'dtype'):\n        ret = ret.dtype.type(ret / rcount)\n    else:\n        ret = ret / rcount\n\n    return ret\n\ndef _std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False, *,\n         where=True):\n    ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n               keepdims=keepdims, where=where)\n\n    if isinstance(ret, mu.ndarray):\n        ret = um.sqrt(ret, out=ret)\n    elif hasattr(ret, 'dtype'):\n        ret = ret.dtype.type(um.sqrt(ret))\n    else:\n        ret = um.sqrt(ret)\n\n    return ret\n\ndef _ptp(a, axis=None, out=None, keepdims=False):\n    return um.subtract(\n        umr_maximum(a, axis, None, out, keepdims),\n        umr_minimum(a, axis, None, None, keepdims),\n        out\n    )\n\ndef _dump(self, file, protocol=2):\n    if hasattr(file, 'write'):\n        ctx = contextlib_nullcontext(file)\n    else:\n        ctx = open(os_fspath(file), \"wb\")\n    with ctx as f:\n        pickle.dump(self, f, protocol=protocol)\n\ndef _dumps(self, protocol=2):\n    return pickle.dumps(self, protocol=protocol)\n",289],"/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_classification.py":["\"\"\"Nearest Neighbor Classification\"\"\"\n\n# Authors: Jake Vanderplas <vanderplas@astro.washington.edu>\n#          Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Sparseness support by Lars Buitinck\n#          Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>\n#\n# License: BSD 3 clause (C) INRIA, University of Amsterdam\n\nimport numpy as np\nfrom scipy import stats\nfrom ..utils.extmath import weighted_mode\nfrom ..utils.validation import _is_arraylike, _num_samples\n\nimport warnings\nfrom ._base import _check_weights, _get_weights\nfrom ._base import NeighborsBase, KNeighborsMixin, RadiusNeighborsMixin\nfrom ..base import ClassifierMixin\n\n\nclass KNeighborsClassifier(KNeighborsMixin,\n                           ClassifierMixin,\n                           NeighborsBase):\n    \"\"\"Classifier implementing the k-nearest neighbors vote.\n\n    Read more in the :ref:`User Guide <classification>`.\n\n    Parameters\n    ----------\n    n_neighbors : int, default=5\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    weights : {'uniform', 'distance'} or callable, default='uniform'\n        weight function used in prediction.  Possible values:\n\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : int, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : str or callable, default='minkowski'\n        the distance metric to use for the tree.  The default metric is\n        minkowski, and with p=2 is equivalent to the standard Euclidean\n        metric. See the documentation of :class:`DistanceMetric` for a\n        list of available metrics.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n        Doesn't affect :meth:`fit` method.\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,)\n        Class labels known to the classifier\n\n    effective_metric_ : str or callble\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    outputs_2d_ : bool\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n        otherwise True.\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> neigh = KNeighborsClassifier(n_neighbors=3)\n    >>> neigh.fit(X, y)\n    KNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.1]]))\n    [0]\n    >>> print(neigh.predict_proba([[0.9]]))\n    [[0.66666667 0.33333333]]\n\n    See Also\n    --------\n    RadiusNeighborsClassifier\n    KNeighborsRegressor\n    RadiusNeighborsRegressor\n    NearestNeighbors\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances\n       but different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n    \"\"\"\n\n    def __init__(self, n_neighbors=5, *,\n                 weights='uniform', algorithm='auto', leaf_size=30,\n                 p=2, metric='minkowski', metric_params=None, n_jobs=None):\n        super().__init__(\n            n_neighbors=n_neighbors,\n            algorithm=algorithm,\n            leaf_size=leaf_size, metric=metric, p=p,\n            metric_params=metric_params,\n            n_jobs=n_jobs)\n        self.weights = weights\n\n    def fit(self, X, y):\n        \"\"\"Fit the k-nearest neighbors classifier from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : {array-like, sparse matrix} of shape (n_samples,) or \\\n                (n_samples, n_outputs)\n            Target values.\n\n        Returns\n        -------\n        self : KNeighborsClassifier\n            The fitted k-nearest neighbors classifier.\n        \"\"\"\n        self.weights = _check_weights(self.weights)\n\n        return self._fit(X, y)\n\n    def predict(self, X):\n        \"\"\"Predict the class labels for the provided data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_queries, n_features), \\\n                or (n_queries, n_indexed) if metric == 'precomputed'\n            Test samples.\n\n        Returns\n        -------\n        y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n            Class labels for each data sample.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n\n        neigh_dist, neigh_ind = self.kneighbors(X)\n        classes_ = self.classes_\n        _y = self._y\n        if not self.outputs_2d_:\n            _y = self._y.reshape((-1, 1))\n            classes_ = [self.classes_]\n\n        n_outputs = len(classes_)\n        n_queries = _num_samples(X)\n        weights = _get_weights(neigh_dist, self.weights)\n\n        y_pred = np.empty((n_queries, n_outputs), dtype=classes_[0].dtype)\n        for k, classes_k in enumerate(classes_):\n            if weights is None:\n                mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n            else:\n                mode, _ = weighted_mode(_y[neigh_ind, k], weights, axis=1)\n\n            mode = np.asarray(mode.ravel(), dtype=np.intp)\n            y_pred[:, k] = classes_k.take(mode)\n\n        if not self.outputs_2d_:\n            y_pred = y_pred.ravel()\n\n        return y_pred\n\n    def predict_proba(self, X):\n        \"\"\"Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_queries, n_features), \\\n                or (n_queries, n_indexed) if metric == 'precomputed'\n            Test samples.\n\n        Returns\n        -------\n        p : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n            of such arrays if n_outputs > 1.\n            The class probabilities of the input samples. Classes are ordered\n            by lexicographic order.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n\n        neigh_dist, neigh_ind = self.kneighbors(X)\n\n        classes_ = self.classes_\n        _y = self._y\n        if not self.outputs_2d_:\n            _y = self._y.reshape((-1, 1))\n            classes_ = [self.classes_]\n\n        n_queries = _num_samples(X)\n\n        weights = _get_weights(neigh_dist, self.weights)\n        if weights is None:\n            weights = np.ones_like(neigh_ind)\n\n        all_rows = np.arange(X.shape[0])\n        probabilities = []\n        for k, classes_k in enumerate(classes_):\n            pred_labels = _y[:, k][neigh_ind]\n            proba_k = np.zeros((n_queries, classes_k.size))\n\n            # a simple ':' index doesn't work right\n            for i, idx in enumerate(pred_labels.T):  # loop is O(n_neighbors)\n                proba_k[all_rows, idx] += weights[:, i]\n\n            # normalize 'votes' into real [0,1] probabilities\n            normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n            normalizer[normalizer == 0.0] = 1.0\n            proba_k /= normalizer\n\n            probabilities.append(proba_k)\n\n        if not self.outputs_2d_:\n            probabilities = probabilities[0]\n\n        return probabilities\n\n\nclass RadiusNeighborsClassifier(RadiusNeighborsMixin,\n                                ClassifierMixin,\n                                NeighborsBase):\n    \"\"\"Classifier implementing a vote among neighbors within a given radius\n\n    Read more in the :ref:`User Guide <classification>`.\n\n    Parameters\n    ----------\n    radius : float, default=1.0\n        Range of parameter space to use by default for :meth:`radius_neighbors`\n        queries.\n\n    weights : {'uniform', 'distance'} or callable, default='uniform'\n        weight function used in prediction.  Possible values:\n\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n\n        Uniform weights are used by default.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : int, default=2\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : str or callable, default='minkowski'\n        the distance metric to use for the tree.  The default metric is\n        minkowski, and with p=2 is equivalent to the standard Euclidean\n        metric. See the documentation of :class:`DistanceMetric` for a\n        list of available metrics.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`sparse graph`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    outlier_label : {manual label, 'most_frequent'}, default=None\n        label for outlier samples (samples with no neighbors in given radius).\n\n        - manual label: str or int label (should be the same type as y)\n          or list of manual labels if multi-output is used.\n        - 'most_frequent' : assign the most frequent label of y to outliers.\n        - None : when any outlier is detected, ValueError will be raised.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Class labels known to the classifier.\n\n    effective_metric_ : str or callable\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    n_samples_fit_ : int\n        Number of samples in the fitted data.\n\n    outlier_label_ : int or array-like of shape (n_class,)\n        Label which is given for outlier samples (samples with no neighbors\n        on given radius).\n\n    outputs_2d_ : bool\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n        otherwise True.\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import RadiusNeighborsClassifier\n    >>> neigh = RadiusNeighborsClassifier(radius=1.0)\n    >>> neigh.fit(X, y)\n    RadiusNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0]\n    >>> print(neigh.predict_proba([[1.0]]))\n    [[0.66666667 0.33333333]]\n\n    See Also\n    --------\n    KNeighborsClassifier\n    RadiusNeighborsRegressor\n    KNeighborsRegressor\n    NearestNeighbors\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n    \"\"\"\n\n    def __init__(self, radius=1.0, *, weights='uniform',\n                 algorithm='auto', leaf_size=30, p=2, metric='minkowski',\n                 outlier_label=None, metric_params=None, n_jobs=None,\n                 **kwargs):\n        super().__init__(\n              radius=radius,\n              algorithm=algorithm,\n              leaf_size=leaf_size,\n              metric=metric, p=p, metric_params=metric_params,\n              n_jobs=n_jobs)\n        self.weights = weights\n        self.outlier_label = outlier_label\n\n    def fit(self, X, y):\n        \"\"\"Fit the radius neighbors classifier from the training dataset.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples, n_samples) if metric='precomputed'\n            Training data.\n\n        y : {array-like, sparse matrix} of shape (n_samples,) or \\\n                (n_samples, n_outputs)\n            Target values.\n\n        Returns\n        -------\n        self : RadiusNeighborsClassifier\n            The fitted radius neighbors classifier.\n        \"\"\"\n        self.weights = _check_weights(self.weights)\n\n        self._fit(X, y)\n\n        classes_ = self.classes_\n        _y = self._y\n        if not self.outputs_2d_:\n            _y = self._y.reshape((-1, 1))\n            classes_ = [self.classes_]\n\n        if self.outlier_label is None:\n            outlier_label_ = None\n\n        elif self.outlier_label == 'most_frequent':\n            outlier_label_ = []\n            # iterate over multi-output, get the most frequent label for each\n            # output.\n            for k, classes_k in enumerate(classes_):\n                label_count = np.bincount(_y[:, k])\n                outlier_label_.append(classes_k[label_count.argmax()])\n\n        else:\n            if (_is_arraylike(self.outlier_label) and\n               not isinstance(self.outlier_label, str)):\n                if len(self.outlier_label) != len(classes_):\n                    raise ValueError(\"The length of outlier_label: {} is \"\n                                     \"inconsistent with the output \"\n                                     \"length: {}\".format(self.outlier_label,\n                                                         len(classes_)))\n                outlier_label_ = self.outlier_label\n            else:\n                outlier_label_ = [self.outlier_label] * len(classes_)\n\n            for classes, label in zip(classes_, outlier_label_):\n                if (_is_arraylike(label) and\n                   not isinstance(label, str)):\n                    # ensure the outlier lable for each output is a scalar.\n                    raise TypeError(\"The outlier_label of classes {} is \"\n                                    \"supposed to be a scalar, got \"\n                                    \"{}.\".format(classes, label))\n                if np.append(classes, label).dtype != classes.dtype:\n                    # ensure the dtype of outlier label is consistent with y.\n                    raise TypeError(\"The dtype of outlier_label {} is \"\n                                    \"inconsistent with classes {} in \"\n                                    \"y.\".format(label, classes))\n\n        self.outlier_label_ = outlier_label_\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the class labels for the provided data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_queries, n_features), \\\n                or (n_queries, n_indexed) if metric == 'precomputed'\n            Test samples.\n\n        Returns\n        -------\n        y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n            Class labels for each data sample.\n        \"\"\"\n\n        probs = self.predict_proba(X)\n        classes_ = self.classes_\n\n        if not self.outputs_2d_:\n            probs = [probs]\n            classes_ = [self.classes_]\n\n        n_outputs = len(classes_)\n        n_queries = probs[0].shape[0]\n        y_pred = np.empty((n_queries, n_outputs), dtype=classes_[0].dtype)\n\n        for k, prob in enumerate(probs):\n            # iterate over multi-output, assign labels based on probabilities\n            # of each output.\n            max_prob_index = prob.argmax(axis=1)\n            y_pred[:, k] = classes_[k].take(max_prob_index)\n\n            outlier_zero_probs = (prob == 0).all(axis=1)\n            if outlier_zero_probs.any():\n                zero_prob_index = np.flatnonzero(outlier_zero_probs)\n                y_pred[zero_prob_index, k] = self.outlier_label_[k]\n\n        if not self.outputs_2d_:\n            y_pred = y_pred.ravel()\n\n        return y_pred\n\n    def predict_proba(self, X):\n        \"\"\"Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_queries, n_features), \\\n                or (n_queries, n_indexed) if metric == 'precomputed'\n            Test samples.\n\n        Returns\n        -------\n        p : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n            of such arrays if n_outputs > 1.\n            The class probabilities of the input samples. Classes are ordered\n            by lexicographic order.\n        \"\"\"\n\n        X = self._validate_data(X, accept_sparse='csr', reset=False)\n        n_queries = _num_samples(X)\n\n        neigh_dist, neigh_ind = self.radius_neighbors(X)\n        outlier_mask = np.zeros(n_queries, dtype=bool)\n        outlier_mask[:] = [len(nind) == 0 for nind in neigh_ind]\n        outliers = np.flatnonzero(outlier_mask)\n        inliers = np.flatnonzero(~outlier_mask)\n\n        classes_ = self.classes_\n        _y = self._y\n        if not self.outputs_2d_:\n            _y = self._y.reshape((-1, 1))\n            classes_ = [self.classes_]\n\n        if self.outlier_label_ is None and outliers.size > 0:\n            raise ValueError('No neighbors found for test samples %r, '\n                             'you can try using larger radius, '\n                             'giving a label for outliers, '\n                             'or considering removing them from your dataset.'\n                             % outliers)\n\n        weights = _get_weights(neigh_dist, self.weights)\n        if weights is not None:\n            weights = weights[inliers]\n\n        probabilities = []\n        # iterate over multi-output, measure probabilities of the k-th output.\n        for k, classes_k in enumerate(classes_):\n            pred_labels = np.zeros(len(neigh_ind), dtype=object)\n            pred_labels[:] = [_y[ind, k] for ind in neigh_ind]\n\n            proba_k = np.zeros((n_queries, classes_k.size))\n            proba_inl = np.zeros((len(inliers), classes_k.size))\n\n            # samples have different size of neighbors within the same radius\n            if weights is None:\n                for i, idx in enumerate(pred_labels[inliers]):\n                    proba_inl[i, :] = np.bincount(idx,\n                                                  minlength=classes_k.size)\n            else:\n                for i, idx in enumerate(pred_labels[inliers]):\n                    proba_inl[i, :] = np.bincount(idx,\n                                                  weights[i],\n                                                  minlength=classes_k.size)\n            proba_k[inliers, :] = proba_inl\n\n            if outliers.size > 0:\n                _outlier_label = self.outlier_label_[k]\n                label_index = np.flatnonzero(classes_k == _outlier_label)\n                if label_index.size == 1:\n                    proba_k[outliers, label_index[0]] = 1.0\n                else:\n                    warnings.warn('Outlier label {} is not in training '\n                                  'classes. All class probabilities of '\n                                  'outliers will be assigned with 0.'\n                                  ''.format(self.outlier_label_[k]))\n\n            # normalize 'votes' into real [0,1] probabilities\n            normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n            normalizer[normalizer == 0.0] = 1.0\n            proba_k /= normalizer\n\n            probabilities.append(proba_k)\n\n        if not self.outputs_2d_:\n            probabilities = probabilities[0]\n\n        return probabilities\n",615]},"functions":{"isspmatrix (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py:1205)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/sparse/base.py",1205],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:437)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",437],"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:458)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",458],"_add_filter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:181)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",181],"simplefilter (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:165)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",165],"asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:23)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py",23],"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py:477)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/warnings.py",477],"_ensure_no_complex_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:452)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",452],"_get_threadlocal_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:16)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py",16],"get_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:24)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py",24],"asanyarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py:110)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_asarray.py",110],"issubclass_ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:285)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py",285],"issubdtype (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py:359)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numerictypes.py",359],"_sum_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2106)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",2106],"<dictcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:71)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",71],"_wrapreduction (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:70)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",70],"sum (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2111)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",2111],"_safe_accumulator_op (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py:838)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/extmath.py",838],"_assert_all_finite (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:83)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",83],"__subclasscheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:100)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py",100],"__instancecheck__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py:96)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/abc.py",96],"_num_samples (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:242)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",242],"check_array (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:459)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",459],"_num_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:185)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",185],"_check_n_features (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:334)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py",334],"_validate_data (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py:379)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/base.py",379],"isclass (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/inspect.py:73)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/inspect.py",73],"<listcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:1095)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",1095],"check_is_fitted (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py:1036)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/validation.py",1036],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:34)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",34],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:280)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",280],"get_active_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:76)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",76],"current_process (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/process.py:37)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/process.py",37],"daemon (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/process.py:198)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/process.py",198],"current_thread (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:1318)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py",1318],"in_main_thread (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:185)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",185],"effective_n_jobs (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:501)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",501],"effective_n_jobs (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:385)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",385],"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:294)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py",294],"_parse_letter_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:416)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py",416],"_parse_local_version (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:455)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py",455],"<lambda> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:482)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py",482],"_cmpkey (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:467)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py",467],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:284)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py",284],"parse (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:65)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py",65],"__lt__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py:92)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py",92],"_init (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:206)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py",206],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:228)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py",228],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:34)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py",34],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py:138)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py",138],"uuid4 (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py:713)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py",713],"hex (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py:333)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/uuid.py",333],"memstr_to_bytes (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/disk.py:42)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/disk.py",42],"get_context (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/context.py:233)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/multiprocessing/context.py",233],"RLock (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:82)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py",82],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:637)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",637],"effective_n_jobs (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:227)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",227],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:609)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",609],"configure (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:389)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",389],"effective_n_jobs (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:200)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",200],"configure (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:70)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",70],"_initialize_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:730)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",730],"_print (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:862)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",862],"start_call (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:80)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",80],"compute_batch_size (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:89)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",89],"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:256)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py",256],"_qsize (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:209)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py",209],"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:259)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py",259],"get (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:154)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py",154],"gen_even_slices (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/__init__.py:726)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/__init__.py",726],"wraps (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/functools.py:65)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/functools.py",65],"update_wrapper (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/functools.py:35)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/functools.py",35],"delayed (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py:188)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py",188],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py:198)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py",198],"delayed_function (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py:190)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py",190],"<genexpr> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:716)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py",716],"get_nested_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:213)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",213],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:245)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",245],"_put (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:213)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py",213],"_is_owned (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:271)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py",271],"notify (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py:351)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/threading.py",351],"put (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:122)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py",122],"_get (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py:217)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/queue.py",217],"__len__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:275)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",275],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:345)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",345],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:184)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",184],"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:216)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",216],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:86)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py",86],"helper (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:242)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py",242],"set_config (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:42)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py",42],"config_context (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py:99)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/_config.py",99],"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:112)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py",112],"copyto (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:1054)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py",1054],"full (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py:288)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py",288],"_tree_query_parallel_helper (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:537)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py",537],"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py:121)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/contextlib.py",121],"__call__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py:203)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/utils/fixes.py",203],"<listcomp> (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:262)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",262],"unregister (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:222)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",222],"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:219)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",219],"__call__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:258)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",258],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:569)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",569],"batch_completed (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:93)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",93],"print_progress (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:875)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",875],"__call__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:350)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",350],"apply_async (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:206)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",206],"_dispatch (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:759)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",759],"dispatch_one_batch (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:796)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",796],"retrieval_context (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:137)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",137],"get (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:574)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",574],"retrieve (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:918)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",918],"_squeeze_time (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/logger.py:23)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/logger.py",23],"short_format_time (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/logger.py:39)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/logger.py",39],"stop_call (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:83)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",83],"terminate (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py:86)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/_parallel_backends.py",86],"_terminate_backend (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:755)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",755],"__call__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py:958)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/joblib/parallel.py",958],"_arrays_for_stack_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:208)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py",208],"_vhstack_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:219)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py",219],"_atleast_2d_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:78)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py",78],"atleast_2d (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:82)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py",82],"concatenate (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:143)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py",143],"vstack (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py:223)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/shape_base.py",223],"kneighbors (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:592)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py",592],"_get_weights (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py:69)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_base.py",69],"_chk_asarray (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py:244)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py",244],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:429)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py",429],"geterr (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:132)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py",132],"seterr (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:32)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py",32],"__enter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:433)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py",433],"__exit__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py:438)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_ufunc_config.py",438],"_contains_nan (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py:215)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py",215],"_transpose_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:598)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",598],"_wrapfunc (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:52)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",52],"transpose (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:602)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",602],"_zeros_like_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py:71)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py",71],"empty_like (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py:75)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/multiarray.py",75],"zeros_like (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py:75)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/numeric.py",75],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py:20)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py",20],"_maybe_view_as_subclass (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py:25)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py",25],"as_strided (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py:38)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/stride_tricks.py",38],"__init__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py:647)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py",647],"__iter__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py:655)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py",655],"__next__ (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py:674)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/index_tricks.py",674],"_unique_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py:133)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py",133],"_nonzero_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1823)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",1823],"nonzero (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1827)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/fromnumeric.py",1827],"_diff_dispatcher (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/function_base.py:1149)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/function_base.py",1149],"diff (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/function_base.py:1153)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/function_base.py",1153],"_unique1d (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py:310)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py",310],"_unpack_tuple (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py:125)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py",125],"unique (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py:138)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/lib/arraysetops.py",138],"_amax (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_methods.py:37)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/numpy/core/_methods.py",37],"_mode1D (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py:558)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py",558],"mode (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py:485)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/scipy/stats/stats.py",485],"predict (/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:179)":["/Users/mathisbatoul/opt/miniconda3/envs/sklbench/lib/python3.9/site-packages/sklearn/neighbors/_classification.py",179]}}}